---
title: "IV: Tidyverse Tricks & SQL"
image: apple-touch-icon.png
execute: 
  message: false
  warning: false
---

::: panel-tabset
# Lesson

```{r, include = FALSE, purl = TRUE}
## -----------------------------------------------------------------------------
##
##' [PROJ: EDH 7916]
##' [FILE: Tidyverse Tricks & SQL]
##' [INIT: Jan 12th 2024]
##' [AUTH: Matt Capaldi] @ttalVlatt
##
## -----------------------------------------------------------------------------

setwd(this.path::here())

## ---------------------------
##' [Libraries]
## ---------------------------

library(tidyverse)
library(dbplyr)
library(RSQLite)

```

[{{< fa code >}} R Code](/r-scripts/09-wrangle-iv.R)

-   In this brand new lesson for 2024 we are going to cover two main topics
    1.  Some more advanced `tidyverse` commands that can save a lot of time
    2.  How the `tidyverse` we have learned so far relates to `SQL` (Structured Query Language) commonly used to access databases

## Tidyverse Tricks

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Tidyverse Tricks Data]
## ---------------------------
```

-   Now you have a decent grasp on the core functions of `tidyverse` we can start exploring some helper functions that can make our lives much easier!
-   These commands have been chosen as I have personally found them wildly helpful in working with IPEDS and other data
    -   To demonstrate, we are going to use IPEDS finance data files for 2018/19 school year
    -   Notice: IPEDS uses separate data files for public `_f1a` non-profit `_f2` and for-profit `_f3` colleges

```{r}
df_18_pub <- read_csv(file.path("data", "ipeds-finance", "f1819_f1a_rv.csv"))
df_18_np <- read_csv(file.path("data", "ipeds-finance", "f1819_f2_rv.csv"))
df_18_fp <- read_csv(file.path("data", "ipeds-finance", "f1819_f3_rv.csv"))
```

-   First, since there should be no college in more than one of these data files, and each college only has one row, we can `bind_rows` to stack each one on top of the other
    -   Then run a quick test to check no UNITID appears more than once (no duplicates)

```{r}
df_18 <- bind_rows(df_18_pub, df_18_np, df_18_fp)

df_18 |>
  count(UNITID) |>
  filter(n > 1)

```

-   Notice: we now have 6069 obs. of 663 variables
    -   Does this pass the "eyeball test"?
        -   The number of obs. looks right as it is just sum of the obs. from the 3 dfs
        -   The number of vars. may look concerning at first
            -   Why, if all we did was "stack" rows on top of each other, would the number of vars increase?
-   What we have created is a somewhat "sparse" data frame, as each institution category has differently named variables (in part due to differing reporting requirements)

Our data looks something like this

|                | Public Vars | Non-Profit Vars | For-Profit Vars |
|----------------|-------------|-----------------|-----------------|
| Public IDs     | `values`    | `NA`            | `NA`            |
| Non-Profit IDs | `NA`        | `values`        | `NA`            |
| For-Profit IDs | `NA`        | `NA`            | `values`        |

: Depiction of Sparse DataFrame

- Hmm, sounds like this could get tricky... Luckily `tidyverse` is here to help!

### `coelesce()` Data Split Across Columns

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [coalesce()-ing Split Data]
## ---------------------------
```

- Let's say we want to combine this information into one variable to show how much all institutions spend on instruction, research, and student services, respectively
  - Side note: If combining variables like this in your own work, check the code book to ensure you know what you're combining and why
  - In this case, the variables we will be combining appear to be close equivalents
  
- First things first, let's `select()` the relevant variables from the larger data set using the IPEDS dictionary files
  - Quick question: How did I find the variable names to use below? 

```{r}
df_18 <- df_18 |>
  select(UNITID,
         F1C011, F1C021, F1C061,
         F2E011, F2E021, F2E051,
         F3E011, F3E02A1, F3E03B1)

print(df_18[100:105,])
print(df_18[3000:3005,])
```

<!--

- For the sake of demonstration, let me show how to do this for instruction without our first "tidyverse trick"
  - Fun fact: I did something very close to this in my final project when I took this class!

```{r}

## Split back up into separate files
pub <- df_18 |> filter(!is.na(F1C011)) |>
  ## Rename the variable
  rename(inst_spend = F1C011) |>
  ## Drop the other variables
  select(UNITID, inst_spend)
np <- df_18 |> filter(!is.na(F2E011)) |>
  rename(inst_spend = F2E011) |>
  select(UNITID, inst_spend)
fp <- df_18 |> filter(!is.na(F3E011)) |>
  rename(inst_spend = F3E011) |>
  select(UNITID, inst_spend)
## Re-bind the colleges back up
rebind <- bind_rows(pub, np, fp)

```

- Quite a lot of code, and that is a relatively simple case

-->

- Luckily, tidyverse has a command to help us, `coalesce()` 
  - Returns the first non-missing value across any number of columns
    - This works perfectly in situations like this, when you only have one data point for each row, it could just be in any column

<!--

- Let's check, are they the same?

```{r}
all.equal(rebind, coalesce)
```

Yep! So let's do all the columns and get a clean data frame

-->

let's do all the columns and get a clean data frame

```{r}
df_18_clean <- df_18 |>
  mutate(inst_spend = coalesce(F1C011, F2E011, F3E011),
         rsch_spend = coalesce(F1C021, F2E021, F3E02A1),
         serv_spend = coalesce(F1C061, F2E051, F3E03B1)) |>
  select(UNITID, inst_spend, rsch_spend, serv_spend)

print(df_18_clean[100:105,])
print(df_18_clean[3000:3005,])
```

- This is a real time saver if working with IPEDS finance data
- Another use case for this might be if you had 30 columns one for each test date and rows for student scores, they all took the test on one of the dates, you want a single column for student scores, but it could have been recorded in any of the date columns

### Finding `if_any()` Issues

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Finding if_any() Issues]
## ---------------------------
```

- Although compliance is a federal requirement, completeness of reporting remains a struggle when using IPEDS data
- From the snapshots of the data we have seen so far, it seems that there's a good number of $0 reports for these values
- Let's try and get a data frame containing only institutions that reported $0 for one of these spending categories

<!--

- Without the helper function

```{r}
df_0_inst <- df_18_clean |> filter(inst_spend == 0)
df_0_rsch <- df_18_clean |> filter(rsch_spend == 0)
df_0_serv <- df_18_clean |> filter(serv_spend == 0)
df_0 <- bind_rows(df_0_inst, df_0_rsch, df_0_serv)

## Plus we end up with duplicates
df_0 |>
  count(UNITID) |>
  filter(n > 1)

```

-->

```{r}
df_0 <- df_18_clean |>
  filter(if_any(everything(), ~ . == 0)) ## h/t https://stackoverflow.com/questions/69585261/dplyr-if-any-and-numeric-filtering

print(df_0)
```

- Let's walk through this code
  1. Assign our results to a new object called `df_0`
  2. Take `df_18_clean` and pipe it into filter
  3. Inside `filter()` we have our `if_any` helper function, which has two arguments
    i. `.cols` which columns to look across
      - Here we have just gone for all columns with `everything()`, but you could input a list of specific column names, or another selection function like `where(is.numeric))`
    ii. `.fns` function to test the columns against
      - Here we have use `~ . == 0`
        - We haven't used `purrr` from `tidyverse` in this class, but the `~` comes from there, in short, it starts a very simple function for us
          - The function takes any value `.` and asks if it equals 0 `== 0`
          - If the function returns `TRUE` (as in it equals 0) for any column, that row will be `filter()`-ed in 

### Working `across()` Multiple Columns

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Working across() Columns]
## ---------------------------
```

- Now let's explore that data a little more with `if_any()`'s sister function `across()`
  - Internally `if_any()` and `across()` are set up the same
    - They both take
      i. `.cols` which columns to look across
      ii. `.fns` function to test the columns against
  - The difference between them comes down to which function they work in
    - `if_any()` is used in a handful of functions like `filter()`
    - `across` is used in most functions like `summarize()` and `mutate()`
      - If you try to use `across()` where you aren't meant to `tidyverse` will throw you a warning
      
- Here, we will use the `across` function inside `count()` to get a breakdown of which spending categories are unreported most often

```{r}
df_0 |>
  select(-UNITID) |>
  count(across(everything(), ~ . == 0))
```

- The internal logic of the `across()` here is identical to the `if_any()` above
  - `everything()` works across all columns, `~ . == 0` is a simple function to test if any value equals 0
- `across()` just works in the `count()` function
  - Outputs a count table counting combinations of the variables equaling 0
    - By far the most common variable to report zero is research spending, with 4411 reporting only that variable as 0
    - 25 schools reported 0 for all three variables
    
    

- Although we've only been working across 3 variables in these examples, the power of these commands is that they can work across an unlimited number of columns, so the bigger your data, the more should be thinking `across()` and `if_any()`

### Moving Beyond `ifelse()` with `case_when()`

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [From ifelse() to case_when()]
## ---------------------------
```

- Keeping digging into differences in spending categories, next, let's say we want to create a new variable that says which order the three spending categories were for each school

- Let's walk through the `case_when()` code
  1. Much like we used `ifelse()` inside mutate to make a new variable in [Data Wranling I](03-wrangle-i.qmd), we can use `case_when()` when we have more than a binary test
    - `case_when()` goes down the list of conditions in order until it finds one that it answers `TRUE` at which point it returns the value on the right hand side of the `~`
    - Here we have listed out all possible orders of spending categories with a label for each scenario
  2. Unlike `ifelse()` there is a unique danger that would don't cover every eventuality with your conditions
    - This is why I like to end with `TRUE`, as that will be `TRUE` no matter what
      - I then assign some kind of catch-all phrase that makes me know I made an error (it will just `NA` if you don't do this)
  3. To check if a `case_when()` worked, I like to pipe it into a `count()` for the new variable we made

```{r}
df_18_clean |>
  mutate(highest_cat = case_when(inst_spend > rsch_spend & rsch_spend > serv_spend ~ "inst_rsch_serv",
                                 inst_spend > serv_spend & serv_spend > rsch_spend ~ "inst_serv_rsch",
                                 rsch_spend > inst_spend & inst_spend > serv_spend ~ "rsch_inst_serv",
                                 rsch_spend > serv_spend & serv_spend > inst_spend ~ "rsch_serv_inst",
                                 serv_spend > inst_spend & inst_spend > rsch_spend ~ "serv_inst_rsch",
                                 serv_spend > rsch_spend & rsch_spend > inst_spend ~ "serv_rsch_inst",
                                 TRUE ~ "You missed a condition Matt")) |>
  count(highest_cat)
```

- Looks like I missed something in my `case_when()`, can anyone guess what it is?

- What would happen to this school
  - inst_spend 35,000,000
  - rsch_spend 20,000,000
  - serv_spend 20,000,000
    - As I have only specified for all order of categories being "greater than" the other, situations where two categories are equal slip through the cracks
      - This was a genuine mistake when writing the lesson, precisely why I always include that catch all
      
- Let's see how our results change if I use "greater than or equal to" signs
      
```{r}
df_18_clean |>
  mutate(highest_cat = case_when(inst_spend >= rsch_spend & rsch_spend >= serv_spend ~ "inst_rsch_serv",
                                 inst_spend >= serv_spend & serv_spend >= rsch_spend ~ "inst_serv_rsch",
                                 rsch_spend >= inst_spend & inst_spend >= serv_spend ~ "rsch_inst_serv",
                                 rsch_spend >= serv_spend & serv_spend >= inst_spend ~ "rsch_serv_inst",
                                 serv_spend >= inst_spend & inst_spend >= rsch_spend ~ "serv_inst_rsch",
                                 serv_spend >= rsch_spend & rsch_spend >= inst_spend ~ "serv_rsch_inst",
                                 TRUE ~ "You missed a condition Matt")) |>
  count(highest_cat)
```

- No missing conditions this time, hooray!

- Also, I'm definitely surprised how few institutions spend most on research, and just how many spend instruction > services > research
  - Does this surprise you as well?

### Tidyverse Tricks Summary

- I hope some of these more advanced `tidyverse` commands will prove helpful, particularly as you move into the world of bigger data sets with more variables!
- Next, we are going to revisit some code from [Data Wrangling I](03-wrangle-i.qmd) and [Data Wrangling II](04-wrangle-ii.qmd) and see how it translates to `SQL`

## From `tidyverse` to `SQL`

- For this section, we are going to see how tasks from [Data Wrangling II](04-wrangle-ii.qmd) could be performed using `SQL`, a common tool used to work with databases

### What is `SQL`?

- SQL, short for Structured Query Language, is a way of retrieving, modifying, and storing data from databases. For a solid background page on `SQL` see [this overview from AWS](https://aws.amazon.com/what-is/sql/)
- One of thing that confuses people about SQL is that there's the base language of SQL that all SQL-based products share and then multiple commercial product implementations of SQL that take the base language and add additional and unique functions
  - The `dbplyr` package we are going to use below attempts to mimic the implementation we tell it to
- A key difference between `R` and `SQL` is that we don't have the option to assign our results to an object like we do with `R`
  - Instead, an `SQL` query is simply the code to retrieve the data, what happens to that data (e.g., does it show up on your screen, is it saved somewhere, etc.) will be determined by the `SQL`-based software you're using

### How does `SQL` Relate to this Class?

- First, `SQL` is a really important tool for data management jobs in higher education. Larger institutions like UF almost certainly store their institutional data in a database that uses `SQL`
  - However, it is rarely taught in Higher Ed degree programs, so, this little intro already sets you up for success
- Second, the `tidyverse` language we have been learning this semester is in many ways similar to `SQL`
  - In fact, a lot of the functions and function names in `tidyverse` come directly from `SQL`
  - There's an entire package `dbplyr` which can translate our standard `dplyr` commands to `SQL` queries
    - That's what we are going to play around with today!
    

### Data Wrangling II in `SQL`

- To explore `SQL` in a familiar environment, we are going to re-visit [Data Wrangling II](04-wrangle-ii.qmd)
  - While more complicated data wrangling is certainly possible in `SQL` the most common use in education is to pull or "query" data out of institutional databases with some simple summaries or joins
  - 

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Data Wrangling II in SQL]
## ---------------------------
```

- First, we have to simulate an `SQL` database for `dbplyr`
  - This tells `dbplyr` exactly how to translate our code
  - For today's class, we will simulate a Microsoft Access database with the `simulate_access()` command
- Second, instead of a normal `df`, we want R to pretend that `df` is a table in a database, which we do with the `memdb_frame()` command
  - If you're curious what this command does, try `print(df)` and `print(db)` to see the difference

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [dbplyr SQL Setup]
## ---------------------------
```

```{r}

df <- read_csv(file.path("data", "sch-test", "all-schools.csv"))

microsoft_access <- simulate_access()

db <- memdb_frame(df)

```

#### Create Summary Table

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Create Summary Table]
## ---------------------------
```

- Our first command is pretty simple, we want to group our data by year then calculate the mean test score, which will give us average test scores for each year

```{r}
#| class-output: SQL
# https://stackoverflow.com/questions/76724279/syntax-highlight-quarto-output
df_sum <- db |>
    ## grouping by year so average within each year
    group_by(year) |>
    ## get mean(<score>) for each test
    summarize(math_m = mean(math),
              read_m = mean(read),
              science_m = mean(science)) |>
  show_query()
```

- We start with `SELECT`
  - We list out what we want, which is the `year` variable and `AVG()`s of math, reading, and science test scores
- We are taking them from our simulated databases `dbplyr_002` which we want `GROUP BY`-ed year
  - As `SQL` code is nested, we can't read it top to bottom, the `GROUP BY` takes place as we pull the data out of `dbplyr_002` before we calculate the `AVG()`

#### Left-Join

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Left-Join]
## ---------------------------
```
  
- Next, we want to join these averages back into the the main data frame

```{r}
#| class-output: SQL
df_joined <- db |>
    ## pipe into left_join to join with df_sum using "year" as key
    left_join(df_sum, by = "year") |>
  show_query()
```

- We see our preceding query nested inside our new query from the second `SELECT` statement to the `GROUP BY` statement
  - That all sits become `RHS`  inside our `LEFT JOIN` statement
- We see this is joined `ON` the `year` variable with the original data `dbplyr_002`

#### Pivot-Longer

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Pivot-Longer]
## ---------------------------
```

- Our next query pivots the data longer, which remember from our original lesson take the math, reading, and science score columns, and turns them into one column for score type and column for score

```{r}
#| class-output: SQL
df_long <- db |>
    ## cols: current test columns
    ## names_to: where "math", "read", and "science" will go
    ## values_to: where the values in cols will go
    pivot_longer(cols = c("math","read","science"),
                 names_to = "test",
                 values_to = "score") |>
  show_query()
```

- What we see here is a bit different, as it's manual
- First we `SELECT` school and year as they are, math as `test` and `math` as `score`
- This then `UNION ALL`-ed (think `bind_rows` style stacking) with the same query for `reading` and then again for `science`

#### Pivot-Wider

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Pivot-Wider]
## ---------------------------
```

- Next, let's pivot that data back wide

```{r}
#| class-output: SQL
df_wide <- df_long |>
    ## names_from: values in this column will become new column names
    ## values_from: values in this column will become values in new cols
    pivot_wider(names_from = "test",
                values_from = "score") |>
  show_query()
```

- Our first `SELECT` statement asks for school and year, then...
- We use `CASE WHEN` for each type of test, creating a new variable for each subject `WHEN` the test type equaled that subject
- Beneath that, we see our previous query that creates the long data that we pivoted back wider

# Assignment

There is no homework assignment associated with this lesson. Instead, your initial analysis for your reproducible report is due Sunday at 11:59pm. See the [final project](99-final.qmd) for details.

:::

<!--

### Data Wrangling I in `SQL`

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Data Wrangling I in SQL]
## ---------------------------
```

- In [Data Wrangling I](03-wrangle-i.qmd) we were given the following task

> Using HSLS09 data, figure out average differences in college degree expectations
> across census regions; for a first pass, ignore missing values and
> use the higher of student and parental expectations if an
> observation has both.

- Our final piped together code to answer the question looked like this

```{r}
df <- read_csv(file.path("data", "hsls-small.csv"))

df |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  ## Drop is high_exp is still NA (neither parent or student answered)
  filter(!is.na(high_exp)) |>
  ## Group the results by region
  group_by(x1region) |>
  ## Get the mean of high_exp (by region)
  summarize(mean_exp = mean(high_exp))
```

- Now, let's see what that looks like in `SQL`
- First, we have to simulate an `SQL` database for `dbplyr`
  - This tells `dbplyr` exactly how to translate our code
  - For today's class, we will simulate a Microsoft Access database with the `simulate_access()` command
- Second, instead of a normal `df`, we want R to pretend that `df` is a table in a database, which we do with the `memdb_frame()` command
  - If you're curious what this command does, try `print(df)` and `print(db)` to see the difference

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [dbplyr SQL Setup]
## ---------------------------
```

```{r}

microsoft_access <- simulate_access()

db <- memdb_frame(df)

```

- Now, let's use `dbplyr`'s `show_query()` function to see what this task would look like in `SQL`
  - It's a little overwhelming, particularly because it's nested code (show some appreciation for the `|>` pipe)
    - Honestly, you wouldn't often do something with this many steps in `SQL`
  - But, as we walk through it piece by piece, you will see a lot of similarities to the `R` code we wrote

```{r}

db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  ## Drop is high_exp is still NA (neither parent or student answereed)
  filter(!is.na(high_exp)) |>
  ## Group the results by region
  group_by(x1region) |>
  ## Get the mean of high_exp (by region)
  summarize(mean_exp = mean(high_exp)) |>
  show_query()

```

#### Step-by-Step Breakdown

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Step-by-Step Breakdown]
## ---------------------------
```

1. First, we selected our four variables from `dbplyr_001` (which is our database `db`)

```{r}
db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  show_query()
```

2. Second, we add in `CASE WHEN` statements, that work in place of our `ifelse()` statements to replace -8 -9 and 11 with `NA` (`NULL` in SQL-speak)
  - These actually work just like the `case_when()` function we covered in the first part of the class

```{r}
db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  show_query()
```

3. Okay, all we have done here is add in the line that creates our `high_exp` variable (see the first full line of code)
  - The confusing part is that `dbplyr` has created `q01` and `SELECT`-ed everything `.*` from it
    - This (and any of time you see `q01`) is just a bit like if we assigned something into a new object midway through processing

```{r}

db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  show_query()

```

4. Here's where things become hard to untangle, mostly because `dbplyr` keeps assigning the results to `q01` then `SELECT`-ing everything back
  - If you made it this far, you've got the main point of this exercise, so don't worry if this becomes too confusing
  - This step we added the first two longer blocks of code that `SELECT` our variables again (as the previous step's output was saved back to `q01`) then perform a `CASE WHEN` that's equivalent to our `ifelse()` statements that took the non-`NA` value if from parent/student expectations if the other was `NA` (`NULL` in SQL-speak)

```{r}
db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  show_query()
```

5. Here we added 2 lines of substance (`FROM (` just links to rest of the code)
- First line: `SELECT q01.*`
  - Select everything from q01 (a temporary object saved from the last step)
- Last line: ``WHERE (NOT((`high_exp` IS NULL)))``
  - Same as `filter(!is.na(high_exp))`
- Note: the reason it's the first and last lines is that everything we have done so far is nested in the middle
  - Revisit the introduction to the `|>` pipe in [Data Wrangling I](03-wrangle-i.qmd) for refresher on nested code


```{r}
db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  ## Drop is high_exp is still NA (neither parent or student answereed)
  filter(!is.na(high_exp)) |>
  show_query()
```

6. Finally, we want to get our mean expectation by region, so we add 2 lines of substance again
  - Last line: ``GROUP BY `x1region``
    - Group the data by region
  - First line: ``SELECT `x1region`, AVG(`high_exp`) AS `mean_exp``
    - Taking `x1region` as the first column and the `AVG` (mean) of `high_exp` as the second column

```{r}
db |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  ## Drop is high_exp is still NA (neither parent or student answered)
  filter(!is.na(high_exp)) |>
  ## Group the results by region
  group_by(x1region) |>
  summarize(mean_exp = mean(high_exp)) |>
  show_query()
```

- There we have it, a walk through our first data wrangling session translated into `SQL`
  - By no means are expected to have any working knowledge of `SQL` from this
  - The point is to demonstrate how similar they are, and get your toes wet if you're interested in developing `SQL` skills (as any future institutional researchers should be)

- Next, we will explore `Data Wrangling II` to see how `SQL` can be used for joining data

#### Pivot-Longer with Compound Names

```{r, include = FALSE, purl = TRUE}
## ---------------------------
##' [Pivot-Longer with Compound Names]
## ---------------------------
```

- Finally, let's read in our an extra-wide version of our data into `df_3` and turn it into a simulated database

```{r}

df_3 <- read_csv(file.path("data", "sch-test", "all-schools-wide.csv"))

db_3 <- memdb_frame(df_3)

print(db_3)

```

- Now using the `contains()` function to `pivot_longer()` any columns with 19 in the name (all but school name)
- We then take the column names and split (`names_sep`) them by the `_` putting the first half into new column `test` and the second half into new column `year`, with the values from all cells going into new column `score`

```{r}
#| class-output: SQL
## Note: change from DWII,dbplyr can't translate separate, or any stringr commands, so we have to be more sophisticated with our pivot_longer
df_long_fix <- db_3 |>
    ## NB: contains() looks for "19" in name: if there, it adds it to cols
    pivot_longer(cols = contains("19"),
                 names_to = c("test", "year"),
                 names_sep = "_",
                 values_to = "score") |>
  show_query()

```

- Similarly to our previous pivot longer, we see the `SQL` effectively just creates a series of queries, one for each year/subject combination, then `UNION ALL` stacks them all together


-->

