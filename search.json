[
  {
    "objectID": "13-Program-Functional.html#dry-vs-wet-programming",
    "href": "13-Program-Functional.html#dry-vs-wet-programming",
    "title": "II: Functions & Loops",
    "section": "DRY vs WET programming",
    "text": "DRY vs WET programming\nThe watchwords for this lesson are DRY vs WET:\n\nDRY: Don’t repeat yourself\nWET: Write every time\n\nLet’s say you have a three-step analysis process for 20 files (read, lower names, add a column). Under a WET programming paradigm in which each command gets its own line of code, that’s 60 lines of code. If the number of your files grows to 50, that’s now 150 lines of code — for just three tasks! When you write every time, you not only make your code longer and harder to parse, you also increase the likelihood that your code will contain bugs while simultaneously decreasing its scalability.\nIf you need to repeat an analytic task (which may be a set of commands), then it’s better to have one statement of that process that you repeat, perhaps in a loop or in a function. Don’t repeat yourself — say it once and have R repeat it for you!\nThe goal of DRY programming is not abstraction or slickness for its own sake. That runs counter to the clarity and replicability we’ve been working toward. Instead, we aspire to DRY code since it is more scalable and less buggy than WET code. To be clear, a function or loop can still have bugs, but the bugs it introduces are often the same across repetitions and fixed at a single point of error. That is, it’s typically easier to debug when the bug has a single root cause than when it could be anywhere in 150 similar but slightly different lines of code.\nAs we work through the lesson examples, keep in the back of your mind:\n\nWhat would this code look like if I wrote everything twice (WET)?\nHow does this DRY process not only reduce the number of lines of code, but also make my intent clearer?"
  },
  {
    "objectID": "13-Program-Functional.html#setup",
    "href": "13-Program-Functional.html#setup",
    "title": "II: Functions & Loops",
    "section": "Setup",
    "text": "Setup\nWe’ll use a combination of nonce data and the school test score data we’ve used in a past lesson. We won’t read in the school test score data until the last section, but we’ll continue following our good organizational practice by setting the directory paths at the top of our script.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "13-Program-Functional.html#part-1-control-flow",
    "href": "13-Program-Functional.html#part-1-control-flow",
    "title": "II: Functions & Loops",
    "section": "Part 1: Control flow",
    "text": "Part 1: Control flow\nAs stated above, by control flow, I simply mean the functions that help you change how your script is read. Repeating commands often involves a loop, which is what it sounds like: upon reaching a loop, R will repeat the code inside the loop (looping back up the beginning of the section) for a certain number of times or until some condition is met. Once completed, R will go back to reading each line in order like normal.\nIf you google around, you may find that loops have a bad reputation in R, mostly for being slow. But they aren’t that slow and they are easy to write and understand.\n\nfor\nThe for() function allows you to build the aptly named for loops. There are few ways to use for(), but its construction is the same: for (variable in sequence).\nReading it backwards, the sequence is just the set of numbers or objects that we’re going to work through. The variable is a new variable that will temporarily hold a value from the sequence in each run through the loop. When the sequence is finished, so is the loop.\nFirst, let’s loop through a sequence of 10 numbers, printing each one at a time as we work through the loop.\n\n## make vector of numbers between 1 and 10\nnum_sequence &lt;- 1:10\n\n## loop through, printing each num_sequence value, one at a time\nfor (i in num_sequence) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice the braces {} that come after for(). This is the code in the loop that will be repeated as long as the loop is run. With each loop, i takes on the next value in the num_sequence. This is why we see 1 through 10 printed to the console.\nLet’s do it again, but this time with characters.\n\n## character vector using letters object from R base\nchr_sequence &lt;- letters[1:10]\n\n## loop through, printing each chr_sequence value, one at a time\nfor (i in chr_sequence) {\n    print(i)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nOnce more, with each loop, i takes on each chr_sequence value in turn and print() prints it to the console.\n\nQuick exercise\nCan you modify the above loop so that it works through both the num_sequence and chr_sequence in the same loop? (HINT: how might you combine/concatenate the two sequences?)\n\nAnother way to make a for loop is work through a vector by its indices, that is, instead of pulling out each item directly and storing it in i, we can use i as an index counter and then call items based on their index: chr_sequence[i]. Here’s an example.\n\n## for loop by indices\nfor (i in 1:length(chr_sequence)) {\n    print(chr_sequence[i])\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nTo understand what’s happening, let’s break the code into pieces to make it clearer.\nInside the for() parentheses, we have i in 1:length(chr_sequence). We know what i in means since it’s like what we’ve seen before. What’s 1:length(chr_sequence)? First, it looks like the colon construction we’ve seen before, &lt;start&gt;:&lt;end&gt;, which means give the full sequence of values from &lt;start&gt; through &lt;end&gt;.\nSince we made it above, we know that there are ten letters in chr_sequence. We could just use 1:10. However, this violates our no magic numbers rule (what happens if we want to add or subtract letters from the list later?).\nWe can get around this issue by using the base-R function, length(), which will return the number of items in a one-dimensional object. Since we know that length(chr_sequence) is equal to 10, that means that 1:length(chr_sequence) is the same thing as saying 1:10. It’s just another more flexible way to get the end number of our sequence. We can show this by just printing i again.\n\n## for loop by indices (just show indices)\nfor (i in 1:length(chr_sequence)) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nBack to the original function, we see that inside the braces ({}), we have print(chr_sequence[i]). If you’ve taken a look at the supplemental lesson for base R, you know that brackets ([]) are way of pulling out specific values from a vector. For example, if you have a vector of 3 items, e.g., x &lt;- c(\"a\", \"b\", \"c\"), you can select the 2nd item by using square brackets and an index value: x[2]. While an index value is a number (think of it like a street address or room number in a hall), we can also use variables that represent numbers, which is useful for programming. Here’s a non-looped test:\n\n## confirm that we can use variables as indices\ni &lt;- 1                     # set i == 1\nchr_sequence[i]      \n\n[1] \"a\"\n\ni &lt;- 2                     # now set i == 2\nchr_sequence[i]            # notice that code is exactly the same here\n\n[1] \"b\"\n\n\nWe know that i is going to take on values 1 through 10 in the loop, which means the print() function will get chr_sequence[1], chr_sequence[2], and so on. Because of the brackets, these will turn into…a, b, and so on. We should get the same thing as before! Let’s put all the pieces together and run again:\n\n## for loop by indices (once again)\nfor (i in 1:length(chr_sequence)) {\n    print(chr_sequence[i])\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nSuccess!\nWhether you decide to loop using actual values from the sequence or indices will usually depend on the code you want to run in the loop. Sometimes one way works better and other times the other. Just do whatever works best for you at that time.\n\nQuick exercise\nAdd another print statement to the last loop that shows the value of i with each loop.\n\n\n\nwhile\nThe while() function is similar to for() except that it doesn’t have a predetermined stopping point. As long the expression inside the parentheses is TRUE, the loop will keep going. Only when it becomes FALSE will it stop.\nOne way to use a while() loop is to set up a counter. When the counter reaches some value, the expression inside the while() parentheses is no longer true and the loop stops.\n\n## set up a counter\ni &lt;- 1\n## with each loop, add one to i\nwhile(i &lt; 11) {\n    print(i)\n    i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nUsing a while() loop with a counter is often the same as using a for() loop with a sequence. If that’s the case, it’s probably better just to use a for() loop.\nwhile() loops are most useful when it’s not clear from the start when the loop should stop. Imagine you have an algorithm that should only stop when a certain number is reached. If the time it takes to reach the number changes depending on the input, then a for() loop probably won’t work, but a while() loop will.\nYou have to careful, however, with while() loops. If you forget to increment the counter (like I did the first time I set up this example), the loop won’t ever stop because i will never get larger and will always be less than 11! If your while() loop will only stop when a certain condition is met, it’s still a good idea to build in a pre-specified number of trials. If your loop has tried X times to meet the condition and still hasn’t done so, it should stop with an error or return what it has so far (depending on your needs).\nYou have been warned!\n\n\nif\nWe’ve already used a version of if, ifelse(), quite a bit. We can use if statements in our script to decide whether a section of code should be run or skipped. We can also use if() inside a for() loop to set a condition that changes behavior on some iterations of the loop.\n\n## only print if number is not 5\nfor (i in num_sequence) {\n    if (i != 5) {\n        print(i)\n    }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice how 5 wasn’t printed to the console. It worked!\n\nQuick exercise\nChange the condition to print only numbers below 3 and above 7.\n\nWe can add one or more else if() / else() partners to if() if we need, for example, option B to happen if option A does not.\n\n## if/else loop\nfor (i in num_sequence) {\n    if (i != 3 & i != 5) {\n        print(i)\n    } else if (i == 3) {\n        print('three')\n    } else {\n        print('five')\n    }\n}\n\n[1] 1\n[1] 2\n[1] \"three\"\n[1] 4\n[1] \"five\"\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nAs with dplyr verbs, the small number of control functions (and there are some others) can be combined in an infinite number of ways to help you control how your script is read. If you find, however, that your script keeps getting more complex, with multiple nested loops and ifelse() exceptions, it might be time to either rethink your approach or to write your own functions that can handle expectations in a clearer way."
  },
  {
    "objectID": "13-Program-Functional.html#part-2-writing-functions",
    "href": "13-Program-Functional.html#part-2-writing-functions",
    "title": "II: Functions & Loops",
    "section": "Part 2: Writing functions",
    "text": "Part 2: Writing functions\nYou can write your own functions in R and should! They don’t need to be complex. In fact, they tend to be best when kept simple. Mostly, you want a function to do one thing really well.\nTo make a function, you use the function() function. Put the code that you want your function to run in the braces {}. Any arguments that you want your function to take should be in the parentheses () right after the word function. The name of your function is the name of the object you assign it to.\nLet’s make one. The function below, say_hi(), doesn’t take any arguments and prints a simple string when called. After you’ve built it, call your function using its name, not forgetting to include the parentheses.\n\n## function to say hi!\nsay_hi &lt;- function() {\n    print(\"Hi!\")\n}\n\n## call it\nsay_hi()\n\n[1] \"Hi!\"\n\n\nLet’s make another one with an argument so that it’s more flexible. Let’s have our function take a name and print it. We’ll use the base-R function, paste0(), to combine all the string parts into one string. paste0() is like paste(), but it assumes we don’t want any space between the pieces. That works for us here because we’ll add our spaces manually.\n\n## function to say hi!\nsay_hi &lt;- function(name) {\n    ## combine (notice we add space after comma)\n    out_string &lt;- paste0(\"Hi, \", name, \"!\")\n    ## print output string\n    print(out_string)\n}\n\n## call it\nsay_hi(\"Leo\")\n\n[1] \"Hi, Leo!\"\n\n\nThis time, we want it to print out a sequence of numbers, but we want to be able to change the number each time we call it.\n\n## new function to print sequence of numbers\nprint_nums &lt;- function(num_vector) {\n    ## this code looks familiar...\n    for (i in num_vector) {\n        print(i)\n    }\n}\n\n## try it out!\nprint_nums(1:10)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice how the variable num_vector is repeated in both the main function argument and inside the for parentheses. The for() function sees num_vector and looks for it in the main function. It finds it because the num_vector you give the main function, print_nums(), is passed through to the code inside. Now for() can see it and use it! Let’s try a few more inputs:\n\n## v1\nprint_nums(1)\n\n[1] 1\n\n## v2\nprint_nums(1:5)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n## v3\nprint_nums(seq(1, 20, by = 2))\n\n[1] 1\n[1] 3\n[1] 5\n[1] 7\n[1] 9\n[1] 11\n[1] 13\n[1] 15\n[1] 17\n[1] 19\n\n\n\nQuick exercise\nWhat happens if you forget to put an argument in your new function? How do you think you might set a default argument for num_vector? Could you set it equal to something?\n\nOne last thing to keep in mind about functions is that they follow Vegas rules: what happens inside the function, stays inside the function. A more technical way of stating this is that the function has its own scope and objects created / processes run within that scope stay within that scope. That’s why even though we created an object called out_string in our say_hi() function, we can’t call it directly from the console. It only exists while the function is running and goes away once the function completes.\nEven if we repeat an object name that exists in our global (working) environment inside our function, the value we give it inside the function will override the global value within the function. But once the function is finished, the global value will again take precedence. Don’t worry to much about scoping (here’s more information if you are interested), but just be aware that it’s generally good practice that if you want an object for use inside your function, you should either create it there or make it an argument."
  },
  {
    "objectID": "13-Program-Functional.html#part-3-practical-examples",
    "href": "13-Program-Functional.html#part-3-practical-examples",
    "title": "II: Functions & Loops",
    "section": "Part 3: Practical examples",
    "text": "Part 3: Practical examples\n\nExample 1: missing data\nNow that we’ve seen some control flow and programming methods, let’s move to a more realistic use case. In this first example, we’ll make a function that fills in missing values, a common task we’ve had. First, we’ll generate some fake data with missing values.\nNote that since we’re using R’s sample() function, your data will look a little different from mine due to randomness in the sample, but everything will work the same.\n\n## create a data frame with around 10% missing values (-97,-98,-99) in\n## three columns\ndf &lt;- tibble(\"id\" = 1:100,\n             \"age\" = sample(c(seq(11,20,1), -97),\n                            size = 100,\n                            replace = TRUE,\n                            prob = c(rep(.09, 10), .1)),\n             \"sibage\" = sample(c(seq(5,12,1), -98),\n                               size = 100,\n                               replace = TRUE,\n                               prob = c(rep(.115, 8), .08)),\n             \"parage\" = sample(c(seq(45,55,1), c(-98,-99)),\n                               size = 100,\n                               replace = TRUE,\n                               prob = c(rep(.085, 11), c(.12, .12)))\n             ) \n## show\ndf\n\n# A tibble: 100 × 4\n      id   age sibage parage\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    12      8    -98\n 2     2    17      8     49\n 3     3    15      9    -99\n 4     4    18     10    -99\n 5     5    13     12     49\n 6     6    15      7     49\n 7     7    11     12     50\n 8     8    13      8     47\n 9     9    19      5     50\n10    10    12      6     45\n# … with 90 more rows\n\n\nWe could fix these manually like we have been in past lessons and assignments, but it would be nice have a shorthand function. The function needs to flexible though, because the missing data values are coded differently in each column.\n\n## function to fix missing values\nfix_missing &lt;- function(x, miss_val) {\n    ## use ifelse(&lt; test &gt;, &lt; do this if TRUE &gt;, &lt; do that if FALSE &gt;)\n    x &lt;- ifelse(x %in% miss_val,        # is x == any value in miss_val?\n                NA,                     # TRUE: replace with NA\n                x)                      # FALSE: return original value as is\n    ## return corrected x\n    return(x)\n}\n\nOur fix_missing() function should meet our needs. It takes the same ifelse() function we’ve used before, but instead of using the name of the object (like df), uses an argument name x that we can set each time. It does the same for miss_val. Instead of choosing a hard-coded value (a magic number), we can change it each time we call the function. Let’s try it out.\n\n## check\ndf %&gt;%\n    count(age)\n\n# A tibble: 11 × 2\n     age     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   -97     3\n 2    11    11\n 3    12     9\n 4    13     9\n 5    14    17\n 6    15    11\n 7    16     9\n 8    17     7\n 9    18     9\n10    19     5\n11    20    10\n\n## missing values in age are coded as -97\ndf &lt;- df %&gt;%\n    mutate(age = fix_missing(age, -97))\n\n## recheck\ndf %&gt;%\n    count(age)\n\n# A tibble: 11 × 2\n     age     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    11    11\n 2    12     9\n 3    13     9\n 4    14    17\n 5    15    11\n 6    16     9\n 7    17     7\n 8    18     9\n 9    19     5\n10    20    10\n11    NA     3\n\n\nIt worked! All the values that were -97 before, are now in the NA table column. Importantly, none of the other values changed.\n\nQuick exercise\nCorrect the missing values in the other columns using our new function. NOTE that parage has missing values of -98 and -99.\n\n\n\nExample 2: batch read files\n\nRead in all files in folder\nIn our lesson on appending, joining, and merging, we read in a few administrative test score files. In that lesson, we read in each file individually and then appended them.\n\n## read in all Bend Gate test score files\ndf_1 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1980.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_2 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1981.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_3 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1982.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_4 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1983.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_5 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1984.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_6 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1985.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## append\ndf &lt;- bind_rows(df_1, df_2, df_3, df_4, df_5, df_6)\n\n## show\ndf\n\n# A tibble: 6 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n4 Bend Gate  1983   491   276     793\n5 Bend Gate  1984   502   310     788\n6 Bend Gate  1985   488   280     789\n\n\nThat was fine then, but when you see that much repeated code, you should immediately consider using a more functional programming approach. What if we wanted to read in all the files for the other schools as well? That would be a total of 24 very similar lines of code — ripe for introducing bugs.\nLet’s combine a number of the skills we’ve learned to this point — functions, regular expressions, and pipes — to read and bind all the test score files in a more functional (and less buggy) manner.\nFirst, we’ll store the names of the files in an object using list.files(). We’ve used the list.files() function a few times in class when checking that we were in the correct working directory. In those situations, we’ve just printed the output to the console. But as with all things R, we can save that output in an object instead and put it to good use.\n\n## get names (with full path) of all school test score files\n\nfiles &lt;- list.files(file.path(\"data\", \"sch_test\", \"by_school\"), full.names = TRUE)\n\n## show\nfiles\n\n [1] \"data/sch_test/by_school/bend_gate_1980.csv\"   \n [2] \"data/sch_test/by_school/bend_gate_1981.csv\"   \n [3] \"data/sch_test/by_school/bend_gate_1982.csv\"   \n [4] \"data/sch_test/by_school/bend_gate_1983.csv\"   \n [5] \"data/sch_test/by_school/bend_gate_1984.csv\"   \n [6] \"data/sch_test/by_school/bend_gate_1985.csv\"   \n [7] \"data/sch_test/by_school/east_heights_1980.csv\"\n [8] \"data/sch_test/by_school/east_heights_1981.csv\"\n [9] \"data/sch_test/by_school/east_heights_1982.csv\"\n[10] \"data/sch_test/by_school/east_heights_1983.csv\"\n[11] \"data/sch_test/by_school/east_heights_1984.csv\"\n[12] \"data/sch_test/by_school/east_heights_1985.csv\"\n[13] \"data/sch_test/by_school/niagara_1980.csv\"     \n[14] \"data/sch_test/by_school/niagara_1981.csv\"     \n[15] \"data/sch_test/by_school/niagara_1982.csv\"     \n[16] \"data/sch_test/by_school/niagara_1983.csv\"     \n[17] \"data/sch_test/by_school/niagara_1984.csv\"     \n[18] \"data/sch_test/by_school/niagara_1985.csv\"     \n[19] \"data/sch_test/by_school/spottsville_1980.csv\" \n[20] \"data/sch_test/by_school/spottsville_1981.csv\" \n[21] \"data/sch_test/by_school/spottsville_1982.csv\" \n[22] \"data/sch_test/by_school/spottsville_1983.csv\" \n[23] \"data/sch_test/by_school/spottsville_1984.csv\" \n[24] \"data/sch_test/by_school/spottsville_1985.csv\" \n\n\n\nQuick exercise\nRerun list.files() again, but set the full.names argument to the default value of FALSE. What does the output look like? Why is this a problem? (Once finished, be sure to set full.names = TRUE again and rerun.)\n\nNow that we have an object, we can read in the files using a loop. We’ll need something to store them in along the way. We’ll use a blank list. We’ve not used lists before, but think of them as special vectors, which we have used.\nOnce we have a list, we’ll read in each file, but instead of storing it in an object like df, we’ll put it in the list. Since we’ll be reading in a large number of data files in the next few steps, we’ll also include the argument show_col_types = FALSE so that read_csv() doesn’t print information for each file (our assumption is that we’ve already spot checked a few files and know what we’re getting).\n\n## init list\ndf_list &lt;- list()\n\n## use loop to read in files\nfor (i in 1:length(files)) {\n    ## read in file (f) and store in list (note double brackets for list)\n    df_list[[i]] &lt;- read_csv(files[i], show_col_types = FALSE)    \n}\n\n## show first 3 items\ndf_list[1:3]\n\n[[1]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\n[[2]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\n[[3]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nNow that the items are in a list, we can again use bind_rows(), which, in addition to individual objects, can take a single list of objects.\n\n## bind our list to single data frame\ndf &lt;- df_list %&gt;%\n    bind_rows()\n\n## show\ndf\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# … with 14 more rows\n\n\nAside from being more aesthetically pleasing, this code is better because it:\n\ndoesn’t rely on repeated lines of code with small changes\nis flexible\n\nImagine you receive three more years of data for each school: 1986, 1987, 1988. As long as the files are in the same format, all you need to do is put the same directory as the other files and rerun your code. It will add them to the files vector and your loop will run just as before.\n\n\nRead in only some files\nWhat if we only want files for Spottsville? Overall, the code is exactly the same except that we want our list to only contain the spottsville_*.csv files. How can we do that? With regular expressions!\nNotice that the second argument in list.files() is pattern with a default value of NULL. This means it doesn’t do any filtering if we leave it out. But if we want to filter which files are stored in files, we should use it.\n\n## filter files to be read in using pattern\nfiles_sp &lt;- list.files(file.path(\"data\", \"sch_test\", \"by_school\"), pattern = \"spottsville\", full.names = TRUE)\n\n## check\nfiles_sp\n\n[1] \"data/sch_test/by_school/spottsville_1980.csv\"\n[2] \"data/sch_test/by_school/spottsville_1981.csv\"\n[3] \"data/sch_test/by_school/spottsville_1982.csv\"\n[4] \"data/sch_test/by_school/spottsville_1983.csv\"\n[5] \"data/sch_test/by_school/spottsville_1984.csv\"\n[6] \"data/sch_test/by_school/spottsville_1985.csv\"\n\n\nLuckily, our regular expression pattern can simply be \"spottsville\". Were our files less consistently named, we might have had trouble (name those files well!).\nThe rest of the code should be as it was before (with the small addition of _sp in various names to keep distinct from our first attempt).\n\n## init list\ndf_sp_list &lt;- list()\n\n## use loop to read in files\nfor (i in 1:length(files_sp)) {\n    ## read in file (f) and store in list\n    df_sp_list[[i]] &lt;- read_csv(files_sp[i], show_col_types = FALSE)    \n}\n\n## bind our list to single data frame\ndf_sp &lt;- df_sp_list %&gt;%\n    bind_rows()\n\n## show\ndf_sp\n\n# A tibble: 6 × 5\n  school       year  math  read science\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Spottsville  1980   498   288     813\n2 Spottsville  1981   494   270     765\n3 Spottsville  1982   507   289     801\n4 Spottsville  1983   515   288     775\n5 Spottsville  1984   475   289     779\n6 Spottsville  1985   515   285     784"
  },
  {
    "objectID": "13-Program-Functional.html#questions",
    "href": "13-Program-Functional.html#questions",
    "title": "II: Functions & Loops",
    "section": "Questions",
    "text": "Questions\n\nUsing a loop, do the following:\n\nRead in each of the individual school test files for Bend Gate and Niagara only. (HINT The vertical pipe operator, |, means OR in regular expression patterns.)\n\nWithin each iteration of the loop, add a column to the data frame that is called relative_path and contains the string relative path to the data file you just read in (e.g., if the file is located at data/sch_test/by_school/bend_gate_1980.csv, then relative_path ==    \"data/schools/by_test/bend_gate_1980.csv\" in that row).\n\nBind all the data sets together.\n\nRead in hsls_small.csv and do the following:\n\nUsing the user-written function fix_missing(), convert missing values in x1ses to NA.\n\nSubset the full data frame to the first 50 rows and pull out the test scores into a vector using the following code:\nr       test_scr &lt;- df %&gt;%           filter(row_number() &lt;= 50) %&gt;%           pull(x1txmtscor)\nUsing a for() loop, print out the index of the missing values (when test_scr equals -8).\nRepeat the same code, but add an else() companion to the initial if() statement that prints the value if non-missing.\nAdd an else if() between the initial if() and final else() in your loop that prints \"Flag: low score\" if the score is less than 40. Also, change your first if() statement to print \"Flag: missing value\" instead of the index if the value is missing.\nWrite your own function to compare two values and return the higher of the two. It should be called return_higher(), take two arguments, and return the higher of two values.\n\nOnce you’ve created it, use it in a dplyr chain to create a new column in the data frame called high_expct that is the represents the higher of x1stuedexpct and x1paredexpct. Don’t forget to account for missing values!\nHINT If stuck on what the inside of your function should look like, go back to the lesson in which we did this already — can you repurpose that code in some way?\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_9.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "11-Data-Viz-IV.html",
    "href": "11-Data-Viz-IV.html",
    "title": "IV: Interactive Graphics",
    "section": "",
    "text": "Lesson (Plotly)Assignment"
  },
  {
    "objectID": "Extra-05-RCPP.html#read-in-data",
    "href": "Extra-05-RCPP.html#read-in-data",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Read in data",
    "text": "Read in data\nThis module uses two data frames. The first has the names and locations of all colleges with a physical campus in 2015. The second has the locations of every census block group in the United States from the 2010 Census.\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## assume we're running this script from the ./scripts subdirectory\ndf_col &lt;- readRDS(file.path(\"data\", \"rcpp\", \"collegeloc.RDS\"))\ndf_cbg &lt;- readRDS(file.path(\"data\", \"rcpp\", \"cblocks.RDS\"))\n\n\nCollege locations\nHere’s a quick peek at the college location data (around 7,600 institutions).\n\n## college locations\ndf_col\n\n# A tibble: 7,647 × 5\n   unitid instnm                              fips5   lon   lat\n    &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 100654 Alabama A & M University            01089 -86.6  34.8\n 2 100663 University of Alabama at Birmingham 01073 -86.8  33.5\n 3 100690 Amridge University                  01101 -86.2  32.4\n 4 100706 University of Alabama in Huntsville 01089 -86.6  34.7\n 5 100724 Alabama State University            01101 -86.3  32.4\n 6 100733 University of Alabama System Office 01125 -87.5  33.2\n 7 100751 The University of Alabama           01125 -87.5  33.2\n 8 100760 Central Alabama Community College   01123 -85.9  32.9\n 9 100812 Athens State University             01083 -87.0  34.8\n10 100830 Auburn University at Montgomery     01101 -86.2  32.4\n# … with 7,637 more rows\n\n\n\n\nCensus block group locations\nAnd here’s the census block group location data (around 217,000 block groups).\n\n## census block group locations\ndf_cbg\n\n# A tibble: 217,740 × 4\n   fips11         pop   lon   lat\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 010010201001   698 -86.5  32.5\n 2 010010201002  1214 -86.5  32.5\n 3 010010202001  1003 -86.5  32.5\n 4 010010202002  1167 -86.5  32.5\n 5 010010203001  2549 -86.5  32.5\n 6 010010203002   824 -86.5  32.5\n 7 010010204001   944 -86.4  32.5\n 8 010010204002  1937 -86.4  32.5\n 9 010010204003   935 -86.4  32.5\n10 010010204004   570 -86.4  32.5\n# … with 217,730 more rows"
  },
  {
    "objectID": "Extra-05-RCPP.html#compute-great-circle-distance-with-haversine",
    "href": "Extra-05-RCPP.html#compute-great-circle-distance-with-haversine",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Compute great circle distance with Haversine",
    "text": "Compute great circle distance with Haversine\nThe Haversine formula is a fairly straightforward trigonometric problem. Since the coordinates in the data are in latitude and longitude and the formula requires radians, a quick helper function deg_to_rad() is used to make the conversion.\n\n## convert degrees to radians\ndeg_to_rad &lt;- function(degree) {\n  m_pi &lt;- 3.141592653589793238462643383280\n  return(degree * m_pi / 180)\n}\n\n## compute Haversine distance between two points\ndist_haversine &lt;- function(xlon, xlat, ylon, ylat) {\n\n  ## radius of Earth in meters\n  e_r &lt;- 6378137\n  \n  ## return 0 if same point\n  if (xlon == ylon & xlat == xlon) { return(0) }\n  \n  ## convert degrees to radians\n  xlon = deg_to_rad(xlon)\n  xlat = deg_to_rad(xlat)\n  ylon = deg_to_rad(ylon)\n  ylat = deg_to_rad(ylat)\n  \n  ## haversine distance formula\n  d1 &lt;- sin((ylat - xlat) / 2)\n  d2 &lt;- sin((ylon - xlon) / 2)\n  \n  return(2 * e_r * asin(sqrt(d1^2 + cos(xlat) * cos(ylat) * d2^2)))\n}\n\nWith the formula, we can compute the distance in meters between the first census block group and the first college in the data set pretty quickly.\n\n## store first census block group point (x) and first college point (y)\nxlon &lt;- df_cbg[[1, \"lon\"]]\nxlat &lt;- df_cbg[[1, \"lat\"]]\nylon &lt;- df_col[[1, \"lon\"]]\nylat &lt;- df_col[[1, \"lat\"]]\n\n## test single distance function\nd &lt;- dist_haversine(xlon, xlat, ylon, ylat)\n\n## show\nd\n\n[1] 258212.3\n\n\n\nQuick exercise\nFind the coordinates of two places you know the distance between pretty well (say, your hometown and where you live now or your first college or the nearest big city). Compute the distance and compare to Google's driving distance. It should be shorter (crows fly very straight), but similar. You may want to convert the meters to kilometers or miles. As always Google is your friend for all these steps.\n\n\nMany to many distance matrix\nNow that we have a core function, let’s write a larger function that can take many input points, many output points, and compute the distances between them.\n\n## compute many to many distances and return matrix\ndist_mtom &lt;- function(xlon,         # vector of starting longitudes\n                      xlat,         # vector of starting latitudes\n                      ylon,         # vector of ending longitudes\n                      ylat,         # vector of ending latitudes\n                      x_names,      # vector of starting point names\n                      y_names) {    # vector of ending point names\n\n  ## init output matrix (n X k)\n  n &lt;- length(xlon)\n  k &lt;- length(ylon)\n  mat &lt;- matrix(NA, n, k)\n  \n  ## double loop through each set of points to get all combinations\n  for(i in 1:n) {\n    for(j in 1:k) {\n      ## compute distance using core function\n      mat[i,j] &lt;- dist_haversine(xlon[i], xlat[i], ylon[j], ylat[j])\n    }\n  }\n  \n  ## add row and column names\n  rownames(mat) &lt;- x_names\n  colnames(mat) &lt;- y_names\n  return(mat)\n}\n\nLet’s test it with a subset of ten starting points.\n\n## test matrix (limit to only 10 starting points)\ndistmat &lt;- dist_mtom(df_cbg$lon[1:10], df_cbg$lat[1:10],\n                     df_col$lon, df_col$lat,\n                     df_cbg$fips11[1:10], df_col$unitid)\n\n## show\ndistmat[1:5,1:5]\n\n               100654   100663   100690   100706   100724\n010010201001 258212.3 119349.8 31495.73 251754.3 21138.09\n010010201002 256255.2 117447.3 32284.61 249798.4 22263.60\n010010202001 256775.4 118210.3 31047.25 250348.1 21059.86\n010010202002 258084.9 119554.9 30210.67 251665.2 20017.60\n010010203001 256958.5 118703.3 29758.10 250567.0 19905.92\n\n\n\nQuick exercise\nCan you find the minimum distance for each starting point? What’s the name of the nearest end point?\n\n\n\nNearest end point\nRather than return a full matrix of distances that we then need to evaluate to find the shortest distance, let’s write a new function, dist_min(), that will take two vectors of points again, but only return the closest point in the second group, with distance, for each point in the first group.\n\n## compute and return minimum distance along with name\ndist_min &lt;- function(xlon,         # vector of starting longitudes\n                     xlat,         # vector of starting latitudes\n                     ylon,         # vector of ending longitudes\n                     ylat,         # vector of ending latitudes\n                     x_names,      # vector of starting point names\n                     y_names) {    # vector of ending point names\n    \n    ## NB: lengths: x coords == x names && y coords == y_names\n    n &lt;- length(xlon)\n    k &lt;- length(ylon)\n    minvec_name &lt;- vector('character', n)\n    minvec_meter &lt;- vector('numeric', n)\n\n    ## init temporary vector for distances between one x and all ys\n    tmp &lt;- vector('numeric', k)\n\n    ## give tmp names of y vector\n    names(tmp) &lt;- y_names\n\n    ## loop through each set of starting points\n    for(i in 1:n) {\n        for(j in 1:k) {\n            tmp[j] &lt;- dist_haversine(xlon[i], xlat[i], ylon[j], ylat[j])\n        }\n\n        ## add to output matrix\n        minvec_name[i] &lt;- names(which.min(tmp))\n        minvec_meter[i] &lt;- min(tmp)\n    }\n\n    return(data.frame('fips11' = x_names,\n                      'unitid' = minvec_name,\n                      'meters' = minvec_meter,\n                      stringsAsFactors = FALSE))\n}\n\nLet’s test it out.\n\n## test matrix (limit to only 10 starting points)\nmindf &lt;- dist_min(df_cbg$lon[1:10], df_cbg$lat[1:10],\n                  df_col$lon, df_col$lat,\n                  df_cbg$fips11[1:10], df_col$unitid)\n## show\nmindf\n\n         fips11 unitid   meters\n1  010010201001 101471 15785.88\n2  010010201002 101471 14228.69\n3  010010202001 101471 13949.21\n4  010010202002 101471 14870.35\n5  010010203001 101471 13362.81\n6  010010203002 101471 14389.79\n7  010010204001 101471 12704.73\n8  010010204002 101471 13270.80\n9  010010204003 101471 14102.44\n10 010010204004 101471 14756.51\n\n\n\nQuick exercise\nHow long will it take to find the closest college to each census block group? Use system.time() and extrapolate to make a best guess."
  },
  {
    "objectID": "Extra-05-RCPP.html#rcpp",
    "href": "Extra-05-RCPP.html#rcpp",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Rcpp",
    "text": "Rcpp\nThese worked well, but we only did 10 starting points. We need to find it for over 200,000 starting points. What we need is to convert the base R functions into Rcpp functions.\nBelow, the full script, dist_func.cpp, is discussed in pieces.\n\nFront matter\nThe head of an Rcpp script is like an R script in that this is place to #include other header files. This is akin to having library(...) in an R script. At the very least, an Rcpp script needs to include it’s own header files, Rcpp.h.\nNext, we define preprocessor replacements. In the process of compiling the script, that is, turning the Rcpp code that’s human readable into machine-readable byte code, the preprocessor first runs through the script. One job is to replace any #define directive with the value it has been given. In our code, that means that anywhere in the script that e_r is found, 6378137.0 literally replaces it.\nFinally, we load the Rcpp namespace with using namespace Rcpp;. This means that we don’t have to keep repeating Rcpp::&lt;...&gt; before every function we want. Sometimes, it’s better to do it the long way, especially if you are using functions from multiple libraries that may overlap, but we’re okay doing it this way this time.\n// header files to include\n#include &lt;Rcpp.h&gt;\n\n// preprocessor replacements\n#define e_r 6378137.0       \n#define m_pi 3.141592653589793238462643383280\n\n// use Rcpp namespace to avoid Rcpp::&lt;...&gt; repetition\nusing namespace Rcpp;\n\n\nUtility functions\nFirst, we have the two utility functions to convert degrees to radians and to compute the Haversine distance between two points. A few things to note right away:\n\nRcpp comments use // or /* ... */\n\nLines must end in a semi-colon, ;\n\nVariables are strongly typed\n\nWhen a language is strongly typed, the user must tell the compiler the variable type, like double, int, etc. R will guess what you want and change on fly. This is a nice interactive feature, but is part of what makes R slow sometimes. By strongly typing a variable, the computer knows right away what you want and doesn’t waste time or memory making adjustments.\nA few other things to notice:\n\ndoubles should end in a . or .0, otherwise they are assumed to be integers; dividing by integers can have weird outcomes so unless you are REALLY sure you want an integer, a floating point number like a double is probably what you want (see degree * m_pi /   180.0)\n\nThe variable type before the function name tells the compiler what form the function output will take\n\nFunction arguments must also be typed\n\nFinally, for the function to be available to you as a user, you must put // [[Rcpp::export]] just before it.\n// convert degrees to radians\n// [[Rcpp::export]]\ndouble deg_to_rad_rcpp(double degree) {\n  return(degree * m_pi / 180.0);\n}\n\n// compute Haversine distance between two points\n// [[Rcpp::export]]\ndouble dist_haversine_rcpp(double xlon,\n               double xlat,\n               double ylon,\n               double ylat) {\n\n  // return 0 if same point\n  if (xlon == ylon && xlat == xlon) return 0.0;\n\n  // convert degrees to radians\n  xlon = deg_to_rad_rcpp(xlon);\n  xlat = deg_to_rad_rcpp(xlat);\n  ylon = deg_to_rad_rcpp(ylon);\n  ylat = deg_to_rad_rcpp(ylat);\n\n  // haversine distance formula\n  double d1 = sin((ylat - xlat) / 2.0);\n  double d2 = sin((ylon - xlon) / 2.0);\n  return 2.0 * e_r * asin(sqrt(d1*d1 + cos(xlat) * cos(ylat) * d2*d2));  \n}\n\n\nMany to many distance matrix\nMoving to the main functions, notice that base R function has been changed only slightly. For the most part, the difference is that variables, argument, and function return must be typed.\nA few other differences to note:\n\nthe length of a vector is measured with .size()\n\nloops in C++ take the form (; ; ), which works as “start i as 0, check if i is less than the number of starting points, n, add 1 to i, then run the loop; if the check fails, skip the loop.\n\n// compute many to many distances and return matrix\n// [[Rcpp::export]]\nNumericMatrix dist_mtom_rcpp(NumericVector xlon,\n                 NumericVector xlat,\n                 NumericVector ylon,\n                 NumericVector ylat,\n                 CharacterVector x_names,\n                 CharacterVector y_names) {\n\n  // init output matrix (x X y)\n  int n = xlon.size();\n  int k = ylon.size();\n  NumericMatrix distmat(n,k);\n\n  // double loop through each set of points to get all combinations\n  for(int i = 0; i &lt; n; i++) {\n    for(int j = 0; j &lt; k; j++) {\n      distmat(i,j) = dist_haversine_rcpp(xlon[i],xlat[i],ylon[j],ylat[j]);\n    }\n  }\n  // add row and column names\n  rownames(distmat) = x_names;\n  colnames(distmat) = y_names;\n  return distmat;\n}   \n\n\nNearest end point\nAgain the Rcpp version of this script is almost identical to the base R version, just with variable typing added. Since we’re returning a data frame (Rcpp version: DataFrame), we have to create it with DataFrame::create() as we return it.\n// compute and return minimum distance along with name\n// [[Rcpp::export]]\nDataFrame dist_min_rcpp(NumericVector xlon,\n                        NumericVector xlat,\n                        NumericVector ylon,\n                        NumericVector ylat,\n                        CharacterVector x_names,\n                        CharacterVector y_names) {\n  \n  // init output matrix (x X 3)\n  int n = xlon.size();\n  int k = ylon.size();\n  CharacterVector minvec_name(n);\n  NumericVector minvec_meter(n);\n  NumericVector tmp(k);\n  \n  // loop through each set of starting points\n  for(int i = 0; i &lt; n; i++) {\n    for(int j = 0; j &lt; k; j++) {\n      tmp[j] = dist_haversine_rcpp(xlon[i],xlat[i],ylon[j],ylat[j]);\n    }\n    // add to output matrix\n    minvec_name[i] = y_names[which_min(tmp)];\n    minvec_meter[i] = min(tmp);\n  }\n  \n  // return created data frame\n  return DataFrame::create(Named(\"fips11\") = x_names,\n                           Named(\"unitid\") = minvec_name,\n                           Named(\"meters\") = minvec_meter,\n                           _[\"stringsAsFactors\"] = false);\n}\n\n\nSource the Rcpp file\nTo compile the Rcpp scripts and have the functions available to us in our R script or session, we need to read in the script with sourceCpp(). This works much like regular source() works, except for Rcpp files. We’ll add the argument rebuild = TRUE so that if need to go back and adjust the source code during a session, it will be rebuilt when we call sourceCpp() again.\nCompiling code can take a while. In essence, we’re trading some time now for faster speed down the road. This is why compiled code isn’t great for interactive coding sessions or for small tasks. But our code isn’t complex and compiles rather quickly.\n\n## source Rcpp code\nsourceCpp(\"site-attachments/dist_func.cpp\", rebuild = TRUE)"
  },
  {
    "objectID": "Extra-05-RCPP.html#quick-comparisons",
    "href": "Extra-05-RCPP.html#quick-comparisons",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Quick comparisons",
    "text": "Quick comparisons\nNow that we have both versions of our distance measuring functions, we should compare the time it takes both to run. But first, let’s make sure that they give the same results.\n\nSingle distance\n\nd_Rcpp &lt;- dist_haversine_rcpp(xlon, xlat, ylon, ylat)\n\n## show\nd_Rcpp\n\n[1] 258212.3\n\n## compare\nidentical(d, d_Rcpp)\n\n[1] TRUE\n\n\nFor one point, our dist_haversine() and dist_haversine_rcpp() give the same result.\n\n\nMany to many\nNext, we’ll compare our many to many matrix, making sure we get the same values.\n\ndistmat_Rcpp &lt;- dist_mtom_rcpp(df_cbg$lon[1:10],\n                               df_cbg$lat[1:10],\n                               df_col$lon,\n                               df_col$lat,\n                               df_cbg$fips11[1:10],\n                               df_col$unitid)\n\n## show\ndistmat_Rcpp[1:5,1:5]\n\n               100654   100663   100690   100706   100724\n010010201001 258212.3 119349.8 31495.73 251754.3 21138.09\n010010201002 256255.2 117447.3 32284.61 249798.4 22263.60\n010010202001 256775.4 118210.3 31047.25 250348.1 21059.86\n010010202002 258084.9 119554.9 30210.67 251665.2 20017.60\n010010203001 256958.5 118703.3 29758.10 250567.0 19905.92\n\n## compare\nall.equal(distmat, distmat_Rcpp)\n\n[1] TRUE\n\n\nAgain, we get the same values (within some very very small floating point rounding error).\n\n\nMinimum distances\nFinally, we’ll compare the minimum distance function.\n\nmindf_Rcpp &lt;- dist_min_rcpp(df_cbg$lon[1:10],\n                            df_cbg$lat[1:10],\n                            df_col$lon,\n                            df_col$lat,\n                            df_cbg$fips11[1:10],\n                            df_col$unitid)\n\n## show\nmindf_Rcpp\n\n         fips11 unitid   meters\n1  010010201001 101471 15785.88\n2  010010201002 101471 14228.69\n3  010010202001 101471 13949.21\n4  010010202002 101471 14870.35\n5  010010203001 101471 13362.81\n6  010010203002 101471 14389.79\n7  010010204001 101471 12704.73\n8  010010204002 101471 13270.80\n9  010010204003 101471 14102.44\n10 010010204004 101471 14756.51\n\n## compare\nall.equal(mindf, mindf_Rcpp)\n\n[1] TRUE\n\n\nAnd once more, the same results. We’re now ready to compare speeds!"
  },
  {
    "objectID": "Extra-05-RCPP.html#benchmarks",
    "href": "Extra-05-RCPP.html#benchmarks",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Benchmarks",
    "text": "Benchmarks\nTo compare really small differences in time, we’ll use the microbenchmark package. Aside from being accurate at small time scales, it makes comparisons based on multiple runs and can plot the differences using the autoplot() function.\nFirst, we’ll compare the core dist_haversine*() functions.\n\n## use microbenchmark to compare \ntm_single &lt;- microbenchmark(base_R = dist_haversine(xlon, xlat, ylon, ylat),\n                            Rcpp = dist_haversine_rcpp(xlon, xlat, ylon, ylat),\n                            times = 1000L)\n## results\ntm_single\n\nUnit: microseconds\n   expr   min    lq     mean median    uq      max neval cld\n base_R 3.651 3.750 4.010598 3.8520 4.029   20.679  1000   a\n   Rcpp 1.906 2.141 4.500577 2.3035 4.185 1378.462  1000   a\n\n## plot\nautoplot(tm_single)\n\n\n\n\nComparing dist_haversine() with dist_haversine_rcpp(), the compiled version isn’t that much faster. Considering we’re on the scale of microseconds, it really isn’t that much faster.\n\n## time for base R to do many to many with 100 starting points\nsystem.time(dist_mtom(df_cbg$lon[1:100],\n                      df_cbg$lat[1:100],\n                      df_col$lon,\n                      df_col$lat,\n                      df_cbg$fips11[1:100],\n                      df_col$unitid))\n\n   user  system elapsed \n  5.532   0.044   6.306 \n\n## ...and now Rcpp version\nsystem.time(dist_mtom_rcpp(df_cbg$lon[1:100],\n                           df_cbg$lat[1:100],\n                           df_col$lon,\n                           df_col$lat,\n                           df_cbg$fips11[1:100],\n                           df_col$unitid))\n\n   user  system elapsed \n  0.062   0.002   0.068 \n\n## compare just 10 many to many\ntm_mtom &lt;- microbenchmark(base_R = dist_mtom(df_cbg$lon[1:10],\n                                             df_cbg$lat[1:10],\n                                             df_col$lon,\n                                             df_col$lat,\n                                             df_cbg$fips11[1:10],\n                                             df_col$unitid),\n                          Rcpp = dist_mtom_rcpp(df_cbg$lon[1:10],\n                                                df_cbg$lat[1:10],\n                                                df_col$lon,\n                                                df_col$lat,\n                                                df_cbg$fips11[1:10],\n                                                df_col$unitid),\n                          times = 100L)\n\n## results\ntm_mtom\n\nUnit: milliseconds\n   expr        min        lq       mean     median         uq        max neval\n base_R 409.045492 446.55024 506.787092 494.631705 554.894677 774.337767   100\n   Rcpp   4.031125   4.10577   5.087788   4.911738   5.869729   9.618541   100\n cld\n  a \n   b\n\n## plot\nautoplot(tm_mtom)\n\n\n\n\nWhere we start to see speed improvements in the main function. The compiled version of the many to many function is nearly two orders of magnitude faster!\n\n## time for base R to do many to many with 100 starting points\nsystem.time(dist_min(df_cbg$lon[1:100],\n                     df_cbg$lat[1:100],\n                     df_col$lon,\n                     df_col$lat,\n                     df_cbg$fips11[1:100],\n                     df_col$unitid))\n\n   user  system elapsed \n  5.308   0.026   5.446 \n\n## ...and now Rcpp version\nsystem.time(dist_min_rcpp(df_cbg$lon[1:100],\n                          df_cbg$lat[1:100],\n                          df_col$lon,\n                          df_col$lat,\n                          df_cbg$fips11[1:100],\n                          df_col$unitid))\n\n   user  system elapsed \n  0.049   0.000   0.050 \n\n## compare just 10 min\ntm_min &lt;- microbenchmark(base_R = dist_min(df_cbg$lon[1:10],\n                                           df_cbg$lat[1:10],\n                                           df_col$lon,\n                                           df_col$lat,\n                                           df_cbg$fips11[1:10],\n                                           df_col$unitid),\n                         Rcpp = dist_min_rcpp(df_cbg$lon[1:10],\n                                              df_cbg$lat[1:10],\n                                              df_col$lon,\n                                              df_col$lat,\n                                              df_cbg$fips11[1:10],\n                                              df_col$unitid),\n                         times = 100)\n## results\ntm_min\n\nUnit: milliseconds\n   expr        min         lq       mean     median         uq       max neval\n base_R 394.187697 450.627588 521.500527 498.440708 587.912705 812.49850   100\n   Rcpp   4.602717   4.807334   5.925393   5.584605   6.413875  16.53974   100\n cld\n  a \n   b\n\n## plot\nautoplot(tm_min)\n\n\n\n\nSimilarly, the compiled minimum distance function is much faster than the base R function.\nIn this case as well as the former, we should note that we aren’t comparing fully optimized versions of either function. That said, while it’s possible that the base R functions could be sped up, neither is likely to come close to matching the speed of the compiled Rcpp versions."
  },
  {
    "objectID": "Extra-05-RCPP.html#full-run-for-rcpp-version",
    "href": "Extra-05-RCPP.html#full-run-for-rcpp-version",
    "title": "Extra: R & C++ Using RCPP",
    "section": "Full run for Rcpp version",
    "text": "Full run for Rcpp version\nThroughout, we’ve been running the functions on a reduced starting data set. But how long does it take to find the nearest college to each census block? Let’s find out!\n\n## find minimum\nsystem.time(full_min &lt;- dist_min_rcpp(df_cbg$lon,\n                                      df_cbg$lat,\n                                      df_col$lon,\n                                      df_col$lat,\n                                      df_cbg$fips11,\n                                      df_col$unitid))\n\n   user  system elapsed \n110.133   0.401 112.701 \n\n## show\nfull_min %&gt;% tibble()\n\n# A tibble: 217,740 × 3\n   fips11       unitid meters\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 010010201001 101471 15786.\n 2 010010201002 101471 14229.\n 3 010010202001 101471 13949.\n 4 010010202002 101471 14870.\n 5 010010203001 101471 13363.\n 6 010010203002 101471 14390.\n 7 010010204001 101471 12705.\n 8 010010204002 101471 13271.\n 9 010010204003 101471 14102.\n10 010010204004 101471 14757.\n# … with 217,730 more rows\n\n\nJust a little over a minute (on my laptop) to compute 217,000 by 7,700 distances, finding and storing the minimally distance college along with its distance!"
  },
  {
    "objectID": "01-Setup-Installing-R.html#getting-started",
    "href": "01-Setup-Installing-R.html#getting-started",
    "title": "I: Installing R & RStudio",
    "section": "Getting started",
    "text": "Getting started\nThis course requires you to install a few bits of software on your computer. Specifically, you need:\n\nR\nRStudio\ngit\nLaTeX\n\nThese instructions should help you find and download what you need. You do not need to use this guide, but it may help, particularly if you aren’t used to downloading and installing open source software.\nI’ve done my best to include screenshots of each step or provide links to external sites that already have excellent instructions (e.g., git). One snag, however, is that while some in the class may use Windows/PC, others use Apple/MacOS (I’m making the assumption that no one is using Linux — if you are, you probably don’t need these instructions!). I personally use MacOS. This means that some of the screenshots are based on what I see as a Mac user on the software websites. But where I can, I show sections for MacOS and Windows downloads.\nI also can’t walk you through each step of the installation once you’ve downloaded the correct files, again, because operating systems differ. That said, he good news is that with only one exception (sorry Windows users!), you should be able to install all software using the default process like you do with most other software."
  },
  {
    "objectID": "01-Setup-Installing-R.html#installing-r",
    "href": "01-Setup-Installing-R.html#installing-r",
    "title": "I: Installing R & RStudio",
    "section": "Installing R",
    "text": "Installing R\nFirst things first, we’ll get R, which you can find at https://cran.r-project.org. Depending on your operating system (OS), you’ll click one of the following links at the top of the home page.\n\n\n\nCRAN homepage\n\n\n\nR for MacOS\nWhen downloading R for MacOS, you’ll want to click the link for the latest version of R: R-&lt;#&gt;.&lt;#&gt;.&lt;#&gt;.pkg where &lt;#&gt;.&lt;#&gt;.&lt;#&gt; represent the major, minor, and patch numbers. As of the writing of this document (May 2020), the latest version of R is R 4.0.0 — it may be different (higher) when you download. Just grab the one inside the red box.\nYou may be asked if you want to allow the download. If so, say yes and pay attention to where you save it (typically your Downloads folder). Once it has finished downloading, double click on the package icon and follow the default directions to install.\n\n\n\nPage to download R for MacOS\n\n\n\n\nR for Windows\nWhen downloading R for Windows, you’ll first be taken to an intermediate screen. Just click the indicated link to go to the next page.\n\n\n\nIntermediate page to download R for Windows\n\n\nOn the next screen, click the link to “Download R-&lt;#&gt;.&lt;#&gt;.&lt;#&gt; for Windows” where &lt;#&gt;.&lt;#&gt;.&lt;#&gt; represent the major, minor, and patch numbers. As of the writing of this document (May 2020), the latest version of R is R 4.0.0 — it may be different (higher) when you download. Just grab the one inside the red box.\nYou may be asked if you want to allow the download. If so, say yes and pay attention to where you save it (typically your Downloads folder). Once it has finished downloading, double click on the installation icon and follow the default directions to install.\nDepending on the level of control you have on your computer and how you typically install software, you may want to install R as an administrator. I would recommend that to head off issues down the road, but if you don’t have administrator privileges then go ahead an install as a user.\n\n\n\nPage to download R for Windows"
  },
  {
    "objectID": "01-Setup-Installing-R.html#installing-rstudio",
    "href": "01-Setup-Installing-R.html#installing-rstudio",
    "title": "I: Installing R & RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nNow that you’ve installed R, it’s time to get RStudio, the program we’ll use to work with R. Start by going to the RStudio home page: https://rstudio.com.\nAt the very top, you’ll see a link to “Downloads”: click that.\n\n\n\nRstudio homepage\n\n\nYou’ll be presented with a number of versions of RStudio to install. We’ll choose the free desktop version (naturally!).\n\n\n\nRStudio version selection\n\n\nYou’ll now see a button to download RStudio. Step (1) is to download R, but we’ve already done that so we’re good.\nOne thing: the RStudio website is smart and tries to guess your OS so that it can present you with a big button to download the correct version. As you can see, it worked for me: I’m shown a button to download RStudio for MacOS. If you go to the website on a computer using Windows, the button should instead be a link to install RStudio for Windows. If all works, then you can click the button either way (yours just may look different), download, and install as normal. If the button doesn’t have your correct OS, then go to the next step.\n\n\n\nRStudio install button\n\n\nJust below the big button, you’ll see the full list of RStudio versions. You can also pick your correct version here. Same as before, just click the link, download, and install as normal.\n\n\n\nRStudio install options"
  },
  {
    "objectID": "01-Setup-Installing-R.html#installing-git",
    "href": "01-Setup-Installing-R.html#installing-git",
    "title": "I: Installing R & RStudio",
    "section": "Installing git",
    "text": "Installing git\nThere are two things you need to do to use git/GitHub in our course: (1) have an installation of git and (2) have a GitHub account. Rather than reinventing the wheel, I suggest following the instructions from Jenny Bryan.\n\nGet a GitHub account\nInstall git on your computer\n\nNOTE As part of registering an account with GitHub, I recommend requesting an Education Discount so you can get free private repositories for future work."
  },
  {
    "objectID": "01-Setup-Installing-R.html#installing-latex",
    "href": "01-Setup-Installing-R.html#installing-latex",
    "title": "I: Installing R & RStudio",
    "section": "Installing LaTeX",
    "text": "Installing LaTeX\nLaTeX is a document typesetting system/language. While it’s probably best known for its ability to nicely typeset mathematical equations, LaTeX works really well quantitative research workflows. That said, it can be difficult to install and work with.\nWe’ll use LaTeX later in the semester so that you can make nice PDF reports. The good news is that you won’t really need to interact with LaTeX at all to do so — other than to install it now.\nSince you don’t need a full TeX distribution on your computer, you can most likely get by using the TinyTeX distribution that we can install directly from R. If you want a full version of TeX on your computer (NOTE: It’s very large), then skip to the full installation for your computer.\n\nTinyTex\nOnce you’ve installed R and RStudio, open RStudio and type the following in the Console:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\nThis will install the tinytex R package and then install the TinyTeX distribution (it may take a minute or two).\n\n\nOPTIONAL: Full installation\nIf you want the full installation, first go the LaTeX home page at https://www.latex-project.org.\n\nIf you scroll down slightly, you’ll see options for MacOS (MacTeX) and Windows (MiKTeX) installations. Click the link that applies to your OS and follow the instructions below.\n\n\nLaTeX for MacOS (MacTeX)\nOn the MacTeX home page, first click the link for the MacTeX Download.\n\nOn the next page, click the link for MacTeX.pkg, agree to download, and then double click on the downloaded file to install. Note that this package is very big (~ 4GB) because you are downloading just about everything TeX-related, including some software. It’s what I use, but I use TeX all the time. Just know this in case your storage space is limited on your computer.\n\n\n\nLaTeX for Windows (MiKTeX)\nOnce you reach the Windows (MiKTeX) homepage, click the Downloads link at the top of the page.\n\nOn the next screen, you’ll want to click on the blue “Download” button, agree to the download, and then install.\n\nONE NOTE MiKTeX is much smaller than MacTeX, but that’s because it doesn’t download everything. Instead, it opts to only install packages as you need them. Cool, expect that doesn’t always work well with RStudio.\nThe fix is this: when going through the installation, on the “Settings” screen, be sure to change the default selection for “Install missing packages on-the-fly” from \"Ask me first\" to \"Always\". Continue the installation with the other default options.\n\n\nprint('Hello, World!')\n\n[1] \"Hello, World!\"\n\n\nR is a port of the S language, which was developed at Bell Labs. As a GNU project, R is open source and free to use and distribute. It can be installed and used on most major operating systems.\nR is best thought of as an integrated language and environment that was designed with statistical computing and data analysis in mind. To that end, its structure is a balance between powerful mathematical computation and high-level functionality that can be used interactively (unlike compiled code). In other words, it’s a great tool for quantitative data analysis since it both allows you to investigate your data easily and, when the time comes, write robust programs.\nOriginally, R was probably best known for its graphing capabilities. As it has matured, it has grown in popularity among data scientists, who have increasingly extended its functionality through user-contributed packages. We will use a number of packages during this course."
  },
  {
    "objectID": "01-Setup-Installing-R.html#rstudio-an-integrated-development-environment-ide-for-r",
    "href": "01-Setup-Installing-R.html#rstudio-an-integrated-development-environment-ide-for-r",
    "title": "I: Installing R & RStudio",
    "section": "RStudio: an integrated development environment (IDE) for R",
    "text": "RStudio: an integrated development environment (IDE) for R\nTo work with the R language, it helps to have an application. While R ships with one (you may have seen it on your computer after you installed R), it’s pretty plain. RStudio, on the other hand, is a powerful integrated design environment (IDE) that does most everything R-related very well and with little fuss: run commands, write scripts, view output, interact with other languages and remote site, etc. There are other options for working with R, but RStudio is a great all-around program that we will use in this course.\n\nRStudio has 3-4 main frames:\n\nConsole\nScript window (will be closed at first if you don’t have any scripts open)\nEnvironment / History / Connections\nFiles / Plots / Packages / Help / Viewer\n\nEach has a useful purpose, but for today, we’ll mostly focus on the console itself.\n\nQuick exercise\nIf you haven’t already, try entering an equation in the console (like 1 + 1). Next, open the script associated with this module and run the first line. Welcome to R!"
  },
  {
    "objectID": "01-Setup-Installing-R.html#assignment",
    "href": "01-Setup-Installing-R.html#assignment",
    "title": "I: Installing R & RStudio",
    "section": "Assignment",
    "text": "Assignment\nR is a type of object-oriented programming environment. This means that R thinks of things in its world as objects, which are like virtual boxes in which we can put things: data, functions, and even other objects.\nBefore discussing data types and structures, the first lesson in R is how to assign values to objects. In R (for quirky reasons), the primary means of assignment is the arrow, &lt;-, which is a less than symbol, &lt;, followed by a hyphen, -.\n\n## assign value to object x using &lt;-\nx &lt;- 1\n\nNOTE: You can also use a single equals sign, =, to assign a value to an object: x = 1. Keep in mind, however, that since = sometimes has other meanings in R and can be confused with ==, which is different, it’s generally clearer to use &lt;-.\n\nBut’s where’s the output?\nR does exactly what you ask it to do — no more, no less. If you don’t ask it to return something, either explicitly from a function or implicitly by printing to the console, it won’t. This can be huge source of frustration to new users.\nThe good-ish news is that by default, R will print an object’s contents to the console if it’s the only thing you type in. Many functions similarly print to the console if you don’t assign the output to an object. You can see this when simply type a number or character into the console.\n\n## when you input a number or character, R returns it back to you\n1\n\n[1] 1\n\n\"a\"\n\n[1] \"a\"\n\n\nBasically, you’ve just told R “Here’s a 1” and R said “The content of 1 is 1”. Same for \"a\". The initial number in the square brackets ([1]) is telling you the index (place within the object) of the first item. Since we only have one item, it’s just [1].\nWhen you store something in an object, you can type the object’s name into the console to see what’s in it.\n\n## what's in x?\nx\n\n[1] 1\n\n\nA neat trick if you want to both assign a value and see the results printed to the output is to to wrap the entire line in ().\n\n## wrap in () to print after assignment\n(x &lt;- 5)\n\n[1] 5\n\n\n\nQuick exercise\nUsing the arrow, assign the output of 1 + 1 to x. Next subtract 1 from x and reassign the result to x. Show the value in x.\n\nA NOTE ON “GOOD-ISH” Keep in mind how much data your object has / might have when printing it to the console. For a small amount, printing is just fine. But if you have, for example, a matrix with 1,000 columns and 1 million rows, printing might not be a useful exercise. There are other ways, such as the function head() that might be more useful in these situations."
  },
  {
    "objectID": "01-Setup-Installing-R.html#comments",
    "href": "01-Setup-Installing-R.html#comments",
    "title": "I: Installing R & RStudio",
    "section": "Comments",
    "text": "Comments\nYou may have noticed already, but comments in R are set off using the hash or pound character at the beginning of the line: #. The comment character tells R to ignore the line, that is, do not try to interpret it as code you the user want run.\n\nQuick exercise\nType the phrase “This is a comment” directly into the R console both with and without a leading “#”. What happens each time?\n\nYou may notice that I use two hashes. This is a stylistic tick that has more to do with the editor I use than an R requirement. You can use only a single # for your comments if you like."
  },
  {
    "objectID": "01-Setup-Installing-R.html#data-types-and-structures",
    "href": "01-Setup-Installing-R.html#data-types-and-structures",
    "title": "I: Installing R & RStudio",
    "section": "Data types and structures",
    "text": "Data types and structures\nR uses variety of data types and structures to represent and work with data. There are many, but the major ones that you’ll use most often are:\n\nlogical\nnumeric (integer & double)\ncharacter\nvector\nmatrix\nlist\ndataframe\n\nUnderstanding the nuanced differences between data types is not important right now. Just know that they exist and that you’ll gain an intuitive understanding of them as you become better aquainted with R."
  },
  {
    "objectID": "01-Setup-Installing-R.html#packages",
    "href": "01-Setup-Installing-R.html#packages",
    "title": "I: Installing R & RStudio",
    "section": "Packages",
    "text": "Packages\nUser-submitted packages are a huge part of what makes R great. You may hear me use the phrases “base R” during class. What I mean by this is the R that comes as you download it with no packages loaded (sometimes also called “vanilla R”). While it’s powerful in and of itself — you can do everything you need with base R — most of your scripts will make use of one of more contributed packages. These will make your data analytic life much nicer. We’ll lean heavily on the tidyverse suite of packages this semester.\n\nInstalling packages from CRAN\nMany contributed packages are hosted on the CRAN package repository. What’s really nice about CRAN is that packages have to go through quite a few checks in order for CRAN to approve and host them. Checks include making sure the package has documentation, works on a variety of systems, and doesn’t try to do odd things to your computer. The upshot is that you should feel okay downloading these packages from CRAN.\nTo download a package from CRAN, use:\n\ninstall.packages(\"&lt;package name&gt;\")\n\nNOTE Throughout this course, if you see something in triangle brackets (&lt;...&gt;), that means it’s a placeholder for you to change accordingly.\nMany packages rely on other packages to function properly. When you use install.packages(), the default option is to install all dependencies. By default, R will check how you installed R and download the right operating system file type.\n\nQuick exercise\nInstall the tidyverse package, which is really a suite of packages that we’ll use throughout the semester. Don’t forget to use double quotation marks around the package name: install.packages(\"tidyverse\")\n\n\n\nLoading package libraries\nPackage libraries can loaded in a number of ways, but the easiest it to write:\n\nlibrary(\"&lt;library name&gt;\")\n\nwhere \"&lt;library name&gt;\" is the name of the package/library. You will need to load these before you can use their functions in your scripts. Typically, they are placed at the top of the script file.\nFor example, let’s load the tidyverse library we just installed:\n\n## load library (note quirk that you don't need quotes here)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that when you load the tidyverse (which, again, is actually loading a number of other libraries), you see a lot of output. Not all packages are this noisy, but the information is useful here because it shows all the libraries that are now loaded and ready for you to use."
  },
  {
    "objectID": "01-Setup-Installing-R.html#help",
    "href": "01-Setup-Installing-R.html#help",
    "title": "I: Installing R & RStudio",
    "section": "Help",
    "text": "Help\nI don’t have every R function and nuance memorized, so I certainly don’t expect that you will. With all the user-written packages, it would be difficult to keep up if I tried! When stuck, there are a few ways to get help.\n\nHelp files\nIn the console, typing a function name immediately after a question mark will bring up that function’s help file (in RStudio, you should see in the bottom right facet):\n\n## get help file for function\n?median\n\nTwo question marks will search for the command name in CRAN packages (again, in the bottom right facet):\n\n## search for function in CRAN\n??median\n\nAt first, using help files may feel like trying to use a dictionary to see how to spell a word — if you knew how to spell it, you wouldn’t need the dictionary! Similarly, if you knew what you needed, you wouldn’t need the help file. But over time, they will become more useful, particularly when you want to figure out an obscure option that will give you exactly what you need.\n\n\nGoogle it!\nGoogle is a coder’s best friend. If you are having a problem, odds are a 1,000+ other people have too and at least one of them has been brave enough to ask about it in a forum like StackOverflow, CrossValidated, or R-help mailing list.\nIf you are lucky, you’ll find the exact answer to your question. More likely, you’ll find a partial answer that you’ll need to modify for your needs. Sometimes, you’ll find multiple partial answers that, in combination, help you figure out a solution. It can feel overwhelming at first, particularly if it’s a way of problem-solving that’s different from what you’re used to. But it does become easier with practice.\nGoogle it!\n\n\nAsking for help: order of operations\nWhen needing help for this class, your order of operations should be:\n\nTry a lot on your own (perhaps using rubber duck debugging)\nR help files\nGoogle\nClass peers (directly or through our Issues page)\nMe\n\nThis is not because I don’t want to help. My concern is the opposite: that I’m likely to just show you. Data analysis is tricky because no two problems are alike. But over time, they do rhyme. The time you put in now learning to figure things out on your own will be well paid in the future."
  },
  {
    "objectID": "01-Setup-Installing-R.html#useful-packages",
    "href": "01-Setup-Installing-R.html#useful-packages",
    "title": "I: Installing R & RStudio",
    "section": "Useful packages",
    "text": "Useful packages\nWe’re going to use a number of packages this semester. While we may need more than this list — and you almost certainly will in your own future work — let’s install these to get us started.\n\nQuick exercise\nInstall the following packages using the install.packages() function: - devtools - here - usethis - gitcreds"
  },
  {
    "objectID": "01-Setup-Installing-R.html#software-installation",
    "href": "01-Setup-Installing-R.html#software-installation",
    "title": "I: Installing R & RStudio",
    "section": "Software installation",
    "text": "Software installation\nIf you weren’t able to get everything set up and working prior to or during our first class meeting, please take time to finish installing all the software required for the course. Please set up a time to meet with me ASAP if you are having issues."
  },
  {
    "objectID": "01-Setup-Installing-R.html#practice-creating-an-r-.r-script",
    "href": "01-Setup-Installing-R.html#practice-creating-an-r-.r-script",
    "title": "I: Installing R & RStudio",
    "section": "Practice creating an R (*.R) script",
    "text": "Practice creating an R (*.R) script\n\nOpen RStudio\nInitialize a New File &gt; R Script in RStudio. Save it as &lt;lastname&gt;_assignment_1.R in the assignments folder in your personal repo, replacing &lt;lastname&gt; with your last name. For example, I would call my assignment skinner_assignment_1.R and it would be located in the following location:\nstudent_skinner/\n|\n|__assignments/\n    |\n    |__skinner_assignment_1.R\n\nNOTE that you can complete these steps using either the drop down menus or the icons.\nAlso, don’t forget to add .R to the end of your file. You will know you’ve done it correctly if the tab with your file name changes to include an icon that looks like a sheet of paper with an “R” superimposed on it.\nCopy the following code snippet into your file and save the file:\n## install rmarkdown package\ninstall.packages(\"rmarkdown\")\nRun the code to install the RMarkdown package either by\n\nHighlighting the code and pushing the Run button\nPutting your cursor on the line of code and using the key combo of Command-Enter (Mac) or Control-Enter (Windows) to run"
  },
  {
    "objectID": "Extra-08-Types-Structures.html#questions",
    "href": "Extra-08-Types-Structures.html#questions",
    "title": "Extra: Data Types & Structures",
    "section": "Questions",
    "text": "Questions\n\nQuick exercise\nCreate two or three equal length vectors. Next, combine to create a data frame. Finally, change one value in the data frame (HINT: think about how you changed vector and matrix values before)."
  },
  {
    "objectID": "05-Data-Viz-I.html#setup",
    "href": "05-Data-Viz-I.html#setup",
    "title": "I: Basics",
    "section": "Setup",
    "text": "Setup\nWe’re using two libraries today:\n\nggplot2\nhaven\n\nThe ggplot2 library is part of the tidyverse, so we don’t need to load it separately (we can just use library(tidyverse) as always).\nWe’re also going to use haven, which allows us to read in data files from other software such as SPSS, SAS, and Stata. We’ll use it to read in a Stata (*.dta) version of the small HSLS data we’ve used before. The Stata version, unlike the plain CSV version, has labels for the variables and values. These will be useful when plotting.\nThough haven is part of the tidyverse (and should have been installed when you installed tidyverse), we’ll have to explicitly call it.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(haven)\n\nNote that since we have two data files this lesson, we’ll give them unique names instead of the normal df:\n\ndf_hs := hsls_small.dta\ndf_ts := all_schools.csv\n\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## read_dta() ==&gt; read in Stata (*.dta) files\n## read_csv() ==&gt; read in comma separated value (*.csv) files\ndf_hs &lt;- read_dta(file.path(\"data\", \"hsls_small.dta\"))\ndf_ts &lt;- read_csv(file.path(\"data\", \"sch_test\", \"all_schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "05-Data-Viz-I.html#plots-using-base-r",
    "href": "05-Data-Viz-I.html#plots-using-base-r",
    "title": "I: Basics",
    "section": "Plots using base R",
    "text": "Plots using base R\nEven though new graphics libraries have been developed, the base R graphics system remains powerful. The base system is also very easy to use in a pinch. When I want a quick visual of a data distribution that’s just for me, I often use base R.\nNote that for the next few plots, I’m not much concerned with how they look. Specifically, the axis labels won’t look very nice. We could spend time learning to make really nice base R plots for publication, but I’d rather we spend that time with ggplot2 graphics.\nAlso note that we’ll switch to using the base R data frame $ notation to pull out the columns we want. If you need some more information on using $ notation, check out the supplemental lesson on data wrangling with base R.\n\nHistogram\nFor continuous variables, a histogram is a useful plot. Though the hist() function has many options to adjust how it looks, the defaults work really well if you just want a quick look at the distribution.\n\n## histogram of math scores (which should be normal by design)\nhist(df_hs$x1txmtscor)\n\n\n\n\n\nQuick exercise\nCheck the distribution of the students’ socioeconomic score (SES).\n\n\n\nDensity\nDensity plots are also really helpful. R doesn’t have single density plot function, but you can get a density plot in one of two ways, each of which will give a slightly different result.\nFirst, you can adjust the hist() function to add the freq = FALSE argument. It looks like the first histogram above, but notice that the y-axis now represents density rather than counts.\n\n## density plot of math scores with hist() function\nhist(df_hs$x1txmtscor, freq = FALSE)\n\n\n\n\nSecond, you can plot() the density() of a continuous variable. Unlike hist(), however, density() doesn’t automatically ignore missing values, so we have to tell it to remove NAs using the na.rm = TRUE argument (a common argument for base R functions that’s useful to remember).\n\n## density plot of math scores\n## read inside out: get density value, then plot values\nplot(density(df_hs$x1txmtscor, na.rm = TRUE))\n\n\n\n\n\nQuick exercise\nPlot the density of SES. Next, add the col argument in plot() to change the color of the line to \"red\": plot(density(df_hs$x1txtmscor, na.rm = TRUE), col = \"red\").\n\n\n\nBox plot\nA box plot will let you see the distribution of a continuous variable at specific values of another discrete variable. For example, test scores ranges at each student expectation level.\nCall a box plot using the boxplot() function. This one is a little trickier because it uses the R formula construction to set the continuous variable against the discrete variable. The formula uses a tilde, ~, and should be constructed like this:\n\n&lt;continuous var&gt; ~ &lt;discrete var&gt;\n\nNotice how we can use the data = df_hs argument instead of adding df_hs$ in front of the variable names. This saves us some typing.\n\n## box plot of math scores against student expectations\nboxplot(x1txmtscor ~ x1stuedexpct, data = df_hs)\n\n\n\n\nFrom the boxplot, we can see that math test scores tend to increase as students’ educational expectations increase (remember that 11 means “I don’t know [how far I’ll go in school]”), though there’s quite a bit of overlap in the marginal distributions.\n\n\nScatter\nPlot two continuous variables against one another using the base plot() function. There are two primary ways to make a scatter plot using plot():\n\nplot(x, y)\nplot(y ~ x)\n\nWith both, x is the variable that will go on the x-axis and y the one that will go on the y-axis. It’s really a matter of which makes sense to you. We’ll use the first.\n\n## scatter plot of math against SES\nplot(df_hs$x1ses, df_hs$x1txmtscor)\n\n\n\n\nWhile the data seem to show a positive correlation between socioeconomic status and math test score, there’s also quite a bit of variation in that association (notice that the cloud-like nature of the circles).\n\nQuick exercise\nRerun the above plot, but this time store it in an object, plot_1. Next, make the same plot, but this time use the second formula construction (~) — store it in an object, plot_2. Visually compare the two, but for a more formal test, use identical(plot_1, plot_2) on the two plot objects to prove they are the same."
  },
  {
    "objectID": "05-Data-Viz-I.html#plots-using-ggplot2",
    "href": "05-Data-Viz-I.html#plots-using-ggplot2",
    "title": "I: Basics",
    "section": "Plots using ggplot2",
    "text": "Plots using ggplot2\nggplot2 is my — and many R users’ — primary system for making plots. It is based on the idea of a grammar of graphics. Just as we can use finite rules of a language grammar to construct an endless number of unique sentences, so too can we use a few graphical grammatical rules to make an endless number of unique figures.\nThe ggplot2 system is too involved to cover in all of its details, but that’s kind of the point of the grammar of graphics: once you see how it’s put together, you can anticipate the commands you need to build your plot.\nWe’ll start by covering the same plots as above.\n\nHistogram\nAs the main help site says, all ggplot2 plots need three things:\n\n[data]: The source of the variables you want to plot\n[aesthetics]: How variables in the data map onto the plot (e.g., what’s on the x-axis? what’s on the y-axis?)\n[geom]: The geometry of the figure or the kind of figure you want to make (e.g., what do you want to do with those data and mappings? A line graph? A box plot?…)\n\nWe’ll start by making a histogram again. To help make these pieces clearer, I’ll use the argument names when possible. The first function, which initializes the plot is ggplot(). Its first argument is the data.\nThe aesthetic mappings, that is, which variables go where or how they function on the plot, go inside the aes() function. Since we only have one variable, x1txmtscor, it is assigned to x.\nIf we assign this first part to an object, p, and print by calling the object…\n\n## init ggplot \np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor))\np\n\n\n\n\n…nothing! Well, not nothing, but no histogram. That’s because the plot object p knows the data and the key variable mapping but doesn’t know what do with them. What do we want?\nSince we want a histogram, we add the geom_histogram() function to the existing plot object with a plus sign(+). Once we do that, we’ll try to print the plot again…\n\n## add histogram instruction (notice we can add pieces using +)\np &lt;- p + geom_histogram()\np\n\n\n\n\nSuccess!\nLet’s repeat it the whole process, but without the middle step:\n\n## create histogram using ggplot\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram()\np\n\n\n\n\nAs you can see, the code to make a ggplot2 figure looks a lot like what we’ve seen with other tidyverse libraries, e.g. dplyr. The key difference between ggplot2 and dplyr, however, is that while dplyr uses the pipe (%&gt;%) to connect different functions, ggplot2 uses a plus sign (+).\nIt may help you remember the difference:\n\ndplyr moves output from left to the input in the right and so needs a pipe (%&gt;%)\nggplot2 adds layer upon layer to build up the final figure and so needs a plus sign (+)\n\n\n\nDensity\nUnlike the base R graphics system, ggplot2 does have a density plotting command, geom_density(). Instead of building up the figure piecemeal, we’ll go ahead and chain the geom to the first command and print.\nNotice how the function chain is the mostly the same as above, but (1) written in a single linked chain and (2) using a different geom_*() command at the end to indicate that we want something different.\n\n## density\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_density()\np\n\n\n\n\n\nQuick exercise\nMake a density plot of SES.\n\nIf we want to superimpose the density plot over the histogram, we only need chain the two commands together with a slight modification in how the histogram is made. This way, the histogram and the density will be on the same scale.\nThe change happens in the geom_histogram() function, where we add a new mapping: aes(y = ..density..). (NOTE: this is similar to what we did above in base R to make a histogram on a density scale.)\n\n## histogram with density plot overlapping\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram(mapping = aes(y = ..density..)) +\n    geom_density()\np\n\n\n\n\nIt worked, but it’s not the greatest visual since the colors are the same and the density plot is thin with no fill.\nAdding to what came before, the geom_histogram() and geom_density() both take on new arguments that change the defaults. Now the resulting plot should look nicer and be easier to read.\n\n## histogram with density plot overlapping (add color to see better)\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram(mapping = aes(y = ..density..),\n                   color = \"black\",\n                   fill = \"white\") +\n    geom_density(fill = \"red\", alpha = 0.2)\np\n\n\n\n\n\nQuick exercise\nTry changing some of the arguments in the last plot. What happens when you change alpha (keep the value between 0 and 1)? What does the color argument change? And fill? What happens if you switch the geom_*() functions, call geom_histogram() after you call geom_density()?\n\nA key thing to note about arguments is that when the are outside of the aes(), they apply uniformly to the whole geom (e.g. all the histogram bars are white with a black outline, the density is light red). When you want some aesthetic of the figure to change as a function of the data, you need to put it inside aes(). We’ll see this in the next plot.\n\n\nTwo-way\nPlotting the difference in a continuous distribution across groups is a common task. Let’s see the difference between student math scores between students with parents who have any postsecondary degree and those without.\nSince we’re using data that was labeled in Stata, we’ll see the labels when we use count()\n\n## see the counts for each group\ndf_hs %&gt;% count(x1paredu)\n\n# A tibble: 7 × 2\n  x1paredu                                         n\n  &lt;dbl+lbl&gt;                                    &lt;int&gt;\n1  1 [Less than high school]                    1010\n2  2 [High school diploma or GED]               5909\n3  3 [Associate's degree]                       2549\n4  4 [Bachelor's degree]                        4102\n5  5 [Master's degree]                          2116\n6  7 [Ph.D/M.D/Law/other high lvl prof degree]  1096\n7 NA                                            6721\n\n\nWe can see that all values of x1paredu greater than 2 represent parents with some college credential. Since we want only two distinct groups, we can use the operator &gt;= to make a new 0/1 binary variable. If a value of x1paredu is above 3, then the new indicator pared_coll will be 1; if not, 0.\nNOTE that in the Stata version of hsls_small, all the missing values, which are normally negative numbers, have already been properly converted to NA values. That’s why we see a count column for NA and not labels for missingness that we might have expected based on prior lessons.\nThe ggplot() function doesn’t need to use our full data. In fact, our data needs to be set up a bit differently to make this plot. We’ll make a new temporary data object that only has the data we need.\n\n## need to set up data\nplot_df &lt;- df_hs %&gt;%\n    ## select the columns we need\n    select(x1paredu, x1txmtscor) %&gt;%\n    ## can't plot NA so will drop\n    drop_na() %&gt;%\n    ## create new variable that == 1 if parents have any college\n    mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0)) %&gt;%\n    ## drop (using negative sign) the original variable we don't need now\n    select(-x1paredu) \n\n## show\nhead(plot_df)\n\n# A tibble: 6 × 2\n  x1txmtscor pared_coll\n  &lt;dbl+lbl&gt;       &lt;dbl&gt;\n1 59.4                1\n2 47.7                1\n3 64.2                1\n4 49.3                1\n5 62.6                1\n6 58.1                1\n\n\nTo plot against the two groups we’ve made, we need to add it to the aesthetic feature, aes(). The math score, x1txmtscor, is still mapped to x, but since we want two side-by-side histograms, we set the fill aesthetic to our new indicator variable. So the function knows that it’s a group (and not just a continuous number with only two values), we wrap it in the factor() function.\nFinally, we add some changes to the geom_histogram() function so that each group is on the same scale.\n\n## two way histogram\np &lt;- ggplot(data = plot_df,\n            aes(x = x1txmtscor, fill = factor(pared_coll))) +\n    geom_histogram(alpha = 0.5, stat = \"density\", position = \"identity\")\np\n\n\n\n\nBy assigning pared_coll to the fill aesthetic, we can see a difference in the distribution of math test scores between students whose parents have at least some college and those whose parents do not.\n\nQuick exercise\nRemove some of the new arguments in geom_histogram(). How does the resulting plot change? Remove the factor() function from around pared_coll: what happens?\n\n\n\nBox plot\nBy this point, you’re hopefully seeing the pattern in how ggplot2 figures are put together. To make a box plot, we need to add a y mapping to the aes() in addition to the x mapping. We’ve also added the same variable to fill as we did to x. We do this so that in addition to having different box and whisker plots along the x-axis, each plot is given its own color.\n\n## box plot using both factor() and as_factor()\np &lt;- ggplot(data = df_hs,\n            mapping = aes(x = factor(x1paredu),\n                          y = x1txmtscor,\n                          fill = as_factor(x1paredu))) +\n    geom_boxplot()\np\n\n\n\n\nIn a way, this plot is similar to the dual histogram above. But since we want to see the distribution of math scores across finer-grained levels of parental education, the box and whisker plot is clearer than trying to overlap seven histograms.\n\nQuick exercise\nChange the as_factor() and factor() functions above. How does the plot change?\n\n\n\nScatter\nTo make a scatter plot, make sure that the aes() has mappings for the x axis and y axis and then use geom_point() to plot. To make things easier to see (remembering the cloud from the base R plot above), we’ll reduce the data to 10% of the full sample using sample_frac() from dplyr. We’ll also limit our 10% to those who aren’t missing information about student education expectations\n\n## sample 10% to make figure clearer\ndf_hs_10 &lt;- df_hs %&gt;%\n    ## drop observations with missing values for x1stuedexpct\n    drop_na(x1stuedexpct) %&gt;%\n    ## sample\n    sample_frac(0.1)\n\n## scatter\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point()\np\n\n\n\n\nNow that we have our scatter plot, let’s say that we want to add a third dimension. Specifically, we want to change the color of each point based on whether a student plans to earn a Bachelor’s degree or higher. That means we need a new dummy variable that is 1 for those with BA/BS plans and 0 for others.\nWe can look at the student base year expectations with count():\n\n## see student base year plans\ndf_hs %&gt;%\n    count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct                                     n\n   &lt;dbl+lbl&gt;                                    &lt;int&gt;\n 1  1 [Less than high school]                      93\n 2  2 [High school diploma or GED]               2619\n 3  3 [Start an Associate's degree]               140\n 4  4 [Complete an Associate's degree]           1195\n 5  5 [Start a Bachelor's degree]                 115\n 6  6 [Complete a Bachelor's degree]             3505\n 7  7 [Start a Master's degree]                   231\n 8  8 [Complete a Master's degree]               4278\n 9  9 [Start Ph.D/M.D/Law/other prof degree]      176\n10 10 [Complete Ph.D/M.D/Law/other prof degree]  4461\n11 11 [Don't know]                               4631\n12 NA                                            2059\n\n\nWe see that x1stuedexpct &gt;= 6 means a student plans to earn a Bachelor’s degree or higher. But since we need to account for the fact that 11 means “I don’t know”, we need to make sure our test includes x1stuedexpct &lt; 11. Remember from a prior lesson that we can connect these two statements together with the operator &. Let’s create our new variable.\n\n## create variable for students who plan to graduate from college\ndf_hs_10 &lt;- df_hs_10 %&gt;%\n    mutate(plan_col_grad = ifelse(x1stuedexpct &gt;= 6 & x1stuedexpct &lt; 11,\n                                  1,        # if T: 1\n                                  0))       # if F: 0\n\nNow that we have our new variable plan_col_grad, we can add it the color aesthetic, aes() in geom_point(). Don’t forget to use factor() so that ggplot knows to treat it like a group!\n\n## scatter\np &lt;- ggplot(data = df_hs_10,\n            mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5)\np\n\n\n\n\n\nQuick exercise\nChange how you make plan_col_grad so that instead of 1 and 0, you use ‘yes’ and ‘no’. Make your figure again. What changes?\n\n\n\nFitted lines\nIt’s often helpful to plot fitted lines against a scatter plot to help see the underlying trend. There are a number of ways to do this with the geom_smooth() function.\n\nLinear fit\nSetting method = lm in geom_smooth() will fit a simple straight line of best fit with 95% confidence interval shaded around it.\n\n## add fitted line with linear fit\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = lm)\np\n\n\n\n\n\n\nLinear fit with polynomials\nIn addition to the method, we can add a formula to allow the fitted line to take a non-linear shape. Using the aes() values of x and y, the argument below uses an R formula, y ~ x, but with the addition of the poly() function. Setting the second argument in poly() to 2 gives the line an extra quadratic term, which allows it to take a more curved shape.\n\n## add fitted line with polynomial linear fit\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = lm, formula = y ~ poly(x,2))\np\n\n\n\n\n\nQuick exercise\nChange the value in poly() to higher numbers. How does the line change?\n\n\n\nLoess\nFinally, we can skip trying to adjust a linear line and just fit a LOESS curve, which is a smooth line produced by fitting a large number of local polynomial regressions on subsets of the data.\n\n## add fitted line with loess\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = loess)\np\n\n\n\n\nTo be clear, these semi-automated lines of best fit should not be used to draw final conclusions about the relationships in your data. You will want to do much more analytic work to make sure any correlations you observe aren’t simply spurious and that fitted lines are telling you something useful. That said, fitted lines via ggplot2 can be useful when first trying to understand your data or to more clearly show observed trends.\n\n\n\nLine graph\nWhen you want to show changes in one variable as a function of another variable, e.g., changes in test scores over time, then a line graph is often a good choice. Since our hsls_small data is cross-sectional, we’ll shift to using our school test score data. Remember that the test score data show three sets of test scores (math, science, and reading) for four schools over a period of six years. This data frame is long in year, but wide in test type. It looks like this:\n\n## show test score data\ndf_ts\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# … with 14 more rows\n\n\nTo keep it simple for our first line plot, we’ll filter our plot data to keep only scores for one school. Notice how we can do that directly with pipes inside the ggplot() function. We want to see changes in test scores over time, so we’ll map year to the x axis and, for now, math to the y axis. To see a line graph, we add geom_line().\n\n## line graph\np &lt;- ggplot(data = df_ts %&gt;% filter(school == \"Spottsville\"),\n            mapping = aes(x = year, y = math)) +\n    geom_line()\np\n\n\n\n\n\nQUICK EXERCISE\nChange the school in filter() to “East Heights” and then “Bend Gate”.\n\nEasy enough, but let’s say that we want to add a third dimension — to show math scores for each school in the same plot area. To do this, we can map a third aesthetic to school. Looking at the help file for geom_line(), we see that lines (a version of a path) can take colour, which means we can change line color based on a variable.\nThe code below is almost exactly the same as before less two things:\n\nWe don’t filter df_ts this time, because we want all schools\nWe add colour = school inside aes()\n\n\n## line graph for math scores at every school over time\np &lt;- ggplot(data = df_ts,\n            mapping = aes(x = year, y = math, colour = school)) +\n    geom_line()\np\n\n\n\n\nThis is nice (though maybe a little messy at the moment) because it allows us to compare math scores across time across schools. But we have two more test types — reading and science — that we would like to include as well. One approach that will let us add yet another dimension is to use facets.\n\n\nFacets\nWith facets, we can put multiple plots together, each showing some subset of the data. For example, instead of plotting changes in math scores across schools over time on the same plot area (only changing the color), we can use facet_wrap() to give each school its own little plot. You might hear me or other refer to plots like this a showing small multiples or as small multiples figures.\nCompared to the code just above, notice how we’ve removed colour = school from aes() and included facet_wrap(~ school). The tilde (~) works like the tilde in plot(y ~ x) above: it means “plot against or by X”. In this case, we are plotting math test scores over time by each school.\n\n## facet line graph\np &lt;- ggplot(data = df_ts,\n            mapping = aes(x = year, y = math)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\nIs this faceted plot better than the color line plot before it? To my eyes, it’s a little clearer, but not so much so that I couldn’t be convinced to use the first one. Whether you use the first or the second would largely depend on your specific data and presentation needs.\nFaceting has a clearer advantage, however, when you want to include the fourth level of comparison: (1) scores across (2) time across (3) schools from (4) different tests. To make this comparison, we first need to reshape our data, which is only long in year, to be long in test, too. As we’ve already seen in a past lesson, we’ll use pivot_longer() to place each test type in its own column (test) with the score next to it.\n\n## reshape data long\ndf_ts_long &lt;- df_ts %&gt;%\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"), # cols to pivot long\n                 names_to = \"test\",                 # where col names go\n                 values_to = \"score\")               # where col values go\n\n## show\ndf_ts_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# … with 62 more rows\n\n\n\nQUICK EXERCISE\nIf we have 4 schools, 6 years, and 3 tests, how many observations should df_ts_long have in total? Does it?\n\nWith our reshaped data frame, we now reintroduce colour into the aes(), this time set to test. We make one other change: y = score now, since that’s the column for test scores in our reshaped data. All else is the same.\n\n## facet line graph, with colour = test and ~school\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\nWell, it worked…we can see each school’s different test score trends over time, with each school in its own facet and test scores set to a different color. But the result is a bit underwhelming. Because the different test types are such different scales (even though they are normed within themselves), within-test changes seem rather flat over time.\nLet’s try something different: in the next figure, we’ll swap the variables we give to colour and within facet_wrap(). This means that each test should have its own facet and each line will represent a different school.\n\n## facet line graph, now with colour = school and ~test\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = school)) +\n    facet_wrap(~ test) +\n    geom_line()\np\n\n\n\n\nOkay. New problem. While it’s maybe a little easier to see same-test differences across schools over time, the different scales of the tests still make the figure less useful than we might hope. It’s not that the students are way better at science than reading; it’s just that the tests are scaled differently. Someone quickly reading this figure, however, might make that incorrect interpretation.\nOne thing we can do is change the y-axis for each facet. The default is to keep the y-axis scale the same. By adding scales = \"free_y\" to facet_wrap(), we’ll let each test have its own y-axis scale.\nHaving different axis scales side-by-side can be confusing, however (this is why the default is to keep them the same). To mitigate that confusion, we’ll also rearrange the facets so they stack rather than sit side by side. To do this, we’ll add ncol = 1 to facet_wrap(). This says our facets have to stick to one column, effectively meaning they will stack vertically.\n\n## facet line graph, with one column so they stack\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = school)) +\n    facet_wrap(~ test, ncol = 1, scales = \"free_y\") +\n    geom_line()\np\n\n\n\n\nThat looks better! But we can do even better than that…\nCurrently, each test score is on its own normed scale. While our new figure allows us to make comparisons across schools over time within test, it’s more difficult to make a good comparison between tests. For example, East Heights has a little over 20 point drop in reading scores from 1981 to 1982 and about the same drop in science scores from 1982 to 1983. How should we think about these drops? Are they about the same or is one drop relatively bigger than the other?\nTo better answer this question, we could re-standardize each test score so that it is centered at 0 and a one unit change is equal to 1 standard deviation difference in score. We’ll use mutate() to create a new variable score_std. Because we group_by() test, score_std will be standardized within test.\n\n## rescale test scores\ndf_ts_long &lt;- df_ts_long %&gt;%\n    group_by(test) %&gt;%\n    mutate(score_std = (score - mean(score)) / sd(score)) %&gt;%\n    ungroup\n\nWe’ll repeat the same code as before, but this time substitute y = score_std. Because all tests are on the same standardized scale, we can also drop scales = \"free_y\".\n\n## facet line graph with standardized test scores\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std, colour = school)) +\n    facet_wrap(~ test, ncol = 1) +\n    geom_line()\np\n\n\n\n\nNotice the lines look the same relative to one another, but now we have a consistent scale to help judge changes. To answer our question from before, it seems that the drop in reading scores (1981 to 1982) and science scores (1982 to 1983) were each about 1.5 standard deviations. We could test more formally, but we have a clearer idea now that all tests are on the same scale.\n\nQUICK EXERCISE\nWhat happens if you use the argument scales = \"free_y\" in the last bit of code? Why might you not use that once we’ve scaled the test scores?\n\nAs a quick change, we can go back to having each school in its own facet and test scores within.\n\n## facet line graph\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\n\nQUICK EXERCISE\nWhy did we drop ncol = 1 from facet_wrap()? What happens if you keep it?\n\nOur plot is looking better, but it still may not contain the information we want. We’ve standardized the test scores over this time window, but maybe what we really want to know is how they’ve changed relative to the beginning of the sample period. You can imagine a superintendent who took over in 1981 would be keen to know how scores have changed during their tenure.\nThis means that while we still want to standardize the scores, we should zero them not at the overall mean, but at the value in the first year. We can do that by grouping by school and test, arranging in year order, making a new variable that is the first() score (within test, within school) and using that rather than the mean test score to make our new variable, score_std_sch.\n\n## standardize test scores within school to first year\ndf_ts_long &lt;- df_ts_long %&gt;%\n    group_by(test, school) %&gt;%\n    arrange(year) %&gt;% \n    mutate(score_year_one = first(score),\n           ## note that we're using score_year_one instead of mean(score)\n           score_std_sch = (score - score_year_one) / sd(score)) %&gt;%\n    ungroup\n\nNow we’ll plot using our new variable score_std_sch.\n\n## facet line graph\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std_sch, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\nWith this final graph, we can see relative changes across schools, across times, across tests. Notice that line shapes within each facet are the same as before, just shifted up or down so that the first value for each test in 1981 is 0.\nIs this the best version of this figure (minus making the axis and legend labels look nicer)? Again, it depends on what you want to show. Remember that figures don’t speak for themselves: it’s up to you to explain to your reader (include your future self) what they mean. That said, a well crafted figure will make that job much easier."
  },
  {
    "objectID": "05-Data-Viz-I.html#questions",
    "href": "05-Data-Viz-I.html#questions",
    "title": "I: Basics",
    "section": "Questions",
    "text": "Questions\n\nWhat is the distribution of household size among students in the sample?\nHow does student socioeconomic status differ between students who ever attended college and those who did not?\nHow do parental educational expectations differ across region?\nHow does the relationship between socioeconomic status and math test score differ across region (use a smoothing line to help show any relationship)?\nAmong students who ever attended college, how does socioeconomic status differ between those who delayed postsecondary enrollment and those who did not delay, when delay is defined as:\n\nmore than 6 months between high school graduation and postsecondary enrollment?\nmore than 12 months?\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_5.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Contemporary research in higher education (and other disciplines) should be both rigorous and reproducible. This is class will teach you the fundamentals of data management and quantitative research workflow with emphasis on rigor and reproducibility.\nOften referred to informally as “the R class”, you will get an introduction to coding using the R programming language, but many of the skills are directly transferable to future work in Python, Stata, or other software."
  },
  {
    "objectID": "index.html#class-overview",
    "href": "index.html#class-overview",
    "title": "Homepage",
    "section": "",
    "text": "Contemporary research in higher education (and other disciplines) should be both rigorous and reproducible. This is class will teach you the fundamentals of data management and quantitative research workflow with emphasis on rigor and reproducibility.\nOften referred to informally as “the R class”, you will get an introduction to coding using the R programming language, but many of the skills are directly transferable to future work in Python, Stata, or other software."
  },
  {
    "objectID": "index.html#credit-to-dr.-skinner",
    "href": "index.html#credit-to-dr.-skinner",
    "title": "Homepage",
    "section": "Credit to Dr. Skinner",
    "text": "Credit to Dr. Skinner\nFirst and foremost, credit for the structure and vast majority of the content on this site goes to Dr. Benjamin T. Skinner who designed this course before leaving UF to take a data scientist position at the National Endowment for the Humanities. I took this class with Dr. Skinner in Spring 2022, then was a Teaching Assistant for this class with him during Spring 2023. In building this class website, I’ve tried to keep the majority of the content consistent whilst making the class work for the format Dr. Tanner and I are going to teach it in.\nIf you’re interested in seeing Dr. Skinner’s versions of the class, here are links to his course site\n\nEDH 7916 Spring 2023\nEDH 7916 Spring 2022"
  },
  {
    "objectID": "index.html#the-im-stuck-workflow",
    "href": "index.html#the-im-stuck-workflow",
    "title": "Homepage",
    "section": "The “I’m Stuck” Workflow",
    "text": "The “I’m Stuck” Workflow\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nAI\n\nPlease acknowledge with a ## h/t &lt;the exact tool & question&gt;\nAcceptable: “what’s wrong with this &lt;code&gt;”\nNot Acceptable: “write me code takes &lt;data&gt; and creates a plot to display &lt;trend&gt;”\nThere’s inevitably a middle ground\n\nYou should be able to explain how any code you submit works\n\n\nMatt’s office hours or email\n\nTrying the above steps first really helps me help you\n\nI’d probably start by going through them\n\nI rarely will give direct answers, I just help you think through the issue"
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Homepage",
    "section": "Useful Links",
    "text": "Useful Links\n Email Matt\n See Examples of Matt’s Code on Github\n Stack Overflow R Questions"
  },
  {
    "objectID": "index.html#good-luck-in-the-class",
    "href": "index.html#good-luck-in-the-class",
    "title": "Homepage",
    "section": "Good luck in the class!",
    "text": "Good luck in the class!\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0."
  },
  {
    "objectID": "10-Data-Viz-III.html#reading-in-data",
    "href": "10-Data-Viz-III.html#reading-in-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in Data",
    "text": "Reading in Data\nOne of the great advantages of using APIs for mapping is that we download spatial data directly into R instead of having to download and handle these quite large files through our computer. If we weren’t using an API or wanted to plot some spatial data not available through one, we would need to find and download a shapefile folder containing a selection of files, then read in the one ending in *.shp — something like this below:\n\n## ---------------------------\n## example of shapefile read\n## ---------------------------\n\n## pseudo code (won't run!)\ndf &lt;- read_sf(file.path(\"&lt;Data-Folder&gt;\", \"&lt;Folder-You-Downloaded&gt;\", \"&lt;Shapefile-Name&gt;.shp\"))\n\nThese shapefiles can sometimes be hard to find, take up a lot of space on our computer (especially if they are overly detailed for our needs), and make it much harder to share our project with others for reproducibility. That is why we are going to use an API."
  },
  {
    "objectID": "10-Data-Viz-III.html#setting-up-apis-and-tidycensus",
    "href": "10-Data-Viz-III.html#setting-up-apis-and-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Setting up APIs and tidycensus",
    "text": "Setting up APIs and tidycensus\nSo what exactly is an API? In short, think of it as a way of R going to a website/database and pulling data directly from the server-side or backend, without our having to ever interact with the website directly. (Note from BS: we avoid point-click at all costs!) We are going to use the API tidycensus today, but all APIs operate on the same basic idea.\nTidycensus is, in my opinion, one of the easiest APIs to get set up and use in R. Most APIs require that you use some kind of key that identifies you as an authorized user. Typically you need to set up the key the first time you use the API, but helpfully, it’s usually possible to store the key on your computer for all future use (think of the way we initially set up GitHub and then it worked without needing to go through that process again — the good news is that API keys are way easier to set up). Most keys are free to obtain and use. If you were using an API to access a private database such as Google Maps, you might need to pay for your key to have access or on a sliding scale depending on how much you use it. But because we are using Census data, which is freely available to the public, there’s no charge.\nHopefully, most of you were able to get your Census API key before class, but if anyone needs a reminder,\n\nsimply go here\nenter your organization name (University of Florida)\nenter your email.\n\nYou will quickly receive an email with your API key, which you will need below.\nTo set up tidycensus for the first time, we first need to set our API key. The tidycensus library makes this much easier than many APIs by having a built-in function that you can use to save your API key to your computer. Simply place your API key in the &lt;&gt; of the code below. The install option means it will save the API key for future use, so you will not need to worry about this step again.\n\n## ---------------------------\n## set API key\n## ---------------------------\n\n## you only need to do this once: replace everything between the\n## quotes with the key in the email you received\n##\n## eg. census_api_key(\"XXXXXXXXXXXXXXXXXX\", install = T)\ncensus_api_key(\"&lt;Your API Key Here&gt;\", install = T)\n\nNow that this is set up, we are ready to start using tidycensus — yay!"
  },
  {
    "objectID": "10-Data-Viz-III.html#reading-in-data-with-tidycensus",
    "href": "10-Data-Viz-III.html#reading-in-data-with-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in data with tidycensus",
    "text": "Reading in data with tidycensus\nThere are five main tidycensus functions that you can use to call in data, with each calling data from a different source operated by the US Census Bureau. For today’s lesson we are going to use get_acs(), which collects data from the American Community Survey (regular sampled surveys of demographic data across the US). There are a selection of other functions to collect data from different sources within the Census; the most useful ones for us start with get_. You can see more info here.\nWe are going to assign &lt;- the data we pull down into the object df_census:\n\n## ---------------------------\n## first data pull\n## ---------------------------\n\ndf_census &lt;- get_acs(geography = \"county\",\n                     state = \"FL\",\n                     year = 2021,\n                     variables = \"DP02_0065PE\", # Pop &gt;=25 with Bachelors\n                     output = \"wide\",\n                     geometry = TRUE)\n\nGetting data from the 2017-2021 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nUsing the ACS Data Profile\n\n\nLet’s walk through each element of this command in turn:\n\ngeography = \"county\" is telling the function to get estimates (and spatial data later) at the county level; this could also be \"state\", for example, to get state level data.\n\nstate = \"FL\" is telling the function to get data only for the state of Florida. You could put a group of states with c(), use full state names, or use FIPS codes — tidycensus is flexible. If you want a narrower set of data, you could also add county =, which works in a similar way. For example, if you added county = \"Alachua\", you would only get county-level data for Alachua County, Florida.\nyear = 2021 is telling the function to pull data for the survey year 2021. For ACS, this will be the survey set ending in that year. Keep in mind that some data are not available for every year. For example, data from the full decennial census are only available for 2010 or 2020.\nvariables = \"DP02_0065PE\" is telling the function to pull the variable coded \"DP02_0065PE\", which is the percentage of the population older than 25 with a Bachelor’s degree. This is the only tricky part of using tidycensus: understanding census API’s variable names. Let me breakdown what we are calling here:\n\nDP02_0065\n\nThis is the main variable code the census uses. You can call this by using the load_variables() command, but doing so creates a massive table in R that is hard to navigate through. An easier way is to go the census API’s list of variables for the dataset you are using, which for the 2021 ACS is here (change the years/data sources as needed for other surveys).\nIn here you can crtl-f or cmd-f search for the variable you are looking for. For this variable we could search “bachelor,” which will highlight all the variables that have “bachelor” in the title. Find the variable you want and copy the name.\n\nPE\n\nYou will notice there are multiple DP02_0065 variables, these are the same underlying variable, but in different forms. The common endings are E or PE, which stand for Estimate and Percentage Estimate. For our purposes, we are most often going to want the percentage estimate (PE), so we will select DP02_0065PE, the percent estimate of Bachelor’s degree attainment for those 25 years old and above, and DP02_0065PM which is the margin of error for the percentage (hence the M at the end). If you want the total count instead, select E.\n\n\noutput = \"wide\" is telling it we want the data in a wide format. Think back to Data Wrangling II: wide data means having a separate column for each variable whereas long data would be in two columns, one with the variable name and one with the variable value. For ease of plotting/mapping, we are going to want it in wide format.\ngeometry = T is telling the function we want to download geometry (a kind of spatial data) to go with our census data. This saves us having to deal with finding, loading, and joining a shapefile to make our map. We will discuss this more shortly.\n\nOkay, let see what the top of our new data looks like.\n\n## show header of census data\nhead(df_census)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -82.57599 ymin: 27.64324 xmax: -80.73292 ymax: 30.14312\nGeodetic CRS:  NAD83\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4\n                        geometry\n1 MULTIPOLYGON (((-81.65856 2...\n2 MULTIPOLYGON (((-82.57599 2...\n3 MULTIPOLYGON (((-81.95616 2...\n4 MULTIPOLYGON (((-81.6809 29...\n5 MULTIPOLYGON (((-82.10621 2...\n6 MULTIPOLYGON (((-82.31133 2...\n\n\nIt looks a bit different than a normal data frame. For now, let’s not worry too much about the first few lines which give a summary of the spatial aspects of the our downloaded data. If you look underneath those lines, from GEOID to DP02_0065PM, you’ll see something that looks more like the tibbles we are familiar with. Then, in the last column, we get to our spatial data in the geometry column. If you open df_census in the viewer, it looks like a normal data frame ending with this slightly different column called geometry.\nNote: I wouldn’t recommend often looking through the file in viewer as the the spatial data can make it slow/laggy. If you need to dig into the data that way, use st_drop_geometry() and assign it to a new object.\n\n## view data frame without geometry data (not assigning, just viewing)\ndf_census %&gt;%\n  st_drop_geometry()\n\n   GEOID                         NAME DP02_0065PE DP02_0065PM\n1  12095       Orange County, Florida        23.0         0.6\n2  12125        Union County, Florida         7.6         2.1\n3  12069         Lake County, Florida        16.0         0.9\n4  12127      Volusia County, Florida        16.8         0.5\n5  12105         Polk County, Florida        14.0         0.5\n6  12119       Sumter County, Florida        19.4         1.4\n7  12073         Leon County, Florida        26.7         0.9\n8  12047     Hamilton County, Florida         6.9         1.6\n9  12093   Okeechobee County, Florida        11.3         1.4\n10 12071          Lee County, Florida        17.8         0.5\n11 12001      Alachua County, Florida        23.2         1.0\n12 12077      Liberty County, Florida         8.3         2.6\n13 12097      Osceola County, Florida        16.5         0.9\n14 12123       Taylor County, Florida         8.3         2.1\n15 12013      Calhoun County, Florida         7.5         2.0\n16 12037     Franklin County, Florida        12.2         2.5\n17 12029        Dixie County, Florida         5.9         1.9\n18 12133   Washington County, Florida         7.9         1.4\n19 12129      Wakulla County, Florida        12.2         2.2\n20 12131       Walton County, Florida        21.1         1.6\n21 12007     Bradford County, Florida         7.3         1.6\n22 12031        Duval County, Florida        21.0         0.6\n23 12033     Escambia County, Florida        17.4         0.8\n24 12089       Nassau County, Florida        20.5         1.4\n25 12009      Brevard County, Florida        19.5         0.6\n26 12086   Miami-Dade County, Florida        19.8         0.3\n27 12053     Hernando County, Florida        12.9         0.6\n28 12107       Putnam County, Florida         8.6         1.2\n29 12023     Columbia County, Florida        10.6         1.1\n30 12049       Hardee County, Florida         7.2         1.8\n31 12017       Citrus County, Florida        11.9         0.9\n32 12117     Seminole County, Florida        27.7         0.8\n33 12039      Gadsden County, Florida        12.8         1.4\n34 12045         Gulf County, Florida        14.6         2.8\n35 12121     Suwannee County, Florida         9.3         1.8\n36 12065    Jefferson County, Florida        14.5         2.2\n37 12075         Levy County, Florida        10.5         1.6\n38 12057 Hillsborough County, Florida        22.5         0.4\n39 12103     Pinellas County, Florida        22.0         0.5\n40 12083       Marion County, Florida        13.8         0.8\n41 12055    Highlands County, Florida        12.2         1.0\n42 12027       DeSoto County, Florida         8.8         1.3\n43 12113   Santa Rosa County, Florida        18.8         1.1\n44 12079      Madison County, Florida         8.8         1.8\n45 12041    Gilchrist County, Florida         9.4         1.7\n46 12087       Monroe County, Florida        21.7         1.3\n47 12111    St. Lucie County, Florida        15.9         0.8\n48 12109    St. Johns County, Florida        28.1         1.1\n49 12003        Baker County, Florida         9.2         2.2\n50 12035      Flagler County, Florida        17.8         1.1\n51 12051       Hendry County, Florida         5.8         1.4\n52 12091     Okaloosa County, Florida        20.7         1.0\n53 12005          Bay County, Florida        16.4         1.0\n54 12099   Palm Beach County, Florida        23.0         0.4\n55 12011      Broward County, Florida        21.2         0.4\n56 12021      Collier County, Florida        22.4         0.8\n57 12081      Manatee County, Florida        19.6         0.6\n58 12085       Martin County, Florida        21.9         1.1\n59 12043       Glades County, Florida         9.5         2.5\n60 12067    Lafayette County, Florida         5.8         2.9\n61 12063      Jackson County, Florida         7.8         1.1\n62 12015    Charlotte County, Florida        15.5         0.9\n63 12059       Holmes County, Florida         6.6         1.7\n64 12101        Pasco County, Florida        16.9         0.5\n65 12019         Clay County, Florida        18.2         1.1\n66 12115     Sarasota County, Florida        21.4         0.6\n67 12061 Indian River County, Florida        19.4         1.3"
  },
  {
    "objectID": "10-Data-Viz-III.html#a-very-brief-overview-of-spatial-data",
    "href": "10-Data-Viz-III.html#a-very-brief-overview-of-spatial-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "A (Very) Brief Overview of Spatial Data",
    "text": "A (Very) Brief Overview of Spatial Data\nWe do not have time to really get into all the complexities of spatial data in this class, so, unfortunately, it will have to remain a bit of black box for now. But below is a quick overview of how R handles it.\nAs we saw above, there is a column on the end of our data called geometry. This is not technically a column like we are used to; for example, you can’t select(geometry) or filter(geometry == x) like we do with other variables in our data frames. Instead, think of it as a special attachment R places on each observation.\n\nVectors vs Rasters\nWhen looking online for spatial data, you might see how spatial data can be either in vector or raster format. For our purposes, everything is going to be vector, which is kind of like a vector in R: collection of data points that represent something spatial. Raster data, on the other hand, is a grid with information assigned to each square, commonly used for satellite imagery analysis.\n\nVector: here’s instructions (e.g., a formula) to draw a line; great for animations and things like maps and scales well\nRaster: here’s a big paint-by-numbers grid and the line you see is where some squares are filled in; great for photographic images, but doesn’t scale well\n\nVector data are usually either as points (think dots on a map), lines (think a line connecting two points on a map), or polygons (think a collection of lines on a map that create a closed shape). In this lesson we are going to use both point and polygon data (you won’t use line data as much). If this sounds complicated, fear not! It is much simpler than it sounds right now!\nFor those interested, this is a nice intro to the types of spatial data.\n\n\nCoordinate Reference Systems (CRS)\nFor purposes of this lesson, the only internal workings of spatial data we need to be aware of is something called the Coordinate Reference System or CRS. Our earth is not flat, but rather is a curved three-dimensional object (NB from BS: this is most likely true). Since we don’t want to carry around globes, we take this 3D object and squish it into two dimensions on a map. This process necessarily involves some kind of transformation, compromise, or projection.\nIn a nutshell, this is a very simplified explanation of what a CRS decides: it’s how we are deciding to twist, pull, squish a 3D Earth surface into a flat surface. Turns out this matters a lot. Do you want your results to have the correct areas? Or maybe correct distances? Or maybe straight lines of bearing (particularly important if you are sailing and don’t want those trips to take any longer than necessary).\nHere’s a (somewhat old) pop culture look at this issue from one of my favorite shows…\n\nThis is a relatively complicated process we are not going to go into here. If you’re interested here’s a nice introduction to CRS by QGIS.\nFor our class we are going to use the CRS ESPG 4326, which is in essence a projection that makes east/west run straight left/right and north/south run straight up/down. All different CRS have their advantages and disadvantages. This is nice and simple for quick descriptive maps, but distorts shapes in ways that might be harmful, particularly if you are going to do any distance calculations, etc.\nNB from BS: If you are going to do spatial work in education research (other than just making maps for display), you really need to know what your projection is doing. Even if you are just making maps for display, some projections are, IMNSHO, more aesthetically pleasing that others in different situations. I personally will tell you if your map offends the dictates of good taste.\nKeep an eye out for crs = 4326 as we go through some examples plotting spatial data below.\nIn short, what you need to know about spatial data for this lesson is this:\n\nR stores spatial data in something called geometry attached to each observation/row\nTo handle spatial data, you can’t just filter() it like normal; instead you have to use functions from a spatial data package such as sf or tigris\nThe CRS (coordinate reference system) is how we choose to account for the earth being curved; what is most important for mapping is that everything we use on the plot is using the same CRS. Using crs   = 4326 will give a nice simple flat projection. This projection has drawbacks, but is easy to work with and so is what we will use for now."
  },
  {
    "objectID": "10-Data-Viz-III.html#lets-make-a-map-finally",
    "href": "10-Data-Viz-III.html#lets-make-a-map-finally",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Let’s Make a Map (finally)!",
    "text": "Let’s Make a Map (finally)!\nIf we have made it this far, things are about to get much more interesting and hands-on!\nWe are going to make an education-focused map based on template I used for a real consulting project last summer as part of my GA-ship. This template is really adaptable for a lot the kind of maps we might want educational research and reports. So let’s get started.\nWe are going to have two layers, a base map with the census data we already downloaded, and a layer of points on top representing colleges.\n\nLayer One: Base Map\nBefore we plot anything, particularly since we are going to have multiple layers, we want to check our CRS\n\n## ---------------------------------------------------------\n## making a map\n## ---------------------------------------------------------\n## ---------------------------\n## layer one: base map\n## ---------------------------\n\n## show CRS for dataframe\nst_crs(df_census)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nThat isn’t our simple flat ESPG 4326, so we are going to st_transform() to set that.\n\n## transform the CRS to 4326\ndf_census &lt;- df_census %&gt;%\n  st_transform(crs = 4326)\n\nThen we can check again…\n\n## show CRS again; notice how it changed from NAD93 to ESPG:4326\nst_crs(df_census) \n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nLooks good!\nOkay, with our CRS now set, let’s plot our base map. We actually use the familiar ggplot() to make our maps because there is a special geom_* that works with spatial data: geom_sf(). Everything works in a similar way to our normal plots, so this should be familiar. Luckily all the tricky spatial aspects are handled by ggplot for us.\nThe below code will make our base map, and store in an object called base_map.\n\n## create base map\nbase_map &lt;- ggplot() +\n  geom_sf(data = df_census,\n          aes(fill = DP02_0065PE),\n          color = \"black\",\n          size = 0.1) +\n  labs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) +\n  scale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\")\n\nLet’s go through each line of the geom_sf() as we did for get_acs() above:\n\ndata = df_census: all we need to do take make our spatial plot is call a data frame with a geometry attachment. geom_sf() will handle how to plot that for us.\naes(fill = DP02_0065PE): much like we would with a box plot, we are simply telling ggplot to fill the shapes (in our case, Florida’s counties) based on that variable. So here we are filling Florida’s counties based on the percent of the population over 25 with a Bachelor’s degree (the variable we chose from tidycensus)\ncolor = \"black\": remember since this is outside the aes() argument it will applied consistenly across the plot. We are telling it to make all the lines black.\nsize = 0.1: similarly, we are telling to make the lines 0.1 thickness (thinner than the default)\n\nThen we have added two visual alterations like we covered in the second plotting lesson. For a quick reminder:\n\nlabs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) is saying to give the legend for fill this title; a new function, str_wrap() says to make a newline (wrap) when there are more than 20 characters\nscale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\") is telling fill with a color gradient starting at with light slate blue and finishing with a dark slate blue; instead of colour names, we’re using hex color codes\n\nNow, let’s call our base_map object to see what this looks like\n\n## call base map by itself\nbase_map\n\n\n\n\nWe have made a map! But we are going to add one more layer.\n\n\nLayer Two: Institution Points\nA lot of education data comes with a latitude and longitude for the institution. Today we are going to use IPEDS, but you can certainly get these for K-12 schools and a whole lot more besides.\nWe are now going to read in some library data from IPEDS that I cleaned and merged earlier.\n\n## ---------------------------\n## layer two: institutions\n## ---------------------------\n\n## read in IPEDS data\ndf_ipeds &lt;- read_csv(file.path(\"data\", \"mapping_api_data.csv\"))\n\nRows: 3764 Columns: 78\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (35): INSTNM, STABBR, COUNTYNM, XLPBOOKS, XLEBOOKS, XLEDATAB, XLPMEDIA, ...\ndbl (43): UNITID, CONTROL, ICLEVEL, FIPS, COUNTYCD, LATITUDE, LONGITUD, LEXP...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our data\n\n## show IPEDS data\nhead(df_ipeds)\n\n# A tibble: 6 × 78\n  UNITID INSTNM     CONTROL ICLEVEL STABBR  FIPS COUNT…¹ COUNT…² LATIT…³ LONGI…⁴\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 100654 Alabama A…       1       1 AL         1 Madiso…    1089    34.8   -86.6\n2 100663 Universit…       1       1 AL         1 Jeffer…    1073    33.5   -86.8\n3 100690 Amridge U…       2       1 AL         1 Montgo…    1101    32.4   -86.2\n4 100706 Universit…       1       1 AL         1 Madiso…    1089    34.7   -86.6\n5 100724 Alabama S…       1       1 AL         1 Montgo…    1101    32.4   -86.3\n6 100751 The Unive…       1       1 AL         1 Tuscal…    1125    33.2   -87.5\n# … with 68 more variables: LEXP100K &lt;dbl&gt;, LCOLELYN &lt;dbl&gt;, XLPBOOKS &lt;chr&gt;,\n#   LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;, LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;,\n#   LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;, LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;,\n#   LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;, LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;,\n#   LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;, LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;,\n#   LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;, LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;,\n#   LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;, LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, …\n\n\nWe see a normal data frame for colleges with bunch of variables (use the IPEDS codebook to unpack the variable names), including latitude and longitude. Latitude and longitude represent something spatial, but they’re not quite spatial data like R knows. Let’s change that!\n\n## convert coordinates columns into a true geometry column; this is\n## much more reliable than simply plotting them as geom_points as it\n## ensures the CRS matches etc.\ndf_ipeds &lt;- df_ipeds %&gt;% \n  st_as_sf(coords = c(\"LONGITUD\", \"LATITUDE\"))\n\nAbove we call st_as_sf(), then tell it the coordinates, coords =, are in columns name LONGITUD and LATITUDE. If we aren’t using argument names (we aren’t) just remember that since longitude tells you were you are east/west on the globe, it translates to the x axis. Because latitude gives you north/south direction, it translates to the y axis.\nIf we look at our data again, we are going to see that spatial summary again as R has attached some point geometry to our college data based on the coordinates.\n\n## show IPEDS data again\nhead(df_ipeds)\n\nSimple feature collection with 6 features and 76 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -87.54598 ymin: 32.36261 xmax: -86.17401 ymax: 34.78337\nCRS:           NA\n# A tibble: 6 × 77\n  UNITID INSTNM     CONTROL ICLEVEL STABBR  FIPS COUNT…¹ COUNT…² LEXP1…³ LCOLE…⁴\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 100654 Alabama A…       1       1 AL         1 Madiso…    1089       1       2\n2 100663 Universit…       1       1 AL         1 Jeffer…    1073       1       2\n3 100690 Amridge U…       2       1 AL         1 Montgo…    1101       1       2\n4 100706 Universit…       1       1 AL         1 Madiso…    1089       1       2\n5 100724 Alabama S…       1       1 AL         1 Montgo…    1101       1       2\n6 100751 The Unive…       1       1 AL         1 Tuscal…    1125       1       2\n# … with 67 more variables: XLPBOOKS &lt;chr&gt;, LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;,\n#   LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;, LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;,\n#   LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;, LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;,\n#   LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;, LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;,\n#   LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;, LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;,\n#   LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;, LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;,\n#   LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, LTCRCLT &lt;dbl&gt;, LILLDYN &lt;dbl&gt;, …\n\n\nBut then look at our CRS: it’s NA! This means R will not be able to turn that data into a map. Basically, R knows we have spatial data, but it doesn’t know how we want to put onto a 2D surface (how to project it). To be sure, let’s check the CRS directly:\n\n## check CRS for IPEDS data\nst_crs(df_ipeds)\n\nCoordinate Reference System: NA\n\n\nStill NA…\nLuckily the fix for this is similar to how we change the CRS for our earlier map.\n\n## add CRS to our IPEDS data\ndf_ipeds &lt;- df_ipeds %&gt;% \n  st_set_crs(4326) # When you first add coordinates to geometry, it doesn't know\n                   # what CRS to use, so we set to 4326 to match our base map data\n\nOkay, let’s have another look…\n\n## check CRS of IPEDS data again\nst_crs(df_ipeds)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAnd we see we have our nice CRS back!\nOkay, now the hard work is done, we just need to call our base_map, add a layer representing the colleges as points, and store it into a new object point_map:\n\npoint_map &lt;- base_map +\n  geom_sf(data = df_ipeds %&gt;% filter(FIPS == 12), # Only want to plot colleges in FL\n          aes(size = LPBOOKS),\n          alpha = 0.8,\n          shape = 23, # Get the diamond shape which stands out nicely on the map\n          fill = \"white\", # This shape has a fill and color for the outline\n          color = \"black\") + # FYI 21 is a circle with both fill and color\n  labs(size = \"Number of Books in Library\")\n\nAs we have done all lesson, we can take a quick look through our second geom_sf() function line by line:\n\ndata = df_ipeds %&gt;% filter(FIPS == 12): for this layer we are using our df_ipeds data, which covers the country, but since our base map is Florida, we only want colleges located in the Sunshine State (which is FIPS code 12).\naes(size = LPBOOKS) is saying we want to change the size of point based on LPBOOKS, which is the total number of books in the college’s library collection. More books, bigger dot!\nalpha = 0.5 is outside the aes() so we are making it all 50% transparent.\nThen we added labs(size = \"Number of Books in Library\") to change the legend title to “Number of Books in Library”\n\nPhew! Last thing, let’s call our new point_map object and take a look at what we created!\n\n## show new map\npoint_map\n\nWarning: Removed 14 rows containing missing values (`geom_sf()`)\n\n\n\n\n\nThere we go! We now have a map that shows us county bachelor’s degree attainment and the number of books in a college’s library. If you notice, UF has most books out of all Florida colleges, Go Gators!\nObviously, this may not be the most useful map in the world, but the template is very adaptable. Using tidycensus we can swap out the base map geography to have most common US geographies and/or swap out any variable available in from the Census Bureau. Equally, we can swap out the point data to represent anything we have coordinate points for and change the aesthetics to represent any data we have for those points."
  },
  {
    "objectID": "10-Data-Viz-III.html#supplemental-material-us-transformations-tigris-basics",
    "href": "10-Data-Viz-III.html#supplemental-material-us-transformations-tigris-basics",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Supplemental Material: US transformations & tigris basics",
    "text": "Supplemental Material: US transformations & tigris basics\nFor the sake of time, I left this until the end as we don’t need it for the assignment. But it may be useful if you are looking to make any maps in your final assignment or the future.\ntigris is a package that offers a direct way of downloading US spatial data that is not tied to census data. (Note: it’s actually used by tidycensus behind the scenes to get your spatial data.) If you get spatial data from tigris it won’t come with any additional data to plot per say, but it comes with identifying variables you could use to pair up with external data using something like left_join().\nSomething we as educational researchers might be interested in plotting is school districts. While we could get these from tidycensus with geography = \"school district (unified)\", it may be the case that we have school district data rather than census data we want to plot. In that case, it might be easier to use tigris directly to get the blank shapefiles. The function names for tigris are really simple. school_districts() for example retrieves a shapefile for US school districts whereas states() retrieves boundries for the all US states and territories.\nLet’s take a quick look at the 50 states.\n\n## ---------------------------------------------------------\n## supplemental using tigris directly\n## ---------------------------------------------------------\n## ---------------------------\n## get states geometries\n## ---------------------------\ndf_st &lt;- states() %&gt;%\n  filter(STATEFP &lt;= 56) # keeping only the 50 states plus D.C.\n\nRetrieving data for the year 2020\n\n\nLike we did before, let’s take a peak at our newly downloaded data.\n\n## look at head of state data\nhead(df_st)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -97.23909 ymin: 24.39631 xmax: -71.08857 ymax: 49.38448\nGeodetic CRS:  NAD83\n  REGION DIVISION STATEFP  STATENS GEOID STUSPS          NAME LSAD MTFCC\n1      3        5      54 01779805    54     WV West Virginia   00 G4000\n2      3        5      12 00294478    12     FL       Florida   00 G4000\n3      2        3      17 01779784    17     IL      Illinois   00 G4000\n4      2        4      27 00662849    27     MN     Minnesota   00 G4000\n5      3        5      24 01714934    24     MD      Maryland   00 G4000\n6      1        1      44 01219835    44     RI  Rhode Island   00 G4000\n  FUNCSTAT        ALAND      AWATER    INTPTLAT     INTPTLON\n1        A  62266296765   489206049 +38.6472854 -080.6183274\n2        A 138958484319 45975808217 +28.3989775 -082.5143005\n3        A 143778461053  6216594318 +40.1028754 -089.1526108\n4        A 206232157570 18949864226 +46.3159573 -094.1996043\n5        A  25151895765  6979171386 +38.9466584 -076.6744939\n6        A   2677759219  1323691129 +41.5964850 -071.5264901\n                        geometry\n1 MULTIPOLYGON (((-81.74725 3...\n2 MULTIPOLYGON (((-86.39964 3...\n3 MULTIPOLYGON (((-91.18529 4...\n4 MULTIPOLYGON (((-96.78438 4...\n5 MULTIPOLYGON (((-77.45881 3...\n6 MULTIPOLYGON (((-71.7897 41...\n\n\nSimilar to before, we have\n\na spatial summary at the top\na set of normal looking columns with different ID codes and names\nan attached geometry for each row\n\nIf we simply plot this with no aesthetics, we get the outline of all states, but there is something about it that makes it less ideal for a quick data visualization:\n\n## quick plot of states\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\nAs we can see, while the map is geographically accurate, there is a lot of open ocean on the map due to the geographic structure of the US. Often when we see maps of the US, Alaska and Hawaii are moved to make it easier to read. Tigris offers an easy way of doing this:\n\n## shift position of Hawaii and Alaska\ndf_st &lt;- df_st %&gt;%\n  shift_geometry(position = \"below\")\n\nThe shift_geometry() should work on any spatial data with Alaska and Hawaii, not only data from tigris. Now let’s run that same plot again.\n\n## replotting with shifted Hawaii and Alaska\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\nAlthough not always a good idea (never do spatial analysis on data you’ve done this to, it will be severely off), if you’re looking to plot the 50 states in an easy-to-read manner, this can be a really useful tool.\nLastly, to re-illustrate what a CRS does, let’s plot this two more times, putting it onto our simple ESPG 4326 CRS, and then using the Peters Projection referenced in the video clip at the start of class.\n\n## change CRS to what we used for earlier map\ndf_st &lt;- df_st %&gt;%\n  st_transform(crs = 4326)\n\n## make make\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1)\n\n\n\n\nSee how the line are now a perfect grid, but the shapes of states (look at Montana) are a little different? That’s the power of a CRS!\nFinally, let’s please the Organization of Cartographers for Social Equality and look at the Peters projection. Note: while this projection is great for showing comparably accurate area across the globe, it does that by other trade offs not acknowledged by Dr. Fallow from CSE, so it’s not universally better, it’s better for the task it was designed for. That’s the key with CRS, find the best one for the task you’re doing.\n\n## change CRS to requirements for Peters projection\n## h/t https://gis.stackexchange.com/questions/194295/getting-borders-as-svg-using-peters-projection\npp_crs &lt;- \"+proj=cea +lon_0=0 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\ndf_st &lt;- df_st %&gt;%\n  st_transform(crs = pp_crs)\n\n## make mape\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1)\n\n\n\n\nSee how to the gap between 45 and 50 degrees north is much smaller than between 20 and 25 degrees north? That’s the projection at work (think about how this reflects how the globe is shaped)."
  },
  {
    "objectID": "Extra-04-Functional-II.html#load-libraries-and-set-paths",
    "href": "Extra-04-Functional-II.html#load-libraries-and-set-paths",
    "title": "Extra: Functional Programming II",
    "section": "Load libraries and set paths",
    "text": "Load libraries and set paths\n\n## ---------------------------\n## libraries\n## ---------------------------\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout"
  },
  {
    "objectID": "Extra-04-Functional-II.html#read-in-and-inspect-data",
    "href": "Extra-04-Functional-II.html#read-in-and-inspect-data",
    "title": "Extra: Functional Programming II",
    "section": "Read in and inspect data",
    "text": "Read in and inspect data\n\n## ---------------------------------------------------------\n## read in data\n## ---------------------------------------------------------\n\ndf &lt;- readRDS(file.path(\"data\", \"kmeans.RDS\"))\n\n## show\ndf\n\n# A tibble: 1,500 × 3\n       x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 33.1  11.6  -16.0\n 2 -8.55  6.01  11.8\n 3 50.7  56.3   97.8\n 4  2.83 27.5   34.4\n 5 -4.66 20.0   25.1\n 6 -3.04 16.2   22.8\n 7 30.5  12.1  -14.3\n 8  7.08 25.3   25.6\n 9  1.33 20.1   17.6\n10 -3.11 13.4   11.5\n# … with 1,490 more rows\n\n\nThe data set is an 1,500 by 3 matrix, with three covariates (or features in machine learning parlance), x, y, and z. Since these columns names don’t tell us much, let’s plot the first two features against each other in a scatter plot.\n\n## plot first two dimensions\ng &lt;- ggplot(df, mapping = aes(x = x, y = y)) + geom_point()\n\n## show\ng\n\n\n\n\nWhile there appears to be an overall positive correlation between x and y, the data seem to cluster into three groups. One group (furthest left on the x axis) seems more distinct from the other two, which look to be on the same trend line, but clumped in lower and higher positions.\nOur task will be to assign each point to an as yet unspecified group with the goal that each group will be contiguous (e.g., will cluster together)."
  },
  {
    "objectID": "Extra-04-Functional-II.html#k-means-clustering",
    "href": "Extra-04-Functional-II.html#k-means-clustering",
    "title": "Extra: Functional Programming II",
    "section": "K-means clustering",
    "text": "K-means clustering\nFormally, the objective function for a k-means clustering algorithm is as follows (from Wikipedia):\n\\[ \\underset{S}{arg\\,min} \\sum_{i=1}^k\\sum_{x\\in S_i} \\vert\\vert x - \\mu_i \\vert\\vert^2\\]\nwhere\n\n\\(k\\) is the number of clusters\n\n\\(S_i\\) is one cluster set of points\n\n\\(\\vert\\vert x - \\mu_i \\vert\\vert^2\\) is the squared Euclidean distance\n\nIn English, the objective is that for a fixed number of clusters, assign each point to a cluster such the variance within each cluster (within-cluster sum of squares or WCSS) is minimized. If you think that this sounds like the objective of ordinary least squares (OLS), you are right. The difference here is that instead of creating a line of best fit (or using parametric assumptions to make inferences), we are simply assigning each point to its “cluster of best fit.”\n\nAlgorithm\nThere are number of algorithms to compute k-means. We’ll use Lloyd’s algorithm, which isn’t the best, but is the simplest and works well enough for our purposes.\n\nChoose number of clusters, \\(k\\).\n\nRandomly choose \\(k\\) points from data to serve as initial cluster means.\n\nRepeat following steps until assignments no longer change:\n\nAssign each point to the cluster with the closest mean (see Step 3.1, below, for formal definition).\n\nUpdate means to reflect points assigned to the cluster (see Step 3.2, below, for formal definition).\n\n\n\nStep 3.1: Assign each point to the closest mean\n\\[ S_i^{(t)} = \\{ x_p : \\vert\\vert x_p - m_i^{(t)} \\vert\\vert^2 \\leq \\vert\\vert x_p - m_j^{(t)} \\vert\\vert^2 \\,\\forall\\, j, 1 \\lt j \\lt k \\}\\]\nAt time \\(t\\), each set, \\(S_i\\), contains all the points, \\(x_p\\) for which its mean, \\(m_i^{(t)}\\), is the nearest of possible cluster means.\n\n\nStep 3.2: Update means to reflect points in the cluster\n\\[ m_i^{(t+1)} = \\frac{1}{\\vert S_i^{(t)} \\vert} \\sum_{x_j\\in S_i^{(t)}} x_j \\]\nAt time \\(t+1\\), each cluster mean \\(m_i^{(t+1)}\\) is the centroid of the points, \\(x_j\\) assigned to the cluster at time \\(t\\)."
  },
  {
    "objectID": "Extra-04-Functional-II.html#lets-just-get-it-to-run",
    "href": "Extra-04-Functional-II.html#lets-just-get-it-to-run",
    "title": "Extra: Functional Programming II",
    "section": "Let’s just get it to run!",
    "text": "Let’s just get it to run!\nFor our first step, let’s see if we can “just get the job done.” With that in mind, we’ll limit the number of features to two, x and y.\n\n## convert data to matrix to make our lives easier (x and y, only, for now)\ndat &lt;- df %&gt;% select(x, y) %&gt;% as.matrix()\n\nAs the first step, we need to pick some means. We’ll do this by sampling 3 numbers between 1 and the number of rows in the data frame (1,500). Next we’ll use these to pull out three rows that will be our starting means.\n\n## get initial means to start\nindex &lt;- sample(1:nrow(dat), 3)           # give k random indexes\nmeans &lt;- dat[index,]\n\n## show\nmeans\n\n            x        y\n[1,]  2.37508 20.79650\n[2,] 31.31455 23.20679\n[3,] 54.76006 59.01664\n\n\nNext, we need to keep track of how we assign each point. To that end, we will first initialize an empty numeric vector with a spot for each point, assign_vec. After that, we’ll loop through each point, compute the distance between that point and each mean, and put the index of the closest mean (effectively the label) in the point’s spot in assign_vec. For example, if the first point is closest to the first mean, assign_vec[1] == 1; if the 10th point is closest to the third mean, assign_vec[10] == 3, and so on.\n\n## init assignment vector\nassign_vec &lt;- numeric(nrow(dat))\n\n## assign each point to one of the clusters\nfor (i in 1:nrow(dat)) {\n  ## init a temporary distance object to hold k distance values\n  distance &lt;- numeric(3)\n  ## compare to each mean\n  for (j in 1:3) {\n    distance[j] &lt;- sum((dat[i,] - means[j,])^2)\n  }\n  ## assign the index of smallest value,\n  ## which is effectively cluster ID\n  assign_vec[i] &lt;- which.min(distance)\n}\n\n## show first few\nassign_vec[1:20]\n\n [1] 2 1 3 1 1 1 2 1 1 1 1 3 3 3 1 2 1 2 1 3\n\n\n\nQuick exercise\nMerge the assignments back to the data (everything should be in order for a quick cbind() or bind_cols()) and then plot, assigning a unique color to each group. How did we do the first iteration?\n\nOkay, we’ve started, but our means were arbitrary and probably weren’t the best. Now we need to pick new means and repeat the code above. We need to keep doing this until the assignments don’t change.\nA while() loop is perfect for this task. We’ll create a variable called identical, which will take the Boolean FALSE. While identical is not FALSE (i.e., TRUE), the loop will continue.\nInside, we’ll compute new means, store the assignments in another object so that we can compare later, run the same code as above again, then compare the old and the new. If the old and new assignments are different, then identical remains FALSE and the loop runs again. If the old and new assignments are the same, however, identical becomes TRUE and the loop stops because !TRUE == FALSE.\n\n## repeat above in loop until assignments don't change\nidentical &lt;- FALSE\nwhile (!identical) {\n  ## update means by subsetting each column\n  ## by assignment group, taking mean\n  for (i in 1:3) {\n    means[i,] &lt;- colMeans(dat[assign_vec == i,])\n  }\n  ## store old assignments, b/c we need to compare\n  old_assign_vec &lt;- assign_vec\n  ## assign each point to one of the clusters\n  for (i in 1:nrow(dat)) {\n    ## init a temporary distance object\n    ## to hold k distance values\n    distance &lt;- numeric(3)\n    ## compare to each mean\n    for (j in 1:3) {\n      distance[j] &lt;- sum((dat[i,] - means[j,])^2)\n    }\n    ## assign the index of smallest value,\n    ## which is effectively cluster ID\n    assign_vec[i] &lt;- which.min(distance)\n  }\n  ## check if assignments change\n  identical &lt;- identical(old_assign_vec, assign_vec)\n}\n\nLet’s check to see how we did.\n\n## check assignment with scatter plot\nplot_df &lt;- bind_cols(df, as.data.frame(assign_vec))\ng &lt;- ggplot(plot_df, mapping = aes(x = x, y = y,\n                                   color = factor(assign_vec))) +\n  geom_point()\n\n## show\ng\n\n\n\n\nLooks like we did it! The clusters are, well, clustered and the points, for the most part, seem to be labeled correctly.\n\nQuick exercise\nThe k-means algorithm can be sensitive to the starting points since it finds locally optimal solutions (no guarantee that the solution is the best of all possible solutions). Run the code again a couple of times and see how your fit changes. Do some points move between groups?\n\n\nProblems\nOur success not withstanding, we have a number of problems:\n\nWe have exactly repeated code\n\nMagic numbers in the code (3)\n\nVariable names that depend on working environment\n\nPrimary purpose of code is obfuscated\n\nProblems 1-3 make it difficult to change things in the code. We have to remember and find each repeated instance that we want to change. Problem 3 makes it difficult to re-use the code in another script (who knows what global variables are currently lurking and, once gone, will mess up our program?).\nProblem 4 means that it will be difficult to see what we did when coming back to the code in the future. With the repeated steps and the loop, it will be easy to lose the forest for the trees. What’s the point of this section of code again? After reading through multiple lines, we’ll see, if we’re lucky, that we were running a k-means clustering algorithm. That’s wasted time."
  },
  {
    "objectID": "Extra-04-Functional-II.html#convert-to-functions",
    "href": "Extra-04-Functional-II.html#convert-to-functions",
    "title": "Extra: Functional Programming II",
    "section": "Convert to functions",
    "text": "Convert to functions\nWhat we need is to convert our spaghetti code to DRY (don’t repeat yourself) code. Each unique bit of code should be wrapped in function that does one thing and does it well. Since functions can use other functions, we can start at the smallest part and build up to something more complex.\nWhat do we need? Looking at the code above, it looks like we would benefit from having unique functions to:\n\ncompute squared Euclidean distance\n\ncompute new means based on assignment labels\n\nmake new cluster assignments for each point\n\n\nEuclidean distance function\n\n## Euclidean distance^2\neuclid_dist_sq &lt;- function(x,y) return(sum((x - y)^2))\n\n\n\nCompute means\n\n## compute new means for points in cluster\ncompute_mean &lt;- function(data, k, assign_vec) {\n  ## init means matrix: # means X # features (data columns)\n  means &lt;- matrix(NA, k, ncol(data))\n  ## for each mean, k...\n  for (i in 1:k) {\n    ## ...get column means, restricting to cluster assigned points\n    means[i,] &lt;- colMeans(data[assign_vec == i,])\n  }\n  return(means)\n}\n\n\n\nMake new assignments\nNotice that we’ve added a bit to our code above. We might find it useful to compute the within-cluster sum of squares (WCSS), so we’ve added and object, wcss, that has one spot for each cluster and holds a running total of the sum of squares within each cluster.\nTo return multiple objects from a function, R requires that they be stored in a list. This function will return both the assignments for each point, like before, as well as the WCSS.\n\n## find nearest mean to each point and assign to cluster\nassign_to_cluster &lt;- function(data, k, means, assign_vec) {\n  ## init within-cluster sum of squares for each cluster\n  wcss &lt;- numeric(k)\n  ## for each data point (slow!)...\n  for (i in 1:nrow(data)) {\n    ## ...init distance vector, one for each cluster mean\n    distance &lt;- numeric(k)\n    ## ...for each mean...\n    for (j in 1:k) {\n      ## ...compute distance to point\n      distance[j] &lt;- euclid_dist_sq(data[i,], means[j,])\n    }\n    ## ...assign to cluster with nearest mean\n    assign_vec[i] &lt;- which.min(distance)\n    ## ...add distance to running sum of squares\n    ## for assigned cluster\n    wcss[assign_vec[i]] &lt;- wcss[assign_vec[i]] + distance[assign_vec[i]]\n  }\n  return(list(\"assign_vec\" = assign_vec, \"wcss\" = wcss))\n}\n\n\n\nBONUS: standardize data\nMany machine learning algorithms perform better if the covariates are on the same scale. We’ve not really had to worry about this so far since our data are scaled roughly the same. That said, we may want to use this algorithm in the future with data that aren’t similarly scaled (say, on data with age in years and family income in dollars).\nThis function simply standardizes a matrix of data and returns the scaled data along with the mean and standard deviations of each column (feature), which are useful for rescaling the data later.\n\n## standardize function that also returns mu and sd\nstandardize_dat &lt;- function(data) {\n  ## column means\n  mu &lt;- colMeans(data)\n  ## column standard deviations\n  sd &lt;- sqrt(diag(var(data)))\n  ## scale data (z-score); faster to use pre-computed mu/sd\n  sdata &lt;- scale(data, center = mu, scale = sd)\n  return(list(\"mu\" = mu, \"sd\" = sd, \"scaled_data\" = sdata))\n}\n\n\n\nK-means function\nNow that we have the pieces, we can put the pieces together in single function. Our function takes the following arguments:\n\ndata: a data frame (converted to matrix inside the function)\n\nk: number of clusters we want\n\niterations: a stop valve in case our algorithm doesn’t want to converge\n\nnstarts: a new feature that says how many times to start the algorithm; while not perfect it may help prevent some sub-optimal local solutions\n\nstandardize: an option to standardize the data\n\nNote that only data and k have to be supplied by the user. The other arguments have default values. If we leave them out of the function call, the defaults will be used.\n\n## k-means function\nmy_kmeans &lt;- function(data,                  # data frame\n                      k,                     # number of clusters\n                      iterations = 100,      # max iterations\n                      nstarts = 1,           # how many times to run\n                      standardize = FALSE) { # standardize data?\n  ## convert to matrix\n  x &lt;- as.matrix(data)\n  ## standardize if TRUE\n  if (standardize) {\n    sdata &lt;- standardize_dat(data)\n    x &lt;- sdata[[\"scaled_data\"]]\n  }\n  ## for number of starts\n  for (s in 1:nstarts) {\n    ## init identical\n    identical &lt;- FALSE\n    ## select k random points as starting means\n    means &lt;- x[sample(1:nrow(x),k),]\n    ## init assignment vector\n    init_assign_vec &lt;- rep(NA, nrow(x))\n    ## first assignment\n    assign_wcss &lt;- assign_to_cluster(x, k, means, init_assign_vec)\n    ## iterate until iterations run out\n    ## or no change in assignment\n    while (iterations &gt; 0 && !identical) {\n      ## store old assignment / wcss object\n      old_assign_wcss &lt;- assign_wcss\n      ## get new means\n      means &lt;- compute_mean(x, k, assign_wcss[[\"assign_vec\"]])\n      ## new assignments\n      assign_wcss &lt;- assign_to_cluster(x, k, means, assign_wcss[[\"assign_vec\"]])\n      ## check if identical (no change in assignment)\n      identical &lt;- identical(old_assign_wcss[[\"assign_vec\"]],\n                             assign_wcss[[\"assign_vec\"]])\n      ## reduce iteration counter\n      iterations &lt;- iterations - 1\n    }\n    ## store best values...\n    if (s == 1) {\n      best_wcss &lt;- assign_wcss[[\"wcss\"]]\n      best_centers &lt;- means\n      best_assignvec &lt;- assign_wcss[[\"assign_vec\"]]\n    } else {\n      ## ...update accordingly if number of starts is &gt; 1\n      ## & wcss is lower\n      if (sum(assign_wcss[[\"wcss\"]]) &lt; sum(best_wcss)) {\n        best_wcss &lt;- assign_wcss[[\"wcss\"]]\n        best_centers &lt;- means\n        best_assignvec &lt;- assign_wcss[[\"assign_vec\"]]\n      }\n    }\n  }\n  ## convert back to non-standard centers if necessary\n  if (standardize) {\n    sd &lt;- sdata[[\"sd\"]]\n    mu &lt;- sdata[[\"mu\"]]\n    for (j in 1:ncol(x)) {\n      best_centers[,j] &lt;- best_centers[,j] * sd[j] + mu[j]\n    }\n  }\n  ## return assignment vector, cluster centers, & wcss\n  return(list(\"assignments\" = best_assignvec,\n              \"centers\" = best_centers,\n              \"wcss\" = best_wcss))\n}\n\n\nQuick exercise\nLook through the function and talk through how the function will run with the following arguments:\n\nmy_kmeans(data, 3)\n\nmy_kmeans(data, 3, standardize = TRUE)\n\nmy_kmeans(data, 3, nstarts = 10)"
  },
  {
    "objectID": "Extra-04-Functional-II.html#dimensions",
    "href": "Extra-04-Functional-II.html#dimensions",
    "title": "Extra: Functional Programming II",
    "section": "2 dimensions",
    "text": "2 dimensions\nNow that we have our proper function, let’s run it! Let’s go back to where we started, with two dimensions.\n\n## 2 dimensions\nkm_2d &lt;- my_kmeans(data = df[,c(\"x\",\"y\")], k = 3, nstarts = 20)\n\n## check assignments, centers, and wcss\nkm_2d$assignments[1:20]\n\n [1] 1 3 2 3 3 3 1 3 3 3 3 2 2 2 3 1 3 1 1 2\n\nkm_2d$centers\n\n           [,1]      [,2]\n[1,] 25.0407539  9.780767\n[2,] 54.6683528 54.725368\n[3,]  0.7741392 25.471911\n\nkm_2d$wcss\n\n[1] 88179.39 99748.81 89184.79\n\n\nNotice how clear and unambiguous our code is now. We are running a k-means clustering algorithm with three clusters. We get results that tell us the assignments for each point, the centers of the clusters, and the WCSS."
  },
  {
    "objectID": "Extra-04-Functional-II.html#dimensions-1",
    "href": "Extra-04-Functional-II.html#dimensions-1",
    "title": "Extra: Functional Programming II",
    "section": "3 dimensions",
    "text": "3 dimensions\nThe nice thing about our function is that it easily scales to more than two dimensions. Instead of subsetting the data, we’ll just include it all for 3 dimensions this time.\n\n## 3 dimensions\nkm_3d &lt;- my_kmeans(data = df, k = 3, nstarts = 20)\n\n## check centers and wcss\nkm_3d$assignments[1:20]\n\n [1] 3 1 2 1 1 1 3 1 1 1 1 2 2 2 1 3 1 3 3 2\n\nkm_3d$centers\n\n           [,1]     [,2]      [,3]\n[1,]  0.8928973 25.51276  25.58783\n[2,] 54.8865406 54.97876 105.13575\n[3,] 25.2858908 10.26503 -24.21246\n\nkm_3d$wcss\n\n[1] 153343.3 159247.5 167007.1"
  },
  {
    "objectID": "Extra-04-Functional-II.html#check-out-plots",
    "href": "Extra-04-Functional-II.html#check-out-plots",
    "title": "Extra: Functional Programming II",
    "section": "Check out plots",
    "text": "Check out plots\nLet’s plot our assignments. First, let’s plot the two dimensional assignments. We’ll use plotly this time so that it’s easier to zoom in on individual points.\n\n## using 2D: looks good 2D...\np &lt;- plot_ly(df, x = ~x, y = ~y, color = factor(km_2d$assignments)) %&gt;%\n  add_markers()\n\n## show\np\nLooks pretty much like our first plot above. But what happens if we include the third feature, z, that we ignored?\n\n## ...but off in 3D\np &lt;- plot_ly(df, x = ~x, y = ~y, z = ~z, color = factor(km_2d$assignments),\n             marker = list(size = 3)) %&gt;%\n  add_markers()\n\n## show\np\nAh! Not so good! Clearly, some of the points appear to fit in one cluster if only x and y are considered, but no longer do when the third dimension is included.\n\nQuick exercise\nManipulate the 3D plotly figure to see if you can recreate the 2D plot, that is adjust the viewing angle. Then move it around to see how that flattened view was deceptive.\n\nLet’s move to our results that used all three features. Again, we’ll first look in just two dimensions.\n\n## using 3D: looks off in 2D...\np &lt;- plot_ly(df, x = ~x, y = ~y, color = factor(km_3d$assignments)) %&gt;%\n  add_markers()\n\n## show\np\nIn two dimensions, our clusters look a bit messy since they overlap on the edges. But looking in three dimensions…\n\n## ...but clearly better fit in 3D\np &lt;- plot_ly(df, x = ~x, y = ~y, z = ~z, color = factor(km_3d$assignments),\n             marker = list(size = 3)) %&gt;%\n    add_markers()\n\n## show\np\nMuch better! Not perfect, but better."
  },
  {
    "objectID": "12-Program-Stats.html#data",
    "href": "12-Program-Stats.html#data",
    "title": "I: Basic Statistical Models",
    "section": "Data",
    "text": "Data\nIn this lesson, we’ll use data from the NCES Education Longitudinal Study of 2002. Much like HSLS, ELS is a nationally representative survey that initially surveyed students in their early high school career (10th grade in 2002) and followed them into college and the workforce. We’ll again use a smaller version of the data, so be sure to get the full data files if you decide to use ELS in a future project. One additional benefit of ELS is that the public use files contain the weights we’ll use to properly account for the survey design later in the lesson.\nHere’s a codebook with descriptions of the variables included in our lesson today:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nstu_id\nstudent id\n\n\nsch_id\nschool id\n\n\nstrat_id\nstratum\n\n\npsu\nprimary sampling unit\n\n\nbystuwt\nstudent weight\n\n\nbysex\nsex-composite\n\n\nbyrace\nstudent’s race/ethnicity-composite\n\n\nbydob_p\nstudent’s year and month of birth\n\n\nbypared\nparents’ highest level of education\n\n\nbymothed\nmother’s highest level of education-composite\n\n\nbyfathed\nfather’s highest level of education-composite\n\n\nbyincome\ntotal family income from all sources 2001-composite\n\n\nbyses1\nsocio-economic status composite, v.1\n\n\nbyses2\nsocio-economic status composite, v.2\n\n\nbystexp\nhow far in school student thinks will get-composite\n\n\nbynels2m\nels-nels 1992 scale equated sophomore math score\n\n\nbynels2r\nels-nels 1992 scale equated sophomore reading score\n\n\nf1qwt\nquestionnaire weight for f1\n\n\nf1pnlwt\npanel weight, by and f1 (2002 and 2004)\n\n\nf1psepln\nf1 post-secondary plans right after high school\n\n\nf2ps1sec\nSector of first postsecondary institution\n\n\nfemale\n== 1 if female\n\n\nmoth_ba\n== 1 if mother has BA/BS\n\n\nfath_ba\n== 1 if father has BA/BS\n\n\npar_ba\n== 1 if either parent has BA/BS\n\n\nplan_col_grad\n== 1 if student plans to earn college degree\n\n\nlowinc\n== 1 if income &lt; $25k\n\n\n\nLet’s load the libraries and data!\n…but first! You’ll most likely need to install the survey package first, using install.packages(\"survey\").\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(haven)\nlibrary(survey)\n\nLoading required package: grid\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\n\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## assume we're running this script from the ./scripts subdirectory\ndf &lt;- read_dta(file.path(\"data\", \"els_plans.dta\"))"
  },
  {
    "objectID": "12-Program-Stats.html#correlations",
    "href": "12-Program-Stats.html#correlations",
    "title": "I: Basic Statistical Models",
    "section": "Correlations",
    "text": "Correlations\nIn prior lessons, we’ve visually checked for correlations between variables through plotting. We can also produce more formal tests of correlation using R’s cor() function. By default, it gives us the Pearson correlation coefficient, though we can also use it to return other versions.\nFirst, let’s check the correlation between math and reading scores. Because this is a base R function, we’ll need to use df$ notation. We’ll also need to be explicit about removing missing values with the argument, use = \"complete.obs\" (one of the few annoying function options that doesn’t really match how other functions work).\n\n## correlation between math and reading scores\ncor(df$bynels2m, df$bynels2r, use = \"complete.obs\")\n\n[1] 0.7521538\n\n\nAs we might have hypothesized, there is a strong positive correlation between math and reading scores.\nWe can also build a correlation matrix if we give cor() a data frame. Our data set, though smaller than the full ELS data set, is still rather large. We’ll only check the correlations between a few variables. In this next example, I’ll show you how you might do this using the tidyverse way we’ve used in other lessons.\n\n## correlation between various columns, using pipes\ndf %&gt;%\n    ## select a few variables\n    select(byses1, bynels2m, bynels2r, par_ba) %&gt;%\n    ## use a . to be the placeholder for the piped in data.frame\n    cor(., use = \"complete.obs\")\n\n            byses1  bynels2m  bynels2r    par_ba\nbyses1   1.0000000 0.4338441 0.4315718 0.7041740\nbynels2m 0.4338441 1.0000000 0.7490148 0.2980608\nbynels2r 0.4315718 0.7490148 1.0000000 0.2865717\npar_ba   0.7041740 0.2980608 0.2865717 1.0000000\n\n\nPer our expectations, the main diagonal is all 1s — every variable is perfectly correlated with itself — and the mirror cells on either side of the line are the same: [2,1] == [1,2] because the correlation between bynels2m and byses1 is the same as byses1 and bynels2m (the order doesn’t matter).\n\nQuick exercise\nRun a correlation of both math and reading scores against whether the father has a Bachelor’s degree (fath_ba) or the mother has a Bachelor’s degree (moth_ba). In what ways, if any, is this different than what you get when using the combined par_ba indicator variable?"
  },
  {
    "objectID": "12-Program-Stats.html#t-test",
    "href": "12-Program-Stats.html#t-test",
    "title": "I: Basic Statistical Models",
    "section": "t-test",
    "text": "t-test\nOne common statistical test is a t-test for a difference in means across groups (there are, of course, other types of t-tests that R can compute). This version of the test can be computed using the R formula syntax: y ~ x. In our example, we’ll compute base-year math scores against parent’s college education level. Notice that since we have the data = df argument after the comma, we don’t need to include df$ before the two variables.\n\n## t-test of difference in math scores across parental education (BA/BA or not)\nt.test(bynels2m ~ par_ba, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  bynels2m by par_ba\nt = -38.341, df = 13269, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.671327 -7.827819\nsample estimates:\nmean in group 0 mean in group 1 \n       41.97543        50.22501 \n\n\nLooking at the bottom of the output, we see the mean math scores across the two groups:\n\nMath score of 41.9754334 for students for whom neither parent has a Bachelor’s degree or higher\nMath score of 50.2250064 for students for whom at least one parent has a Bachelor’s degree or higher\n\nAre these differences statistically meaningful? Looking in the third line, we see a large t stat of -38.34 which is highly statistically significant at conventional levels. Taken together, we can say that among this sample of students, those from households in which at least one parent had a Bachelor’s degree or higher tended to score higher on the math exam than their peers whose parents did not have a Bachelor’s degree or higher. Furthermore, this result is statistically significant, meaning that we can reject the null that there is no difference between the groups, that is, the difference we observe is just sampling noise.\nIs the difference practically significant? Maybe, but to really know that we need to understand more about the design and scaling of the math test. Once we do, we can apply our content-area knowledge and place our findings in their proper context.\n\nQuick exercise\nRun a t-test of reading scores against whether the father has a Bachelor’s degree (fath_ba)."
  },
  {
    "objectID": "12-Program-Stats.html#using-survey-weights",
    "href": "12-Program-Stats.html#using-survey-weights",
    "title": "I: Basic Statistical Models",
    "section": "Using survey weights",
    "text": "Using survey weights\nSo far we haven’t used survey weights, but they are very important if we want to make population-level inferences using surveys with complex sampling designs. Yes, we can produce means, compute t-tests, and fit regressions without weights, but our results may not be externally valid due the fact that observations in our sample are almost certainly out of proportion to their occurrence in the population. Otherwise stated, it’s likely that our sample under-represents some groups while over-representing others. The upshot is that any unweighted estimates will be a function of what we observe (sample) and not necessarily what we want (population). Furthermore, our standard errors — which we use when determining statistical significance — may be incorrect if we don’t use survey weights, meaning that we may be more likely to commit Type I or Type II errors.\nSo that we can make population-level inferences, survey designers often include weights that allow us to adjust the amount each observation contributes to our estimates. With this adjustment our estimate should better reflect the population value. To use survey weights, you’ll need to use the survey package (which we’ve already loaded above).\nAs a first step, you need to set the survey design using the svydesign() function. You could do this in the svymean() or svyglm() functions we’ll use to actually produce our weighted estimates, but it’s easier and clearer to do it first, store it in an object, and then re-use that object.\nELS has a complex sampling design that we won’t get into, but the appropriate columns from our data frame, df, are set to the proper arguments in svydesign():\n\nids are the primary sampling units or psus\n\nstrata are indicated by the strat_ids\n\nweight is the base-year student weight or bystuwt\ndata is our data frame object, df\nnest = TRUE because the psus are nested in strat_ids\n\nFinally, notice the ~ before each column name, which is necessary in this function.\n\n## subset data\nsvy_df &lt;- df %&gt;%\n    select(psu,                         # primary sampling unit\n           strat_id,                    # stratum ID\n           bystuwt,                     # weight we want to use\n           bynels2m,                    # variables we want...\n           moth_ba,\n           fath_ba,\n           par_ba,\n           byses1,\n           lowinc,\n           female) %&gt;%\n    ## go ahead and drop observations with missing values\n    drop_na()\n\n## set svy design data\nsvy_df &lt;- svydesign(ids = ~psu,\n                    strata = ~strat_id,\n                    weight = ~bystuwt,\n                    data = svy_df,\n                    nest = TRUE)\n\nNow that we’ve set the survey design, let’s compare the unweighted mean with one that accounts for the survey design.\n\n## compare unweighted and survey-weighted mean of math scores\ndf %&gt;% summarise(bynels2m_m = mean(bynels2m, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  bynels2m_m\n       &lt;dbl&gt;\n1       45.4\n\nsvymean(~bynels2m, design = svy_df, na.rm = TRUE)\n\n           mean    SE\nbynels2m 44.424 0.262\n\n\nIt’s a little different!\nWe can also use the survey package to properly weight our t-tests and regression models — again, using the object svy_df in the design argument in place of our unset df data frame.\n\n## get svymeans by group\nsvyby(~bynels2m, by = ~par_ba, design = svy_df, FUN = svymean, na.rm = TRUE)\n\n  par_ba bynels2m        se\n0      0 41.43669 0.2481650\n1      1 49.35088 0.3370024\n\n## t-test using survey design / weights\nsvyttest(bynels2m ~ par_ba, design = svy_df)\n\n\n    Design-based t-test\n\ndata:  bynels2m ~ par_ba\nt = 22.988, df = 389, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in mean is not equal to 0\n95 percent confidence interval:\n 7.237325 8.591066\nsample estimates:\ndifference in mean \n          7.914195 \n\n\nNotice that unlike the base R t.test() function, svyttest() gives the difference between the groups. That’s part of the reason we began by using svyby() function with FUN = svymean. Looking at the full output between the two functions, our results are a little different from what we got earlier without weights.\n\nQUICK EXERCISE\nCompare this to the output from t.test() above. How is it different?\n\nTwo notes:\nFirst, the survey library has a ton of features and is worth diving into if you regularly work with survey data. We’ve only scratched the surface of what it can do. But keep in mind that whatever statistical test you want to perform, there’s likely a version that works with survey weights via this library.\nSecond, you’ll have to spend a lot of time reading the survey methodology documents in order to understand which weights you should you use (context always matters) so that you can properly set up your survey design with svydesign(). But be warned: even then it won’t always be clear which weights are the “correct” weights. That said, do as you always do: (1) your due diligence, and (2) be prepared to defend your choice as well as note potential limitations.\nIn this lesson, we’ll move beyond t.tests to regression. As before, this lesson is too short to stand in for a full course on regression analyses. However, it should give you the basics of how to fit regression and store key parameters. We’ll also cover the basics of prediction and the estimate of marginal “effects” (nothing causal here!)."
  },
  {
    "objectID": "12-Program-Stats.html#data-1",
    "href": "12-Program-Stats.html#data-1",
    "title": "I: Basic Statistical Models",
    "section": "Data",
    "text": "Data\nIn this lesson, we’ll use the same data from the NCES Education Longitudinal Study of 2002. So you don’t have to go back to the prior lesson, here again is a codebook with descriptions of the variables included in our lesson today:\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nstu_id\nstudent id\n\n\nsch_id\nschool id\n\n\nstrat_id\nstratum\n\n\npsu\nprimary sampling unit\n\n\nbystuwt\nstudent weight\n\n\nbysex\nsex-composite\n\n\nbyrace\nstudent’s race/ethnicity-composite\n\n\nbydob_p\nstudent’s year and month of birth\n\n\nbypared\nparents’ highest level of education\n\n\nbymothed\nmother’s highest level of education-composite\n\n\nbyfathed\nfather’s highest level of education-composite\n\n\nbyincome\ntotal family income from all sources 2001-composite\n\n\nbyses1\nsocio-economic status composite, v.1\n\n\nbyses2\nsocio-economic status composite, v.2\n\n\nbystexp\nhow far in school student thinks will get-composite\n\n\nbynels2m\nels-nels 1992 scale equated sophomore math score\n\n\nbynels2r\nels-nels 1992 scale equated sophomore reading score\n\n\nf1qwt\nquestionnaire weight for f1\n\n\nf1pnlwt\npanel weight, by and f1 (2002 and 2004)\n\n\nf1psepln\nf1 post-secondary plans right after high school\n\n\nf2ps1sec\nSector of first postsecondary institution\n\n\nfemale\n== 1 if female\n\n\nmoth_ba\n== 1 if mother has BA/BS\n\n\nfath_ba\n== 1 if father has BA/BS\n\n\npar_ba\n== 1 if either parent has BA/BS\n\n\nplan_col_grad\n== 1 if student plans to earn college degree\n\n\nlowinc\n== 1 if income &lt; $25k\n\n\n\nWe’ll load the same libraries and data!\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(survey)\n\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## assume we're running this script from the ./scripts subdirectory\ndf &lt;- read_dta(file.path(\"data\", \"els_plans.dta\"))"
  },
  {
    "objectID": "12-Program-Stats.html#linear-model",
    "href": "12-Program-Stats.html#linear-model",
    "title": "I: Basic Statistical Models",
    "section": "Linear model",
    "text": "Linear model\nLinear models are the go-to method of making inferences for many data analysts. In R, the lm() command is used to compute an ordinary least squares (OLS) regression. Unlike above, where we just let the t.test() output print to the console, we can and will store the output in an object.\nFirst, let’s compute the same t-test as in the prior inferential lesson, but in a regression framework. This time, we’ll assume equal variances between the distributions in the t-test above (var.equal = TRUE), so we should get the same results as we did before.\n\n## t-test of difference in math scores across parental education (BA/BA or not)\nt.test(bynels2m ~ par_ba, data = df, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  bynels2m by par_ba\nt = -38.54, df = 15234, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -8.669138 -7.830008\nsample estimates:\nmean in group 0 mean in group 1 \n       41.97543        50.22501 \n\n\n\n## compute same test as above, but in a linear model\nfit &lt;- lm(bynels2m ~ par_ba, data = df)\nfit\n\n\nCall:\nlm(formula = bynels2m ~ par_ba, data = df)\n\nCoefficients:\n(Intercept)       par_ba  \n      41.98         8.25  \n\n\nThe output is a little thin: just the coefficients. To see the full range of information you want from regression output, use the summary() function wrapped around the fit object.\n\n## use summary to see more information about regression\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ par_ba, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.515  -9.685   0.595   9.885  37.015 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.9754     0.1375  305.35   &lt;2e-16 ***\npar_ba        8.2496     0.2141   38.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.01 on 15234 degrees of freedom\n  (924 observations deleted due to missingness)\nMultiple R-squared:  0.08884,   Adjusted R-squared:  0.08878 \nF-statistic:  1485 on 1 and 15234 DF,  p-value: &lt; 2.2e-16\n\n\nWe’ll more fully discuss this output in the next section. For now, let’s compare the key findings to those returned from the t test.\nBecause our right-hand side (RHS) variable is an indicator variable that == 0 for parents without a BA/BS or higher and == 1 if either parent has a BA/BS or higher, then the intercept reflects the math test score for students when par_ba == 0. This matches the mean in group 0 value from the t test above.\nIn a regression framework, the coefficient on par_ba is the marginal difference when par_ba increases by one unit. Since pared is the only parameter on the RHS (besides the intercept) and only takes on values 0 and 1, we can add its coefficient to the intercept to get the math test score mean for students with parents with a BA/BS or higher.\n\n## add intercept and par_ba coefficient\nfit$coefficients[[\"(Intercept)\"]] + fit$coefficients[[\"par_ba\"]]\n\n[1] 50.22501\n\n\nLooks like this value matches what we saw before (within rounding). Going the other way, the coefficient on par_ba, 8.2495729, is the same as the difference between the groups in the t test. Finally, notice that the absolute value of the test statistic for the t test and the par_ba coefficient are the same value: 38.54. Success!\n\nMultiple regression\nTo fit a multiple regression, use the same formula framework that we’ve use before with the addition of all the terms you want on right-hand side of the equation separated by plus (+) signs.\nNB From here on out, we’ll spend less time interpreting the regression results so that we can focus on the tools of running regressions. That said, let me know if you have questions of interpretation.\n\n## linear model with more than one covariate on the RHS\nfit &lt;- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,\n          data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + \n    lowinc, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.456  -8.775   0.432   9.110  40.921 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.7155     0.1811 252.420  &lt; 2e-16 ***\nbyses1        6.8058     0.2387  28.511  &lt; 2e-16 ***\nfemale       -1.1483     0.1985  -5.784 7.42e-09 ***\nmoth_ba       0.4961     0.2892   1.715  0.08631 .  \nfath_ba       0.8242     0.2903   2.840  0.00452 ** \nlowinc       -2.1425     0.2947  -7.271 3.75e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.24 on 15230 degrees of freedom\n  (924 observations deleted due to missingness)\nMultiple R-squared:  0.1929,    Adjusted R-squared:  0.1926 \nF-statistic: 728.1 on 5 and 15230 DF,  p-value: &lt; 2.2e-16\n\n\nThe full output tells you:\n\nthe model that you fit, under Call:\na table of coefficients with\n\nthe point estimates (Estimate)\nthe point estimate errors (Std. Error)\nthe test statistic for each point estimate (t value with this model)\nthe p value for each point estimate (Pr(&gt;|t|))\n\nsignificance stars (. and *) along with legend\nthe R-squared values (Multiple R-squared and Adjusted     R-squared)\nthe model F-statistic (F-statistic)\nnumber of observations dropped if any\n\nIf observations were dropped due to missing values (lm() does this automatically by default), you can recover the number of observations actually used with the nobs() function.\n\n## check number of observations\nnobs(fit)\n\n[1] 15236\n\n\nThe fit object also holds a lot of other information that is sometimes useful.\n\n## see what fit object holds\nnames(fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\n\nIn addition to the coefficients, which you pulled out of the first model, both fitted.values and residuals are stored in the object. You can access these “hidden” attributes by treating the fit object like a data frame and using the $ notation.\n\n## see first few fitted values and residuals\nhead(fit$fitted.values)\n\n       1        2        3        4        5        6 \n42.86583 48.51465 38.78234 36.98010 32.82855 38.43332 \n\nhead(fit$residuals)\n\n         1          2          3          4          5          6 \n  4.974173   6.785347  27.457659  -1.650095  -2.858552 -14.153323 \n\n\n\nQuick exercise\nAdd the fitted values to the residuals and store in an object (x). Compare these values to the math scores in the data frame.\n\nAs a final note, the model matrix used fit the regression can be retrieved using model.matrix(). Since we have a lot of observations, we’ll just look at the first few rows.\n\n## see the design matrix\nhead(model.matrix(fit))\n\n  (Intercept) byses1 female moth_ba fath_ba lowinc\n1           1  -0.25      1       0       0      0\n2           1   0.58      1       0       0      0\n3           1  -0.85      1       0       0      0\n4           1  -0.80      1       0       0      1\n5           1  -1.41      1       0       0      1\n6           1  -1.07      0       0       0      0\n\n\nWhat this shows is that the fit object actually stores a copy of the data used to run it. That’s really convenient if you want to save the object to disk (with the save() function) so you can review the regression results later. But keep in mind that if you share that file, you are sharing the part of the data used to estimate it. Because a lot of education data is restricted in some way — via memorandums of understanding (MOUs) or IRB — be careful about sharing the saved output object. Typically you’ll only share the results in a table or figure, but just be aware.\n\n\nUsing categorical variables or factors\nIt’s not necessary to pre-construct dummy variables if you want to use a categorical variable in your model. Instead you can use the categorical variable wrapped in the factor() function. This tells R that the underlying variable shouldn’t be treated as a continuous value, but should be discrete groups. R will make the dummy variables on the fly when fitting the model. We’ll include the categorical variable bystexp in this model.\n\n## check values of student expectations\ndf %&gt;%\n    count(bystexp)\n\n# A tibble: 9 × 2\n  bystexp                                           n\n  &lt;dbl+lbl&gt;                                     &lt;int&gt;\n1 -1 [{don^t know}]                              1450\n2  1 [less than high school graduation]           128\n3  2 [high school graduation or ged only]         983\n4  3 [attend or complete 2-year college/school]   879\n5  4 [attend college, 4-year degree incomplete]   561\n6  5 [graduate from college]                     5416\n7  6 [obtain master^s degree or equivalent]      3153\n8  7 [obtain phd, md, or other advanced degree]  2666\n9 NA                                              924\n\n\nEven though student expectations of eventual degree attainment are roughly ordered, let’s use them in our model as discrete groups. That way we can leave in “Don’t know” and don’t have to worry about that “attend college, 4-year degree incomplete” is somehow higher than “attend or complete 2-year college/school”.\n\n## add factors\nfit &lt;- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba\n          + lowinc + factor(bystexp),\n          data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + \n    lowinc + factor(bystexp), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.680  -8.267   0.541   8.423  38.696 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       42.9534     0.3366 127.603  &lt; 2e-16 ***\nbyses1             5.1616     0.2297  22.468  &lt; 2e-16 ***\nfemale            -2.3828     0.1909 -12.481  &lt; 2e-16 ***\nmoth_ba            0.4119     0.2742   1.502   0.1331    \nfath_ba            0.6250     0.2754   2.270   0.0232 *  \nlowinc            -2.2017     0.2794  -7.880 3.50e-15 ***\nfactor(bystexp)1 -10.0569     1.0710  -9.390  &lt; 2e-16 ***\nfactor(bystexp)2  -5.4527     0.4813 -11.329  &lt; 2e-16 ***\nfactor(bystexp)3  -1.2000     0.4966  -2.416   0.0157 *  \nfactor(bystexp)4  -3.5317     0.5771  -6.119 9.62e-10 ***\nfactor(bystexp)5   3.6345     0.3446  10.546  &lt; 2e-16 ***\nfactor(bystexp)6   7.6366     0.3736  20.442  &lt; 2e-16 ***\nfactor(bystexp)7   7.5114     0.3860  19.460  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.6 on 15223 degrees of freedom\n  (924 observations deleted due to missingness)\nMultiple R-squared:  0.2754,    Adjusted R-squared:  0.2748 \nF-statistic: 482.1 on 12 and 15223 DF,  p-value: &lt; 2.2e-16\n\n\nIf you’re using labeled data like we have been for the past couple of modules, you can use the as_factor() function from the haven library in place of the base factor() function. You’ll still see the as_factor(&lt;var&gt;) prefix on each coefficient, but now you’ll have labels instead of the underlying values, which should make parsing the output a little easier.\n\n## same model, but use as_factor() instead of factor() to use labels\nfit &lt;- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba\n          + lowinc + as_factor(bystexp),\n          data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + \n    lowinc + as_factor(bystexp), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.680  -8.267   0.541   8.423  38.696 \n\nCoefficients:\n                                                           Estimate Std. Error\n(Intercept)                                                 42.9534     0.3366\nbyses1                                                       5.1616     0.2297\nfemale                                                      -2.3828     0.1909\nmoth_ba                                                      0.4119     0.2742\nfath_ba                                                      0.6250     0.2754\nlowinc                                                      -2.2017     0.2794\nas_factor(bystexp)less than high school graduation         -10.0569     1.0710\nas_factor(bystexp)high school graduation or ged only        -5.4527     0.4813\nas_factor(bystexp)attend or complete 2-year college/school  -1.2000     0.4966\nas_factor(bystexp)attend college, 4-year degree incomplete  -3.5317     0.5771\nas_factor(bystexp)graduate from college                      3.6345     0.3446\nas_factor(bystexp)obtain master^s degree or equivalent       7.6366     0.3736\nas_factor(bystexp)obtain phd, md, or other advanced degree   7.5114     0.3860\n                                                           t value Pr(&gt;|t|)    \n(Intercept)                                                127.603  &lt; 2e-16 ***\nbyses1                                                      22.468  &lt; 2e-16 ***\nfemale                                                     -12.481  &lt; 2e-16 ***\nmoth_ba                                                      1.502   0.1331    \nfath_ba                                                      2.270   0.0232 *  \nlowinc                                                      -7.880 3.50e-15 ***\nas_factor(bystexp)less than high school graduation          -9.390  &lt; 2e-16 ***\nas_factor(bystexp)high school graduation or ged only       -11.329  &lt; 2e-16 ***\nas_factor(bystexp)attend or complete 2-year college/school  -2.416   0.0157 *  \nas_factor(bystexp)attend college, 4-year degree incomplete  -6.119 9.62e-10 ***\nas_factor(bystexp)graduate from college                     10.546  &lt; 2e-16 ***\nas_factor(bystexp)obtain master^s degree or equivalent      20.442  &lt; 2e-16 ***\nas_factor(bystexp)obtain phd, md, or other advanced degree  19.460  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.6 on 15223 degrees of freedom\n  (924 observations deleted due to missingness)\nMultiple R-squared:  0.2754,    Adjusted R-squared:  0.2748 \nF-statistic: 482.1 on 12 and 15223 DF,  p-value: &lt; 2.2e-16\n\n\nIf you look at the model matrix, you can see how R created the dummy variables from bystexp: adding new columns of only 0/1s that correspond to the bystexp value of each student.\n\n## see what R did under the hood to convert categorical to dummies\nhead(model.matrix(fit))\n\n  (Intercept) byses1 female moth_ba fath_ba lowinc\n1           1  -0.25      1       0       0      0\n2           1   0.58      1       0       0      0\n3           1  -0.85      1       0       0      0\n4           1  -0.80      1       0       0      1\n5           1  -1.41      1       0       0      1\n6           1  -1.07      0       0       0      0\n  as_factor(bystexp)less than high school graduation\n1                                                  0\n2                                                  0\n3                                                  0\n4                                                  0\n5                                                  0\n6                                                  0\n  as_factor(bystexp)high school graduation or ged only\n1                                                    0\n2                                                    0\n3                                                    0\n4                                                    0\n5                                                    0\n6                                                    0\n  as_factor(bystexp)attend or complete 2-year college/school\n1                                                          1\n2                                                          0\n3                                                          0\n4                                                          0\n5                                                          0\n6                                                          0\n  as_factor(bystexp)attend college, 4-year degree incomplete\n1                                                          0\n2                                                          0\n3                                                          0\n4                                                          0\n5                                                          0\n6                                                          1\n  as_factor(bystexp)graduate from college\n1                                       0\n2                                       0\n3                                       0\n4                                       1\n5                                       1\n6                                       0\n  as_factor(bystexp)obtain master^s degree or equivalent\n1                                                      0\n2                                                      0\n3                                                      0\n4                                                      0\n5                                                      0\n6                                                      0\n  as_factor(bystexp)obtain phd, md, or other advanced degree\n1                                                          0\n2                                                          1\n3                                                          0\n4                                                          0\n5                                                          0\n6                                                          0\n\n\n\nQuick exercise\nAdd the categorical variable byincome to the model above. Next use model.matrix() to check the RHS matrix.\n\n\n\nInteractions\nAdd interactions to a regression using an asterisks (*) between the terms you want to interact. This will add both main terms and the interaction(s) between the two to the model. Any interaction terms will be labeled using the base name or factor name of each term joined by a colon (:).\n\n## add interactions\nfit &lt;- lm(bynels2m ~ byses1 + factor(bypared)*lowinc, data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ byses1 + factor(bypared) * lowinc, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.998  -8.852   0.326   9.063  39.257 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              44.0084     0.6345  69.355  &lt; 2e-16 ***\nbyses1                    7.6082     0.2772  27.448  &lt; 2e-16 ***\nfactor(bypared)2          1.5546     0.6544   2.376 0.017533 *  \nfactor(bypared)3          0.6534     0.7136   0.916 0.359863    \nfactor(bypared)4          1.8902     0.7198   2.626 0.008646 ** \nfactor(bypared)5          1.5059     0.7200   2.091 0.036501 *  \nfactor(bypared)6          1.4527     0.7386   1.967 0.049235 *  \nfactor(bypared)7          2.0044     0.8286   2.419 0.015569 *  \nfactor(bypared)8          0.8190     0.9239   0.887 0.375360    \nlowinc                    2.0347     0.8112   2.508 0.012140 *  \nfactor(bypared)2:lowinc  -2.9955     0.9298  -3.222 0.001278 ** \nfactor(bypared)3:lowinc  -4.0551     1.0682  -3.796 0.000147 ***\nfactor(bypared)4:lowinc  -4.8143     1.1126  -4.327 1.52e-05 ***\nfactor(bypared)5:lowinc  -4.6890     1.0947  -4.283 1.85e-05 ***\nfactor(bypared)6:lowinc  -4.5252     1.0556  -4.287 1.82e-05 ***\nfactor(bypared)7:lowinc  -7.2222     1.3796  -5.235 1.67e-07 ***\nfactor(bypared)8:lowinc  -9.8773     1.6110  -6.131 8.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 15219 degrees of freedom\n  (924 observations deleted due to missingness)\nMultiple R-squared:  0.1948,    Adjusted R-squared:  0.194 \nF-statistic: 230.2 on 16 and 15219 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nPolynomials\nTo add quadratic and other polynomial terms to the model, use the I() function, which lets you raise the term to the power you want in the regression using the caret (^) operator. In the model below, we add a quadratic version of the reading score to the right-hand side.\n\n## add polynomials\nfit &lt;- lm(bynels2m ~ bynels2r + I(bynels2r^2), data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = bynels2m ~ bynels2r + I(bynels2r^2), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.462  -5.947  -0.156   5.780  46.645 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   12.7765815  0.6241194  20.471   &lt;2e-16 ***\nbynels2r       1.1197116  0.0447500  25.021   &lt;2e-16 ***\nI(bynels2r^2) -0.0006246  0.0007539  -0.828    0.407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.921 on 15881 degrees of freedom\n  (276 observations deleted due to missingness)\nMultiple R-squared:  0.5658,    Adjusted R-squared:  0.5657 \nF-statistic: 1.035e+04 on 2 and 15881 DF,  p-value: &lt; 2.2e-16\n\n\n\nQuick exercise\nFit a linear model with both interactions and a polynomial term. Then look at the model matrix to see what R did under the hood."
  },
  {
    "objectID": "12-Program-Stats.html#generalized-linear-model-for-binary-outcomes",
    "href": "12-Program-Stats.html#generalized-linear-model-for-binary-outcomes",
    "title": "I: Basic Statistical Models",
    "section": "Generalized linear model for binary outcomes",
    "text": "Generalized linear model for binary outcomes\nIn some cases when you have binary outcomes — 0/1 — it may be appropriate to continue using regular OLS, fitting what is typically called a linear probability model or LPM. In those cases, just use lm() as you have been.\nBut in other cases, you’ll want to fit a generalized linear model, in which case you’ll need to switch to the glm() function. It is set up just like lm(), but it has an extra argument, family. Set the argument to binomial() when your dependent variable is binary. By default, the link function is a logit link.\n\n## logit\nfit &lt;- glm(plan_col_grad ~ bynels2m + as_factor(bypared),\n           data = df,\n           family = binomial())\nsummary(fit)\n\n\nCall:\nglm(formula = plan_col_grad ~ bynels2m + as_factor(bypared), \n    family = binomial(), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6467  -0.9581   0.5211   0.7695   1.5815  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.824413   0.090000 -20.271  &lt; 2e-16 ***\nbynels2m                   0.056427   0.001636  34.491  &lt; 2e-16 ***\nas_factor(bypared)hsged    0.042315   0.079973   0.529   0.5967    \nas_factor(bypared)att2yr   0.204831   0.088837   2.306   0.0211 *  \nas_factor(bypared)grad2ry  0.480828   0.092110   5.220 1.79e-07 ***\nas_factor(bypared)att4yr   0.499019   0.090558   5.511 3.58e-08 ***\nas_factor(bypared)grad4yr  0.754817   0.084271   8.957  &lt; 2e-16 ***\nas_factor(bypared)ma       0.943558   0.101585   9.288  &lt; 2e-16 ***\nas_factor(bypared)phprof   1.052006   0.121849   8.634  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 17545  on 15235  degrees of freedom\nResidual deviance: 15371  on 15227  degrees of freedom\n  (924 observations deleted due to missingness)\nAIC: 15389\n\nNumber of Fisher Scoring iterations: 4\n\n\nIf you want a probit model, just change the link to probit.\n\n## probit\nfit &lt;- glm(plan_col_grad ~ bynels2m + as_factor(bypared),\n           data = df,\n           family = binomial(link = \"probit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = plan_col_grad ~ bynels2m + as_factor(bypared), \n    family = binomial(link = \"probit\"), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7665  -0.9796   0.5238   0.7812   1.5517  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.0522131  0.0539072 -19.519  &lt; 2e-16 ***\nbynels2m                   0.0326902  0.0009357  34.938  &lt; 2e-16 ***\nas_factor(bypared)hsged    0.0325415  0.0488225   0.667   0.5051    \nas_factor(bypared)att2yr   0.1316456  0.0539301   2.441   0.0146 *  \nas_factor(bypared)grad2ry  0.2958810  0.0554114   5.340 9.31e-08 ***\nas_factor(bypared)att4yr   0.3065176  0.0544813   5.626 1.84e-08 ***\nas_factor(bypared)grad4yr  0.4553127  0.0505009   9.016  &lt; 2e-16 ***\nas_factor(bypared)ma       0.5525198  0.0588352   9.391  &lt; 2e-16 ***\nas_factor(bypared)phprof   0.6115358  0.0688820   8.878  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 17545  on 15235  degrees of freedom\nResidual deviance: 15379  on 15227  degrees of freedom\n  (924 observations deleted due to missingness)\nAIC: 15397\n\nNumber of Fisher Scoring iterations: 4\n\n\nNote that the interpretation of parameters in logit and probit models differs from that in linear model with a normal / gaussian link function. For example, a one-unit change for a parameter in a logistic regression is associated with a change in log odds of the outcome, \\[\nlog(\\frac{p}{1-p})\n\\] , holding all other values at some fixed value (their means, for example). If this doesn’t sound particularly interpretable, it’s not! This is one reason people continue to use linear probability models (LPMs) rather than logits/probits (there are other reasons too). As always, make the choice that best fits your data, research questions, and intended audience.\n\nQuick exercise\nFit a logit or probit model to another binary outcome."
  },
  {
    "objectID": "12-Program-Stats.html#using-survey-weights-in-a-regression",
    "href": "12-Program-Stats.html#using-survey-weights-in-a-regression",
    "title": "I: Basic Statistical Models",
    "section": "Using survey weights in a regression",
    "text": "Using survey weights in a regression\nWe spent time in the Inferential I lesson on setting up a data frame that accounted for survey weights. Review that if need a reminder of the intuition.\nImportant for this lesson is that you can (and should!) use survey weights in a regression framework. As a reminder, here’s the information you need to set up the svydesign() that accounts for ELS’s complex sampling design:\n\nids are the primary sampling units or psus\n\nstrata are indicated by the strat_ids\n\nweight is the base-year student weight or bystuwt\ndata is our data frame object, df\nnest = TRUE because the psus are nested in strat_ids\n\nBefore running our survey-weighted regression, we’ll once again set up our data in a new object.\n\n## subset data\nsvy_df &lt;- df %&gt;%\n    select(psu,                         # primary sampling unit\n           strat_id,                    # stratum ID\n           bystuwt,                     # weight we want to use\n           bynels2m,                    # variables we want...\n           moth_ba,\n           fath_ba,\n           par_ba,\n           byses1,\n           lowinc,\n           female) %&gt;%\n    ## go ahead and drop observations with missing values\n    drop_na()\n\n## set svy design data\nsvy_df &lt;- svydesign(ids = ~psu,\n                    strata = ~strat_id,\n                    weight = ~bystuwt,\n                    data = svy_df,\n                    nest = TRUE)\n\nNow that we’ve done that, here’s how we run a weighted regression. Notice that it’s svyglm(), even though we used lm() before (the default \"link\" function in glm() is gaussian or normal; if we had binary outcomes and wanted to use a logit link then we could include family = binomial() as we did before).\n\n## fit the svyglm regression and show output\nsvyfit &lt;- svyglm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,\n                 design = svy_df)\nsummary(svyfit)\n\n\nCall:\nsvyglm(formula = bynels2m ~ byses1 + female + moth_ba + fath_ba + \n    lowinc, design = svy_df)\n\nSurvey design:\nsvydesign(ids = ~psu, strata = ~strat_id, weight = ~bystuwt, \n    data = svy_df, nest = TRUE)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.2221     0.2710 166.867  &lt; 2e-16 ***\nbyses1        6.9470     0.2913  23.849  &lt; 2e-16 ***\nfemale       -1.0715     0.2441  -4.389 1.47e-05 ***\nmoth_ba       0.6633     0.3668   1.809   0.0713 .  \nfath_ba       0.5670     0.3798   1.493   0.1363    \nlowinc       -2.4860     0.3644  -6.822 3.49e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 150.5809)\n\nNumber of Fisher Scoring iterations: 2\n\n\nAs when compared t.test results when using unweighted and weighted data, our regression results are little different when using the weights. As a reminder, there’s no single quick answer to using weights in a regression framework. It’s up to you and your investigation of the code book to decide:\n\nthe appropriate weights to choose (there are usually many options!)\nwhat weights mean in the context of missing data\nwhat weights mean in a complex research design\n\n\nPredictions\nBeing able to generate predictions from new data can be a powerful tool. Above, we were able to return the predicted values from the fit object. We can also use the predict() function to return the standard error of the prediction in addition to the predicted values for new observations.\nFirst, we’ll get predicted values using the original data along with their standard errors.\n\n## predict from first model\nfit &lt;- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,\n          data = df)\n\n## old data\nfit_pred &lt;- predict(fit, se.fit = TRUE)\n\n## show options\nnames(fit_pred)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n\nhead(fit_pred$fit)\n\n       1        2        3        4        5        6 \n42.86583 48.51465 38.78234 36.98010 32.82855 38.43332 \n\nhead(fit_pred$se.fit)\n\n[1] 0.1755431 0.2587681 0.2314676 0.2396327 0.2737971 0.2721818\n\n\nWith the standard errors, we can get a better feel for how much faith we want to put in our predictions. If the standard errors are low, then maybe we feel more secure than if the errors are large. Alternatively, perhaps the errors are uniformly lower for some parts of our data than others. If so, that might suggest more investigation or a note on the limitations of our predictions for parts of the sample.\n\nTwo other types of predictions\nWe won’t practice these since in application they work similarly to what we did above. However, I do want to note two other types of predictions that you might want to make. Both involve making predictions for data that you didn’t use to fit your model. Predictions using new data or held-out data in a train/test framework are another way to evaluate your model. If you predict well to new/held-out data, that can be a good sign for the utility of your model.\n\nPredictions with new data\nIdeally, we would have a new observations with which to make predictions. Then we could test our modeling choices by seeing how well they predicted the outcomes of the new observations.\nWith discrete outcomes (like binary 0/1 data), for example, we could use our model and right-hand side variables from new observations to predict whether the new observation should have a 0 or 1 outcome. Then we could compare those predictions to the actual observed outcomes by making a 2 by 2 confusion matrix that counted the numbers of true positives and negatives (correct predictions) and false positives and negatives (incorrect predictions).\nWith continuous outcomes, we could follow the same procedure as above, but rather than using a confusion matrix, instead assess our model performance by measuring the error between our predictions and the observed outcomes. Depending on our problem and model, we might care about minimizing the root mean square error, the mean absolute error, or some other metric of the error.\n\n\nPredictions using training and testing data\nIn the absence of new data, we instead could have separated our data into two data sets, a training set and test set. After fitting our model to the training data, we could have tested it by following either above procedure with the testing data (depending on the outcome type). Setting a rule for ourselves, we could evaluate how well we did, that is, how well our training data model classified test data outcomes, and perhaps decide to adjust our modeling assumptions. This is a fundamental way that many machine learning algorithm assess fit.\n\n\n\n\nMargins\nUsing the predict() function alongside some other skills we have practiced, we can also make predictions on the margin a la Stata’s -margins- suite of commands.\nFor example, after fitting our multiple regression, we might ask ourselves, what is the marginal association of coming from a family with low income on math scores, holding all other terms in our model constant? In other words, if student A and student B are similar along dimensions we can observe (let’s say the average student in our sample) except for the fact that student A’s family is considered lower income and student B’s is not, what if any test score difference might we expect?\nTo answer this question, we first need to make a “new” data frame with a column each for the variables used in the model and rows that equal the number of predictive margins that we want to create. In our example, that means making a data frame with two rows and five columns.\nWith lowinc, the variable that we want to make marginal predictions for, we have two potential values: 0 and 1. This is the reason our “new” data frame has two rows. If lowinc took on four values, for example, then our “new” data frame would have four rows, one for each potential value. But since we have two, lowinc in our “new” data frame will equal 0 in one row and 1 in the other row.\nAll other columns in the “new” data frame should have consistent values down their rows. Often, each column’s repeated value is the variable’s average in the data. Though we could use the original data frame (df) to generate these averages, the resulting values may summarize different data from what was used to fit the model if there were observations that lm() dropped due to missing values. That happened with our model. We could try to use the original data frame and account for dropped observations, but I think it’s easier to use the design matrix that’s retrieved from model.matrix().\nThe code below goes step-by-step to make the “new” data frame.\n\n## create new data that has two rows, with averages and one marginal change\n\n## (1) save model matrix\nmm &lt;- model.matrix(fit)\nhead(mm)\n\n  (Intercept) byses1 female moth_ba fath_ba lowinc\n1           1  -0.25      1       0       0      0\n2           1   0.58      1       0       0      0\n3           1  -0.85      1       0       0      0\n4           1  -0.80      1       0       0      1\n5           1  -1.41      1       0       0      1\n6           1  -1.07      0       0       0      0\n\n## (2) drop intercept column of ones (predict() doesn't need them)\nmm &lt;- mm[,-1]\nhead(mm)\n\n  byses1 female moth_ba fath_ba lowinc\n1  -0.25      1       0       0      0\n2   0.58      1       0       0      0\n3  -0.85      1       0       0      0\n4  -0.80      1       0       0      1\n5  -1.41      1       0       0      1\n6  -1.07      0       0       0      0\n\n## (3) convert to data frame so we can use $ notation in next step\nmm &lt;- as_tibble(mm)\n\n## (4) new data frame of means where only lowinc changes\nnew_df &lt;- tibble(byses1 = mean(mm$byses1),\n                 female = mean(mm$female),\n                 moth_ba = mean(mm$moth_ba),\n                 fath_ba = mean(mm$fath_ba),\n                 lowinc = c(0,1))\n\n## see new data\nnew_df\n\n# A tibble: 2 × 5\n  byses1 female moth_ba fath_ba lowinc\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0421  0.503   0.274   0.320      0\n2 0.0421  0.503   0.274   0.320      1\n\n\nNotice how the new data frame has the same terms that were used in the original model, but has only two rows. In the lowinc column, the values switch from 0 to 1. All the other rows are averages of the data used to fit the model. This of course makes for a somewhat nonsensical average (what does it mean that a single father to have .32 of a BA/BS?), but that’s okay. Again, what we want right now are two students who represent the “average” student except one is low income and the other is not.\nTo generate the prediction, we use the same function call as before, but use our new_df object with the newdata argument.\n\n## predict margins\npredict(fit, newdata = new_df, se.fit = TRUE)\n\n$fit\n       1        2 \n45.82426 43.68173 \n\n$se.fit\n        1         2 \n0.1166453 0.2535000 \n\n$df\n[1] 15230\n\n$residual.scale\n[1] 12.24278\n\n\nOur results show that compared to otherwise similar students, those with a family income less than $25,000 a year are predicted to score about two points lower on their math test. To be clear, this is not a casual estimate, but rather associational. This means we would be wrong to say that having a lower family income causes lower math test scores (which doesn’t jibe with our domain knowledge either; low income is almost certainly a proxy for other omitted variables — access to educational resources for just one thing — that are much more directly linked to test scores).\nI also want to note that we held the other covariates at their means. We could have instead chosen other values (e.g. fath_ba == 1 or female == 1), which would have given us different marginal associations. Dropping or including other covariates likely would change our results, as well. The takeaway is that is that margins you compute are a function of your model as well as (obviously) the margin you investigate."
  },
  {
    "objectID": "99-Final-Project.html",
    "href": "99-Final-Project.html",
    "title": "Final Project",
    "section": "",
    "text": "ProposalInitial AnalysesDraft ReportPresentationFinal ReportRubric\n\n\nFrom the syllabus:\n\nFor your final project, you must produce a 3-5 page report on a higher education topic of interest. The report should be a combination of writing, tables, and figures, have minimal citations (if any), and be fully reproducible with minimal effort. You must use data that is either publicly available or that can be shared with others (no IRB restrictions). Everyone will submit three preliminary assignments in addition to the final report.\n\nThis assignment represents the first of those three preliminary assignments.\nFor your final project proposal, I need the following in a cleanly formatted Markdown (.md) file:\n\nA paragraph describing your project:\n\nWhat will you be predicting?\nWhy is it interesting?\n\nA description of where you will find this data.\nA few lines describing your dependent variable in detail. What is the nature of this outcome as a variable?\n\n\nSubmission details\n\nSave your file as &lt;name&gt;_proposal.md where your last name replaces &lt;name&gt; (e.g. skinner_proposal.md) in your final_project folder\nPush to your GitHub repo prior to the start of the class in which it is due. If you are unsure whether you have successfully pushed your changes, check the online version of your repo at GitHub.com. If you can see your changes there, I can see them too.\n\n\n\n\nFrom the syllabus:\n\nFor your final project, you must produce a 3-5 page report on a higher education topic of interest. The report should be a combination of writing, tables, and figures, have minimal citations (if any), and be fully reproducible with minimal effort. You must use data that is either publicly available or that can be shared with others (no IRB restrictions). Everyone will submit three preliminary assignments in\n\naddition to the final report.\nThis assignment represents the second of those three preliminary assignments.\nFor your initial analyses, I need the following in either a cleanly formatted R (.R) script or RMarkdown (.Rmd) file:\n\nCode that:\n\nReads in the data set that contains your dependent variable\n[If appropriate] Converts missing values to NA\n[If appropriate] Reshapes data\n[If appropriate] Joins data\n\n3 of the 4 following plots:\n\nA univariate graphic describing the dependent variable (e.g. histogram)\nThe conditional mean of your dependent variable at levels of at least one independent variable (e.g. box-and-whisker)\nThe distribution of your dependent variable at levels of at least one independent variable (e.g. grouped density plot, faceted density plot)\nA bivariate graphic that compares your dependent variable with at least one other independent variable (e.g. scatter plot)\n\n\n\nSubmission details\n\nSave your file as:\n\n&lt;name&gt;_analyses.R (if submitting an R script)\n&lt;name&gt;_analyses.Rmd (if submitting an Rmd file)\n\nwhere your last name replaces &lt;name&gt; (e.g. skinner_analyses.R or skinner_analyses.Rmd) in your final_project folder.\nPush to your GitHub repo prior to the start of the class in which it is due. If you are unsure whether you have successfully pushed your changes, check the online version of your repo at GitHub.com. If you can see your changes there, I can see them too.\n\n\n\n\nFrom the syllabus:\n\nFor your final project, you must produce a 3-5 page report on a higher education topic of interest. The report should be a combination of writing, tables, and figures, have minimal citations (if any), and be fully reproducible with minimal effort. You must use data that is either publicly available or that can be shared with others (no IRB restrictions). Everyone will submit three preliminary assignments in addition to the final report.\n\nThis assignment represents the third of those three preliminary assignments.\nFor your draft report, I need the following in a cleanly formatted RMarkdown (.Rmd) file:\n\nWell commented code that:\n\nReads in your raw data set\nPerforms all necessary data wrangling tasks to clean, join, and reshape your data as necessary for your project\nPerforms all necessary analyses\n\nPlots (as necessary, but required) to show the results of your analyses\nTables (as necessary, but not required) to show the results of your analyses\nFirst draft of the written elements of your report; these elements should:\n\nMotivate your analyses\nDescribe your data\nDescribe your methodological approach, in high-level, audience-appropriate manner (think your boss or colleagues, not me specifically)\nDescribe your findings\nAppropriately contextualize your results (e.g. limitations, validity) and make appropriate recommendations\n[If necessary] Include citations\nBe well organized\n\n\nPlease also submit your raw data files so that I can run your script. If your files are very large (too large push to GitHub, contact me so that we can figure out another way for you to get your data to me.\n\nSubmission details\n\nSave your file as: &lt;name&gt;_draft_report.Rmd (if submitting an Rmd file) where your last name replaces &lt;name&gt; (e.g. skinner_draft_report.Rmd) in your final_project folder.\nSubmit data files (zipped if necessary)\nPush to your GitHub repo prior to the start of the class in which it is due. If you are unsure whether you have successfully pushed your changes, check the online version of your repo at GitHub.com. If you can see your changes there, I can see them too.\n\n\n\n\n\n\n\nFrom the syllabus:\n\nFor your final project, you must produce a 3-5 page report on a higher education topic of interest. The report should be a combination of writing, tables, and figures, have minimal citations (if any), and be fully reproducible with minimal effort. You must use data that is either publicly available or that can be shared with others (no IRB restrictions). Everyone will submit three preliminary assignments in addition to the final report.\n\nThis assignment represents the final report.\nFor your final report, I need:\n\nA clearly formatted RMarkdown (.Rmd) file that will build your report\nEither:\n\nA copy of your data\nVery clear instructions on how to get your data\n\n\nYour report will be graded according to the final report rubric.\n\nSubmission details\n\nSave your file as: &lt;name&gt;_final_report.Rmd (if submitting an Rmd file) where your last name replaces &lt;name&gt; (e.g. skinner_final_report.Rmd) in your final_project folder.\nPush to your GitHub repo prior to the start of the class in which it is due. If you are unsure whether you have successfully pushed your changes, check the online version of your repo at GitHub.com. If you can see your changes there, I can see them too.\n\n\n\n\nThere are five major areas to the final project:\n\nData Analysis\nGraphical/Tabular Presentation\nWritten Description\nOrganization, Clarity, and Formatting\nCoding\n\n\nData Analysis\n\nA strong final project will have a data analysis that cleanly wrangles raw data into an analysis-ready data set; correctly performs all descriptive and statistical analyses necessary to answer the research question; and well presents the results. The analyses do not necessarily have to be complex, but they should represent the best reasonable approach.\nAn acceptable final project will have a data analysis that includes some some mistakes or inaccuracies in how the data are cleaned, analyses are run, and/or results are presented. The analyses may not be quite appropriate for the analytic task.\nA weak final project data analysis will not correctly wrangle raw data into a clean data set, will not well describe relationships between predictors and the outcome, will not use appropriate analytic tools, and will not present results well or at all. In general, the analysis will be messy and unable to provide insights into the research questions/problems motivated in the report.\n\n\n\nGraphical/Tabular Presentation\n\nA strong final project will include nicely labeled, easy to understand graphics that describe exactly what is happening with the patterns in the data. The graphics may be simple or complex, but they clearly connect to the analysis (e.g. not just a figure for the sake of a figure). The response could include (but doesn’t have to include) interactive graphics. A table or two may be included, but only sparingly and in a clear format.\nAn acceptable final project will include graphics, but these figures may not be easy to read, may not be sufficiently detailed, or may not represent the most appropriate way to show the relationships in the data. A table or two may be included, but not appropriately formatted or without a clear rationale for its inclusion (i.e. why a table and not a figure).\nA weak final project will include graphics and/or tables that are poorly labeled and don’t make much sense.\n\n\n\nWritten Description\n\nA strong final project will include clear and concise written sections that are easily understandable by an interested layperson. Assume that your audience is your boss or a colleague—not me.\nAn acceptable final project will be written generally well, but technical details may be poorly described or not described at all, and sentences will be hard to follow.\nA weak final project will be poorly written, with many mistakes regarding both the analysis and good writing practices.\n\n\n\nOrganization, Clarity, Formatting\n\nA strong final project will have an .Rmd file that generates a very nicely formatted document, suitable for professional presentation. What kind of report would you want to give to a supervisor or have given to you? That’s what I want back from you. The organization should be very clear and easy to understand.\nAn acceptable final project will have some formatting problems and may not look very nice.\nA weak final project will include code chunks in the output, poor formatting, and in general will just be messy.\n\n\n\nCoding\n\nA strong final project will have code that can generate results from the raw data in an easy to understand way. The code will be commented and will run on my computer without my having to tweak it in any way. (NOTE: An easy test on your end is transfer your files to a new location with the appropriate directory structure and attempt to knit the document)\nAn acceptable final project will have code that is relatively clear, but that may not be commented in ways that make sense and that has some problems that require debugging on my end.\nA weak final project will have code that is messy, hard to understand and not commented. It will not run on my computer, and cannot be easily debugged."
  },
  {
    "objectID": "08-Data-Wrangling-III.html#setup",
    "href": "08-Data-Wrangling-III.html#setup",
    "title": "III: Working with strings & dates",
    "section": "Setup",
    "text": "Setup\nAs before, we’ll continue working within the tidyverse. We’ll focus, however, on using two specific libraries:\n\nstringr for strings\nlubridate for dates\n\nYou may have noticed already that when we load the tidyverse library with library(tidyverse), the stringr library is already loaded. The lubridate library, though part of the tidyverse, is not. We need to load it separately.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NB: The stringr library is loaded with tidyverse, but\n## lubridate is not, so we need to load it separately\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\n\nNB: As we have done in the past few lessons, we’ll run this script assuming that our working directory is set to the scripts directory."
  },
  {
    "objectID": "08-Data-Wrangling-III.html#part-1-working-with-strings",
    "href": "08-Data-Wrangling-III.html#part-1-working-with-strings",
    "title": "III: Working with strings & dates",
    "section": "Part 1: Working with strings",
    "text": "Part 1: Working with strings\nTo practice working with strings, we’ll use data from Integrated Postsecondary Education Data System (IPEDS):\n\nThe National Center for Education Statistics (NCES) administers the Integrated Postsecondary Education Data System (IPEDS), which is a large-scale survey that collects institution-level data from postsecondary institutions in the United States (50 states and the District of Columbia) and other U.S. jurisdictions. IPEDS defines a postsecondary institution as an organization that is open to the public and has the provision of postsecondary education or training beyond the high school level as one of its primary missions. This definition includes institutions that offer academic, vocational and continuing professional education programs and excludes institutions that offer only avocational (leisure) and adult basic education programs. Definitions for other terms used in this report may be found in the IPEDS online glossary.\nNCES annually releases national-level statistics on postsecondary institutions based on the IPEDS data. National statistics include tuition and fees, number and types of degrees and certificates conferred, number of students applying and enrolled, number of employees, financial statistics, graduation rates, student outcomes, student financial aid, and academic libraries.\n\nYou can find more information about IPEDS here. As higher education scholars, IPEDS data are a valuable resource that you may often turn to (I do).\nWe’ll use one file (which can be found here), that covers institutional characteristics for one year:\n\nDirectory information, 2007 (hd2007.csv)\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data and lower all names using rename_all(tolower)\ndf &lt;- read_csv(file.path(\"data\", \"hd2007.csv\")) %&gt;%\n    rename_all(tolower)\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "08-Data-Wrangling-III.html#finding-str_detect",
    "href": "08-Data-Wrangling-III.html#finding-str_detect",
    "title": "III: Working with strings & dates",
    "section": "Finding: str_detect()",
    "text": "Finding: str_detect()\nSo far, we’ve filtered data using dplyr’s filter() verb. When matching a string, we have used == (or != for negative match). For example, if we wanted to limit our data to only those institutions in Florida, we could filter using the stabbr column:\n\n## filter using state abbreviation (not saving, just viewing)\ndf %&gt;%\n    filter(stabbr == \"FL\")\n\n# A tibble: 316 × 59\n   unitid instnm     addr  city  stabbr zip    fips obereg chfnm chfti…¹ gentele\n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 132268 Wyotech-D… 470 … Ormo… FL     32174    12      5 Stev… Presid… 3.86e12\n 2 132338 The Art I… 1799… Fort… FL     3331…    12      5 Char… Presid… 9.54e13\n 3 132374 Atlantic … 4700… Coco… FL     3306…    12      5 Robe… Direct… 7.54e 9\n 4 132408 The Bapti… 5400… Grac… FL     32440    12      5 Thom… Presid… 8.50e 9\n 5 132471 Barry Uni… 1130… Miami FL     3316…    12      5 Sist… Presid… 8.01e 9\n 6 132523 Gooding I… 615 … Pana… FL     32401    12      5 Dr. … CRNA P… 8.51e 9\n 7 132602 Bethune-C… 640 … Dayt… FL     3211…    12      5 Dr T… Presid… 3.86e 9\n 8 132657 Lynn Univ… 3601… Boca… FL     3343…    12      5 Kevi… Presid… 5.61e 9\n 9 132666 Bradenton… 5505… Brad… FL     34209    12      5 A. P… CEO     9.42e 9\n10 132675 Bradford-… 609 … Star… FL     32091    12      5 Rand… Direct… 9.05e 9\n# … with 306 more rows, 48 more variables: ein &lt;chr&gt;, opeid &lt;chr&gt;,\n#   opeflag &lt;dbl&gt;, webaddr &lt;chr&gt;, adminurl &lt;chr&gt;, faidurl &lt;chr&gt;, applurl &lt;chr&gt;,\n#   sector &lt;dbl&gt;, iclevel &lt;dbl&gt;, control &lt;dbl&gt;, hloffer &lt;dbl&gt;, ugoffer &lt;dbl&gt;,\n#   groffer &lt;dbl&gt;, fpoffer &lt;dbl&gt;, hdegoffr &lt;dbl&gt;, deggrant &lt;dbl&gt;, hbcu &lt;dbl&gt;,\n#   hospital &lt;dbl&gt;, medical &lt;dbl&gt;, tribal &lt;dbl&gt;, locale &lt;dbl&gt;, openpubl &lt;dbl&gt;,\n#   act &lt;chr&gt;, newid &lt;dbl&gt;, deathyr &lt;dbl&gt;, closedat &lt;chr&gt;, cyactive &lt;dbl&gt;,\n#   postsec &lt;dbl&gt;, pseflag &lt;dbl&gt;, pset4flg &lt;dbl&gt;, rptmth &lt;dbl&gt;, ialias &lt;chr&gt;, …\n\n\nThis works well because the stabbr column, even though it uses strings, is regular. But what happens when the strings aren’t so regular? For example, let’s look the different titles chief college administrators take.\n\n## see first few rows of distinct chief titles\ndf %&gt;%\n    distinct(chftitle)\n\n# A tibble: 556 × 1\n   chftitle          \n   &lt;chr&gt;             \n 1 Commandant        \n 2 President         \n 3 Chancellor        \n 4 Interim President \n 5 CEO               \n 6 Acting President  \n 7 Director          \n 8 President/CEO     \n 9 Interim Chancellor\n10 President/COO     \n# … with 546 more rows\n\n\nWe find over 500 unique titles. Just looking at the first 10 rows, we see that some titles are pretty similar — President vs. CEO vs. President/CEO — but not exactly the same. Let’s look again, but this time get counts of each distinct title and arrange from most common to least.\n\n## return the most common titles\ndf %&gt;%\n    ## get counts of each type\n    count(chftitle) %&gt;%\n    ## arrange in descending order so we see most popular at top\n    arrange(desc(n))\n\n# A tibble: 556 × 2\n   chftitle               n\n   &lt;chr&gt;              &lt;int&gt;\n 1 President           3840\n 2 Director             560\n 3 Chancellor           265\n 4 Executive Director   209\n 5 Owner                164\n 6 Campus President     116\n 7 Superintendent       105\n 8 CEO                   90\n 9 &lt;NA&gt;                  85\n10 Interim President     75\n# … with 546 more rows\n\n\n\nQuick exercise\nWhat do you notice about the data frames returned by distinct() and count()? What’s the same? What does count() do that distinct() does not?\n\nGetting our counts and arranging, we can see that President is by far the most common title. That said, we also see Campus President and Interim President (and before we saw Acting President as well).\nIf your research question asked, how many chief administrators use the title of “President”? regardless the various iterations, you can’t really use a simple == filter any more. In theory, you could inspect your data, find the unique versions, get counts of each of those using ==, and then sum them up — but that’s a lot of work and likely to be error prone!\nInstead, we can use the stringr function str_detect(), which looks for a pattern in a vector of strings:\nstr_detect(&lt; vector of strings &gt;, &lt; pattern &gt;)\nGoing item by item in the vector, it compares what it sees to the pattern. If it matches, then it returns TRUE; it not, then FALSE. Here’s a toy example:\n\n## string vector example\nfruits &lt;- c(\"green apple\", \"banana\", \"red apple\")\n\n## search for \"apple\", which should be true for the first and third item\nstr_detect(fruits, \"apple\")\n\n[1]  TRUE FALSE  TRUE\n\n\nWe can use str_detect() inside filter() to select only certain rows in our data frame. In our case, we want only those observations in which the title \"President\" occurs in the chftitle column. Because we’re only detecting, as long as \"President\" occurs anywhere in the title, we’ll get that row back.\n\n## how many use some form of the title president?\ndf %&gt;%\n    ## still starting with our count\n    count(chftitle) %&gt;%\n    ## ...but keeping only those titles that contain \"President\"\n    filter(str_detect(chftitle, \"President\")) %&gt;%\n    ## arranging as before\n    arrange(desc(n))\n\n# A tibble: 173 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# … with 163 more rows\n\n\nNow we’re seeing many more versions. We can even more clearly see a few titles that are almost certainly the same title, but were just inputted differently — President/CEO vs. President and CEO vs. President & CEO.\n\nQuick exercise\nIgnoring the sub-counts of the various versions, how many chief administrators have the word “President” in their title?\n\nSeeing the different versions of basically the same title should have us stopping to think: since it seems that this data column contains free form input (e.g. Input chief administrator title:), maybe we should allow for typos? The easiest: Is there any reason to assume that “President” will be capitalized?\n\nQuick exercise\nWhat happens if we search for “president” with a lowercase “p”?\n\nAh! We find a few stragglers. How can we restructure our filter so that we get these, too? There are at least two solutions.\n\n1. Use regular expressions\nRegular expressions (aka regex) are strings that use a special syntax to create patterns that can be used to match other strings. They are very useful when you need to match strings that have some general form, but may differ in specifics.\nWe already used this technique in the a prior lesson when we matched columns in the all_schools_wide.csv with contains(\"19\") so that we could pivot_longer(). Instead of naming all the columns specifically, we recognized that each column took the form of &lt;test&gt;_19&lt;YY&gt;. This is a type of regular expression.\nIn the tidyverse some of the stringr and tidyselect helper functions abstract-away some of the nitty-gritty behind regular expressions. Knowing a little about regular expression syntax, particularly how it is used in R, can go a long way.\nIn our first case, we can match strings that have a capital P President or lowercase p president using square brackets ([]). If we want either “P” or “p”, then we can use the regex, [Pp], in place of the first character: \"[Pp]resident\". This will match either \"President\" or \"president\".\n\n## solution 1: look for either P or p\ndf %&gt;%\n    count(chftitle) %&gt;%\n    filter(str_detect(chftitle, \"[Pp]resident\")) %&gt;%\n    arrange(desc(n))\n\n# A tibble: 175 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# … with 165 more rows\n\n\nThough we don’t see the new observations in the abbreviated output, we note that the number of rows has increased by two. This means that there are at least two title formats in which \"president\" is lowercase and that we weren’t picking up when we only used the uppercase version of \"President\" before.\n\n\n2. Put everything in the same case and match with that case\nAnother solution, which is probably much easier in this particular case, is to set all potential values in chftitle to the same case and then match using that case. In many situations, this is preferable since you don’t need to guess cases up front.\nWe won’t change the values in chftitle permanently — only while filtering. To compare apples to apples (rather than \"Apples\" to \"apples\"), we’ll wrap our column name with the function str_to_lower(), which will make character lowercase, and match using lowercase \"president\".\n\n## solution 2: make everything lowercase so that case doesn't matter\ndf %&gt;%\n    count(chftitle) %&gt;%\n    filter(str_detect(str_to_lower(chftitle), \"president\")) %&gt;%\n    arrange(desc(n))\n\n# A tibble: 177 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# … with 167 more rows\n\n\nWe recover another two titles when using this second solution. Clearly, our first solution didn’t account for other cases (perhaps “PRESIDENT\"?).\nIn general, I find it’s a good idea to try a solution like the second one before a more complicated one like the first. But because every problem is different, so too are the solutions. You may find yourself using a combination of the two.\n\nNot-so-quick exercise\nAnother chief title that was high on the list was “Owner.” How many institutions have an “Owner” as their chief administrator? Of these, how many are private, for-profit institutions (control == 3)? How many have the word “Beauty” in their name?\n\n\n\nReplace using string position: str_sub()\nIn addition to filtering data, we sometimes need to create new variables from pieces of exiting variables. For example, let’s look at the zip code values that are included in the file.\n\n## show first few zip code values\ndf %&gt;%\n    select(unitid, zip)\n\n# A tibble: 7,052 × 2\n   unitid zip       \n    &lt;dbl&gt; &lt;chr&gt;     \n 1 100636 36112-6613\n 2 100654 35762     \n 3 100663 35294-0110\n 4 100690 36117-3553\n 5 100706 35899     \n 6 100724 36101-0271\n 7 100733 35401     \n 8 100751 35487-0166\n 9 100760 35010     \n10 100812 35611     \n# … with 7,042 more rows\n\n\nWe can see that we have both regular 5 digit zip codes as well as those that include the extra 4 digits (ZIP+4). Let’s say we don’t need those last four digits for our analysis (particularly because not every school uses them anyway). Our task is to create a new column that pulls out only the main part of the zip code. It is has to work both for zip values that include the additional hyphen and 4 digits as well as those that only have the primary 5 digits to begin with.\nOne solution in this case is to take advantage of the fact that zip codes — minus the sometimes extra 4 digits — should be regular: 5 digits. If want the sub-part of a string and that sub-part is always in the same spot, we can use the function, str_sub(), which takes a string or column name first, and has arguments for the starting and ending character that mark the sub-string of interest.\nIn our case, we want the first 5 digits so we should start == 1 and end == 5:\n\n## pull out first 5 digits of zip code\ndf &lt;- df %&gt;%\n    mutate(zip5 = str_sub(zip, start = 1, end = 5))\n\n## show (use select() to subset so we can set new columns)\ndf %&gt;%\n    select(unitid, zip, zip5)\n\n# A tibble: 7,052 × 3\n   unitid zip        zip5 \n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 100636 36112-6613 36112\n 2 100654 35762      35762\n 3 100663 35294-0110 35294\n 4 100690 36117-3553 36117\n 5 100706 35899      35899\n 6 100724 36101-0271 36101\n 7 100733 35401      35401\n 8 100751 35487-0166 35487\n 9 100760 35010      35010\n10 100812 35611      35611\n# … with 7,042 more rows\n\n\nA quick visual inspection of the first few rows shows that our str_sub() function performed as expected (for a real analysis, you’ll want to do more formal checks).\n\n\nReplace using regular expressions: str_replace()\nWe can also use a more sophisticated regex pattern with the function str_replace(). The pieces of our regex pattern, \"([0-9]+)(-[0-9]+)?\", are translated as this:\n\n[0-9] := any digit, 0 1 2 3 4 5 6 7 8 9\n+ := match the preceding one or more times\n? := match the preceding 0 or more times\n() := subexpression\n\nPut together, we have:\n\n([0-9]+) := first, look for 1 or more digits\n(-[0-9]+)? := second, look for a hyphen and one or more digits, but you may not find any of that\n\nBecause we used parentheses, (), to separate our subexpressions, we can call them using their numbers (in order) in the last argument of str_replace():\n\n\"\\\\1\" := return the first subexpression\n\nSo what’s happening? If given a zip code that is \"32605\", the regex pattern will collect each digit — \"3\" \"2\" \"6\" \"0\" \"5\" — into the first subexpression because it never sees a hyphen. That first subexpression, \"\\\\1\", is returned: \"32605\". That’s what we want.\nIf given \"32605-1234\", it will collect the first 5 digits in the first subexpression, but will stop adding characters there when it sees the hyphen. From then on out, it adds everything it sees the second subexpression: \"-\" \"1\" \"2\" \"3\" \"4\". But because str_replace() only returns the first subexpression, we still get the same answer: \"32605\". This is what we want.\nLet’s try it on the data.\n\n## drop last four digits of extended zip code if they exist\ndf &lt;- df %&gt;%\n    mutate(zip5_v2 = str_replace(zip, \"([0-9]+)(-[0-9]+)?\", \"\\\\1\"))\n\n## show (use select() to subset so we can set new columns)\ndf %&gt;%\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 7,052 × 4\n   unitid zip        zip5  zip5_v2\n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  \n 1 100636 36112-6613 36112 36112  \n 2 100654 35762      35762 35762  \n 3 100663 35294-0110 35294 35294  \n 4 100690 36117-3553 36117 36117  \n 5 100706 35899      35899 35899  \n 6 100724 36101-0271 36101 36101  \n 7 100733 35401      35401 35401  \n 8 100751 35487-0166 35487 35487  \n 9 100760 35010      35010 35010  \n10 100812 35611      35611 35611  \n# … with 7,042 more rows\n\n\n\nQuick exercise\nWhat if you wanted to the get the last 4 digits (after the hyphen)? What bit of two bits of code above would you change so that you can store the last 4 digits without including the hyphen? Make a new variable called zip_plus4 and store these values. HINT Look at the help file for str_replace().\n\nLet’s compare our two versions: do we get the same results?\n\n## check if both versions of new zip column are equal\nidentical(df %&gt;% select(zip5), df %&gt;% select(zip5_v2))\n\n[1] FALSE\n\n\nNo! Let’s see where they are different:\n\n## filter to rows where zip5 != zip5_v2 (not storing...just looking)\ndf %&gt;%\n    filter(zip5 != zip5_v2) %&gt;%\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 4 × 4\n  unitid zip        zip5  zip5_v2   \n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     \n1 108199 90015--350 90015 90015--350\n2 113953 92113--191 92113 92113--191\n3 431707 06360--709 06360 06360--709\n4 435240 551012595  55101 551012595 \n\n\n\nQuick exercise\nWhat happened? In this scenario, which string subsetting technique worked better?\n\nDepending on the task, regular expressions can either feel like a blessing or a curse. To be honest, I’ve spent more time cursing than thanking them. That said, regular expressions are often the only way to perform a data wrangling task on unstructured string data. They are also a cornerstone of natural language processing techniques, which are increasingly of interest to education researchers.\nWe’ve only scratched the surface of what regular expressions can do. If you face string data in the future, taking a little time to craft a regular expression can be well worth it."
  },
  {
    "objectID": "08-Data-Wrangling-III.html#part-2-working-with-dates",
    "href": "08-Data-Wrangling-III.html#part-2-working-with-dates",
    "title": "III: Working with strings & dates",
    "section": "Part 2: Working with dates",
    "text": "Part 2: Working with dates\nIn opening section, we’ve seen that dates often come in many different formats. While you can format and clean them using regular expressions, you may also want to format them such that R knows they are dates.\nWhy?\nWhen dealing with something straightforward like years, it’s easy enough to store the years a regular numbers and then subtract the recent year from the past year to get a duration: 2020 - 2002 equals 18 years.\nBut what if you have daily data for the school year and you want to know how many days a student had between a first and second test? What if the differences were more than a month of days and every student took the first and second tests on different days? What if you had a panel data set, with students across years, some of which were leap years? You can see how calculating the exact number days between tests for each student could quickly become difficult if trying to do it using regular numerical values.\nR makes this easier by having special time-based data types that will keep track of these issues for us and allow us to work with dates almost as we do with regular numbers.\nIn our IPEDS data set, we can see that few institutions closed in 2007 and 2008. We’ll limit our next analyses to these institutions.\n\n## subset to schools who closed during this period\ndf &lt;- df %&gt;%\n    filter(closedat != -2)\n\n## show first few rows\ndf %&gt;% select(unitid, instnm, closedat)\n\n# A tibble: 83 × 3\n   unitid instnm                                                  closedat\n    &lt;dbl&gt; &lt;chr&gt;                                                   &lt;chr&gt;   \n 1 103440 Sheldon Jackson College                                 6/29/07 \n 2 104522 DeVoe College of Beauty                                 3/29/08 \n 3 105242 Mundus Institute                                        Sep-07  \n 4 105880 Long Technical College-East Valley                      3/31/07 \n 5 119711 New College of California                               Jan-08  \n 6 136996 Ross Medical Education Center                           7/31/07 \n 7 137625 Suncoast II the Tampa Bay School of Massage Therapy LLC 5/31/08 \n 8 141583 Hawaii Business College                                 Sep-07  \n 9 150127 Ball Memorial Hospital School of Radiologic Technology  May-07  \n10 160144 Pat Goins Shreveport Beauty School                      3/1/08  \n# … with 73 more rows\n\n\nWe can see that closedat is stored as a string. Based on our domain knowledge and context clues, we know that the dates are generally in a MM/DD/YYYY (American) format.\nWe can use the lubridate command mdy() to make a new variable that contains the same information, but in a format that R recognizes as a date.\n\n## create a new close date column \ndf &lt;- df %&gt;%\n    mutate(closedat_dt = mdy(closedat))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `closedat_dt = mdy(closedat)`.\nCaused by warning:\n!  35 failed to parse.\n\n## show\ndf %&gt;% select(starts_with(\"close\"))\n\n# A tibble: 83 × 2\n   closedat closedat_dt\n   &lt;chr&gt;    &lt;date&gt;     \n 1 6/29/07  2007-06-29 \n 2 3/29/08  2008-03-29 \n 3 Sep-07   NA         \n 4 3/31/07  2007-03-31 \n 5 Jan-08   NA         \n 6 7/31/07  2007-07-31 \n 7 5/31/08  2008-05-31 \n 8 Sep-07   NA         \n 9 May-07   NA         \n10 3/1/08   2008-03-01 \n# … with 73 more rows\n\n\nWell, we are part of the way there. It seems that mdy() didn’t really work with dates like Sep-2007. What can we do?\nOne solution is to add in a fake day for the ones that didn’t parse and then convert using mdy(). We’ll use regular expressions with an str_replace().\n\n## convert MON-YYYY to MON-01-YYYY\ndf &lt;- df %&gt;%\n    mutate(closedat_fix = str_replace(closedat, \"-\", \"-01-\"),\n           closedat_fix_dt = mdy(closedat_fix))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `closedat_fix_dt = mdy(closedat_fix)`.\nCaused by warning:\n!  7 failed to parse.\n\n## show\ndf %&gt;% select(starts_with(\"close\"))                                \n\n# A tibble: 83 × 4\n   closedat closedat_dt closedat_fix closedat_fix_dt\n   &lt;chr&gt;    &lt;date&gt;      &lt;chr&gt;        &lt;date&gt;         \n 1 6/29/07  2007-06-29  6/29/07      2007-06-29     \n 2 3/29/08  2008-03-29  3/29/08      2008-03-29     \n 3 Sep-07   NA          Sep-01-07    2007-09-01     \n 4 3/31/07  2007-03-31  3/31/07      2007-03-31     \n 5 Jan-08   NA          Jan-01-08    2008-01-01     \n 6 7/31/07  2007-07-31  7/31/07      2007-07-31     \n 7 5/31/08  2008-05-31  5/31/08      2008-05-31     \n 8 Sep-07   NA          Sep-01-07    2007-09-01     \n 9 May-07   NA          May-01-07    2007-05-01     \n10 3/1/08   2008-03-01  3/1/08       2008-03-01     \n# … with 73 more rows\n\n\n\nQuick exercise\nWe had 7 parsing errors. Can you figure out which rows failed to parse and guess why? HINT if mdy() failed to parse closedat, then the subsequent new columns are likely missing values.\n\nNow that we’ve successfully converted the string date into a proper date type, it’s easy to pull out the pieces of that date, including:\n\nyear with year()\nmonth with month()\nday with day()\nday of week with wday()\n\n\n## add columns for\n## - year\n## - month\n## - day\n## - day of week (dow)\ndf &lt;- df %&gt;%\n    mutate(close_year = year(closedat_fix_dt),\n           close_month = month(closedat_fix_dt),\n           close_day = day(closedat_fix_dt),\n           close_dow = wday(closedat_fix_dt, label = TRUE))\n## show\ndf %&gt;%\n    select(closedat_fix_dt, close_year, close_month, close_day, close_dow)\n\n# A tibble: 83 × 5\n   closedat_fix_dt close_year close_month close_day close_dow\n   &lt;date&gt;               &lt;dbl&gt;       &lt;dbl&gt;     &lt;int&gt; &lt;ord&gt;    \n 1 2007-06-29            2007           6        29 Fri      \n 2 2008-03-29            2008           3        29 Sat      \n 3 2007-09-01            2007           9         1 Sat      \n 4 2007-03-31            2007           3        31 Sat      \n 5 2008-01-01            2008           1         1 Tue      \n 6 2007-07-31            2007           7        31 Tue      \n 7 2008-05-31            2008           5        31 Sat      \n 8 2007-09-01            2007           9         1 Sat      \n 9 2007-05-01            2007           5         1 Tue      \n10 2008-03-01            2008           3         1 Sat      \n# … with 73 more rows\n\n\n\nQuick exercise\nCan we trust our close_dow variable? Why?\n\nIt’s also easy to calculate differences of time. We can use normal arithmetic — future date - past date — and R will take care of all the underlying calendar calculations for us (e.g., days in a given month, leap years, etc).\n\n## how long since the institution closed\n## - as of 1 January 2020\n## - as of today\ndf &lt;- df %&gt;%\n    mutate(time_since_close_jan = ymd(\"2020-01-01\") - closedat_fix_dt,\n           time_since_close_now = today() - closedat_fix_dt)\n\n## show\ndf %&gt;% select(starts_with(\"time_since_close\"))\n\n# A tibble: 83 × 2\n   time_since_close_jan time_since_close_now\n   &lt;drtn&gt;               &lt;drtn&gt;              \n 1 4569 days            6015 days           \n 2 4295 days            5741 days           \n 3 4505 days            5951 days           \n 4 4659 days            6105 days           \n 5 4383 days            5829 days           \n 6 4537 days            5983 days           \n 7 4232 days            5678 days           \n 8 4505 days            5951 days           \n 9 4628 days            6074 days           \n10 4323 days            5769 days           \n# … with 73 more rows\n\n\nAs with strings and regular expressions, we’ve only scratched the surface of working with dates in R. For example, you can also work with times (hours, minutes, seconds, etc). Now that you’ve been introduced, however, you should have a starting point for working with panel and administrative data that includes strings and dates that you need to process before conducting your analyses."
  },
  {
    "objectID": "08-Data-Wrangling-III.html#questions",
    "href": "08-Data-Wrangling-III.html#questions",
    "title": "III: Working with strings & dates",
    "section": "Questions",
    "text": "Questions\nNB To answer the questions, you will need to join the two IPEDS data sets using the common unitid key. Note that column names in hd2007.csv are uppercase (UNITID) while those in ic2007mission.csv are lowercase (unitid). There are a few ways to join when the keys don’t exactly match. One is to set all column names to the same case. If you want to use left_join() starting with hd2007.csv, you can first use the the dplyr verb rename_all(tolower) in your chain to lower all column names. See the help file for left_join() for other ways to join by different variable names.\n\nHow many chief administrator names start with “Dr.”?\nNB Many chief administrators are listed on more than one line due to branch campuses. Make sure to take this into account by keeping only distinct names.\nBONUS How many chief administrator names end with the title “PH.D.” or some variant?\nAmong those schools that give their mission statement:\n\nHow many repeat their institutional name in their mission statement?\n\nHow many use the word civic?\nWhich top 3 states have the most schools with mission statements that use the word future?\n\nWhich type of schools (public, private-non-profit, private-for-profit) are most likely to use the word skill in their mission statement?\n\nAmong the schools that closed in 2007 or 2008 and give a date with at least a month and year:\n\nWhich has been closed for the longest time? How many months has it been from its close date to the beginning of this current month (1 February 2020)?\n\nHow many days were there between the first school to close and the last?\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_8.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "04-Data-Wrangling-II.html#data",
    "href": "04-Data-Wrangling-II.html#data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Data",
    "text": "Data\nAfter you download and unzip the data for today’s lesson, move the full folder, sch_test, into the data subdirectory. It should look something like this:\n|__ data/\n    |-- ...\n    |__ sch_test/\n        |-- all_schools.csv\n        |-- all_schools_wide.csv\n        |__ by_school/\n            |-- bend_gate_1980.csv\n            |-- bend_gate_1981.csv\n            |...\n            |-- spottsville_1985.csv\nThese fake data represent test scores across three subjects — math, reading, and science — across four schools over six years. Each school has a file for each year in the by_school subdirectory. The two files in sch_test directory, all_schools.csv and all_schools_wide.csv, combine the individual files but in different formats. We’ll use these data sets to practice appending, joining, and reshaping."
  },
  {
    "objectID": "04-Data-Wrangling-II.html#setup",
    "href": "04-Data-Wrangling-II.html#setup",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Setup",
    "text": "Setup\nAs always, we begin by reading in the tidyverse library and assigning our paths to macros we can reuse below.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAs we did in the past lesson, we run this script assuming that our working directory is set to the scripts directory. Notice that we also include macros for our subdirectories within the data directory. Since they are nested, we can use the previous macros to set new macros."
  },
  {
    "objectID": "04-Data-Wrangling-II.html#appending-data",
    "href": "04-Data-Wrangling-II.html#appending-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Appending data",
    "text": "Appending data\nOur first task is the most straightforward. When appending data, we simply add similarly structured rows to an exiting data frame. What do I mean by similarly structured? Imagine you have a data frame that looks like this:\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\n\nNow, assume you are given data that look like this:\n\n\n\nid\nyear\nscore\n\n\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\nThese data are similarly structured: same column names in the same order. If we know that the data came from the same process (e.g., ids represent students in the same classroom with each file representing a different test day), then we can safely append the second to the first:\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\nData that are the result of the exact same data collecting process across locations or time may be appended. In education research, administrative data are often recorded each term or year, meaning you can build a panel data set by appending. The NCES IPEDS data files generally work like this.\nHowever, it’s incumbent upon you as the researcher to understand your data. Just because you are able to append (R will try to make it work for you) doesn’t mean you always should. What if the score column in our data weren’t on the same scale? What if the test date mattered but isn’t included in the file? What if the files actually represent scores from different grades or schools? It’s possible that we can account for each of these issues as we clean our data, but it won’t happen automatically — append with care!\n\n\n\n\n\nExample\nLet’s practice with an example. First, we’ll read in three data files from the by_school directory.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data, storing in df_*, where * is a unique number\ndf_1 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1980.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_2 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1981.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_3 &lt;- read_csv(file.path(\"data\", \"sch_test\", \"by_school\", \"bend_gate_1982.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLooking at each, we can see that they are similarly structured, with the following columns in the same order: school, year, math, read, science:\n\n## ---------------------------\n## process\n## ---------------------------\n\n## show each\ndf_1\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\ndf_2\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\ndf_3\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nFrom the dplyr library, we use the bind_rows() function to append the second and third data frames to the first.\n\n## append files\ndf &lt;- bind_rows(df_1, df_2, df_3)\n\n## show\ndf\n\n# A tibble: 3 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n\n\nThat’s it!\n\nQuick exercise\nRead in the rest of the files for Bend Gate and append them to the current data frame."
  },
  {
    "objectID": "04-Data-Wrangling-II.html#joining-data",
    "href": "04-Data-Wrangling-II.html#joining-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Joining data",
    "text": "Joining data\nMore often than appending your data files, however, you will need to merge or join them. With a join, you add to your data frame new columns (new variables) that come from a second data frame. The key difference between joining and appending is that a join requires a key, that is, a variable or index common to each data frame that uniquely identifies observations. It’s this key that’s used to line everything up.\nFor example, say you have these two data sets,\n\n\n\nid\nsch\nyear\nscore\n\n\n\n\nA\n1\n2020\n98\n\n\nB\n1\n2020\n95\n\n\nC\n2\n2020\n85\n\n\nD\n3\n2020\n94\n\n\n\n\n\n\nsch\ntype\n\n\n\n\n1\nelementary\n\n\n2\nmiddle\n\n\n3\nhigh\n\n\n\nand you want to add the school type to the first data set. You can do this because you have a common key between each set: sch. A pseudocode description of this join would be:\n\nAdd a column to the first data frame called type\nFill in each row of the new column with the type value that corresponds to the matching sch value in both data frames:\n\nsch == 1 --&gt; elementary\nsch == 2 --&gt; middle\nsch == 3 --&gt; high\n\n\nThe end result would then look like this:\n\n\n\nid\nsch\nyear\nscore\ntype\n\n\n\n\nA\n1\n2020\n98\nelementary\n\n\nB\n1\n2020\n95\nelementary\n\n\nC\n2\n2020\n85\nmiddle\n\n\nD\n3\n2020\n94\nhigh\n\n\n\n\nExample\nA common join task in education research involves adding group-level aggregate statistics to individual observations: for example, adding school-level average test scores to each student’s row. With a panel data set (observations across time), we might want within-year averages added to each unit-by-time period row. Let’s do the second, adding within-year across school average test scores to each school-by-year observation.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in all_schools data\ndf &lt;- read_csv(file.path(\"data\", \"sch_test\", \"all_schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLooking at the data, we see that it’s similar to what we’ve seen above, with additional schools.\n\n## show\ndf\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# … with 14 more rows\n\n\nOur task is two-fold:\n\nGet the average of each test score (math, reading, science) across all schools within each year and save the summary data frame in an object.\nJoin the new summary data frame to the original data frame.\n\n\n1. Get summary\n\n## ---------------------------\n## process\n## ---------------------------\n\n## get test score summary \ndf_sum &lt;- df %&gt;%\n    ## grouping by year so average within each year\n    group_by(year) %&gt;%\n    ## get mean(&lt;score&gt;) for each test\n    summarize(math_m = mean(math),\n              read_m = mean(read),\n              science_m = mean(science))\n\n## show\ndf_sum\n\n# A tibble: 6 × 4\n   year math_m read_m science_m\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  1980   507    295.      798.\n2  1981   496.   293.      788.\n3  1982   506    302.      802.\n4  1983   500    293.      794.\n5  1984   490    300.      792.\n6  1985   500.   290.      794.\n\n\n\nQuick exercise\nThinking ahead, why do you think we created new names for the summarized columns? Why the _m ending?\n\n\n\n2. Join\nWhile one can merge using base R, dplyr uses the SQL language of joins, which can be conceptually clearer (particularly for those who already have experience with relational database structures). Here are the most common joins you will use:\n\nleft_join(x, y): keep all x, drop unmatched y\nright_join(x, y): keep all y, drop unmatched x\ninner_join(x, y): keep only matching\nfull_join(x, y): keep everything\n\n\n\n\nIn the Venn diagrams above, blue represents the observations that are kept, white the observations that are dropped.\n\n\nFor example, the result of a left join between data frame X and data frame Y will include all observations in X and those in Y that are also in X.\nX\n\n\n\nid\ncol_A\ncol_B\n\n\n\n\n001\na\n1\n\n\n002\nb\n2\n\n\n003\na\n3\n\n\n\nY\n\n\n\nid\ncol_C\ncol_D\n\n\n\n\n001\nT\n9\n\n\n002\nT\n9\n\n\n004\nF\n9\n\n\n\nXY (result of left join)\n\n\n\nid\ncol_A\ncol_B\ncol_C\ncol_D\n\n\n\n\n001\na\n1\nT\n9\n\n\n002\nb\n2\nT\n9\n\n\n003\na\n3\nNA\nNA\n\n\n\nObservations in both X and Y (001 and 002, above), will have data for the columns that were separately in X and Y before. Those in X only (003), will have missing values in the new columns that came from Y because they didn’t exist there. Observations in Y but not X (004) are dropped entirely.\nBack to our example…\nSince we want to join a smaller aggregated data frame, df_sum, to the original data frame, df, we’ll use a left_join(). The join functions will try to guess the joining variable (and tell you what it picked) if you don’t supply one, but we’ll specify one to be clear.\n\n## start with data frame...\ndf_joined &lt;- df %&gt;%\n    ## pipe into left_join to join with df_sum using \"year\" as key\n    left_join(df_sum, by = \"year\")\n\n## show\ndf_joined\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# … with 14 more rows\n\n\n\nQuick exercise\nLook at the first 10 rows of df_joined. What do you notice about the new summary columns we added?"
  },
  {
    "objectID": "04-Data-Wrangling-II.html#reshaping-data",
    "href": "04-Data-Wrangling-II.html#reshaping-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Reshaping data",
    "text": "Reshaping data\nReshaping data is a common data wrangling task. Whether going from wide to long format or long to wide, it can be a painful process. But with a little practice, the ability to reshape data will become a powerful tool in your toolbox.\n\nDefinitions\nWhile there are various definitions of tabular data structure, the two you will most often come across are wide and long. Wide data are data structures in which all variable/values are columns. At the extreme end, every id will only have a single row:\n\n\n\n\n\n\n\n\n\n\nid\nmath_score_2019\nread_score_2019\nmath_score_2020\nread_score_2020\n\n\n\n\nA\n93\n88\n92\n98\n\n\nB\n99\n92\n97\n95\n\n\nC\n89\n88\n84\n85\n\n\n\nNotice how each particular score (by year) has its own column? Compare this to long data in which each observational unit (id test score within a given year) will have a row:\n\n\n\nid\nyear\ntest\nscore\n\n\n\n\nA\n2019\nmath\n93\n\n\nA\n2019\nread\n88\n\n\nA\n2020\nmath\n92\n\n\nA\n2020\nread\n98\n\n\nB\n2019\nmath\n99\n\n\nB\n2019\nread\n92\n\n\nB\n2020\nmath\n97\n\n\nB\n2020\nread\n95\n\n\nC\n2019\nmath\n89\n\n\nC\n2019\nread\n88\n\n\nC\n2020\nmath\n84\n\n\nC\n2020\nread\n85\n\n\n\nThe first wide and second long table present the same information in a different format. So why bother reshaping? The short answer is that you sometimes need one format and sometimes the other due to the demands of the analysis you want to run, the figure you want to plot, or the table you want to make.\nNB: Data in the wild are often some combination of these two types: wide-ish or long-ish. For an example, see our all_schools.csv data below, which is wide in some variables (test), but long in others (year). The point of defining long vs wide is not to have a testable definition, but rather to have a framework for thinking about how your data are structured and if that structure will work for your data analysis needs.\n\n\nExample: wide –&gt; long\nTo start, we’ll go back to the all_schools.csv file.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## reading again just to be sure we have the original data\ndf &lt;- read_csv(file.path(\"data\", \"sch_test\", \"all_schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice how the data are wide in test: each school has one row per year, but each test gets its own column. While this setup can be efficient for storage, it’s not always the best for analysis or even just browsing. What we want is for the data to be long.\nInstead of each test having its own column, we would like to make the data look like our long data example above, with each row representing a single school, year, test, score:\n\n\n\n\n\n\n\n\n\nschool\nyear\ntest\nscore\n\n\n\n\nBend Gate\n1980\nmath\n515\n\n\nBend Gate\n1980\nread\n281\n\n\nBend Gate\n1980\nscience\n808\n\n\n…\n…\n…\n…\n\n\n\nAs with joins, you can reshape data frames using base R commands. But again, we’ll use tidyverse functions in the tidyr library. Specifically, we’ll rely on the tidyr pivot_longer() and pivot_wider() commands.\n\npivot_longer()\nThe pivot_longer() function can take a number of arguments, but the core things it needs to know are:\n\ndata: the name of the data frame you’re reshaping (we can use %&gt;% to pipe in the data name)\ncols: the names of the columns that you want to pivot into values of a single new column (thereby making the data frame “longer”)\nnames_to: the name of the new column that will contain the names of the cols you just listed\nvalues_to: the name of the column where the values in the cols you listed will go\n\nIn our current situation, our cols to pivot are \"math\", \"read\", and \"science\". Since they are test types, we’ll call our names_to column \"test\" and our values_to column \"score\".\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df %&gt;%\n    ## cols: current test columns\n    ## names_to: where \"math\", \"read\", and \"science\" will go\n    ## values_to: where the values in cols will go\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"),\n                 names_to = \"test\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# … with 62 more rows\n\n\n\nQuick (ocular test) exercise\nHow many rows did our initial data frame df have? How many unique tests did we have in each year? When reshaping from wide to long, how many rows should we expect our new data frame to have? Does our new data frame have that many rows?\n\n\n\n\nExample: long –&gt; wide\n\npivot_wider()\nNow that we have our long data, let’s reshape it back to wide format using pivot_wider(). In this case, we’re doing just the opposite from before — here are the main arguments you need to attend to:\n\ndata: the name of the data frame you’re reshaping (we can use %&gt;% to pipe in the data name)\nnames_from: the name of the column that contains the values which will become new column names\nvalues_from: the name of the column that contains the values associated with the values in names_from column; these will go into the new columns.\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## long to wide\ndf_wide &lt;- df_long %&gt;%\n    ## names_from: values in this column will become new column names\n    ## values_from: values in this column will become values in new cols\n    pivot_wider(names_from = \"test\",\n                values_from = \"score\")\n\n## show\ndf_wide\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# … with 14 more rows\n\n\n\nQuick exercise\nIn this case, our new wide data frame, df_wide, should be the same as our initial data frame. Is it? How can you tell?\n\n\n\n\nExample: wide –&gt; long with corrections\nUnfortunately, it’s not always so clear cut to reshape data. In this second example, we’ll again reshape from wide to long, but we’ll have to munge our data a bit after the reshape to make it analysis ready.\nFirst, we’ll read in a second file all_schools_wide.csv. This file contains the same information as before, but in a very wide format: each school has only one row and each test by year value gets its own column in the form &lt;test&gt;_&lt;year&gt;.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in very wide test score data\ndf &lt;- read_csv(file.path(\"data\", \"sch_test\", \"all_schools_wide.csv\"))\n\nRows: 4 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): school\ndbl (18): math_1980, read_1980, science_1980, math_1981, read_1981, science_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## show\ndf\n\n# A tibble: 4 × 19\n  school math_…¹ read_…² scien…³ math_…⁴ read_…⁵ scien…⁶ math_…⁷ read_…⁸ scien…⁹\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend …     515     281     808     503     312     814     514     316     816\n2 East …     501     318     782     487     323     813     496     294     818\n3 Niaga…     514     292     787     499     268     762     507     310     771\n4 Spott…     498     288     813     494     270     765     507     289     801\n# … with 9 more variables: math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;,\n#   science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;, read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;,\n#   math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;, science_1985 &lt;dbl&gt;, and abbreviated\n#   variable names ¹​math_1980, ²​read_1980, ³​science_1980, ⁴​math_1981,\n#   ⁵​read_1981, ⁶​science_1981, ⁷​math_1982, ⁸​read_1982, ⁹​science_1982\n\n\nSecond, we can pivot_longer() as we did before using the following values for our key arguments:\n\ndata : df (but piped in using %&gt;%)\ncols : use special tidyselect helper function contains() to select all test by year columns\nnames_to: test_year\nvalues_to: score\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df %&gt;%\n    ## NB: contains() looks for \"19\" in name: if there, it adds it to cols\n    pivot_longer(cols = contains(\"19\"),\n                 names_to = \"test_year\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 3\n   school    test_year    score\n   &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n 1 Bend Gate math_1980      515\n 2 Bend Gate read_1980      281\n 3 Bend Gate science_1980   808\n 4 Bend Gate math_1981      503\n 5 Bend Gate read_1981      312\n 6 Bend Gate science_1981   814\n 7 Bend Gate math_1982      514\n 8 Bend Gate read_1982      316\n 9 Bend Gate science_1982   816\n10 Bend Gate math_1983      491\n# … with 62 more rows\n\n\n\nQuick exercise\nWhy did we use “19” as our value in the contains() function? HINT: use the names() function to return a list of the original data frame (df) column names.\n\nThis mostly worked to get our data long, but now we have this weird combined test_year column. What we really want are two columns, one for the year and one for the test type. We can fix this using tidyr separate() function with the following arguments:\n\ndata: our df_long object, piped in using %&gt;%\ncol: the column we want to split (test_year)\ninto: the names of the new columns to create from col (test and year)\nsep: the name of the character that splits the values in col, so R knows how to fill each of the into columns (\"_\")\n\n\n## separate test_year into two columns, filling appropriately\ndf_long_fix &lt;- df_long %&gt;%\n    ## col: the column to split\n    ## into: names of resulting splits\n    ## sep: the split point --&gt; left to \"test\", right to \"year\"\n    separate(col = \"test_year\",\n             into = c(\"test\", \"year\"),\n             sep = \"_\")\n\n## show\ndf_long_fix\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# … with 62 more rows\n\n\n\nQuick exercise\nRedo the last few steps in a single combined chain using pipes. That is, start with df (which contains all_schools_wide.csv), reshape long, and fix so that you end up with four columns — all in a single piped chain."
  },
  {
    "objectID": "04-Data-Wrangling-II.html#final-note",
    "href": "04-Data-Wrangling-II.html#final-note",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Final note",
    "text": "Final note\nJust as all data sets are unique, so too are the particular steps you may need to take to append, join, or reshape your data. Even experienced coders rarely get all the steps correct the first try. Be prepared to spend time getting to know your data and figuring out, through trial and error, how to wrangle it so that it meets your analytic needs. Code books, institutional/domain knowledge, and patience are your friends here!"
  },
  {
    "objectID": "04-Data-Wrangling-II.html#questions",
    "href": "04-Data-Wrangling-II.html#questions",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Questions",
    "text": "Questions\n\nCompute the average test score by region and join back into the full data frame. Next, compute the difference between each student’s test score and that of the region. Finally, return the mean of these differences by region.\nCompute the average test score by region and family income level. Join back to the full data frame. HINT You can join on more than one key.\nSelect the following variables from the full data set:\n\nstu_id\nx1stuedexpct\nx1paredexpct\nx4evratndclg\n\nFrom this reduced data frame, reshape the data frame so that it is long in educational expectations, meaning that each observation should have two rows, one for each educational expectation type.\ne.g. (your column names and values may be different)\n\n\n\nstu_id\nexpect_type\nexpectation\nx4evratndclg\n\n\n\n\n0001\nx1stuedexpct\n6\n1\n\n\n0001\nx1paredexpct\n7\n1\n\n\n0002\nx1stuedexpct\n5\n1\n\n\n0002\nx1paredexpct\n5\n1\n\n\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_4.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#inspect-the-web-site",
    "href": "Extra-03-Web-Scraping.html#inspect-the-web-site",
    "title": "Extra: Web Scraping",
    "section": "Inspect the web site",
    "text": "Inspect the web site\nFirst, let’s check out the table we want to scrape. The table we see looks like a regularly formatted table, much like we would see in a paper document. But unlike a printed document, a web page relies on hidden-from-the-user code to generate what we see. By doing it this way instead of serving a static image, websites can adjust to the wide array of user screen sizes, devices, and operating systems. Instructions that tell the user device how to generate the page are also smaller than sending a preformatted image, so bandwidth and time to load are also reduced.\nBut as web scrapers, we don’t need this. We need the underlying HTML/CSS/XML code used to generate the page. To see it, you’ll need to use a web site inspector. With Firefox and Chrome, you should be able to right-click the page and see the underlying code (you may need to turn on developer tools first). With Safari, you will have to enable the developer tools first.\nThe top code of the page should look something like this:\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;!-- Current year pub navigation function --&gt;\nMoving further down, we find the table data, but in a very different format (first row):\n...\n&lt;tr&gt;\n  &lt;th class=\"TblCls009\" scope=\"row\" nowrap=\"nowrap\"&gt;1960 &lt;/th&gt;\n  &lt;td class=\"TblCls010\"&gt;1,679&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(44.5)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;756&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(32.3)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;923&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(30.1)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;45.1&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(2.16)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;54.0&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(3.23)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;37.9&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(2.85)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n&lt;/tr&gt;\n...\nThe task is to convert these data into a data frame that we can then store or use in tables and figures. This is what the rvest helps us do."
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#read-web-site",
    "href": "Extra-03-Web-Scraping.html#read-web-site",
    "title": "Extra: Web Scraping",
    "section": "Read web site",
    "text": "Read web site\nThe first step is to read the web page code into an object using the read_html() function.\n\n## set site\nurl &lt;- \"https://nces.ed.gov/programs/digest/d17/tables/dt17_302.10.asp\"\n\n## get site\nsite &lt;- read_html(url)\n\nShowing our object, we can see that the basic structure of the web page is stored.\n\n## show\nsite\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body bgcolor=\"#ffffff\" text=\"#000000\"&gt;\\r\\n\\t\\r\\n\\t&lt;!-- Main NCES Header  ..."
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#select-nodes",
    "href": "Extra-03-Web-Scraping.html#select-nodes",
    "title": "Extra: Web Scraping",
    "section": "Select nodes",
    "text": "Select nodes\nRight now, we have a structured, but not particularly useful object holding our web page data. To pull out specific data, we use the html_nodes() function. Selecting a node is somewhat akin to using dplyr’s filter() on a data frame.\nGreat…but what’s a node and how do I know which ones to use? First, a node is a particular element that is comprised of some information stored between, for example, HTML tags like &lt;p&gt;...&lt;/p&gt; or &lt;h1&gt;...&lt;/h2&gt;. Good web design says that information on page should be organized by its purpose and similarity to other data. For example, major headers should be wrapped in &lt;h1&gt; tags and similar page sections should be given the same CSS class. We can use CSS ids and classes with the html_nodes() function to pull the exact data we need.\nGreat!…but what are the classes that we need? Well, we could just inspect the web page manually and guess. For some pages, that works great. But it certainly looks like a chore for this page. Luckily, there’s a great tool that will help us.\n\nSelectorGadget\nSelectorGadget is a (now very old) plugin that allows you to click on a web page and, through process of elimination, get the exact combination of HTML tags and CSS ids and classes you need to pull only the data you need.\nYou can get it from the link above or by following the instructions here.\nThe SelectorGadget page has instructions, but briefly, this is the process:\n\nOn the first click, SelectorGadget will make its best guess about what you want based on the item you clicked (e.g., table column). The particular element you clicked will be green. The other elements it assumed you want will turn yellow. Sometimes it’s right and you’re finished!\n\nOften, it will select something you don’t want. In that case, click on the yellow item you don’t want. Again, SelectorGadget will make and informed guess. Sometimes it will drop all extraneous elements and sometimes you will need to click multiple times. These elements will be red.\nOn the other hand, SelectorGadget may not have given you everything you want. Keep clicking on new elements (and dropping the extra) until only what you want is highlighted in either green or yellow.\n\nAs you’re clicking, you’ll see a box with a string of element ids and classes changing. When you’re finished, copy this string. This is your node you’ll use in the html_nodes() function!\n\nQuick exercise\nGet the SelectorGadget plugin and play with it for a few minutes. See if you can select only a specific column then only a specific row."
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#first-column-of-data",
    "href": "Extra-03-Web-Scraping.html#first-column-of-data",
    "title": "Extra: Web Scraping",
    "section": "First column of data",
    "text": "First column of data\nAs a first step, let’s get the first column of data in Table 302.10: the total number of recent high school graduates. Using SelectorGadget, I see that the node string I should use is '.tableBracketRow td:nth-child(2)'. After selecting the node, we use html_text() to convert the data into a vector like we’re used to seeing.\n\n## subset to just first column\ntot &lt;- site %&gt;%\n  html_nodes(\".tableBracketRow td:nth-child(2)\") %&gt;%\n  html_text()\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \" \"     \"2,659\" \"2,612\" \"2,525\"\n[10] \"2,606\" \"2,842\" \" \"     \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \" \"    \n[19] \"3,185\" \"2,986\" \"3,141\" \"3,163\" \"3,160\" \" \"     \"3,088\" \"3,056\" \"3,100\"\n[28] \"2,963\" \"3,012\" \" \"     \"2,668\" \"2,786\" \"2,647\" \"2,673\" \"2,450\" \" \"    \n[37] \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \" \"     \"2,599\" \"2,660\" \"2,769\"\n[46] \"2,810\" \"2,897\" \" \"     \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\" \" \"    \n[55] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \" \"     \"3,160\" \"3,079\" \"3,203\"\n[64] \"2,977\" \"2,868\" \" \"     \"2,965\" \"3,137\"\n\n\nSo far so good, but we can see a few problems. First, the blank rows in the table show up in our data. While those blank table spaces are good for the eyes, they aren’t good in our data set. Let’s try to remove them using the trim = TRUE option.\n\n## ...this time trim blank spaces\ntot &lt;- site %&gt;%\n  html_nodes(\".tableBracketRow td:nth-child(2)\") %&gt;%\n  html_text(trim = TRUE)\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \"\"      \"2,659\" \"2,612\" \"2,525\"\n[10] \"2,606\" \"2,842\" \"\"      \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \"\"     \n[19] \"3,185\" \"2,986\" \"3,141\" \"3,163\" \"3,160\" \"\"      \"3,088\" \"3,056\" \"3,100\"\n[28] \"2,963\" \"3,012\" \"\"      \"2,668\" \"2,786\" \"2,647\" \"2,673\" \"2,450\" \"\"     \n[37] \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \"\"      \"2,599\" \"2,660\" \"2,769\"\n[46] \"2,810\" \"2,897\" \"\"      \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\" \"\"     \n[55] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \"\"      \"3,160\" \"3,079\" \"3,203\"\n[64] \"2,977\" \"2,868\" \"\"      \"2,965\" \"3,137\"\n\n\nBetter, but the empty elements are still there. We can use str_subset() from the stringr library (loaded with tidyverse) to remove them.\n\n## remove blank values; str_subset removes pattern (\"\")\ntot &lt;- tot %&gt;% str_subset(pattern = \".+\")\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \"2,659\" \"2,612\" \"2,525\" \"2,606\"\n[10] \"2,842\" \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \"3,185\" \"2,986\" \"3,141\"\n[19] \"3,163\" \"3,160\" \"3,088\" \"3,056\" \"3,100\" \"2,963\" \"3,012\" \"2,668\" \"2,786\"\n[28] \"2,647\" \"2,673\" \"2,450\" \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \"2,599\"\n[37] \"2,660\" \"2,769\" \"2,810\" \"2,897\" \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\"\n[46] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \"3,160\" \"3,079\" \"3,203\" \"2,977\"\n[55] \"2,868\" \"2,965\" \"3,137\"\n\n\nGetting closer. Next, let’s convert our numbers to actual numbers, which R thinks are strings at the moment. To do this, we need to get rid of the commas. The str_replace() function is perfect for this. Regular expressions can become complicated, but our use here is simple:\n\n## remove commas, replacing with empty string\ntot &lt;- tot %&gt;% str_replace(pattern = \",\", replacement = \"\")\n\n## show\ntot\n\n [1] \"1679\" \"1763\" \"1838\" \"1741\" \"2145\" \"2659\" \"2612\" \"2525\" \"2606\" \"2842\"\n[11] \"2758\" \"2875\" \"2964\" \"3058\" \"3101\" \"3185\" \"2986\" \"3141\" \"3163\" \"3160\"\n[21] \"3088\" \"3056\" \"3100\" \"2963\" \"3012\" \"2668\" \"2786\" \"2647\" \"2673\" \"2450\"\n[31] \"2362\" \"2276\" \"2397\" \"2342\" \"2517\" \"2599\" \"2660\" \"2769\" \"2810\" \"2897\"\n[41] \"2756\" \"2549\" \"2796\" \"2677\" \"2752\" \"2675\" \"2692\" \"2955\" \"3151\" \"2937\"\n[51] \"3160\" \"3079\" \"3203\" \"2977\" \"2868\" \"2965\" \"3137\"\n\n\nNow we’re ready to convert to a number.\n\n## convert to numeric\ntot &lt;- tot %&gt;% as.integer()\n\n## show\ntot\n\n [1] 1679 1763 1838 1741 2145 2659 2612 2525 2606 2842 2758 2875 2964 3058 3101\n[16] 3185 2986 3141 3163 3160 3088 3056 3100 2963 3012 2668 2786 2647 2673 2450\n[31] 2362 2276 2397 2342 2517 2599 2660 2769 2810 2897 2756 2549 2796 2677 2752\n[46] 2675 2692 2955 3151 2937 3160 3079 3203 2977 2868 2965 3137\n\n\nFinished!"
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#add-year",
    "href": "Extra-03-Web-Scraping.html#add-year",
    "title": "Extra: Web Scraping",
    "section": "Add year",
    "text": "Add year\nSo that these numbers make sense, let’s grab the years column and create and data frame so that we can make a figure of long term high school completer totals. Again, the first step is to use SelectorGadget to get the node string. This time, it’s \"tbody th\".\n\n## get years column\nyears &lt;- site %&gt;%\n  html_nodes(\"tbody th\") %&gt;%\n  html_text(trim = TRUE)\n\n## remove blank spaces like before\nyears &lt;- years %&gt;% str_subset(pattern = \".+\")\n\n## show\nyears\n\n [1] \"1960\"  \"1961\"  \"1962\"  \"1963\"  \"1964\"  \"1965\"  \"1966\"  \"1967\"  \"1968\" \n[10] \"1969\"  \"1970\"  \"1971\"  \"1972\"  \"1973\"  \"1974\"  \"1975\"  \"1976\"  \"1977\" \n[19] \"1978\"  \"1979\"  \"1980\"  \"1981\"  \"1982\"  \"1983\"  \"1984\"  \"1985\"  \"1986\" \n[28] \"1987\"  \"1988\"  \"1989\"  \"1990\"  \"1991\"  \"1992\"  \"1993\"  \"1994\"  \"1995\" \n[37] \"1996\"  \"1997\"  \"1998\"  \"1999\"  \"2000\"  \"2001\"  \"2002\"  \"2003\"  \"2004\" \n[46] \"2005\"  \"2006\"  \"2007\"  \"2008\"  \"2009\"  \"20103\" \"20113\" \"20123\" \"20133\"\n[55] \"20143\" \"20153\" \"20163\"\n\n\nWe’ve gotten rid of the blank items, but now we have a new problem: the footnotes in the last few years has just be added to the year. Instead of 2010, we have 20103, and so on through 2016. Since the problem is small (it’s easy to see all the bad items) and regular (always extra 3 as the 5th digit), we can fix it using str_sub().\n\n## trim footnote that's become extra digit\nyears &lt;- years %&gt;% str_sub(start = 1, end = 4)\n\n## show\nyears\n\n [1] \"1960\" \"1961\" \"1962\" \"1963\" \"1964\" \"1965\" \"1966\" \"1967\" \"1968\" \"1969\"\n[11] \"1970\" \"1971\" \"1972\" \"1973\" \"1974\" \"1975\" \"1976\" \"1977\" \"1978\" \"1979\"\n[21] \"1980\" \"1981\" \"1982\" \"1983\" \"1984\" \"1985\" \"1986\" \"1987\" \"1988\" \"1989\"\n[31] \"1990\" \"1991\" \"1992\" \"1993\" \"1994\" \"1995\" \"1996\" \"1997\" \"1998\" \"1999\"\n[41] \"2000\" \"2001\" \"2002\" \"2003\" \"2004\" \"2005\" \"2006\" \"2007\" \"2008\" \"2009\"\n[51] \"2010\" \"2011\" \"2012\" \"2013\" \"2014\" \"2015\" \"2016\"\n\n\nFixed! Now we bind together with our high school completers total. Because we want to make a time period line graph, we’ll also convert the years to a date format. We’ll use ymd from the lubridate library. Since we only have years, we’ll include the argument truncated = 2L, which means that we have an incomplete date (no month or day).\nNB Since we dropped blank elements in each vector separately, it’s important to check that all the data line up properly now that we’ve bound them together. If we wanted to be safer, we could have bound the data first, then dropped the rows with double missing values.\n\n## put in data frame\ndf &lt;- bind_cols(years = years, total = tot) %&gt;%\n  mutate(years = ymd(years, truncated = 2L))\n\n## show\ndf\n\n# A tibble: 57 × 2\n   years      total\n   &lt;date&gt;     &lt;int&gt;\n 1 1960-01-01  1679\n 2 1961-01-01  1763\n 3 1962-01-01  1838\n 4 1963-01-01  1741\n 5 1964-01-01  2145\n 6 1965-01-01  2659\n 7 1966-01-01  2612\n 8 1967-01-01  2525\n 9 1968-01-01  2606\n10 1969-01-01  2842\n# … with 47 more rows\n\n\nYou can see that the date format adds a month and day (January 1st by default). While these particular dates probably aren’t right, we won’t use them later when graphing so they can stay.\nLet’s plot our trends.\n\n## plot\ng &lt;- ggplot(df, mapping = aes(x = years, y = total)) +\n  ## line for the main estimate\n  geom_line() +\n  ## make x-axis look nice\n  ## major breaks: every 5 years, from min year to max year\n  ## minor breaks: every 1 year, from min year to max year\n  ## labels: formate to only show year (\"%Y\")\n  scale_x_date(breaks = seq(min(df$years),\n                            max(df$years),\n                            \"5 years\"),\n               minor_breaks = seq(min(df$years),\n                                  max(df$years),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## nice labels and titles\n  labs(x = \"Year\",\n       y = \"High school completers (1000s)\",\n       title = \"Total number of high school completers: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\")\ng\n\n\n\n\n\nQuick exercise\nPull in total percentage of enrollment (column 5), add to data frame, and plot against year."
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#scrape-entire-table",
    "href": "Extra-03-Web-Scraping.html#scrape-entire-table",
    "title": "Extra: Web Scraping",
    "section": "Scrape entire table",
    "text": "Scrape entire table\nNow that we’ve pulled two columns, let’s try to grab the entire table. Once again, we’ll use SelectorGadget to get our node string.\n\n## save node\nnode &lt;- paste0(\".TblCls002 , td.TblCls005 , tbody .TblCls008 , \",\n               \".TblCls009 , .TblCls011 , .TblCls010\")\n\n## save more dataframe-friendly column names that we\n## get from looking at the table online\nnms &lt;- c(\"year\",\"hs_comp_tot\", \"hs_comp_tot_se\",\n         \"hs_comp_m\", \"hs_comp_m_se\",\n         \"hs_comp_f\", \"hs_comp_f_se\",\n         \"enr_pct\", \"enr_pct_se\",\n         \"enr_pct_2\", \"enr_pct_2_se\",\n         \"enr_pct_4\", \"enr_pct_4_se\",\n         \"enr_pct_m\", \"enr_pct_m_se\",\n         \"enr_pct_2_m\", \"enr_pct_2_m_se\",\n         \"enr_pct_4_m\", \"enr_pct_4_m_se\",\n         \"enr_pct_f\", \"enr_pct_f_se\",\n         \"enr_pct_2_f\", \"enr_pct_2_f_se\",\n         \"enr_pct_4_f\", \"enr_pct_4_f_se\")\n\n## whole table\ntab &lt;- site %&gt;%\n  ## use nodes\n  html_nodes(node) %&gt;%\n  ## to text with trim\n  html_text(trim = TRUE)\n\n## show first few elements\ntab[1:30]\n\n [1] \"1960\"   \"1,679\"  \"(44.5)\" \"756\"    \"(32.3)\" \"923\"    \"(30.1)\" \"45.1\"  \n [9] \"(2.16)\" \"—\"      \"(†)\"    \"—\"      \"(†)\"    \"54.0\"   \"(3.23)\" \"—\"     \n[17] \"(†)\"    \"—\"      \"(†)\"    \"37.9\"   \"(2.85)\" \"—\"      \"(†)\"    \"—\"     \n[25] \"(†)\"    \"1961\"   \"1,763\"  \"(46.7)\" \"790\"    \"(33.7)\"\n\n\nOkay. It looks like we have it, but it’s all in single dimension vector. Since we eventually want a data frame, let’s convert to a matrix.\n\n## convert to matrix\ntab &lt;- tab %&gt;%\n  ## we know the size by looking at the table online\n  matrix(., ncol = 25, byrow = TRUE)\n\n## dimensions\ndim(tab)\n\n[1] 68 25\n\n## show first few columns using base R [&lt;rows&gt;,&lt;cols&gt;] notation\ntab[1:10,1:5]\n\n      [,1]   [,2]    [,3]     [,4]    [,5]    \n [1,] \"1960\" \"1,679\" \"(44.5)\" \"756\"   \"(32.3)\"\n [2,] \"1961\" \"1,763\" \"(46.7)\" \"790\"   \"(33.7)\"\n [3,] \"1962\" \"1,838\" \"(44.3)\" \"872\"   \"(32.0)\"\n [4,] \"1963\" \"1,741\" \"(44.9)\" \"794\"   \"(32.6)\"\n [5,] \"1964\" \"2,145\" \"(43.6)\" \"997\"   \"(32.3)\"\n [6,] \"\"     \"\"      \"\"       \"\"      \"\"      \n [7,] \"1965\" \"2,659\" \"(48.5)\" \"1,254\" \"(35.7)\"\n [8,] \"1966\" \"2,612\" \"(45.7)\" \"1,207\" \"(34.4)\"\n [9,] \"1967\" \"2,525\" \"(38.5)\" \"1,142\" \"(28.9)\"\n[10,] \"1968\" \"2,606\" \"(38.0)\" \"1,184\" \"(28.7)\"\n\n\n\nQuick exercise\nWhat happens if you don’t use byrow = TRUE in the matrix command?\n\nIt’s getting better, but now we have a lot of special characters that we need to clean out. This section relies more heavily on regular expressions, but the idea is the same as above.\n\n## clean up table\ntab &lt;- tab %&gt;%\n  ## convert to tibble, leaving name repair as minimal for now\n  as_tibble(.name_repair = \"minimal\") %&gt;%\n  ## rename using names above\n  set_names(nms) %&gt;%\n  ## remove commas\n  mutate(across(everything(), ~ str_replace(., \",\", \"\"))) %&gt;%\n  ## remove dagger and parentheses\n  mutate(across(everything(), ~ str_replace_na(., \"\\\\(\\U2020\\\\)\"))) %&gt;%\n  ## remove hyphens\n  mutate(across(everything(), ~ str_replace_na(., \"\\U2014\"))) %&gt;%\n  ## remove parentheses, but keep any content that was inside\n  mutate(across(everything(), ~ str_replace(., \"\\\\((.*)\\\\)\", \"\\\\1\"))) %&gt;%\n  ## remove blank strings (^ = start, $ = end, so ^$ = start to end w/ nothing)\n  mutate(across(everything(), ~ str_replace_na(., \"^$\"))) %&gt;%\n  ## fix years like above\n  mutate(year = str_sub(year, 1, 4)) %&gt;%\n  ## convert to numbers, suppressing warnings about NAs b/c we know\n  mutate(across(everything(), ~ suppressWarnings(as.numeric(.)))) %&gt;%\n  ## drop rows with missing year (blank online)\n  drop_na(year)\n\n## show\ntab\n\n# A tibble: 57 × 25\n    year hs_co…¹ hs_co…² hs_co…³ hs_co…⁴ hs_co…⁵ hs_co…⁶ enr_pct enr_p…⁷ enr_p…⁸\n   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1960    1679    44.5     756    32.3     923    30.1    45.1    2.16      NA\n 2  1961    1763    46.7     790    33.7     973    31.8    48      2.12      NA\n 3  1962    1838    44.3     872    32       966    30.4    49      2.08      NA\n 4  1963    1741    44.9     794    32.6     947    30.5    45      2.12      NA\n 5  1964    2145    43.6     997    32.3    1148    28.9    48.3    1.92      NA\n 6  1965    2659    48.5    1254    35.7    1405    32.5    50.9    1.73      NA\n 7  1966    2612    45.7    1207    34.4    1405    29.5    50.1    1.74      NA\n 8  1967    2525    38.5    1142    28.9    1383    24.7    51.9    1.44      NA\n 9  1968    2606    38      1184    28.7    1422    24.2    55.4    1.41      NA\n10  1969    2842    36.6    1352    27.3    1490    24.2    53.3    1.36      NA\n# … with 47 more rows, 15 more variables: enr_pct_2_se &lt;dbl&gt;, enr_pct_4 &lt;dbl&gt;,\n#   enr_pct_4_se &lt;dbl&gt;, enr_pct_m &lt;dbl&gt;, enr_pct_m_se &lt;dbl&gt;, enr_pct_2_m &lt;dbl&gt;,\n#   enr_pct_2_m_se &lt;dbl&gt;, enr_pct_4_m &lt;dbl&gt;, enr_pct_4_m_se &lt;dbl&gt;,\n#   enr_pct_f &lt;dbl&gt;, enr_pct_f_se &lt;dbl&gt;, enr_pct_2_f &lt;dbl&gt;,\n#   enr_pct_2_f_se &lt;dbl&gt;, enr_pct_4_f &lt;dbl&gt;, enr_pct_4_f_se &lt;dbl&gt;, and\n#   abbreviated variable names ¹​hs_comp_tot, ²​hs_comp_tot_se, ³​hs_comp_m,\n#   ⁴​hs_comp_m_se, ⁵​hs_comp_f, ⁶​hs_comp_f_se, ⁷​enr_pct_se, ⁸​enr_pct_2\n\n\nGot it!"
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#reshape-data",
    "href": "Extra-03-Web-Scraping.html#reshape-data",
    "title": "Extra: Web Scraping",
    "section": "Reshape data",
    "text": "Reshape data\nWe could stop where we are, but to make the data more usable in the future, let’s convert to a long data frame. This takes a couple of steps, but the idea is to have each row represent a year by estimate, with a column for the estimate value and a column for the standard error on that estimate. It may help to run the code below one line at a time, checking the progress at each step.\n\n## gather for long data\ndf &lt;- tab %&gt;%\n  ## pivot_longer estimates, leaving standard errors wide for the moment\n  pivot_longer(cols = -c(year, ends_with(\"se\")),\n               names_to = \"group\",\n               values_to = \"estimate\") %&gt;%\n  ## pivot_longer standard errors\n  pivot_longer(cols = -c(year, group, estimate),\n               names_to = \"group_se\",\n               values_to = \"se\") %&gt;% \n  ## drop \"_se\" from standard error estimates\n  mutate(group_se = str_replace(group_se, \"_se\", \"\")) %&gt;%\n  ## filter where group == group_se\n  filter(group == group_se) %&gt;%\n  ## drop extra column\n  select(-group_se) %&gt;%\n  ## arrange\n  arrange(year) %&gt;%\n  ## drop if missing year after reshaping\n  drop_na(year)\n\n## show\ndf\n\n# A tibble: 684 × 4\n    year group       estimate    se\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1  1960 hs_comp_tot   1679   44.5 \n 2  1960 hs_comp_m      756   32.3 \n 3  1960 hs_comp_f      923   30.1 \n 4  1960 enr_pct         45.1  2.16\n 5  1960 enr_pct_2       NA   NA   \n 6  1960 enr_pct_4       NA   NA   \n 7  1960 enr_pct_m       54    3.23\n 8  1960 enr_pct_2_m     NA   NA   \n 9  1960 enr_pct_4_m     NA   NA   \n10  1960 enr_pct_f       37.9  2.85\n# … with 674 more rows"
  },
  {
    "objectID": "Extra-03-Web-Scraping.html#plot-trends",
    "href": "Extra-03-Web-Scraping.html#plot-trends",
    "title": "Extra: Web Scraping",
    "section": "Plot trends",
    "text": "Plot trends\nLet’s look at overall college enrollment percentages for recent graduates over time. Because our data are nicely formatted, it’s easy to subset the full table to data to only those estimates we need as well as generate 95% confidence intervals.\n\n## adjust data for specific plot\nplot_df &lt;- df %&gt;%\n  filter(group %in% c(\"enr_pct\", \"enr_pct_m\", \"enr_pct_f\")) %&gt;%\n  mutate(hi = estimate + se * qnorm(.975),\n         lo = estimate - se * qnorm(.975),\n         year = ymd(as.character(year), truncated = 2L),\n         group = ifelse(group == \"enr_pct_f\", \"Women\",\n                        ifelse(group == \"enr_pct_m\", \"Men\", \"All\")))\n\n## show\nplot_df\n\n# A tibble: 171 × 6\n   year       group estimate    se    hi    lo\n   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1960-01-01 All       45.1  2.16  49.3  40.9\n 2 1960-01-01 Men       54    3.23  60.3  47.7\n 3 1960-01-01 Women     37.9  2.85  43.5  32.3\n 4 1961-01-01 All       48    2.12  52.2  43.8\n 5 1961-01-01 Men       56.3  3.14  62.5  50.1\n 6 1961-01-01 Women     41.3  2.81  46.8  35.8\n 7 1962-01-01 All       49    2.08  53.1  44.9\n 8 1962-01-01 Men       55    3     60.9  49.1\n 9 1962-01-01 Women     43.5  2.84  49.1  37.9\n10 1963-01-01 All       45    2.12  49.2  40.8\n# … with 161 more rows\n\n\nFirst, let’s plot the overall average. Notice that we use the filter() function in the ggplot() function to remove the subgroup estimates for men and women.\n\n## plot overall average\ng &lt;- ggplot(plot_df %&gt;% filter(group == \"All\"),\n            mapping = aes(x = year, y = estimate)) +\n  ## create shaded ribbon for 95% CI\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"grey70\") +\n  ## line for main estimate\n  geom_line() +\n  ## make x-axis look nice\n  ## major breaks: every 5 years, from min year to max year\n  ## minor breaks: every 1 year, from min year to max year\n  ## labels: formate to only show year (\"%Y\")\n  scale_x_date(breaks = seq(min(plot_df$year),\n                            max(plot_df$year),\n                            \"5 years\"),\n               minor_breaks = seq(min(plot_df$year),\n                                  max(plot_df$year),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## good labels and titles\n  labs(x = \"Year\",\n       y = \"Percent\",\n       title = \"Percent of recent high school completers in college: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\")    \n\n## show\ng\n\n\n\n\nAfter a small dip in the early 1970s enrollment trends have steadily risen over time.\nNow let’s compare enrollments over time between men and women (dropping the overall average so our plot is clearer).\n\n## plot comparison between men and women\ng &lt;- ggplot(plot_df %&gt;% filter(group %in% c(\"Men\",\"Women\")),\n            ## add colour == group to separate between men and women\n            mapping = aes(x = year, y = estimate, colour = group)) +\n  ## ribbon for 95% CI, but lower alpha so more transparent\n  geom_ribbon(aes(ymin = lo, ymax = hi, fill = group), alpha = 0.2) +\n  ## primary estimate line\n  geom_line() +\n  ## neat x-axis breaks as before\n  scale_x_date(breaks = seq(min(plot_df$year),\n                            max(plot_df$year),\n                            \"5 years\"),\n               minor_breaks = seq(min(plot_df$year),\n                                  max(plot_df$year),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## good labels and titles\n  labs(x = \"Year\",\n       y = \"Percent\",\n       title = \"Percent of recent high school completers in college: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\") +\n  ## set legend title, drop legend for colour since it's redundant with fill\n  guides(fill = guide_legend(title = \"Group\"),\n         colour = \"none\") +\n  ## position legend so that it sits on plot face, in lower right-hand corner\n  theme(legend.position = c(1,0), legend.justification = c(1,0))\n\n## show\ng\n\n\n\n\nThough a greater proportion of men enrolled in college in the 1960s and early 1970s, women have been increasing their enrollment percentages faster than men since the 1980s and now have comparatively higher rates of college participation."
  },
  {
    "objectID": "Extra-07-Ben-Mapping.html#libraries-and-paths",
    "href": "Extra-07-Ben-Mapping.html#libraries-and-paths",
    "title": "Extra: Ben’s Mapping Lesson",
    "section": "Libraries and paths",
    "text": "Libraries and paths\nIn addition to tidyverse, we’ll use the sf library and usmap, the latter of which provides some nicely formatted geospatial data for the United States. Before you start the lesson, you’ll need to install both libraries: install.packages(c(\"sf\", \"usmap\")).\nNOTE Once upon a time, getting the requisite spatial software installed on your computer could be very difficult, with lots of separate installations that could be quite finicky. But these days — unless you have a strange installation of R — you should be able to just install the sf library and have things work. If you run into trouble, it’s probably some background program that needs to be updated or installed. If you end up with errors, copy and paste them into Google. You will almost certainly find others who’ve had the same problem and, hopefully (!), the solution. You can also check out the sf library webpage.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(usmap)"
  },
  {
    "objectID": "Extra-07-Ben-Mapping.html#projections-and-choropleths",
    "href": "Extra-07-Ben-Mapping.html#projections-and-choropleths",
    "title": "Extra: Ben’s Mapping Lesson",
    "section": "Projections and choropleths",
    "text": "Projections and choropleths\nIn this first section, we’ll work with a map of the United States to look at differences in Bachelor’s degree attainment across the states. But before we read in the data, let’s talk about a common format for geographic data files, the shapefile.\nFirst of all, what is a shapefile? While we can always rely on Wikipedia for the technical answer, the gist is that it’s a special data structure that, in addition to the normal data frames we are used to working with, contains geometric information. This geographic information includes points, lines, and polygons that can be used to create multidimensional figures such as maps or to perform spatial analyses. Linking these geometric features with the data frame, a researcher can describe and analyze the spatial dimensions of data. For example, here are some questions that are similar to some that I’ve explored in the past:\n\nHow close is this high school to the nearest open admissions college?\nHow many colleges are within 10/50/100 miles of this census block?\nAre there differences in household earnings or average degree attainment between these proximate zip codes?\n\nOf course, there are many more types of spatial questions to ask. If you have a research question that has a spatial dimension, then you are likely to use a shapefile or another spatial data format (we use another type below). If you look inside the cb_2018_us_state directory that’s in data/geo/, you’ll see a number of files with the same name but different file endings:\n./data/geo/cb_2018_us_state_20m/\n├── cb_2018_us_state_20m.cpg\n├── cb_2018_us_state_20m.dbf\n├── cb_2018_us_state_20m.prj\n├── cb_2018_us_state_20m.shp\n├── cb_2018_us_state_20m.shp.ea.iso.xml\n├── cb_2018_us_state_20m.shp.iso.xml\n└── cb_2018_us_state_20m.shx\nWhile the file ending in *.shp is technically the “shapefile,” the other files contain important information and are linked to the primary shapefile. In other words, think of the collection of files as the shapefile. So keep in mind: in the future when you download and/or share a shapefile — often in a *.zip format — you need to keep/include all the other files. If you are curious about what the other files do, check out this ArcGIS page.\n\nRead in the data\nLet’s read in the data. Notice that we use a special function from the sf library, st_read() and that we use the *.shp file ending. Other than that, this should look like most of the other read functions we’ve used before. Pipes from the tidyverse work here, so after reading in the data, we’ll go ahead and lower the data frame column names (to save our pinkies from all the shift key work!) and convert the state FIPS codes, which uniquely identify each state, from character strings to integers. This will make our lives easier later when we filter our data set.\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## reading in the least detailed cartographic boundary (cb) file from 2018\ndf_us &lt;- st_read(file.path(\"data\", \"geo\", \"cb_2018_us_state_20m\", \"cb_2018_us_state_20m.shp\")) %&gt;%\n  ## renaming the data frame columns to all lowercase\n  rename_all(tolower) %&gt;%\n  ## converting state fips codes (two digit) to numbers for later filtering\n  mutate(statefp = as.integer(statefp))\n\nReading layer `cb_2018_us_state_20m' from data source \n  `/Users/Matt/Desktop/7916/data/geo/cb_2018_us_state_20m/cb_2018_us_state_20m.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n\nAs it’s read in, we get a little more information that tells us:\n\nThe file path and driver (what’s understanding the data structure): ESRI Shapefile\nThat it’s a Simple feature collection, that is, a special data structure with 52 features (unique items: points, polygons, etc) and 9 fields (columns of information)\nThe Geometry type, which is this case is a MULTIPOLYGON\nThe coordinate dimension, which are xy here (think 2D)\nHow big the full area covered is, in the scale of the underlying data. Since the default for this data file is geodesic (on the globe), we get longitude (x) and latitude (y) values\nThe Coordinate Reference System (CRS), which determines how (if at all) the shapes are projected. In this case, they are not — they remain specified as if on a globe — with a datum (center point inside the globe) NAD83\n\nWe’ll talk a little more about projections below. First, let’s call the data frame, df_us, by itself.\n\n## show\ndf_us\n\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\nFirst 10 features:\n   statefp  statens    affgeoid geoid stusps         name lsad        aland\n1       24 01714934 0400000US24    24     MD     Maryland   00  25151100280\n2       19 01779785 0400000US19    19     IA         Iowa   00 144661267977\n3       10 01779781 0400000US10    10     DE     Delaware   00   5045925646\n4       39 01085497 0400000US39    39     OH         Ohio   00 105828882568\n5       42 01779798 0400000US42    42     PA Pennsylvania   00 115884442321\n6       31 01779792 0400000US31    31     NE     Nebraska   00 198956658395\n7       53 01779804 0400000US53    53     WA   Washington   00 172112588220\n8       72 01779808 0400000US72    72     PR  Puerto Rico   00   8868896030\n9        1 01779775 0400000US01    01     AL      Alabama   00 131174048583\n10       5 00068085 0400000US05    05     AR     Arkansas   00 134768872727\n        awater                       geometry\n1   6979966958 MULTIPOLYGON (((-76.04621 3...\n2   1084180812 MULTIPOLYGON (((-96.62187 4...\n3   1399985648 MULTIPOLYGON (((-75.77379 3...\n4  10268850702 MULTIPOLYGON (((-82.86334 4...\n5   3394589990 MULTIPOLYGON (((-80.51989 4...\n6   1371829134 MULTIPOLYGON (((-104.0531 4...\n7  12559278850 MULTIPOLYGON (((-123.2371 4...\n8   4922382562 MULTIPOLYGON (((-65.34207 1...\n9   4593327154 MULTIPOLYGON (((-88.46866 3...\n10  2962859592 MULTIPOLYGON (((-94.61792 3...\n\n\nWe get the same information as above, but now we can see the data frame, too. This shapefile is pretty spare, giving us some basic identifying data, such as the state name and land and water area within the boundaries. In the last column, you can see the geometry, which has the geometry type (MULTIPOLYGON here), as well the beginnings of some numbers nested in parentheses. This is the extra geometry information that we can use to make our maps.\n\n\nMapping and changing projections\nLet’s make our first map. The sf library is powerful because it smoothly integrates with ggplot. The new geom_* we’ll use is geom_sf(), which tells ggplot what it needs to know to make the map. Because we’re making a map and don’t really care about axes or all that (at least in this instance), we’ll also add theme_void() at the end of the call to remove everything but the map we plot.\n\n## ---------------------------\n## initial plot\n## ---------------------------\n\np &lt;- ggplot(df_us) +\n  geom_sf() +\n  theme_void()\n\n## show\np\n\n\n\n\nOk…\nIt’s a map, but not super useful. What’s going on? Well — depending on your screen resolution — you might be able to tell that this map of the United States includes various territories. Because these span the globe, the map has to fit a huge spherical area into a smaller, flatter space. This is what we get.\nSince our plan is look at degree attainment in the states, we’ll reduce our data to only include states. We’ll also start by removing Alaska and Hawaii so that we can first practice with the lower 48 or contiguous states.\n\n## ---------------------------\n## lower 48 only plot\n## ---------------------------\n\n## filter to keep only states (fips &lt;= 56, Wyoming) and then drop AK/HI\ndf_l48 &lt;- df_us %&gt;%\n  filter(statefp &lt;= 56) %&gt;%\n  filter(statefp != 02 & statefp != 15)\n\n## same plot with reduced data\np &lt;- ggplot(df_l48) +\n  geom_sf() +\n  theme_void()\n\n## show\np\n\n\n\n\nNow that we’ve limited our data and effectively zoomed in on the lower 48 states, our map is clearer and much more useful.\nWe have another issue, however. If you are used to looking at maps of the United States, our figure looks a little flat and elongated. The US - Canada border is very straight and the western part of the country looks sort of stretched. This is because our data was unprojected and sf had to pick a projection in order to make the figure.\nWhat do we mean by projection? The fundamental problem of mapping is this: we live on a spherical globe (technically closer to an oblate spheroid) but want to put maps on a flat, 2D surface. Imagine peeling the skin off an orange in one piece and trying to lay it flat. If you want the peel to have the same size and everything to roughly line up, then you’ll have gaps between the edges. Alternately, you can remove the gaps and keep straight lines by stretching the peel, but then you have a stretched surface! In other words, there’s always a compromise. Most common projections try to preserve one metric:\n\nConformal (preserve angles)\nEqual area (preserve area)\nEquidistant (preserve distances between points)\n\nThere is no single correct projection: it really depends on what you want to do in your analyses. If you are calculating distances, then you need to make sure your project preserves distance. If you want to know about areas of overlapping shapes, then you will want to preserve area with the projection you use. Check out this pdf for more information about projections or this report from the U.S. Geological Survey if you really want to know more!\nAlternately, if you aren’t doing any spatial analyses, then you can just select the projection you find most aesthetically pleasing. When faced with unprojected data, the sf library will by default choose an equirectangular projection in which longitudinal and latitudinal lines are straight and at right angles. This is how we get our stretched United States.\nLet’s choose a different Coordinate Reference System (CRS) so that our map takes on a different shape. We’ll use the USA Contiguous Albers Equal Area Conic, which is good for North America and specifically the lower 48 states. To reset the CRS, we’ll use st_transform() with the correct EPSG numeric code for the projection we want: 5070.\n\n## ---------------------------\n## transform CRS (re-project)\n## ---------------------------\n\n## change CRS to something that looks better for US\ndf_l48 &lt;- st_transform(df_l48, crs = 5070)\n\n## same plot, but should look different b/c we changed projection\np &lt;- ggplot(df_l48) +\n  geom_sf() +\n  theme_void()\n\n## show\np\n\n\n\n\nMuch nicer!\n\nQuick exercise\nRe-project and re-plot your lower 48 states map df_l48 using a CRS code of 3338 (good for Alaska) and 32136 (good for Tennessee). What happens to the map each time?\n\nOf course, we dropped Alaska and Hawaii to make this figure and need to bring them back. A common solution is to place them somewhere under the southwestern states. We could do this ourselves, but it takes a lot of manipulation of our the geometric data. The good news is that someone has already done this work for us! Since we’re only concerned about state-level data, we’ll use the us_map() function from the usmaps library.\n\n## ---------------------------\n## use usmaps to get AK/HI\n## ---------------------------\n\n## load usmap data for states\ndf_usmap &lt;- us_map(regions = \"states\")\n\nThe usmaps library has its own plotting function that uses ggplot under the hood, but since not all geospatial libraries have that, we’ll set up the data to work with sf. To do that, we’ll use the st_as_sf() function to set the underlying data as simple features data and then combine the data points into polygons using a chain of group_by(), summarise(), and then st_cast(). Run each line on its own if you want to see what’s happening with each step.\n\n## need to set up to work with sf\n## start by setting up sf data frame with correct projection\ndf_usmap &lt;- st_as_sf(df_usmap, coords = c(\"x\", \"y\"), crs = 5070) %&gt;%\n  ## group by state / group (some states have non-contiguous parts like islands)\n  group_by(fips, group) %&gt;%\n  ## combine these points into single geometry\n  summarise(geometry = st_combine(geometry),\n            .groups = \"drop\") %&gt;%\n  ## reset these points as polygons (think dots to lines that connect)\n  st_cast(\"POLYGON\")\n\nNow that the data are set up correctly, we can run the same plotting code as before, only changing the data.\n\n## Alaska and Hawaii now included, but moved for tighter plot\np &lt;- ggplot(df_usmap) +\n  geom_sf() +\n  theme_void()\n\n## show\np\n\n\n\n\nAnd now we have all the states in a nice projection, ready to make a choropleth map.\n\n\nChoropleth\nA choropleth map is just a map that uses color to show differences across areas. Probably the most common choropleth maps people see are during an election to show winners by political party in different precincts or states. For our example, we’ll show differences in Bachelor’s degree attainment rates for 25 to 44 year-olds from 2005 to 2019 across the states. First, we’ll read in our data, which come from the U.S. Census’s American Community Survey.\n\n## ---------------------------\n## BA attainment 2005-2019\n## ---------------------------\n\n## read in BA attainment (25-44yo) data\ndf_ba &lt;- read_csv(file.path(\"data\", \"geo\", \"ba_25_44.csv\"),\n                  show_col_types = FALSE)\n\n## show top of data\ndf_ba\n\n# A tibble: 780 × 5\n   stfips stabbr stname   year bapct\n    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1      1 AL     Alabama  2005  23.5\n 2      1 AL     Alabama  2006  22.6\n 3      1 AL     Alabama  2007  23.6\n 4      1 AL     Alabama  2008  24.1\n 5      1 AL     Alabama  2009  24.6\n 6      1 AL     Alabama  2010  23.8\n 7      1 AL     Alabama  2011  24.7\n 8      1 AL     Alabama  2012  26.2\n 9      1 AL     Alabama  2013  25  \n10      1 AL     Alabama  2014  25.7\n# … with 770 more rows\n\n\nFirst thing we need to do is join our new attainment data to our spatial data frame. After creating a new join variable with the same name and data type (stfips = as.integer(fips)), we can use left_join() just as we’ve done in prior lessons.\n\n## join data to mapping data\ndf_usmap &lt;- df_usmap %&gt;%\n  ## create a join variable, stfips, that matches what's in df_ba\n  mutate(stfips = as.integer(fips)) %&gt;%\n  ## left_join as usual\n  left_join(df_ba, by = \"stfips\")\n\nWarning in sf_column %in% names(g): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\nFor our first map, we’ll filter to the latest year, 2019. So that the color changes by BA percentage, we’ll add aes(fille = bpct) in geom_sf(). Everything else is the same as before.\n\n## plot with one year of BA attainment\np &lt;- ggplot(df_usmap %&gt;% filter(year == 2019)) +\n  geom_sf(aes(fill = bapct)) +\n  theme_void()\n\n## show\np\n\n\n\n\nTo show changes over time, we’ll filter in more years — 2005, 2010, 2015, 2019 — and include facet_wrap(~ year), which will split our plot into four panels, one for each year. Again, this is very similar to plotting we’ve done before with ggplot.\n\n## plot with 4 years of BA attainment\np &lt;- ggplot(df_usmap %&gt;% filter(year %in% c(2005, 2010, 2015, 2019))) +\n  facet_wrap(~ year) +\n  geom_sf(aes(fill = bapct)) +\n  theme_void()\n\n## show\np\n\n\n\n\nJust like our first figure, the differences are somewhat difficult to see. Perhaps another color gradient would help. Alternately, we can split our continuous percentages into a binary variable at some threshold. Since a fairly common national statistic of Bachelor’s degree attainment is around 1/3 of adults, we’ll pick 33% and use an ifelse() statement to create bacut.\n\n## create new variable that's == 1 if BA attainment is &gt;= 33%\ndf_usmap &lt;- df_usmap %&gt;%\n  ## thinking ahead: use strings in yes/no options that will look good in legend\n  mutate(bacut = ifelse(bapct &gt;= 33, \"33%+\", \"&lt;33%\"))\n\nThis time, we’ll use our new variable bacut in the fill aesthetic. We’ll also make our legend nicer with a title using the name option with scale_fill_discrete().\n\n## plot with 4 years of BA attainment\np &lt;- ggplot(df_usmap %&gt;% filter(year %in% c(2005, 2010, 2015, 2019))) +\n  facet_wrap(~year) +\n  geom_sf(aes(fill = bacut)) +\n  scale_fill_discrete(name = \"BA attainment\") +\n  theme_void()\n\n## show\np\n\n\n\n\nThis last figure makes cross-sectional and diachronic changes much clearer. We could add further titles and a caption, but the figure already looks pretty good!\n\nQuick exercise\nChoose a different cut point for BA attainment and different years. Plot another small multiples plots showing changes over time."
  },
  {
    "objectID": "Extra-07-Ben-Mapping.html#spatial-joins",
    "href": "Extra-07-Ben-Mapping.html#spatial-joins",
    "title": "Extra: Ben’s Mapping Lesson",
    "section": "Spatial joins",
    "text": "Spatial joins\nIn this second section, we’ll perform a couple of simple spatial joins. With spatial work, you’ll often have one of these two situations:\n\nSpecific locations, like a school, that you want to place in its specific geographic location, such as its attendance zone or county\nTwo different spatial polygons, like zip codes and census tracts, that don’t perfectly align but that you want to intersect\n\nWe’ll practice both of these procedures, which aren’t too complicated if your data are properly set up, meaning that you’re using spatial data sets that use the same projection (CRS). That’s our case here, so we’re good to go. Instead of the entire country, we’ll use school attendance zones, school locations, and zip code areas in Alachua County, Florida, to practice spatial joins.\nFirst, we’ll read it the data. Rather than using shapefiles, we’ll use GeoJSON files this time, which are a special type of JSON file (a plain text data file often used in web applications) that contain geometric information.\n\nRead in data\n\n## ---------------------------\n## Alachua County: school/zip\n## ---------------------------\n\n## school attendance zones; school locations; zip codes\ndf_zon &lt;- st_read(file.path(\"data\", \"geo\", \"ac_school_zones.geojson\"))\n\nReading layer `ac_school_zones' from data source \n  `/Users/Matt/Desktop/7916/data/geo/ac_school_zones.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 41 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -82.65812 ymin: 29.41713 xmax: -82.0497 ymax: 29.94532\nGeodetic CRS:  WGS 84\n\ndf_sch &lt;- st_read(file.path(\"data\", \"geo\", \"ac_school_locs.geojson\"))\n\nReading layer `ac_school_locs' from data source \n  `/Users/Matt/Desktop/7916/data/geo/ac_school_locs.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 41 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -82.61531 ymin: 29.52004 xmax: -82.09089 ymax: 29.83443\nGeodetic CRS:  WGS 84\n\ndf_zip &lt;- st_read(file.path(\"data\", \"geo\", \"ac_zipcodes.geojson\"))\n\nReading layer `ac_zipcodes' from data source \n  `/Users/Matt/Desktop/7916/data/geo/ac_zipcodes.geojson' using driver `GeoJSON'\nSimple feature collection with 22 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -82.76944 ymin: 29.40396 xmax: -81.88346 ymax: 30.00484\nGeodetic CRS:  WGS 84\n\n\n\n\nPoints in polygons\nFirst, we’ll plot each set of school attendance zones with school locations as points. We’ll explicitly convert the school level variable into a factor() so that we can order the plot from elementary to middle to high school — rather than alphabetical order.\n\n## plot school zones + schools\np &lt;- ggplot(df_zon) +\n  ## convert school level to a factor so we can order it\n  facet_wrap(~ factor(level, levels = c(\"elementary\",\n                                        \"middle\",\n                                        \"high\"))) +\n  ## change the color of zones by their code (just for variety)\n  geom_sf(aes(fill = code)) +\n  ## remove legend since it doesn't really tell us much\n  scale_fill_discrete(guide = \"none\") +\n  theme_void()\n\n## show\np\n\n\n\n\nNow that we’ve seen the attendance zones, let’s plot the zip code areas.\n\n## plot zip codes zones + schools\np &lt;- ggplot(df_zip) +\n  geom_sf(aes(fill = zip)) +\n  scale_fill_discrete(guide = \"none\") +\n  theme_void()\n\n## show\np\n\n\n\n\nSince zip codes are really about postal routes — meaning they are lines that have been converted into loose “areas” — the overall shape is a little strange and not as easily recognizable as Alachua County.\nLet’s answer this question: Which zip code contains the most schools? To begin, we need to join our school data set to the zip code data set. But instead of joining on a shared key like we’ve done in prior lessons, we want schools to join based on the zip code they fall inside.\nTo join school locations to the zip code areas in this way, we use st_join() which is like left_join(), but takes into account the geometry column in the data.\n\n## join schools to zip codes\ndf_sch_zip &lt;- st_join(df_sch, df_zip)\n\n## show\ndf_sch_zip\n\nSimple feature collection with 41 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -82.61531 ymin: 29.52004 xmax: -82.09089 ymax: 29.83443\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   objectid.x schoolcode                                  facility\n1           1     010461 HIGH SPRINGS ELEMENTARY AND MIDDLE SCHOOL\n2           3     010520             MEADOWBROOK ELEMENTARY SCHOOL\n3           4     010271                      SANTA FE HIGH SCHOOL\n4           5     010221                      MEBANE MIDDLE SCHOOL\n5           7     010161                 ALACHUA ELEMENTARY SCHOOL\n6           8     010561                  TALBOT ELEMENTARY SCHOOL\n7           9     010041                  FOSTER ELEMENTARY SCHOOL\n8          10     010341                RAWLINGS ELEMENTARY SCHOOL\n9          11     010331            GLEN SPRINGS ELEMENTARY SCHOOL\n10         12     010481                   FT CLARKE MIDDLE SCHOOL\n           function_ grades            fulladdr addressid grade2019\n1  ELEMENTARY/MIDDLE   PK-8      1015 N MAIN ST      6989         A\n2         ELEMENTARY   PK-5    11525 NW 39TH AV    116451         A\n3               HIGH   9-12 16213 NW US HWY 441      6988         B\n4             MIDDLE    6-8   16401 NW 140TH ST      6890         C\n5         ELEMENTARY    3-5   13800 NW 152ND PL     62717         C\n6         ELEMENTARY   PK-5     5701 NW 43RD ST     51416         B\n7         ELEMENTARY   PK-5      3800 NW 6TH ST     22495         B\n8         ELEMENTARY   PK-5     3500 NE 15TH ST      3712         C\n9         ELEMENTARY   PK-5     2826 NW 31ST AV      7258         C\n10            MIDDLE    6-8     9301 NW 23RD AV     30480         B\n                               link objectid.y      po_name   zip shape__area\n1   https://www.sbac.edu/Page/11863         15 HIGH SPRINGS 32643  2764000220\n2  https://www.sbac.edu/meadowbrook          4  GAINESVILLE 32606   489129738\n3      https://www.sbac.edu/santafe          9      ALACHUA 32615  3966453434\n4       https://www.sbac.edu/mebane          9      ALACHUA 32615  3966453434\n5      https://www.sbac.edu/alachua          9      ALACHUA 32615  3966453434\n6       https://www.sbac.edu/talbot         16  GAINESVILLE 32653  1082671494\n7       https://www.sbac.edu/foster          7  GAINESVILLE 32609  3423179965\n8     https://www.sbac.edu/rawlings          7  GAINESVILLE 32609  3423179965\n9  https://www.sbac.edu/glensprings          3  GAINESVILLE 32605   273759072\n10  https://www.sbac.edu/fortclarke          4  GAINESVILLE 32606   489129738\n   shape__length                   geometry\n1       399292.3 POINT (-82.58993 29.83443)\n2       158624.8 POINT (-82.46278 29.68815)\n3       433830.7 POINT (-82.52388 29.80632)\n4       433830.7 POINT (-82.49382 29.80557)\n5       433830.7  POINT (-82.4924 29.79456)\n6       248871.3 POINT (-82.38717 29.70745)\n7       347927.6  POINT (-82.3318 29.68818)\n8       347927.6 POINT (-82.30691 29.68426)\n9       128932.4 POINT (-82.36473 29.68239)\n10      158624.8 POINT (-82.44178 29.67324)\n\n\nNow that we’ve placed schools in zip codes, we can answer the question: which zip code has the most schools? Other than the second line of code, st_drop_geometry(), which removes the geometry features of the data set and makes it a plain data frame that’s much easier to manipulate, the rest of the dplyr chain should look familiar. To answer our question, we:\n\ngroup_by() zip column so that we are looking within zip code\nsummarise() and use n() to count the number of rows within each zip code, which, after our join, is the number of unique schools\narrange() using desc() so that the zip code with the maximum number is at the top of the printed data frame\n\n\n## what zip code has the most schools?\ndf_sch_zip %&gt;%\n  ## drop geometry so that we are left with simple data frame\n  st_drop_geometry() %&gt;%\n  ## group by zip code\n  group_by(zip) %&gt;%\n  ## use n() to get number of rows (i.e., schools) in each zip group\n  summarise(num_schools = n()) %&gt;%\n  ## arrange results in descending order so that max is first\n  arrange(desc(num_schools))\n\n# A tibble: 13 × 2\n   zip   num_schools\n   &lt;chr&gt;       &lt;int&gt;\n 1 32641           7\n 2 32609           6\n 3 32608           5\n 4 32605           4\n 5 32606           4\n 6 32615           4\n 7 32669           3\n 8 32601           2\n 9 32640           2\n10 32603           1\n11 32618           1\n12 32643           1\n13 32653           1\n\n\nThere’s our answer!\n\nQuick exercise\nRerun the last bit of code, but store the results in a tibble this time. Join the tibble back to the df_zip sf object and make a plot that color codes each zip code by the number of schools it contains.\n\n\n\nOverlapping polygons\nFor this last task, we’ll zoom into a single school zone, Lake Forest Elementary. We’ll plot it first.\n\n## what zip code has the most schools?\np &lt;- ggplot(df_zon %&gt;% filter(facility == \"Lake Forest Elementary\")) +\n  geom_sf() +\n  theme_void()\n\n## show\np\n\n\n\n\nKnowing that zip codes and schools aren’t perfectly aligned, that is, a single school zone can be made up of multiple zip code areas and vice versa, we ask: how many zip codes are make up the Lake Forest Elementary” attendance zone?\nSince we have two sets of polygons, we’ll join using st_intersection(). This particular join will use the left data frame as the starting point and then keep the parts of the second data frame that fit within (intersect) the first. Therefore, order matters here.\nOnce we’ve done that, we’ll plot the intersected data frame and fill using zip so that we can see the zip code areas clearly.\n\n## join schools to zip codes\ndf_zon_zip &lt;- st_intersection(df_zon, df_zip)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n## what zip code has the most schools?\np &lt;- ggplot(df_zon_zip %&gt;% filter(facility == \"Lake Forest Elementary\")) +\n  geom_sf(aes(fill = zip)) +\n  scale_fill_discrete(name = \"Zip Code\") +\n  theme_void()\n\n## show\np\n\n\n\n\nNow we can see that the Lake Forest Elementary attendance zone is made up of three zip codes: mostly 32609 and 32641 with a little bit of 32640 in the southeast corner. We didn’t project our data using an area-preserving projection before doing these joins, so we would want to be careful computing relative areas (though at this very small area, the projected differences aren’t likely to change much)."
  },
  {
    "objectID": "Extra-07-Ben-Mapping.html#further-information",
    "href": "Extra-07-Ben-Mapping.html#further-information",
    "title": "Extra: Ben’s Mapping Lesson",
    "section": "Further information",
    "text": "Further information\nAs I said at the beginning of the lesson, we’ve only started with the kinds of spatial analyses possible in R. If you want to learn more, you might check out this online resource or this book."
  },
  {
    "objectID": "09-Data-Wrangling-IV.html",
    "href": "09-Data-Wrangling-IV.html",
    "title": "IV: Tidyverse Tricks & SQL",
    "section": "",
    "text": "LessonAssignment\n\n\nYou can download R  here."
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#why-use-git",
    "href": "Extra-01-Setup-GitHub.html#why-use-git",
    "title": "Extra: Git & GitHub",
    "section": "Why use git?",
    "text": "Why use git?\nWith so many other (read: easier!) ways to share and collaborate on documents, why use git? Isn’t it a bit overkill to learn an entirely new syntax? Why not just email files or use something like DropBox? Because it is very easy to end up with something like this:\n\n\n\n\n\nCredit: Jorge Chan\n\nAs complexity increases, so does the need for git\n\\[ Project = f(size, scope, collaborators) \\]\nAs any part of that function grows, so too does the need for a work flow that:\n\nAllows for many moving parts\nAllows for peaceful collaboration (no overwrites)\nAllows for timely collaboration (synchronous)\nAllows for distant collaboration (centralized file location)\nAllows for version control (so you can go back if necessary)\n\nGit was designed to do all these things, so it works better than other systems.\n\n\nWhat’s the difference between git and GitHub?\nYep, you guessed it. Git is the system/language that supports version control. GitHub is an online service that will host your remote repositories, for free if public or a small fee if private. (Students get an education discount and some free repos. Check out https://education.github.com/.)\nRStudio is nice because it provides an nice point-and-click interface for git. (Sourcetree and GitHub Desktop are also really nice GUIs.) If you want to run git at the command line, go for it! But using RStudio or another GUI is fine.\n\n\nHow does git/GitHub work?\n\n\n\n\n\nCredit: Lbhtw (Own work)\n\n\nPlain text and git\nGit works best with plain text files. This is because it notes the differences across two plain text files rather than just copying and re-copying the same file over and over again as it changes. When you’ve only changed one word, git’s method of version control is much more efficient than making a whole new copy.\nIt’s also useful when you need to merge two files. If file B is just file A with new section, file B can be easily merged into file A by inserting the new section — just like you would add a paragraph in a paper you’re writing. R scripts are plain text. Some data files, like *.csv, are plain text. This is why git works really well with data analysis workflows.\nOn the other hand, git does not work as well with binary files. These files are stored in a format closer to what your computer understands, which comes with benefits. Data files, like Stata’s .dta and R’s .Rdata, as well as MS Office files — .docx, .xlsx, *.pptx, etc — are binary files. Git will keep track of your MS Word document, but due to its underlying structure, you won’t be able to merge and every small change will just make a whole new copy of the file. This is why we generally don’t commit large binary data files to Git: your repo just becomes larger and larger with each small change to your data.\n\n\nSome notes on work flow (good habits)\n\nAlways pull from your local repo with your remote repo before starting any work. This makes sure you have the latest files. RStudio has a pull button in the Git tab.\nDon’t wait to push your commits. Just like you save your papers every few lines or paragraphs, you should push to your remote repo. This way, you’re less likely to lose work in the event of a computer crash. Also, should you want to return to a prior version, small changes make it easier to find what you want.\n\nAdd useful commit messages. RStudio will make you add something. Don’t say “stuff.” A version history of 95 “stuff”s is pretty non-helpful. Also, I would keep it clean. Even in a private repository, work as if someone will see what you write someday.\n\nDon’t freak out! If you accidentally push and overwrite or delete something on the remote, you can get it back. That’s the point of version control! Sometimes weird things like merges happen (two files with the same name are shoved together). They can be a pain to fix, but they can be fixed.\nAvoid tracking overly large files and pushing them to your remote repository. GitHub has a file size limit of 100 MB per file and 1 GB per repo. The history of large files compounds quickly, so think carefully before adding them. Usually this means that small data sets are okay; big ones are better backed up somewhere else and left on your machine.\nRemember: even in a private repository, your files are potentially viewable. If any of your data sets or files are restricted, do no push them remotely. Use .gitignore to make sure that git doesn’t track or push them.\n\nEvery class and work session should go like this:\n\npull from GitHub remote\ndo work\nadd or stage (if new file)\ncommit with message\npush back to GitHub remote\n\nIf you’d like you can also use terminal commands to accomplish the same things. Many people (including me) like to use the terminal commands directly.\n\nPull: git pull\nStage: git add &lt;filenames&gt;\nCommit: git commit -m \"&lt;your message here&gt;\"\nPush: git push\n\nThat said, you should use whatever works best for you."
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#getting-your-class-repo-and-linking-with-git",
    "href": "Extra-01-Setup-GitHub.html#getting-your-class-repo-and-linking-with-git",
    "title": "Extra: Git & GitHub",
    "section": "Getting your class repo and linking with git",
    "text": "Getting your class repo and linking with git\nIn this class, you’ll work in and submit your work through a private git repository that only you and I will be able to access. I’ve already set that up in our course organization on GitHub. Let’s get your repo onto your computer.\n\nStep 0: Install git on your computer\nFollow these directions to install git on your computer, if you need it and haven’t already done so. If you use a Mac, there’s a good chance you already have git on your machine. You can check by opening the Terminal application and typing which git at the prompt. If you get a response like /usr/bin/git then you have git. It’s unlikely to be the newest version, but is probably good enough for our class. That said, it generally doesn’t hurt to install a newer version.\n\n\nStep 1: setting a personal access token or PAT\nBefore we try to use git through RStudio, we need to set up your GitHub user credentials on your computer. This is a security feature of GitHub that prevents others from accessing your account. Think of a personal access token or PAT as a special type of password that links your computer to GitHub and that can be set with different levels of permission on your account. It can be very limited, allowing the computer user only a very number of options or very broad, giving the computer user a many ways to interact with GitHub. It can also be set to expire if you want.\nTo set up your PAT, we’ll once again use Jenny Bryan’s GitHub guide. Specifically, we’ll use instructions found here. Repeating the instructions, they are:\nIn the RStudio console, type the following and press enter:\nusethis::create_github_token()\n\n\n\nRStudio gitcred prompt\n\n\nThis should open up a web page on GitHub to create a token with some presents. The presets are fine, though I would recommend setting the PAT to last at least until the course is over (so you don’t have to do all of this again in a month). In this example, I put this course id in the use Note box and set the expiration for middle of the summer after the course has ended. Once you’ve set these, scroll to the bottom and click the button to generate the PAT.\n\n\n\nGitHub PAT page\n\n\nYou see a screen that looks like this:\n\n\n\nGitHub PAT page\n\n\nI’ve blocked out the letters and numbers after gh_ (not that I’m using this PAT since it’s now public!), but you should see a full string. Keep this window open and head back to RStudio.\nIn the console type and press enter:\ngitcreds::gitcreds_set()\n\n\n\nGitHub PAT page\n\n\nIt will as you to add your password. Paste the gh_ PAT and press enter. If it worked, you should see the following:\n\n\n\nGitHub PAT page: finish\n\n\nIf you (or someone you share your computer with) has already set up a PAT in the past, you will be given the option to keep your current set up or replace. I’ll leave that to you. All else being equal, it’s probably better to replace the old PAT with the new PAT unless you have compelling reason not to do so.\nYou should now be set to use GitHub with class!\n\n\nStep 2: clone your GitHub repo to your computer\nNavigate to your GitHub repo at github.com. Once you’ve signed into your account, you should be able to navigate to it through GitHub’s interface. Because the repos are structured the same way, you can also use this link:\nhttps://github.com/edquant/&lt;...&gt;\nWhere &lt;...&gt; is your repo name. Unless otherwise said, it should take the form of student_&lt;last name&gt; (students with shared last names will have a slightly different repo name since repo names within the organization must be unique — ask me or check your email). For example, my repo is student_skinner and is found at:\nhttps://github.com/edquant/student_skinner\nIf you click on this an you aren’t me, you’ll get a 404 error page. That’s because it’s a private repository, meaning only those with access can see it — GitHub pretends it doesn’t exist otherwise. If you can’t find your repo, double check two things:\n\nthat you have the correct repo name (no typos)\nthat you are signed into GitHub\n\nIf you are signed in and have the right link, you should see your page. For example, mine looks like this:\n\n\n\nGitHub student repo\n\n\nJust above the list of files, click on the button that says clone (it’s green by default).\n\n\n\nGitHub clone\n\n\nYou’ll see a drop down. Confirm that HTTPS is bold and underlined. You’ll want to copy to the web address in the text box for the next step (you can use the overlapping box symbol just to the right of the address to copy it to your clipboard).\nNow, return to RStudio. In the upper right hand corner, you’ll see Project: (none) (if you’ve been using RStudio, you may have a project going, meaning it won’t say “none”, but that’s okay). Click on that to see a drop down list of options. You will click on the first option New Project.\n\n\n\nRStudio new project\n\n\nThis window will pop up.\n\n\n\nRStudio new project: version control\n\n\nChoose the option Version Control option. In the next window that opens, choose Git.\n\n\n\nRStudio new project: git\n\n\nIn the final window, paste the URL for your GitHub repo in the first text box. The middle text box should auto-fill with your repo name: student_&lt;last_name&gt;. The last text box is where you want to store your repo. On my computer, it shows the Desktop. If you’d rather place your course files in another place on your computer (e.g. a projects folder or maybe a class folder), then use the Browse button to select a new location. The main thing is to remember where you saved it! Once you’re satisfied, click the Create Project button.\n\n\n\nRStudio new project: create\n\n\nIf everything works, you’ll see a drop down window that looks like it’s doing some work with files. It will quickly close and RStudio will look like it is restarting, when it reopens, you should see your repo files in the bottom right pane, a new tab that says Git, and your repo name in the upper right as the new project name. You are now ready to work with your class files!\n\n\n\nRStudio new project: complete"
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#stagingadding-committing-and-pushing-files-from-your-computer-to-github",
    "href": "Extra-01-Setup-GitHub.html#stagingadding-committing-and-pushing-files-from-your-computer-to-github",
    "title": "Extra: Git & GitHub",
    "section": "Staging/adding, committing, and pushing files from your computer to GitHub",
    "text": "Staging/adding, committing, and pushing files from your computer to GitHub\nThere is nothing special about working in a git directory compared to a normal directory: you still open your files, edit them, and save as normal. The difference is that along the way, you take an extra step that involves:\n\nStaging/adding your files (telling git to pay attention to the changes you’ve made)\nCommitting your files (confirming that you want git to store your changes in your local repository)\nPushing your changes (copying changes to your local repository to your remote repository — GitHub)\n\nIf you click on the Git tab, you’ll a new file listed with some boxes next to it.\n\n\n\nRStudio new project: Git tab\n\n\nThis particular file is a settings file and not that important for us, but it will be good to use as an example. If you want to see information about the file, click the Diff button on the far left, just under the Environment tab.\nA new window will open. The bottom window shows the changes made to the file. Lines with additions are in green; lines with subtractions are in red. Because this is a new file, the entire text is green.\n\n\n\nRStudio new project: Git diff\n\n\nIf you click on the History button, you can see the history of changes to the repo. There’s not much right now, but over time, the history can be very helpful for finding deleted code or pinpointing when a bug was introduced.\n\n\n\nRStudio new project: Git history\n\n\nGoing back by clicking on Changes window, let’s stage/add our file, commit it, and push it.\n\nStage/add\nTo stage/add a file, click the box next to the file name. You’ll notice that the two boxes under the Status header become a single box with an “A” in it for Add.\n\n\n\nRStudio new project: Git add\n\n\n\n\nCommit\nNext, you need to Commit the changes to the local repository. Do this by writing a message in the box on the right. You can write whatever you want, but per the suggestions above, I recommend something short and informative. Once you’re done, click the Commit button.\n\n\n\nRStudio new project: Git commit\n\n\nYou’ll see a drop down menu telling you what has happened. At first this can seem like nothing, but over time, you’ll get the hang of the messages. This tells you that 1 file (our Rproj file changed with 13 new lines (if there were deletions, it would say that, too). You can close this out.\n\n\n\nRStudio new project: Git commit result\n\n\nAfter closing, notice that you don’t see the file name any more or the changes in the bottom panel. However, there is new line towards the top left that tells you “Your branch is ahead of origin/main by 1 commit.” origin/main is the git name of your remote repo: origin is the remote (GitHub) and main is the specific branch (we will only use the “main” branch in this course). In other words, your local repo has changes that are one step ahead of what’s on GitHub. Time to sync them up!\n\n\n\nRStudio new project: Git post commit\n\n\n\n\nPush\nPushing the changes to GitHub is simple: just click the Push button in the far upper right (up arrow). Once again, you’ll get a drop down window with messages from git. There’s no error (it’ll let you know), so you can close the window.\n\n\n\nRStudio new project: Git push\n\n\nWhen you go back to the main RStudio window, you’ll notice that file is no longer listed in the Git tab. Should you make changes to it, it will reappear. Similarly, any files in your local repo that you change, add, or delete will show up here.\n\n\n\nRStudio new project: Git post push\n\n\nIf you want to confirm that the push worked, go to your repo at GitHub. If you can see the new file and the commit message, then you know your changes have made it to your GitHub repo. Importantly for this course, if you see the changes on GitHub, that means I can see them. Conversely, if you don’t see the changes on GitHub, then I can’t see them. Double check!\n\n\n\nRStudio new project: GitHub post push"
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#pulling-down-changes-from-github",
    "href": "Extra-01-Setup-GitHub.html#pulling-down-changes-from-github",
    "title": "Extra: Git & GitHub",
    "section": "Pulling down changes from GitHub",
    "text": "Pulling down changes from GitHub\nClosing the loop, you also need to pull down changes from GitHub. In general, this is necessary when are you are using multiple computers or working with a collaborator. For this course, you’ll need to pull down updates and comments I make to your repo.\nTo pull, all you need to do is click the Pull button in the Git tab (with the down arrow).\n\n\n\nRStudio: pull\n\n\nIf there’s nothing new, you’ll get a message like this. If so, you’re good to go!\n\n\n\nRStudio: post pull\n\n\nIf there are changes, you’ll be informed of those and you’ll see them reflected in your local files. If there’s a new file, you’ll see it appear in your local directory. If a file is deleted, it will be removed from your local directory. If there were any changes in a file, you’ll see them when you next open the file.\n\nCommitting changes before pulling down\nOne thing that catches new (and experienced!) git users is forgetting to stage/add and commit changes before pulling down new changes from the remote repo. If you try to pull from GitHub and there’s a change to a file you have changed but haven’t add/stage + committed, the pull won’t work. It’s annoying, but it’s actually a nice feature: this prevents changes that you aren’t tracking and therefore can’t recover.\nIf this happens, stage/add and commit your local changes and then pull. If you have made changes that don’t align with the new changes you just pulled down, you’ll end up with a merged file. It can look messy, but it’s totally fixable. If this happens and you have trouble, let me know."
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#final-note-about-git-and-this-course",
    "href": "Extra-01-Setup-GitHub.html#final-note-about-git-and-this-course",
    "title": "Extra: Git & GitHub",
    "section": "Final note about git and this course",
    "text": "Final note about git and this course\nBy construction of the scripts I use to build this course, I have the ability to overwrite files you use. That said, I will only overwrite stuff that I put in there, like lessons, assignments, class scripts, etc. I will never overwrite anything you add. Also, I will not overwrite anything you put in the working folder. That’s yours.\nIf you want to, for example, take notes in class scripts using comments (a very good idea), then simply rename the script. For example, you could rename the intro_r.R script to intro_r_notes.R. That way, when I update your repo, you won’t lose your notes when intro_r.R is reset."
  },
  {
    "objectID": "Extra-01-Setup-GitHub.html#complete-your-github-profile",
    "href": "Extra-01-Setup-GitHub.html#complete-your-github-profile",
    "title": "Extra: Git & GitHub",
    "section": "Complete your GitHub profile",
    "text": "Complete your GitHub profile\nIf you haven’t already, please do the following things for your GitHub profile:\n\nAdd a picture (I don’t care whether you use a photo of yourself or something else more private, but please use something other than the default icon).\nCreate a profile README using these instructions. It doesn’t have to be fancy and can include minimal information that you are comfortable publicly sharing, but at the very least please have something. You can use the default prompts or create your own."
  },
  {
    "objectID": "06-Data-Viz-II.html#libraries-functions-and-paths",
    "href": "06-Data-Viz-II.html#libraries-functions-and-paths",
    "title": "II: Customization",
    "section": "Libraries, functions, and paths",
    "text": "Libraries, functions, and paths\nIn addition to tidyverse, we’ll add a new library, patchwork, that we’ll use toward the end of the lesson. If you haven’t already downloaded it, be sure to install it first using install.packages(\"patchwork\").\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\nWe’ll need to convert and then replace some missing values in the lesson, so we’ll include our user-written function, fix_missing(), that we first used in the programming lesson.\n\n## ---------------------------\n## functions\n## ---------------------------\n\n## utility function to convert values to NA\nfix_missing &lt;- function(x, miss_val) {\n  x &lt;- ifelse(x %in% miss_val, NA, x)\n  return(x)\n}\n\nFinally, we’ll load the data file we’ll be using, hsls_small.csv. Since we already know about the structure of hsls_small.csv, we’ll use the read_csv() argument show_col_types = FALSE to prevent all the extra console output when we read in the data file.\n\n## ---------------------------\n## input data\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls_small.csv\"), show_col_types = FALSE)"
  },
  {
    "objectID": "06-Data-Viz-II.html#initial-plot-with-no-formatting",
    "href": "06-Data-Viz-II.html#initial-plot-with-no-formatting",
    "title": "II: Customization",
    "section": "Initial plot with no formatting",
    "text": "Initial plot with no formatting\nRather than make a variety of plots, we’ll focus on making and incrementally improving a single figure (with some slight variations along the way). In general, we’ll be looking at math test scores via the x1txtmscor data column.\nLet’s start with the most basic histogram we can make. But first, we need to fix our variable of interest. As you may recall from an earlier lesson, x1txmtscor is a normed variable with a mean of 50 and standard deviation of 10. That means the negative values need to be converted to NA values and, for our plotting purposes, dropped.\n\n## -----------------------------------------------------------------------------\n## initial plain plot\n## -----------------------------------------------------------------------------\n\n## fix missing values for text score and then drop missing\ndf &lt;- df %&gt;%\n  mutate(math_test = fix_missing(x1txmtscor, -8)) %&gt;%\n  drop_na(math_test)\n\nNow we can make our histogram with no extra settings:\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram()\n\n## show\np\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSo there it is. Let’s make it better."
  },
  {
    "objectID": "06-Data-Viz-II.html#titles-and-captions",
    "href": "06-Data-Viz-II.html#titles-and-captions",
    "title": "II: Customization",
    "section": "Titles and captions",
    "text": "Titles and captions\nThe easiest things to improve on a figure are the title, subtitle, axis labels, and caption. As with a lot of ggplot2 commands, there are a few different ways to set these labels, but the most straightforward way is to use the labs() function as part of the ggplot() chain. Notice that we’ve added it to the end. (Also notice that we’ve set the bins = 30 argument within geom_histogram(), which is the default and will prevent a message from popping up each time.)\n\n## -----------------------------------------------------------------------------\n## titles and captions\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot, showing placeholder titles/labels/captions\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Title\",\n       subtitle = \"Subtitle\",\n       caption = \"Caption\",\n       x = \"X axis label\",\n       y = \"Y axis label\")\n\n## show \np\n\n\n\n\nRather than accurately labeling the figure, I’ve repeated the arguments in strings so that it’s clearer where every piece goes. The title is of course on top, with the subtitle in a smaller font just below. The x and y axis labels go with their respective axes and the caption is right-aligned below the figure. You don’t have to use all of these options for every figure. If you don’t want to use one, you have a couple of options:\n\nIf the argument would otherwise be blank (title, subtitle, and caption), you can just leave the argument out of labs()\nIf the argument will be filled, as is the case on the axes (ggplot will use the variable name by default), you can use NULL\n\nTo make our figure nicer, we’ll add a title, axis labels, and caption describing the data source. We don’t really need a subtitle and since there’s no default value, we’ll just leave it out.\n\n## ---------------------------\n## titles and captions: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nThat looks better. Now we’ll move to improving the axis scales."
  },
  {
    "objectID": "06-Data-Viz-II.html#axis-formatting",
    "href": "06-Data-Viz-II.html#axis-formatting",
    "title": "II: Customization",
    "section": "Axis formatting",
    "text": "Axis formatting\nIn general, the default tick mark spacing and accompanying labels are pretty good. But sometimes we want to change them, sometimes to have fewer ticks and sometimes to have more. For this figure, we could use more ticks on the x axis to make differences in math test score clearer. While we’re at, we’ll increase the number of tick marks on the y axis too.\nTo change these values, we need to use scale_&lt; aesthetic &gt;_&lt; distribution &gt; function. These may seem strange at first, but they follow a logic. Specifically:\n\n&lt; aesthetic &gt;: x, y, fill, colour, etc (what is being changed?)\n&lt; distribution &gt;: is the underlying variable continuous, discrete, or do you want to make a manual change?\n\nTo change our x and y tick marks we use:\n\nscale_x_continuous()\nscale_y_continuous()\n\nWe use x and y because those are the aesthetics being adjusted and we use continuous in both cases because math_test on the x axis and the histogram counts on the y axis are both continuous variables.\nThere are a number of options within the scale_*() family of functions — and they can change depending on which scale_*() function you use — but we’ll focus on using two:\n\nbreaks: where the major lines are going (they get numbers on the axis)\nminor_breaks: where the minor lines are going (they don’t get numbers on the axis)\n\nBoth breaks and minor_breaks take a vector of numbers. We can put each number in manually using c() (e.g., c(0, 10, 20, 30, 40)), but a better way is to use R’s seq() function: seq(start, end, by). Notice that within each scale_*() function, we use the same start and stop arguments for each seq(). We only change the by argument. This will give us axis numbers at spaced intervals with thinner, unnumbered lines between.\n\n## -----------------------------------------------------------------------------\n## axis formatting\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 100),\n                     minor_breaks = seq(from = 0, to = 2500, by = 50)) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nWe certainly have more lines now. Maybe too many on the y axis, which is a sort of low-information axis (do we need really that much detail for histogram counts?). Let’s keep what we have for the x axis and increase the by values of the y axis.\n\n## ---------------------------\n## axis formatting: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nThat seems like a better balance. We’ll stick with that and move on to legend labels."
  },
  {
    "objectID": "06-Data-Viz-II.html#legend-labels",
    "href": "06-Data-Viz-II.html#legend-labels",
    "title": "II: Customization",
    "section": "Legend labels",
    "text": "Legend labels\nLet’s make our histogram a little more complex by separating math scores by parental education. Specifically, we’ll use a binary variable that represents, did either parent attend college? First, we need to create a new variable, pared_coll, from the ordinal variable, x1paredu. You can check the discussion of why we create the variable this way from the first plotting lesson.\n\n## -----------------------------------------------------------------------------\n## legend labels\n## -----------------------------------------------------------------------------\n\n## add indicator that == 1 if either parent has any college\ndf &lt;- df %&gt;%\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0))\n\nNow we’ll make our same histogram, but add the fill aesthetic. As we’ve done in the past, we’ll wrap our new binary variable in as_factor() so ggplot understands that 0/1 are discrete values. We’ll also modify geom_histogram() to use smaller bins, a new \"identity\" position, and make the fill colors semi-transparent with alpha.\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test, fill = as_factor(pared_coll))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nExcept for our labels and tick mark adjustments, this looks similar to what we’ve made before. The problem with this figure is two-fold:\n\nThe legend title is not very nice — it’s just the variable name wrapped in the as_factor() function\nThe legend itself isn’t very informative: what do 0 and 1 mean?\n\nTo fix this, we’ll switch from using as_factor() to factor(), which has more options. We’ll add the following function to aes() in the initial ggplot() function:\nfill = factor(pared_coll,\n              levels = c(0,1),\n              labels = c(\"No college\",\"College\"))\nWith factor(), we first say which variable should be converted to a factor, pared_coll. Next, we manually set the levels of the factor. That’s easy here because we only have two levels, 0 and 1, which we can set using levels = c(0,1). Finally, we can add labels to the levels. The main thing to make sure of is that the order of our labels match the order of the levels. Since\n\n0 := no parental college\n1 := at least one parent went to college\n\nwe use labels = c(\"No college\",\"College\") which match the c(0,1) order in levels. Other than that, everything else is the same.\nNOTE: we could have made pared_coll a factor when we initially created it. In general, that is easier if we want the variable to always be a factor and we’re making a large number of figures. But for our purposes at the moment, we just convert it on the fly inside ggplot.\n\n## ---------------------------\n## legend labels: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test,\n                          fill = factor(pared_coll,\n                                        levels = c(0,1),\n                                        labels = c(\"No college\",\n                                                   \"College\")))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nCloser, but not quite! The 0/1 have been given proper labels, but the legend title is even worse! Not only is it not nice to look at it, it’s now so long that it squishes our plot. What we need to add is a scale_*() function to fix it. Since we’re working with fill and a discrete variable (remember: the factor only takes on countable values, two in this case), then we’ll use scale_fill_discrete(). We don’t really need to do anything other than give the legend that goes with the fill aesthetic a name, so that’s the argument we use: name.\nLet’s add that to the chain just below our other scale_*() functions before labs().\n\n## ---------------------------\n## legend labels: ver 3\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test,\n                          fill = factor(pared_coll,\n                                        levels = c(0,1),\n                                        labels = c(\"No college\",\n                                                   \"College\")))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  scale_fill_discrete(name = \"Parental education\") +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "06-Data-Viz-II.html#facet-labels",
    "href": "06-Data-Viz-II.html#facet-labels",
    "title": "II: Customization",
    "section": "Facet labels",
    "text": "Facet labels\nNow that we’ve done the hard work of setting a factor, we can use the same bit of code to more properly label facets. Instead of splitting the test score histogram by color within the same plot area like we do above, let’s say we use facet_wrap() instead. This will give us discrete plot areas for each value of pared_coll.\nTo convert to a facetted figure, we’ll just move the factor(...) information from fill to facet_wrap(). Since we don’t have color changes based on fill, we can remove alpha and position from geom_histogram().\n\n## -----------------------------------------------------------------------------\n## facet labels\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\nNotice how each facet has a proper label. Easy enough! Note that there is another way to fix facet labels using the labeller() function, but setting the labels using factor() will work for most situations."
  },
  {
    "objectID": "06-Data-Viz-II.html#themes",
    "href": "06-Data-Viz-II.html#themes",
    "title": "II: Customization",
    "section": "Themes",
    "text": "Themes\nNow that we’ve largely set our various labels, we can adjust the overall look of the figure. If you did the mapping lesson you may have noticed the we called theme_void() on all of our maps, which completely removed all the plotting structure: titles, labels, ticks, axes, etc. That’s the extreme end of adjusting the theme!\nLet’s start with simply removing the gray area of the figure. To do this, we use the theme() function at the end of our ggplot chain. Specifically, we’ll call the argument panel.background and remove it using element_blank().\n\n## -----------------------------------------------------------------------------\n## themes\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank())\n\n## show \np\n\n\n\n\nSo we removed the panel, but since our grid lines were white to offset the gray, we don’t have grid lines any more. These would be helpful! We can add them back in, but make them gray using panel.grid.major and panel.grid.minor (notice the similar construction of the names) and setting them with element_line(colour = \"gray\").\n\n## ---------------------------\n## themes: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major = element_line(colour = \"gray\"),\n        panel.grid.minor = element_line(colour = \"gray\"))\n\n## show \np\n\n\n\n\nThat returned our lines, but let’s say that we don’t really care about the horizontal lines. Rather than have the reader focus on counts, we really just want them to focus on the distribution around the math score. If we want to adjust the panel grids one axis at a time, we use the same stub and add *.x and *.y as necessary. Notice how for the x panel grids we use the old code, but for the y panel grids, return to using element_blank().\n\n## ---------------------------\n## themes: ver 3\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        panel.grid.minor.x = element_line(colour = \"grey\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n## show \np\n\n\n\n\nGreat! Now we only have vertical grid lines. Of course, we don’t really need the y axis ticks and labels now. We can ditch them by setting axis.title.y, axis.text.y, and axis.ticks.y to element_blank(). Notice that since we call this after labs(), our label for y is ignored.\n\n## ---------------------------\n## themes: ver 4\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        panel.grid.minor.x = element_line(colour = \"grey\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n## show \np\n\n\n\n\nOkay! We have what we set out to get.\nRemember, all the elements of a ggplot figure can be adjusted. That said, there are some shortcut theme_*() functions we can use that will save some typing. For example, theme_bw() will give something very similar to what we built before removing the horizontal lines.\n\n## ---------------------------\n## themes: ver 5\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw()\n\n## show \np\n\n\n\n\nThere are other complete themes you might find useful in your work. If you want to make manual changes, here’s the full list of arguments and here are options for theme elements. Check them out!"
  },
  {
    "objectID": "06-Data-Viz-II.html#multiple-plots-with-patchwork",
    "href": "06-Data-Viz-II.html#multiple-plots-with-patchwork",
    "title": "II: Customization",
    "section": "Multiple plots with patchwork",
    "text": "Multiple plots with patchwork\nIn this final section, we’ll practice putting multiple figures together. All the plots we’ve made so far have used the same underlying data. Even when we’ve used facet_wrap() to make multiple plot areas, they were related in some way. But what if we want to neatly paste different unrelated plots into a single figure, laid out exactly the way we want?\nWe use the patchwork library!\nWe’ll start by making a new figure. Rather than splitting math scores by parental education, we’ll split by whether the student is below or above 185% of the federal poverty level. As before, we’ll remove missing values from the variable, x1poverty185, and create a new variable, pov185, that takes a binary 0 (below) / 1 (above) set of values.\n\n## -----------------------------------------------------------------------------\n## multiple plots with patchwork\n## -----------------------------------------------------------------------------\n\n## remove missing values\ndf &lt;- df %&gt;%\n  mutate(pov185 = fix_missing(x1poverty185, c(-8,-9))) %&gt;%\n  drop_na(pov185)\n\n## make histogram\np2 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pov185,\n                      levels = c(0,1),\n                      labels = c(\"Below 185% poverty line\",\n                                 \"Above 185% poverty line\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by poverty level\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw()\n\n## show\np2\n\n\n\n\nNow that we have our new figure, let’s paste it side by side (left-right) with our first figure. Once we’ve loaded the patchwork library (like we already did at the top of the script), we can use a + sign between out two ggplot objects: p + p2. We’ll store that in a new object, pp, and then call that.\n\n## ---------------------------\n## patchwork: side by side\n## ---------------------------\n\n## use plus sign for side by side\npp &lt;- p + p2\n\n## show\npp\n\n\n\n\nDefinitely works, but it’s a little squished. Rather than side by side, let’s stack them this time. To stack two plots with patchwork, use a forward slash, /.\n\n## ---------------------------\n## patchwork: stack\n## ---------------------------\n\n## use forward slash to stack\npp &lt;- p / p2\n\n## show\npp\n\n\n\n\nThat looks better!\nPatchwork is sufficiently flexible that you can arrange many figures. Let’s create yet another figure: test score by socioeconomic status. After cleaning up that variable, we make a new plot.\n\n## ---------------------------\n## patchwork: 2 over 1\n## ---------------------------\n\n## drop missing SES values\ndf &lt;- df %&gt;%\n  mutate(ses = fix_missing(x1ses, -8)) %&gt;%\n  drop_na(ses)\n\n## create third histogram of just SES\np3 &lt;- ggplot(data = df,\n             mapping = aes(x = x1ses)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = -5, to = 5, by = 1),\n                     minor_breaks = seq(from = -5, to = 5, by = 0.5)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1000, by = 100),\n                     minor_breaks = seq(from = 0, to = 1000, by = 25)) +\n  labs(title = \"Socioeconomic score\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"SES\",\n       y = \"Count\") +\n  theme_bw()\n\n## show\np3\n\n\n\n\nNow that we have this new plot, let’s paste it to the other figures in a 2 over 1 pattern. To make that clear to patchwork, we use parentheses just like we might in algebra (remember PEMDAS?) to set priority. The parentheses paste the first two figures side by side and then stack this new combined plot above the new plot.\n\n## use parentheses to put figures together (like in algebra)\npp &lt;- (p + p2) / p3\n\n## show\npp\n\n\n\n\nBecause of the new structure, the side by side of the first two figures doesn’t look quite as squished as before. That said, labels and titles still overlap. We also have redundant information. Do we really need that data caption three times?\nLet’s do some clean up to make a nice final figure. The easiest thing will be to remake the figures. This time we’ll:\n\nremove the caption argument from labels (we’ll add it in later)\nuse theme_bw(base_size = 8) to change the overall size of the font. This should help with all the overlapping text.\n\n\n## ---------------------------\n## patchwork: clean up\n## ---------------------------\n\n## Redo the above plots so that:\n## - remove some redundant captions\n## - change base_size so font is smaller\n\n## test score by parental education\np1 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw(base_size = 8)\n\n## test score by poverty level\np2 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pov185,\n                      levels = c(0,1),\n                      labels = c(\"Below 185% poverty line\",\n                                 \"Above 185% poverty line\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by poverty level\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw(base_size = 8)\n\n## create third histogram of just SES\np3 &lt;- ggplot(data = df,\n             mapping = aes(x = x1ses, y = math_test)) + \n  geom_point() +\n  scale_x_continuous(breaks = seq(from = -5, to = 5, by = 1),\n                     minor_breaks = seq(from = -5, to = 5, by = 0.5)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 100, by = 10),\n                     minor_breaks = seq(from = 0, to = 100, by = 5)) +\n  labs(title = \"Math test scores by socioeconomic status\",\n       x = \"SES\",\n       y = \"Math score\") +\n  theme_bw(base_size = 8)\n\n## use parentheses to put figures together (like in algebra)\npp &lt;- (p1 + p2) / p3\n\nWe’ve remade our figures and used patchwork to put them together. But as a final step, we’ll use patchwork’s plot_annotation() argument to add:\n\noverall title\na single caption\nplot-specific tags that are useful for referencing certain plots (i.e., you can say “plot / facet A” rather than “the top left plot / facet”)\n\nWe add plot_annotation() using a + sign just like with a normal ggplot chain.\n\n## add annotations\npp &lt;- pp + plot_annotation(\n  title = \"Math scores across various factors\",\n  caption = \"Data: High School Longitudinal Study, 2009\",\n  tag_levels = \"A\"\n)\n\n## show\npp\n\n\n\n\nDone and looking pretty good! Well, the blob of in plot C maybe isn’t that useful…\nWe can always do more, of course, but remember that a figure doesn’t need to be complicated to be good. In fact, simpler is often better. The main thing is that it is clean and clear and tells the story you want the reader to hear. What exactly that looks like is up to you and your project!"
  },
  {
    "objectID": "06-Data-Viz-II.html#question",
    "href": "06-Data-Viz-II.html#question",
    "title": "II: Customization",
    "section": "Question",
    "text": "Question\n\nMake 3-4 different figures showing relationships between variables in hsls_small. You can remake some figures we made in prior lesson, but whatever you do, make sure that data are clean, everything is properly labeled, tick marks are appropriately spaced and numbered — just that the figures look nice. Once done, put them together in a nice arrangement using patchwork. This may mean making some adjustments so that there’s no redundant information in the final figure.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_6.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#example-data-analysis-task",
    "href": "Extra-02-Program-Vanilla.html#example-data-analysis-task",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Example data analysis task",
    "text": "Example data analysis task\nLet’s imagine we’ve been given the following data analysis task with the HSLS09 data:\n\nFigure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R. Instead, it’s up to you to (1) translate a research question into the discrete steps coding steps necessary to provide an answer, and then (2) translate the answer such that everyone understands what you’ve found.\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nArrange in order so it’s easier to rank and share\nWrite out the results to a file so we have it for later\n\nLet’s do it!\nNOTE: Since we’re not using the tidyverse, we don’t need to call it this time. Even with non-tidyverse R, you may need to call libraries. This analysis, however, does not require any.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NONE"
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#check-working-directory",
    "href": "Extra-02-Program-Vanilla.html#check-working-directory",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Check working directory",
    "text": "Check working directory\nThis script — like the one from the organizing lesson — assumes that the scripts subdirectory is the working directory, that the required data file is in the data subdirectory, and that both subdirectories are at the same level in the course directory. Like this:\nstudent_skinner/         &lt;--- Top-level\n|\n|__/data                 &lt;--- Sub-level 1\n|    |--+ hsls_small.csv\n|\n|__/scripts              &lt;--- Sub-level 1 (Working directory)\n     |--+ dw_one_base_r.R\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that I’m not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, I tell you where you need to be (a common landmark), let you get there, and then rely on relative paths afterwards."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#read-in-data",
    "href": "Extra-02-Program-Vanilla.html#read-in-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Read in data",
    "text": "Read in data\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES / NCES data set that features:\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link (after a little navigation).\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read.csv(), which is base R function\ndf &lt;- read.csv(file.path(\"data\", \"hsls_small.csv\"))\n\nUnlike the read_csv() function we’ve used before, read.csv() (notice the difference: a . instead of an _) doesn’t print anything. So that we can see our data, well print to the console. BUT before we do that…\nread.csv() returns a base R data.frame() rather than the special data frame or tibble() that the tidyverse uses. It’s mostly the same, but one difference is that whereas R will only print the first 10 rows of a tibble, it will print the entire data.frame. We don’t need to see the whole thing, so we’ll use the head() function to print only the first 10 rows.\n\n## show first 10 rows\nhead(df, n = 10)\n\n   stu_id x1sex x1race x1stdob x1txmtscor x1paredu x1hhnumber x1famincome\n1   10001     1      8  199502    59.3710        5          3          10\n2   10002     2      8  199511    47.6821        3          6           3\n3   10003     2      3  199506    64.2431        7          3           6\n4   10004     2      8  199505    49.2690        4          2           5\n5   10005     1      8  199505    62.5897        4          4           9\n6   10006     2      8  199504    58.1268        3          6           5\n7   10007     2      8  199409    49.4960        2          2           4\n8   10008     1      8  199410    54.6249        7          3           7\n9   10009     1      8  199501    53.1875        2          3           4\n10  10010     2      8  199503    63.7986        3          4           4\n   x1poverty185   x1ses x1stuedexpct x1paredexpct x1region x4hscompstat\n1             0  1.5644            8            6        2            1\n2             1 -0.3699           11            6        1            1\n3             0  1.2741           10           10        4            1\n4             0  0.5498           10           10        3            1\n5             0  0.1495            6           10        3            1\n6             0  1.0639           10            8        3           -8\n7             0 -0.4300            8           11        1            1\n8             0  1.5144            8            6        1            1\n9             0 -0.3103           11           11        3            1\n10            0  0.0451            8            6        1           -8\n   x4evratndclg x4hs2psmos\n1             1          3\n2             1          3\n3             1          4\n4             0         -7\n5             0         -7\n6            -8         -8\n7             1          2\n8             1          3\n9             1          8\n10           -8         -8\n\n\n\nQuick exercise\nread.csv() is special version of read.table(), which can read various delimited file types, that is, tabular data in which data cells are separated by a special character. What’s the special character used to separate CSV files? Once you figure it out, re-read in the data using read.table(), being sure to set the sep argument to the correct character.\n\nYou’ll also notice that the data.frame doesn’t tell use the types of our columns. If we really want to know, we can use the class() function. So that we can see all columns, we can use the sapply() function, which will let us apply the class() function across all columns.\n\n## show column types\nsapply(df, class)\n\n      stu_id        x1sex       x1race      x1stdob   x1txmtscor     x1paredu \n   \"integer\"    \"integer\"    \"integer\"    \"integer\"    \"numeric\"    \"integer\" \n  x1hhnumber  x1famincome x1poverty185        x1ses x1stuedexpct x1paredexpct \n   \"integer\"    \"integer\"    \"integer\"    \"numeric\"    \"integer\"    \"integer\" \n    x1region x4hscompstat x4evratndclg   x4hs2psmos \n   \"integer\"    \"integer\"    \"integer\"    \"integer\""
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#select-variables-columns",
    "href": "Extra-02-Program-Vanilla.html#select-variables-columns",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\nData frames are like special matrices. They have rows and columns. You can access these rows and columns using square bracket notation ([]). Because a matrix has two dimensions, you use a comma inside the square brackets to indicate what you mean ([,]):\n\ndf[&lt;rows&gt;,&lt;cols&gt;]\n\nAt it’s most basic, you can use numbers to represent the index of the cell or cells you’re interested in. For example, if you want to access the value of the cell in row 1, column 4, you can use:\n\n## show value at row 1, col 4\ndf[1, 4]\n\n[1] 199502\n\n\nBecause data frames have column names (the variable names in our data set), we can also refer to them by name. The fourth column is the student date of birth variable, x1stdob. Using that instead of 4 (notice the quotation marks \"\"):\n\n## show value at row 1, x1stdob column\ndf[1, \"x1stdob\"]\n\n[1] 199502\n\n\nIf we want to see more than one column, we can put the names in a concatenated vector using the c() function:\n\n## show values at row 1, stu_id & x1stdob column\ndf[1, c(\"stu_id\", \"x1stdob\")]\n\n  stu_id x1stdob\n1  10001  199502\n\n\nSo far, we’ve not assigned these results to anything, so they’ve just printed to the console. However, we can assign them to a new object. If we want to slice our data so that we only have selected columns, we can leave the rows section blank (meaning we want all rows) and include all the columns we want to keep in our new data frame object.\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- df[, c(\"stu_id\", \"x1stuedexpct\", \"x1paredexpct\", \"x1region\")]\n\n## show 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region\n1   10001            8            6        2\n2   10002           11            6        1\n3   10003           10           10        4\n4   10004           10           10        3\n5   10005            6           10        3\n6   10006           10            8        3\n7   10007            8           11        1\n8   10008            8            6        1\n9   10009           11           11        3\n10  10010            8            6        1"
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#mutate-data-into-new-forms",
    "href": "Extra-02-Program-Vanilla.html#mutate-data-into-new-forms",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\n\nChanging existing variables (columns)\nTo conditionally change a variable, we’ll once again use the bracket notation to target our changes. This time, however, we do a couple of things differently:\n\ninclude square brackets on the LHS of the assignment\nuse conditions in the &lt;rows&gt; part of the bracket\n\nAs before, we need to account for the fact that our two expectation variables, x1stuedexpct and x1paredexpct, have values that need to be converted to NA: -8, -9, and 11. See the first data wrangling lesson for the rationale behind these changes.\nFirst, let’s look at the unique values using the table() function. So that we see any missing values, we’ll include an extra argument:\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n  -8    1    2    3    4    5    6    7    8    9   10   11 \n2059   93 2619  140 1195  115 3505  231 4278  176 4461 4631 \n\n## see unique values for parental expectation\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n  -9   -8    1    2    3    4    5    6    7    8    9   10   11 \n  32 6715   55 1293  149 1199  133 4952   76 3355   37 3782 1725 \n\n\nNotice that we use a dollar sign, $, to call the column name directly. Unlike with the tidyverse, we cannot just use the column name. Base R will look for that column name not as a column in a data frame, but as its own object. It probably won’t find it (or worse, you’ll have another object in memory that it will find and you’ll get the wrong thing!).\nRemember how data.frames are special matrices? One of the special features is that you can use the $ sign with the column name as a short cut or double square brackets without the comma.\n\n## each version pulls the column of data for student expectations\n## TRUE == 1, so if the mean of all values == 1, then all are TRUE\nmean(df_tmp$x1stuedexpct == df_tmp[, \"x1stuedexpct\"]) == 1\n\n[1] TRUE\n\nmean(df_tmp$x1stuedexpct == df_tmp[[\"x1stuedexpct\"]]) == 1\n\n[1] TRUE\n\n\nBack to replacing our missing values with NA…\nThe conditions we care about are when df_tmp$x1stuedexpct == -8, for example. Using that condition in the row section of the square bracket, we can replace only what we want.\n\n## replace student expectation values\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8] &lt;- NA\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == 11] &lt;- NA\n\n## replace parent expectation values\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == -8] &lt;- NA\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == -9] &lt;- NA\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == 11] &lt;- NA\n\nWhat each of these lines says is (first one for example): In the data.frame df_tmp, replace the value in x1stuedexpct — where the value of df_tmp$x1stuedexpct is -8 — with NA\nAs less convoluted way of saying this might be (more generally stated): Where the value in column A equals X, replace the value in column A with Y.\nLet’s confirm using table() again. The values that were in -8, -9, and 11 should now be summed under NA.\n\n## see unique values for student expectation (confirm changes)\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  93 2619  140 1195  115 3505  231 4278  176 4461 6690 \n\n## see unique values for parental expectation (confirm changes)\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  55 1293  149 1199  133 4952   76 3355   37 3782 8472 \n\n\n\n\nAdding new variables (columns)\nAdding a new variable to our data frame is just like modifying an existing column. The only difference is that instead of putting an existing column name after the first $ sign, we’ll make up a new name. This tells R to add a new column to our data frame.\nAs with the tidyverse version, we’ll use the ifelse() function to create a new variable that is the higher of student or parental expectations.\n\n## add new column\ndf_tmp$high_expct &lt;- ifelse(df_tmp$x1stuedexpct &gt; df_tmp$x1paredexpct, # test\n                            df_tmp$x1stuedexpct,                       # if TRUE\n                            df_tmp$x1paredexpct)                       # if FALSE\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1         NA\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1         NA\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\nAgain, our “ocular test” shows that this doesn’t handle NA values correctly. Look at student 10002 in the second row: while the student doesn’t have an expectation (or said “I don’t know”), the parent does. However, our new variable records NA. Let’s fix it with this test:\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\n\n## NB: We have to include [is.na(df_tmp$high_expct)] each time so that\n## everything lines up\n\n## step 1 student\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]), \n    ## if TRUE do this...\n    df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n## step 2 parent\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1paredexpct[is.na(df_tmp$high_expct)]),\n    ## if TRUE do this...\n    df_tmp$x1paredexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\nThat’s a lot of text! What’s happening is that we are trying to replace a vector of values with another vector of values, which need to line up and be the same length. That’s why we have\n## what we'll use to replace when TRUE\ndf_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]\nWhen our high_expct column has missing values, we want to replace with non-missing x1stuedexpct values in the same row. That means we also need to subset that column to only include values in rows that have missing high_expct values. Because we must do this each time, our script gets pretty long and unwieldy.\nLet’s check to make sure it worked as intended.\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue. Looking at rows 7 and 9, it seems like those situations are correctly handled as well."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#filter-observations-rows",
    "href": "Extra-02-Program-Vanilla.html#filter-observations-rows",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\nLet’s check the counts of our new variable:\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ntable(df_tmp$high_expct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  71 2034  163 1282  132 4334  191 5087  168 6578 3463 \n\n\nSince we’re not going to use the missing values (we really can’t, even if we wanted to do so), we’ll drop those observations from our data frame.\nAs when we selected columns above, we’ll use the square brackets notation. As with dplyr’s filter(), we want to filter in what we want. We set this condition before the comma in the square brackets. Because we want all the columns, we leave the space after the comma blank.\n\n## filter in values that aren't missing\ndf_tmp &lt;- df_tmp[!is.na(df_tmp$high_expct),]\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n10  10010            8            6        1          8\n11  10011            8            6        3          8\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values). Since we haven’t removed rows until now, we can compare the number of rows in the original data frame, df, to what we have now.\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column. While not a formal test, it does support what we expected (in other words, if the number were different, we’d definitely want to go back and investigate)."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#summarize-data",
    "href": "Extra-02-Program-Vanilla.html#summarize-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. For an overall average, we can just use the mean() function.\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nmean(df_tmp$high_expct)\n\n[1] 7.272705\n\n\nOverall, we can see that students and parents have high postsecondary expectations on average: to earn some graduate credential beyond a bachelor’s degree. However, this isn’t what we want. We want the values across census regions.\n\n## check our census regions\ntable(df_tmp$x1region, useNA = \"ifany\")\n\n\n   1    2    3    4 \n3128 5312 8177 3423 \n\n\nWe’re not missing any census data, which is good. To calculate our average expectations, we need to use the aggregate function. This function allows to compute a FUNction by a group. We’ll use it to get our summary.\n\n## get average (assigning this time)\ndf_tmp &lt;- aggregate(df_tmp[\"high_expct\"],                # var of interest\n                    by = list(region = df_tmp$x1region), # by group\n                    FUN = mean)                          # function to run\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n2      2   7.168110\n3      3   7.357833\n4      4   7.125329\n\n\nSuccess! Expectations are similar across the country, but not the same by region."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#arrange-data",
    "href": "Extra-02-Program-Vanilla.html#arrange-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Arrange data",
    "text": "Arrange data\nAs our final step, we’ll arrange our data frame from highest to lowest (descending). For this, we’ll use sort() and the decreasing option.\n\n## -----------------\n## arrange\n## -----------------\n\n## arrange from highest expectations (first row) to lowest\ndf_tmp &lt;- df_tmp[order(df_tmp$high_expct, decreasing = TRUE),]\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n3      3   7.357833\n2      2   7.168110\n4      4   7.125329"
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#write-out-updated-data",
    "href": "Extra-02-Program-Vanilla.html#write-out-updated-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Write out updated data",
    "text": "Write out updated data\nWe can use this new data frame as a table in its own right or to make a figure. For now, however, we’ll simply save it using the opposite of read.csv() — write.csv() — which works like writeRDS() we’ve used before.\n\n## write with useful name\nwrite.csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\nAnd with that, we’ve met our task: we can show average educational expectations by region. To be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region. This caveat doesn’t necessarily make our analysis less useful, but rather sets its scope. Furthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference."
  },
  {
    "objectID": "Extra-02-Program-Vanilla.html#questions",
    "href": "Extra-02-Program-Vanilla.html#questions",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average standardized math test score?\nWhat is the average standardized math test score by gender?\nIn what year and month were the oldest students in the data set born? The youngest?\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income (give the category and what that category reprents).\nOf the students who earned a high school credential (diploma or GED), what percentage earned a GED or equivalency? How does this differ by region?\nWhat percentage of students ever attended a postsecondary institution by February 2016? Give the cross tabulations for:\n\nfamily incomes less than or equal to $35,000 and greater than $35,000\n\nregion\n\nThis means you should have percentages for 8 groups: above/below $35k within each region.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_10.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "07-Quarto.html#wysiwyg-vs-wysiwym",
    "href": "07-Quarto.html#wysiwyg-vs-wysiwym",
    "title": "Reproducable Reports with Quarto",
    "section": "WYSIWYG vs WYSIWYM",
    "text": "WYSIWYG vs WYSIWYM\n\n“What you see is what you get”\nWYSIWYG (pronounced how you might guess - “whizzy-whig”) stands for what you see is what you get. MS Word and similar programs are WYSIWYG writing tools. Want some text bolded? You highlight the text, click on the bold text button (or hit Control/Command-B) and the text becomes bold. Want 1.05” margins to increase the page count (teachers always know, by the way…), then you adjust the margins and watch the text squeeze a little and the page count increase.\nThe point is that as you write, you control the content and how it’s formatted at the same time. This is really powerful. You can see your document (literally) taking shape and when you’re done writing, you’re mostly done formatting, too.\nBut one problem from a research perspective is that WYSIWYG document preparation programs don’t always share well, meaning that the formatting isn’t always preserved across computers or operating systems. Sometimes equations don’t open correctly; the font you selected doesn’t exist on another computer. While programs may be able to open each other’s files (e.g. OpenOffice can open a .docx file), that’s not always the case. They open different-program files with many errors or even not at all.\nThe second problem is they don’t work well with reproducible workflows. Let’s say you’ve done some data analysis and make 10 tables and 10 figures. You’ve carefully placed and formatted in your MS Word report. Perfect! But before giving to your supervisor or submitting to a journal, you get some new data and need to rerun everything…ah! You’ll have to go through the whole transfer and formatting process again, increasing the likelihood of introducing errors.\n\n\n“What you see is what you mean”\nWYSIWIM (“whizzy-whim”), on the other hand, separates formatting from content. Rather than making bold text bold, you instead add a bit of markup — some special syntax — to the text you want to be bold. Only when the document is finally compiled into the final form will the text be bold.\nMarkdown syntax uses plain text characters to indicate formatting. This lesson was written in Markdown, so to show you an example, here’s the prior paragraph, in plain Markdown syntax:\n_WYSIWIM_ (\"whizzy-whim\"), on the other hand, separates formatting\nfrom content. Rather than making bold text bold, you instead add a bit\nof markup --- some special syntax --- to the text you want to be\nbold. Only when the document is finally _compiled_ into the final\nform will the **text be bold**. \nMarkdown solves the two problems noted above. First, it’s written in plain text, which means that it can be opened on any computer running any operating system. Even if the end user doesn’t have a way to compile the raw Markdown syntax into the nice-looking final form, the text is still very legible. In fact, this feature — ability to read uncompiled — was a motivating force behind the development of Markdown:\n\nThe idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. (John Gruber, Markdown website)\n\nSecond, because it’s plain text, it integrates well with scripting languages like R. Remember our example from before? If your final report was in Markdown instead of MS Word, you could rerun your analyses with the updated data and then recompile your final report — tables and figures updated automatically!\nIn fact, R and Markdown work so well together that you can combine them in a single — appropriately named — RMarkdown document that takes a combined file ending:\n\nR script: analysis.R\nMarkdown script: write_up.md\nRMarkdown script: report.Rmd\n\nor\nanalysis.R + write_up.md = report.Rmd\nWe’ll keep our R and Markdown scripts separate at first, but know that you’ll submit an RMarkdown file for your final project."
  },
  {
    "objectID": "07-Quarto.html#markdown-syntax",
    "href": "07-Quarto.html#markdown-syntax",
    "title": "Reproducable Reports with Quarto",
    "section": "Markdown syntax",
    "text": "Markdown syntax\nRather than list Markdown syntax here, I’ll direct you to an excellent resource: The Markdown Guide. On this site, you find example of both basic syntax (headers, italics, bold, links) and more advanced syntax (tables and footnotes).\nHere’s an example from the Markdown Guide basic syntax page for making headers:\n\n\n\nMarkdown Guide\n\n\nOn the left you have the Markdown syntax. To make a header, just put a pound sign / hash (#) in front of the line. As you want smaller headers, just keep adding pound signs. The middle column shows you the underlying HTML (web markup language) code. This isn’t that important for us. The last column, however, shows the text as it will render in your final document.\nThe site is also nice in that it shows you a few different ways, when they exist, of doing the same thing. Take some time to go through the site — it won’t take long — and keep it in mind as a reference for the future."
  },
  {
    "objectID": "07-Quarto.html#other-references",
    "href": "07-Quarto.html#other-references",
    "title": "Reproducable Reports with Quarto",
    "section": "Other references",
    "text": "Other references\n\nThe Plain Person’s Guide to Plain Text Social Science, by Kieran Healy (very short, but the first chapters make a strong case for using plain text when doing quantitative social science)\n\nIn this lesson, we’ll combine many of the pieces we’ve already covered — reading in data, cleaning data, making figures — into a single RMarkdown document. We’ll purposefully keep it simple at first by reusing some code we’ve seen before."
  },
  {
    "objectID": "07-Quarto.html#getting-started",
    "href": "07-Quarto.html#getting-started",
    "title": "Reproducable Reports with Quarto",
    "section": "Getting started",
    "text": "Getting started\nTo properly compile a PDF documents from Markdown, you’ll need some version of LaTeX, a typesetting system best known for being able to nicely render mathematical notation but that is really useful for making reproducible documents. You should have already downloaded this at the start of the course. If you didn’t, visit the software page for information on how to get it.\nIf you are unable to get LaTeX to install properly or cannot get the document to compile as PDF, you should be able to compile to HTML instead.\nYou will also need the R knitr and rmarkdown libraries. You should have rmarkdown already, but if you haven’t already installed either, type\ninstall.packages(c(\"knitr\",\"rmarkdown\"))\ninto your R console. NOTE that even if you’ve already installed rmarkdown, install.packages() will just quickly reinstall it."
  },
  {
    "objectID": "07-Quarto.html#what-is-rmarkdown",
    "href": "07-Quarto.html#what-is-rmarkdown",
    "title": "Reproducable Reports with Quarto",
    "section": "What is RMarkdown?",
    "text": "What is RMarkdown?\nFrom the RStudio website:\n\nR Markdown is a file format for making dynamic documents with R. An R Markdown document is written in markdown (an easy-to-write plain text format) and contains chunks of embedded R code…\n\nIn other words, an RMarkdown (hereafter RMD) document has two basic components:\n\nR code (in code chunks)\nMarkdown text (most everything else outside of the code chunks)\n\nRMD documents use the file ending, *.Rmd, which makes sense as they combine R code with md text. To compile an RMD file, meaning to\n\nconvert the plain Markdown text into formatted text\nrun R code, producing all output along the way\ncombine the Markdown text plus R output into a finished document\n\nYou will use the rmarkdown render() function, which in turn uses the knitr knit() function under the hood. It can be a bit confusing how all the pieces work together, but luckily, you can use RStudio’s point-and-click interface to knit your documents."
  },
  {
    "objectID": "07-Quarto.html#starting-a-new-document",
    "href": "07-Quarto.html#starting-a-new-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Starting a new document",
    "text": "Starting a new document\nWhen you open a blank RMarkdown document, RStudio will by default fill it with some example text that looks like this. You can change this in RStudio’s settings, but I think it’s helpful to see the skeleton of an RMarkdown document (plus, it’s not a big deal to just erase the parts you don’t need).\n\nExample text in new file started via RStudio (default)\n---\ntitle: \"Document Title\"\nauthor: \"Benjamin Skinner\"\ndate: \"1/30/21\"\noutput: pdf_document\n---\n\n\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "07-Quarto.html#including-plots",
    "href": "07-Quarto.html#including-plots",
    "title": "Reproducable Reports with Quarto",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nRather than going through this example text to learn about RMarkdown, we’ll use our own example document, test_scores.Rmd, which is linked above."
  },
  {
    "objectID": "07-Quarto.html#compile-an-rmarkdown-document",
    "href": "07-Quarto.html#compile-an-rmarkdown-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Compile an RMarkdown document",
    "text": "Compile an RMarkdown document\nWe’ll go through the main sections our example RMD document below. So you can follow along better, you should compile test_scores.Rmd as your first step.\nOnce you’ve downloaded the file and data (if you don’t have it already), place the RMD file in the scripts directory and the unzipped data in the data directory.\nWhen you open test_scores.Rmd in RStudio, you should see a button in the upper left facet that says Knit with a ball of string icon.\n\nIf you have the working directory correctly set to scripts and have placed the sch_test/ data folder inside data, you should be able to click Knit and have the document compile into a PDF. By default, RStudio will open a PDF viewer window or show you the file in the lower right facet in the Viewer tab.\nNOTE that if you’ve had trouble with LaTeX, you should be able to compile into an HTML file, which you can open in your browser."
  },
  {
    "objectID": "07-Quarto.html#sections-of-our-document",
    "href": "07-Quarto.html#sections-of-our-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Sections of our document",
    "text": "Sections of our document"
  },
  {
    "objectID": "07-Quarto.html#yaml-header",
    "href": "07-Quarto.html#yaml-header",
    "title": "Reproducable Reports with Quarto",
    "section": "YAML header",
    "text": "YAML header\nYAML, which stands for “YAML Ain’t a Markup Language”, is a common way to configure dynamic documents like RMD documents. It’s the first thing you see at the top of an RMarkdown file. The YAML header is this piece of code:\nNotice the opening and closing three hyphens (---). This is how R knows that this section of code is special. The YAML can become complex, as you add document options, but for now we keep it simple:\n\ntitle: the document title (printed)\nauthor: the document author (printed)\ndate: manually set date (printed)\n\nleave date out of YAML and the date on which the document is compiled will be added automatically, or you can use the LaTeX macro \\today, which will also print today’s date\nset date to \"\" (empty string) for no printed date\n\noutput: document output\n\npdf_output: for PDF (uses LaTeX)\nhtml_output: for web page output (open in browser)\nword_output: for MSWord output (uses MSWord)\n\n\nWe’re using pdf_output but you can either change this setting or override it when compiling the final document.\nNOTE that the colon (:) is a special character in YAML. Notice that I don’t necessarily have to use quotation marks for strings with spaces — I do for the title, but not for my name. That said, if your title includes a colon, you need to wrap the entire title string in double quotation marks (\") — otherwise the document won’t compile."
  },
  {
    "objectID": "07-Quarto.html#code-chunks",
    "href": "07-Quarto.html#code-chunks",
    "title": "Reproducable Reports with Quarto",
    "section": "Code chunks",
    "text": "Code chunks\nIn general, an RMD code chunk looks like a markdown code chunk. The key difference between the two is that while a plain markdown code chunk is purely about formatting, the RMD code chunk will by default try to run the code and print any output:\n\nMarkdown code chunk\n## this is just a representation \n## when compiled: nothing happens, only code is printed\nx &lt;- rnorm(1000)\nx\n\n\nRMarkdown code chunk\n\n## this is active R code\n## when compiled: the R code is run, and both code and results are printed\nx &lt;- rnorm(1000)\nx\n\n   [1]  0.3203419132 -0.3958450170 -0.4702623056 -0.3010348637  0.1094308252\n   [6] -0.9914744603 -0.8211847538 -0.6086277187 -0.4836891270 -0.3136799201\n  [11]  0.5771551825  0.2462118495  1.1073390651 -0.5388312001  2.0261234879\n  [16]  0.2862790010 -1.4842876468 -0.5224768575  0.6280410021 -0.3402735281\n  [21] -1.1813994993 -0.1194678137 -0.6381276862  1.3994926837  2.1436276735\n  [26]  0.7407446712 -0.7647577965  1.5412104133 -1.0511642763 -1.8051986754\n  [31]  1.3866890195  0.0878458241 -0.7029824420  0.1548425363 -1.3085710623\n  [36]  2.1764929106 -0.0345558570 -1.3279304620 -0.2976252187  0.2215325120\n  [41]  0.0840041887  1.4547585474 -0.0852082514 -0.2001399141 -0.4835853080\n  [46]  1.3726359538 -1.2042455151  0.2826491306 -2.2271093531 -0.8459270349\n  [51] -1.9007507056  1.5045182084 -0.7923116356  0.6403060005 -0.0879579931\n  [56] -1.8556018701  0.8181556150  0.0378951475  0.0234162398  2.2858127323\n  [61]  1.1510872251  1.2822784027 -0.1065125739  1.8139914397  0.1959588335\n  [66] -1.8612614915  0.3400000430 -0.0789679473 -1.4651890967  1.3007820739\n  [71] -0.1029038174 -2.7772151762 -0.0782227364  1.7249383373  0.6802392538\n  [76] -0.7069869157  1.3923688749 -0.1378265583 -0.6792018413 -1.5846625583\n  [81]  0.8550261229 -1.2827351997  0.5673491769 -1.5860950355 -0.7596942229\n  [86]  0.7104963882  0.8901797520  0.9126934248 -0.7062995098 -1.1208961637\n  [91]  1.1792471284  0.6585977478 -0.9083607582  0.1181326412  0.3278518179\n  [96]  0.3038815307  0.1903625611  0.1267861876  0.2984510064 -0.7918614768\n [101]  1.1053423562 -0.1947845099  1.6148946235  1.4068323444 -0.2108328227\n [106]  1.6055286669 -1.0520073823  0.6437954266  0.6699065530 -0.1550807316\n [111]  1.3858851292  1.2356011916  1.0011893306 -0.3112278752  0.6377165624\n [116] -1.9716485427  0.0594652192 -0.5711044152 -0.3333984714 -0.4369537903\n [121]  1.9567646351 -0.0041027729 -0.8574287612 -0.4863986963 -0.8086801071\n [126]  1.4233981981  0.9597489476  0.3382454162  1.0310545631  0.5630343681\n [131] -0.2066886029 -1.4600116492  0.5306696560  0.4307733833  0.8102844966\n [136]  0.4302878957  0.7015393348  0.1876509500 -0.1758517763  0.1838630938\n [141]  0.6070826054  1.0261172602  1.0573622971 -1.4423765773  1.0708981505\n [146] -1.2181190711  1.0869040472 -0.0592874855  0.4597104532 -0.8552569923\n [151] -0.6446851650 -0.2146063485  0.4778495939  0.9468643520 -0.8078460173\n [156] -0.5140139220  2.0648090579  1.1660151077  0.7755895129  0.0004466735\n [161] -0.7977831999 -0.1664382598  0.6960414338 -0.7271125524  0.9293212961\n [166] -0.1537933546  0.3609171410 -0.1983286153 -0.4049816166 -0.5941892894\n [171]  0.3134582314  1.3586708353  0.3664020597  0.9197981611 -0.1475890282\n [176]  1.5306963335 -0.7874846490  2.0728374510  0.2714195947  1.7234487500\n [181] -1.0217809708  0.1289462567 -0.8356648016 -0.3518941276  0.1188842605\n [186] -0.9723015228 -0.7092303312  1.1801607731 -0.8600648045  0.5776675409\n [191] -0.6215540163  0.9687580251  1.6162556686  0.3630063835  1.2714747880\n [196]  0.1470605440  0.0301944994 -0.1112880181  0.9013513131 -1.1478805699\n [201]  0.0718764632  0.6965954151 -0.0731788786 -0.3415759507 -0.6942279943\n [206]  0.4630006862 -1.2134779043  1.1018909556  0.1625245931 -0.1543945081\n [211]  0.1677040506 -0.0591989262 -0.6342017290 -0.6471136373 -2.0225303069\n [216]  0.0206649450  0.2548755566 -0.7448554474 -0.6050640239 -0.2049173092\n [221]  0.2853415436  1.8391534139  0.1020581213  0.7367443385  0.3374497399\n [226]  0.1082705217 -1.1188057142  0.6847337772  1.2624010616  1.2233441796\n [231]  0.2878133840 -0.2256924126 -1.0354113875 -0.2585282212  1.8866182678\n [236]  0.6337893828 -0.2033216083 -1.3907680378  0.7866027612 -0.9930524344\n [241]  1.2512711462  0.6758097759 -0.1622438902 -1.4747507050  0.3116233025\n [246] -0.3272611870 -0.8238473487  0.1407878357  0.5583401389 -1.8180996381\n [251]  1.3772407186  0.2523022277 -0.8987827824 -0.5863801646 -0.3596673497\n [256] -0.3544997848 -1.5069953374  0.0471713337  1.0657506347  1.8956547130\n [261]  1.0448746930  0.1487747072  0.6777894456  0.6457523892  1.6685187793\n [266] -0.8243191946  0.0068717420 -0.9212841849 -0.1911959829  1.1044275722\n [271] -1.0320679155 -2.1546394674  0.0741846345  0.0701664256  0.2474443950\n [276]  0.6174224818 -1.1320392353 -0.6253465412  0.5374451569 -0.0576786715\n [281] -0.1249748538 -1.2555936076  0.0350440920  0.0846414148 -0.2567534431\n [286]  1.4587569719  0.0588684553  0.0382474890 -0.9107293357  0.4590138645\n [291] -0.7357615121 -0.4225544998  0.8640145393 -1.3949983283 -1.2312005562\n [296] -0.2882736353 -0.9478364601  0.3236256773 -0.3570461470  0.3628855773\n [301] -1.6532835038  0.6877467658 -1.7558447910 -0.5509084672 -0.9925123682\n [306]  0.3594373460 -0.6815047753  1.1570102912 -1.1505500438  0.8081442025\n [311] -1.9061136488  0.2698270488 -1.0286033372 -1.4518202680  0.2968984648\n [316]  0.4743972474 -0.6725489670 -2.0411529313 -0.8026462065  0.6307925673\n [321]  0.5493351799 -0.8007828355 -0.1510604685  0.5595089305  0.8695329936\n [326] -0.0106348662  1.4179065974  0.7831511118  1.1246270243  0.5959439789\n [331] -1.0841609631  0.7532520719 -1.6473678454  0.4061509482 -0.1935002480\n [336] -0.3170932217  0.5127504914  0.0467816537  0.1481882911 -0.4954356750\n [341] -0.2082830254  1.3521981769  0.1317890020  0.1445243909  0.1158734231\n [346]  0.4558976518  0.0395709659 -0.0306081266  0.8918077442 -0.3802810745\n [351] -0.7331202704  1.1518977047 -0.4341204608 -0.2757508429 -0.4257600014\n [356] -0.4428609021 -0.3079880725 -0.2910368195 -0.0848774379  0.1597908740\n [361]  0.4302531898  1.0864985733 -0.7570857691  0.3466669013 -0.5749498855\n [366] -1.6262368165  1.3545601012  1.2969579176 -2.2139761973 -0.3917312964\n [371]  0.5306112181  0.5329062801  0.4395606159  2.2209331764 -0.0917251069\n [376]  0.0565362950  0.4224853377 -1.2662972135  0.1656022040 -1.0303246161\n [381] -0.8988018678  1.4412308546  0.5157682012 -0.9147744258 -0.8331343219\n [386] -1.4342160651  2.5059430231 -0.2255643113 -0.0572236983 -1.2320889479\n [391]  1.2967604888 -0.4145463733  1.7319852438 -0.2356568327  1.2902172005\n [396] -0.7963541746  1.2453438202  0.4989179189  1.5941451296  0.0984614660\n [401]  0.9979185452  1.6146363932 -1.8592449925  0.6205393696 -1.7870274600\n [406]  0.2567910217 -0.8054782413 -1.0409602143  1.5809240801  1.3591029839\n [411] -0.3166361696  0.2120319813 -0.0328244681 -1.1416775501 -0.4230948628\n [416]  0.4700977364  1.0234306153  0.9599346063 -0.7467026919 -0.5305064221\n [421]  0.2559999336  1.3283281501 -0.0265707348  0.4507914802 -0.2428768520\n [426] -0.7688469567  0.3706650056 -0.2655922831 -0.2554153824 -0.1773305758\n [431]  0.2166434509  0.7001549885  0.7978228143 -0.5061322476 -1.7155869478\n [436] -0.8540083516  0.2823856681 -2.3651355103 -0.1813686610 -1.7151807441\n [441]  1.3336239089  0.9687503046 -0.5323321326  1.0303586447 -1.7213784224\n [446]  0.3443912381 -1.0636904582  0.9033920291  0.4714738737 -0.2639373427\n [451] -2.5070136467 -1.1167113016  0.3407241383 -1.0126272527  1.6073054186\n [456] -1.0865779196  2.5127732431  1.1136426207 -0.4439007036 -1.1210523886\n [461]  3.2185102851  0.6440449549  1.1151138790 -0.8808338353 -0.8430838701\n [466]  0.8779216235 -0.1514580196  1.4993963820  0.5261069631 -0.4097060444\n [471] -0.6226631617 -0.6875332539  0.0609454293 -0.4258912761 -1.2129079577\n [476] -1.0722670041  0.6571277117  0.2827266900 -1.6639256534  0.9965753217\n [481]  0.2229615083 -1.2964962571 -0.3180875618  0.6373382858 -0.8695751040\n [486] -0.5355622766  0.3215103440 -1.7443384323  0.6631018968  1.0724817229\n [491] -2.5302912481  1.8071226924  0.7800725118 -1.7082565964  2.0001857060\n [496] -1.8098687889  0.6795204180 -0.7184627380  2.5645547038  0.5743216996\n [501]  0.2909517221 -0.3888019648 -0.6546272994 -1.5796863014 -1.0378722918\n [506]  2.1227954774 -0.4719451144  0.8216780266 -2.3752042915  0.6541160290\n [511] -0.1314816644  0.6215144616  0.6348514752 -0.5006077095  1.1711244706\n [516] -0.4353506155  1.5604153140  1.5949277968 -0.3998580313  0.3684926051\n [521] -0.9199959270  0.2648908429  0.6527628823  1.0848805966  0.3326431755\n [526] -1.2479264375  0.8347312808 -0.2909031749  0.3041706472 -0.4253731371\n [531] -2.2016829888 -1.4305223859  0.2271238221 -0.2149015370 -0.1673790131\n [536] -0.4299283094  0.6994530825 -0.5130766106 -0.8863895309 -2.4528656636\n [541]  0.5255622050 -0.8048629882  1.7270853886 -1.3664616928 -0.9798937224\n [546] -0.1776038107 -1.9214812797 -0.2438666845  0.6525053887  1.9547710431\n [551] -0.4813510742  0.1607451121 -1.7750830514  0.0419745756  0.0175829771\n [556]  0.0713832643  1.5086349650 -0.2033632601  0.0672850012 -0.6312014832\n [561]  0.6084903504 -0.0711000217 -1.5384495436  0.0459637889  0.5021449390\n [566]  1.6398568491 -0.7763958531  1.3178514688 -0.4410786650 -0.0404870984\n [571] -0.1577335820  1.1569344521  0.3006984697 -1.5730299690 -0.5665520669\n [576] -0.7875748684  1.0334407852  0.8267501906  0.9189543318  1.9717086905\n [581]  0.5083824334  0.8636001896 -0.9750318325 -0.5804188320 -0.9511876910\n [586]  0.6109818453 -0.2611050992 -0.2467943383 -0.8647732296 -0.6107436923\n [591] -0.4195556432 -0.3046081452  0.8038388655  0.3963862687 -0.8247481106\n [596] -0.8716108786  1.7338009280  0.6930514364 -0.4488719148 -1.7266008569\n [601]  0.2926718090 -0.6838198911  1.2583033925  0.1534948598 -0.9306597016\n [606] -0.7864347790  0.1590741383  0.6389888492 -0.9093765903 -0.2501982105\n [611] -3.2168346325  0.4180843547  1.4303127380 -0.8672221051  0.2586723098\n [616]  0.4812442340  0.0092562644  1.4034230231 -0.7913438406 -0.2442630220\n [621] -0.2569780122  0.5933300558  1.4827189028  0.9396743167 -0.7889072051\n [626]  0.9531558464  1.4615192093  1.5209175185 -1.4528983499  0.3739912595\n [631] -1.5044942782  1.5550011908 -0.1092143791 -0.7375106804 -0.0296257326\n [636] -0.1456523457  0.1661123736  0.5638619034 -0.1105478472 -0.2378238234\n [641] -0.5471988550 -0.0203702284  1.9068761734  0.9604916590 -0.6573017162\n [646] -0.3995457779  0.9338039269  1.3278942673  1.2927597405 -0.6002577994\n [651]  1.8312055167  1.7485002217 -0.3764876982  0.6075059690  1.0270032615\n [656]  0.5975105605 -0.8281763896 -0.5459737364  0.3949383174  1.4059659981\n [661]  0.0354090906 -0.0420114367  0.7259491484  0.9803807894 -2.6614685089\n [666]  0.6001428233  0.0235394922 -0.2701728425 -0.2613857715  0.6440332048\n [671] -0.9398794683  0.9370679186  0.2393282180  0.7505964933  2.1044562437\n [676]  0.1661471454  1.2591151698 -0.5631981777  1.6376447656  0.2928820125\n [681] -0.0910391861 -0.8910599467 -0.8217879913 -1.1982697756  0.4461940240\n [686] -1.2868158760 -1.7241406460 -2.1268359207  1.9852873842  0.1579891453\n [691]  0.8639817187  0.5512183711  0.3782449768 -0.4827639041  1.3516332544\n [696]  1.0616738157 -0.7632248630  1.1175024954  0.1343017288  0.4431425051\n [701] -1.1278820264  1.2320062886 -1.7943201120 -0.2962455924 -1.4795465143\n [706] -1.4267275296  0.3629077936  0.9787691460 -0.3365689952 -0.3615094589\n [711] -0.6917815259 -0.8541798061  0.7621290019  0.6716523993  2.1284573717\n [716]  0.5759444078  0.0688513780  1.5585488663 -0.5361074604  1.4637984807\n [721] -0.0963851546 -0.5762330551  0.9205591930  1.6380702026  2.0068883365\n [726] -0.1635919436  1.1996796188  0.2164570544 -1.2409739504  0.9969238379\n [731]  0.2714663056  0.8283008916  0.1036775562 -0.0948184178  0.0996869035\n [736] -0.2498123805  0.5575443898  0.4326215787 -0.7047397638  2.0223826708\n [741] -0.8415959165 -0.8260856725  1.7343758995  0.1808701720  0.4177407045\n [746] -1.1640867629  1.1260113612  1.2786993155  0.7034381223 -0.3918415942\n [751]  1.2511778887  0.6644030432 -1.0515197494  0.8618256627  0.4982685407\n [756] -0.3235897610 -0.5505008150  1.0928799822  1.0147729792  2.6685455717\n [761]  0.1053682678  0.4130968450 -0.6651010749 -2.0255822036 -0.3455738776\n [766] -1.6522463032 -0.6805345226 -0.4582089824 -0.7722779467 -0.2920105600\n [771]  0.8009889363  1.5962270097 -0.1631398668 -0.3165547192 -0.7891851433\n [776] -1.0056941925  0.4655076102 -1.7850439128  0.1642332735  0.3899031722\n [781]  0.5732746886  1.9411695275  0.1412065539 -0.2812411913  2.9253909928\n [786]  0.2373868156 -0.0249791172  0.8479614379  1.8487603055 -0.5771004939\n [791]  0.9267779274 -0.3807427409  1.1547653065 -0.7126550640 -1.1605853372\n [796]  0.9435057576 -1.1502568126 -0.9290391759 -0.0186662908 -0.9075145699\n [801] -1.0853549511 -0.2899493929 -1.0210595179  0.2161821584 -1.5174616806\n [806] -0.6927205595  0.7673728119 -0.7573426467 -0.6380997118  0.1455747604\n [811] -0.4550626142  0.0279586288 -0.4861918059 -0.9336583576  0.1784445659\n [816]  0.1631473545 -1.1958357552 -0.3911885940 -1.6132695387  0.9602669112\n [821] -1.2622534197  1.4066259587 -1.4829406862 -2.2343166803  0.9076468639\n [826] -0.0734260526 -0.5628447435  0.8656908556  0.5896332042  0.1878880770\n [831] -0.6418611452 -0.1044942199  0.4353362527  0.3790815554  1.7435892511\n [836]  0.4462701067 -0.1067429942 -1.1261200611  2.0729537320  2.6840490502\n [841]  0.9276907260 -0.6310548787  0.4361930352  1.4078884513  0.3284284744\n [846]  1.6829154788 -0.7540081173  0.0934783429  2.0321779837 -0.0288778960\n [851] -0.8108305516 -0.1058005549 -0.0465210314  0.1637358509  1.9730023365\n [856] -0.8577799245 -0.8524892283 -1.3294014991  2.7272179504  0.0567596538\n [861] -0.5321157882 -0.8475545008 -0.2474820622 -0.0797805728  0.2774519156\n [866] -0.7846210042 -1.6220488478 -0.1158060379 -1.2190203058 -1.0202058326\n [871] -0.6650843936  1.5492874827  0.8800514200  0.1377305945 -2.0669687675\n [876] -0.9428242824 -1.2735337386 -0.1864651647  0.4425533348  0.0298935418\n [881]  1.3899275304 -0.8055751273 -0.0648828901 -1.0261352305  0.0597754516\n [886] -1.7586752043 -1.5025064580 -2.3728425835 -0.0798373175 -0.4453467982\n [891]  0.1742088218  1.4900460441  0.9580658337  1.1482878615  0.0742958010\n [896]  1.5775601530  0.5707517106 -0.0526929353 -1.1327620840  1.8777104548\n [901] -0.4101886047  0.3284437737  1.6990845379  1.2217871962 -1.0718957183\n [906]  0.6008206726  0.6241507502  0.0373072767 -0.2231937455 -0.3839854507\n [911] -1.6649372417  0.1195876834 -2.7060320307 -0.1753958203  1.4899986752\n [916]  0.9531230440 -1.3706400094 -1.4021763984 -0.2783638065 -0.5722089298\n [921] -1.4860336500  2.0336998029 -1.4497048105  0.9856813313  0.2365184039\n [926]  0.4022232357 -0.1996211943  0.3487474126  0.9862201889 -0.0427633840\n [931] -1.0691072247  0.6696596748  0.9476568672  0.7119479030 -1.2683681494\n [936]  0.6023412033  0.8024107485 -1.3410797011 -0.1782103606  1.5763891120\n [941] -1.1541087493 -0.3699898123  0.3355094029 -0.8345968226 -1.1767589111\n [946]  0.4597330134  0.6531136017 -0.6827799002 -0.5044599817 -1.2270913677\n [951]  0.0417789848  0.6675630502  0.9738640393 -1.0270609155  0.2033830867\n [956]  0.6953600859 -0.8437139448 -0.0420575229  2.7822300054 -0.7403027511\n [961]  1.6952770759 -0.3086080813 -0.0971656144  0.6515441564  1.4678506705\n [966] -0.1709637708 -1.3039096712  0.3176909672  2.0136669509 -0.0818630512\n [971] -2.2482825171  0.0691263087 -0.2817182068 -0.3197495452  2.0569740273\n [976] -0.1166076503  0.2906758920  1.2870149742 -0.6593858081  0.3770887269\n [981]  0.0870850626 -1.9416303038  0.0487619190 -0.2629749206  0.4933658879\n [986]  0.5123611433  1.7070571785 -0.0832149201 -0.6476890987 -0.5942882233\n [991] -0.4680365286  2.0753877840  1.0552154865 -0.5172141391  0.2030761342\n [996] -0.2221157183  0.6300412990  0.9068657815  1.1158055576 -0.0865327807\n\n\nSee the difference? It’s subtle, but notice that the RMD chunk places braces around the r after the tick marks: {r}. In a normal markdown document, the braces won’t mean anything. But in an RMD document, it’s the difference between just printing the code and running the code before printing the code and its output.\n\n\nCode chunk options\nIn our first code chunk, notice how we still load our libraries and set our file paths. For the libraries, we need to load knitr with library(knitr) in addition to whichever libraries we need for our analysis. As usual, we also load the tidyverse.\nIn addition to our normal analysis setup, notice that we add knitr-specific options in two places.\n\nLocal code chunk options (only affect this code chunk)\nFirst, we can set local code chunk options within the braces that start the code chunk. These options will only affect this particular code chunk.\nAfter r, the first word is the name of the chunk. I’ve called it setup, since that’s what this chunk is doing, but you can name it anything you want. It’s not strictly necessary to name your chunks, but it can come in handy as your documents become more complex: if you get an error, it’s much easier to find data_input chunk than unnamed_chunk_38. NOTE that all named chunks need to be unique or your document will not compile. If really like a particular chunk and want to reuse it, you can always add a number at the end: data_input_1, data_input_2, data_input_3, etc.\nThere are a lot of options you can set for your chunks. Here we set the following:\n\necho=F (FALSE): don’t repeat this code in output\ninclude=F (FALSE): run code, but don’t include output (unless a plot)\nmessage=F (FALSE): don’t output any messages\nwarning=F (FALSE): don’t output any warnings\nerror=F (FALSE): don’t output any errors\n\nAs of knitr 1.35, you can also include chunk options in rows below the opening line using the #| symbol pair. We could rewrite our example options using:\nHowever you choose to include them, these options keep our chunk from echoing the input code into our document and prevents any output. Basically, silence. Sometimes we want our code to echo; sometimes we want output. But since we are making a report, we generally want the underlying code to remain hidden. Readers of our report should only see the write up and any relevant tables and figures — but not all the hard coding we did to make them!\n\n\nGlobal code chunk options (affect all code chunks)\nAfter this first chunk, we can save some typing by setting these options for the rest of the document using knitr::opt_chunks$set(). Notice that we include the same settings as above plus a few more:\n\nfig.path: path + prefix for all figures (put them in our /figures folder and add \"ts-\" to the name)\ndpi (dots per inch): the print quality of our figures; 300 dpi is a nice standard for print (72dpi is sufficient for most web output)\nout.width: our figures should fill the line width; if it’s an 8.5 by 11 inch page with 1 inch margins, then a width of 7.5 inches\ncomment: if we return code output, don’t prepend with # or anything — just the output.\n\nThere are other options we can use. We can also override these setting as necessary for individual code chunks using local settings like we did in the first chunk (as you’ll see below). The main idea with the set up code chunk is to get our document settings as close as possible to the way we generally want them.\n\n\n\nChunk to chunk\nBelow, you see two code chunks with some Markdown text in the middle.\nAn important thing to remember is that your coding environment carries from chunk to chunk, meaning that if you read in data in the code chunk (named input) as df, then df will still be available to you in the next chunk (named table_all) after writing some Markdown text. This means that you can still organize your RMD scripts like your R scripts (no need to do everything at once in a single huge chunk).\n\n\nMake a nice(r) table with kable()\nSince we want to show all of our data (which isn’t very big in this case) and because the data frame df already is organized in the way we want to show the data (school by year with different columns for each test), we can just print out the data frame. To be clear, often our data will be too big to do this, but in this instance, we are okay.\nWe could just print the data frame by calling df in a chunk. But to make it look nicer with a better format, we use kable() which is part of knitr.\nEven using mostly default options, kable() will make a nice looking table for us. We add digits = 0 to make sure that we only show whole numbers and we change our column names to something nicer (leaving a blank \"\" for the school name column, which is obvious). Notice that in the chunk braces we add include = T so that the output — our table — for just this one code chunk will be printed. This is an example of using local code chunk options to override global chunk options.\n\n\nInline code\nYou can also call R code inline, that is, R code that sits outside of code chunks proper and instead is mixed in with your Markdown text.\nInside the code chunk called table_averages we do three things:\n\nUse dplyr to munge our data to get averages\nStore names and scores for high test scores in distinct well named objects:\n\nhi_&lt;test&gt;_sch: school name with highest average math/read/science score\nhi_&lt;test&gt;_scr: highest average math/read/science score\n\nMake/print table using kable()\n\nIn the Markdown text below this code chunk, we call the values using the inline code method\n`r `\nthat is, single back tick, an r, the code we want, then closing back tick. We also pipe the object value to the round() function so that we don’t return averages with extra and unnecessary decimal points. We could have simply run all the code inline (included what we did in step 2 of the code chunk above), but that would have made for extra messy code.\nWhy do this? One reason is that being able to incorporate data-driven values directly in your test is very powerful. Imagine you need to reproduce the same report on a monthly or quarterly basis when data are updated. Part of the written report includes values directly taken or calculated from the data. Rather than update these “magic numbers” each time (potentially missing some), you can use inline R code like we’ve done here. All you need to do then is update the data and recompile the report. Voila! Everything is properly updated.\nTaking it a step further, you can include all kinds of ifelse() logic to make complex dynamic documents. If the value of X is equal to or greater than the value of Y, then print “equals or exceeds”; else print “remains less than”. Be aware, however, that your document text still needs to make sense. It can be difficult enough writing one clear sentence; having to write a sentence that will remain coherent despite variable inputs can be very tough!\n\n\nFigures\nFinally, making figures is pretty much the same as making tables:\nHaving reshaped our original data frame long (df_long), we make a figure just as we’ve done in the past — with some formatting improvements to make it nicer looking. While it’s not strictly necessary to store the figure in an object (p) that we then call, it works just fine.\nNotice that again added include = T to the chunk brace. Because we added figure options to opt_chunks$set() in the setup code chunk, this figure (as well as the next one) is sized so that it fills up the page width (with height determined as a ratio of that width) and printed at 300 dpi quality. If you look in the figures folder, you’ll see the figure named ts-fig_unadjusted, which is the prefix we set above with the name of the code chunk."
  },
  {
    "objectID": "07-Quarto.html#text",
    "href": "07-Quarto.html#text",
    "title": "Reproducable Reports with Quarto",
    "section": "Text",
    "text": "Text\nThroughout our RMD file, we’ve include Markdown text. This text lives outside of the code chunks and is always printed in the final document. It follows normal Markdown text rules, but can have R code placed inline, as we saw above."
  },
  {
    "objectID": "07-Quarto.html#writing-an-rmd-document",
    "href": "07-Quarto.html#writing-an-rmd-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Writing an RMD document",
    "text": "Writing an RMD document\nJust as when you write a plain R script, your progress from initial RMD draft to final product will be iterative. While you can run R code from inside code chunks just as you’ve been all semester, you may find it useful to start your analyses in plain R files first and only add them to an RMD document later.\nFor big projects, such as dissertation, it also doesn’t make much sense to put everything — data reading, cleaning, analysis, table/figure making — inside a single RMD document. You have to redo your entire workflow each time you compile! For large projects, it might make sense to do all the heavy lifting in separate R scripts — saving cleaned up data sets, tables, and figures along the way — and putting all the pre-establish pieces together at the end. But for small projects, such a descriptive policy report, a single RMD document might suffice."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#core-processes-in-a-data-analysis",
    "href": "03-Data-Wrangling-I.html#core-processes-in-a-data-analysis",
    "title": "I: Enter the tidyverse",
    "section": "Core processes in a data analysis",
    "text": "Core processes in a data analysis\nLarge or small, a typical data analysis will involve most — if not all — of the following steps:\n\nRead in data\nSelect variables (columns)\nMutate data into new forms\nFilter observations (rows)\nSummarize data\nArrange data\nWrite out updated data\n\nReturning to our cooking metaphor from the organizing lesson: if the first and last steps represent our raw ingredients and finished dish, respectively, then the middle steps are the core processes we use to prepare the meal.\nAs you can see, there aren’t that many core processes to use. Their power comes from the infinite ways they can be ordered and combined. There are, of course, many specialized tools for specialized data wrangling tasks — too many to cover in this course (Google is your friend here!). But I call these processes “core processes” for a reason — they will be at the center of most of your data analytic work."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#tidyverse",
    "href": "03-Data-Wrangling-I.html#tidyverse",
    "title": "I: Enter the tidyverse",
    "section": "Tidyverse",
    "text": "Tidyverse\nThe tidyverse is shorthand for a number of packages that are built to work well together and can be used in place of base R functions. A few of the tidyverse packages that you will often use are:\n\ndplyr for data manipulation\n\ntidyr for making data tidy\n\nreadr for flat file I/O\n\nreadxl for Excel file I/O\n\nhaven for other file format I/O\n\nggplot2 for making graphics\n\nstringr for working with strings\n\nlubridate for working with dates\n\npurrr for working with functions\n\nMany R users find functions from these libraries to be more intuitive than base R functions. In some cases, tidyverse functions are faster than base R, which is an added benefit when working with large data sets.\nToday we will primarily use functions from the dplyr and readr libraries. We’ll also use some common base R functions as necessary.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#check-working-directory",
    "href": "03-Data-Wrangling-I.html#check-working-directory",
    "title": "I: Enter the tidyverse",
    "section": "Check working directory",
    "text": "Check working directory\nThis script — like the one from the organizing lesson — assumes that the scripts subdirectory is the working directory, that the required data file is in the data subdirectory, and that both subdirectories are at the same level in the course directory. Like this:\nstudent_skinner/         &lt;--- Top-level\n|\n|__/data                 &lt;--- Sub-level 1\n|    |--+ hsls_small.csv\n|\n|__/scripts              &lt;--- Sub-level 1 (Working directory)\n     |--+ dw_one.R\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that I’m not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, I tell you where you need to be (a common landmark), let you get there, and then rely on relative paths afterwards.\n\nSCRAP DIR OBJECT SETTING\n\n## ---------------------------\n## directory paths\n## ---------------------------\n\n## assume we're running this script from the ./scripts subdirectory\n# dat_dir &lt;- file.path(\"..\", \"data\")"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#example-data-analysis-task",
    "href": "03-Data-Wrangling-I.html#example-data-analysis-task",
    "title": "I: Enter the tidyverse",
    "section": "Example data analysis task",
    "text": "Example data analysis task\nLet’s imagine we’ve been given the following data analysis task with the HSLS09 data:\n\nFigure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R. Instead, it’s up to you to (1) translate a research question into the discrete steps coding steps necessary to provide an answer, and then (2) translate the answer such that everyone understands what you’ve found.\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nArrange in order so it’s easier to rank and share\nWrite out the results to a file so we have it for later\n\nLet’s do it!"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#read-in-data",
    "href": "03-Data-Wrangling-I.html#read-in-data",
    "title": "I: Enter the tidyverse",
    "section": "Read in data",
    "text": "Read in data\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES / NCES data set that features:\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link (after a little navigation).\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read_csv() from the readr library\ndf &lt;- read_csv(file.path(\"data\", \"hsls_small.csv\"))\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUnlike the readRDS() function we’ve used before, read_csv() prints out information about the data just read in. Nothing is wrong! The read_csv() function, like many other functions in the tidyverse, assumes you’d rather have more rather than less information and acts accordingly. We can see that all the columns were read in as doubles (dbl(16)), which is just a type of number that the computer understands in a special way (a distinction that’s not important for us in this case). For other data (or if we had told read_csv() how to parse the columns), we might see other column types like:\n\nint(): another type of number (again, an important distinction for the computer, but not usually for us)\nchr(): strings (e.g., \"Ben\" or \"1\" [notice the quotes])\nlgl(): Boolean values of TRUE or FALSE\n\n\nQuick exercise\nread_csv() is special version of read_delim(), which can read various delimited file types, that is, tabular data in which data cells are separated by a special character. What’s the special character used to separate CSV files? Once you figure it out, re-read in the data using read_delim(), being sure to set the delim argument to the correct character."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#select-variables-columns",
    "href": "03-Data-Wrangling-I.html#select-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\nTo choose variables, either when making a new data frame or dropping them, use select(). Like the other dplyr functions we’ll use, the first argument select() takes is the data frame (or tibble) object. After that, we list the column names we want to keep.\nSome pseudocode for using select() is:\n## pseudocode (not to be run)\nselect(&lt; df object &gt;, column_1_name, column_2_name)\nBecause we don’t want to overwrite our original data in memory, we’ll assign (&lt;-) the output to a new object called df_tmp.\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- select(df, stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n# … with 23,493 more rows"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#mutate-data-into-new-forms",
    "href": "03-Data-Wrangling-I.html#mutate-data-into-new-forms",
    "title": "I: Enter the tidyverse",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\nTo add variables and change existing ones, use the mutate() function.\nJust like select(), the mutate() function takes the data frame as the first argument, followed by variable name, new or old, that is created/modified by some function:\n## pseudocode (not to be run)\nmutate(&lt; df object &gt;, column_1_name = function(...))\nIn this case, the function(...) (or, stuff we want to do) is add a new column that is the larger of x1stuedexpct and x1paredexpct.\n\nUnderstanding our data\nFirst things first, however, we need to check the code book to see what the numerical values for our two education expectation variables represent. To save time, I’ve copied them here:\n\nx1stuedexpct\nHow far in school 9th grader thinks he/she will get\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n\n\n\nx1paredexpct\nHow far in school parent thinks 9th grader will go\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n-9\nMissing\n\n\n\nThe good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression. The bad news is that we have three values — -8, -9, and 11 — that we need to deal with so that the averages we compute later represent what we mean.\nLet’s see how many observations are affected by these values using count() (notice that we don’t assign to a new object; this means we’ll see the result in the console, but nothing in our data or object will change):\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ncount(df_tmp, x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -8  2059\n 2            1    93\n 3            2  2619\n 4            3   140\n 5            4  1195\n 6            5   115\n 7            6  3505\n 8            7   231\n 9            8  4278\n10            9   176\n11           10  4461\n12           11  4631\n\n## see unique values for parental expectation\ncount(df_tmp, x1paredexpct)\n\n# A tibble: 13 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -9    32\n 2           -8  6715\n 3            1    55\n 4            2  1293\n 5            3   149\n 6            4  1199\n 7            5   133\n 8            6  4952\n 9            7    76\n10            8  3355\n11            9    37\n12           10  3782\n13           11  1725\n\n\nDealing with -8 and -9 is straightforward — we’ll convert it missing. In R, missing values are technically stored as NA. Not all statistical software uses the same values to represent missing values (for example, Stata uses a dot .). Likely because they want to be software agnostic, NCES has decided to represent missing values as a limited number of negative values. In this case, -8 and -9 represent missing values.\nHow to handle missing values is a very important topic, one we could spend all semester discussing. For now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results.\nDeciding what to do with 11 is a little trickier. While it’s not a missing value per se, it also doesn’t make much sense in its current ordering, that is, to be “higher” than completing a professional degree. We’ll make a decision to convert these to NA as well, effectively deciding that an answer of “I don’t know” is the same as missing an answer."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#changing-existing-variables-columns",
    "href": "03-Data-Wrangling-I.html#changing-existing-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Changing existing variables (columns)",
    "text": "Changing existing variables (columns)\nSo first step: convert -8, -9, and 11 in both variables to NA. We can do this by overwriting cells in each variable with NA when they equal one of these two values. For this, we’ll use the ifelse() function, which has three parts:\n## pseudo code (not to be run)\nifelse(&lt; test &gt;, &lt; return this if TRUE &gt;, &lt; return this if FALSE &gt;)\nifelse() works by asking: if the &lt; test &gt; is TRUE do this else (i.e. the &lt; test &gt; is FALSE) do that. By test, I mean a code statement that evaluates to either TRUE or FALSE. For example:\n\n1 == 1 (TRUE)\n1 == 2 (FALSE)\n1 + 1 == 2 (TRUE)\n\nNOTE When checking whether something equals something else, use a double equals sign (==); a single equals sign typically means assignment = or the same thing as the arrow, &lt;-.\nWhat we want is to go row by row (that is, observation by observation) through both x1stuedexpct and x1paredexpct and test whether the value is either -8, -9, or 11 — if it is, then replace with NA, otherwise, just replace it with the value it found (i.e. leave it alone).\nWe could develop a sophisticated test that looked for any of these conditions, but we can also just do it with multiple ifelse() functions. Notice, however, that we can test for both -8 and -9 in the same test since all the categories we want are positive.\n\n## use case_when to overwrite -8 and 11 with NA in our two expectation variables\ndf_tmp &lt;- mutate(df_tmp,\n                 ## correct student expectations \n                 x1stuedexpct = ifelse(x1stuedexpct &lt; 0,   # is value &lt; 0?\n                                       NA,                 # T: replace with NA\n                                       x1stuedexpct),      # F: replace with self\n                 x1stuedexpct = ifelse(x1stuedexpct == 11, # is value == 11?\n                                       NA,                 # T: replace with NA\n                                       x1stuedexpct),      # F: replace with self\n                 ## correct parental expectations\n                 x1paredexpct = ifelse(x1paredexpct &lt; 0,   # (same as above...)\n                                       NA,                 \n                                       x1paredexpct),       \n                 x1paredexpct = ifelse(x1paredexpct == 11,\n                                       NA,\n                                       x1paredexpct))\n\nNotice that we used df_tmp rather than df. That’s because we want to carry through the work we did with select() before. If we used df instead, then we’d be back to the original data object — not what we want.\nLet’s confirm that our code worked as we planned by using count() again.\n\n## again see unique values for student expectation\ncount(df_tmp, x1stuedexpct)\n\n# A tibble: 11 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    93\n 2            2  2619\n 3            3   140\n 4            4  1195\n 5            5   115\n 6            6  3505\n 7            7   231\n 8            8  4278\n 9            9   176\n10           10  4461\n11           NA  6690\n\n## again see unique values for parental expectation\ncount(df_tmp, x1paredexpct)\n\n# A tibble: 11 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    55\n 2            2  1293\n 3            3   149\n 4            4  1199\n 5            5   133\n 6            6  4952\n 7            7    76\n 8            8  3355\n 9            9    37\n10           10  3782\n11           NA  8472"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#adding-new-variables-columns",
    "href": "03-Data-Wrangling-I.html#adding-new-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Adding new variables (columns)",
    "text": "Adding new variables (columns)\nAdding a new variable to our data frame is just like modifying an existing column. The only difference is that instead of putting an existing column name on the LHS of the = sign in mutate(), we’ll make up a new name. This tells R to make a new column in our data frame that contains the results from the the RHS function(s).\n## pseudocode (not to be run)\nmutate(&lt; df object &gt;, new_column_name = function(...))\nNow that we’ve corrected our expectation variables, we create a new variable that is the higher of the two (per our initial instructions to choose the higher of the two if both existed).\nUsing ifelse() again, we can test whether each student’s degree expectation is higher than that of their parent; if true, we’ll put the student’s value into the new variable — if false, we’ll put the parent’s value into the new variable.\n\n## mutate (notice that we use df_tmp now)\ndf_tmp &lt;- mutate(df_tmp,\n                 high_expct = ifelse(x1stuedexpct &gt; x1paredexpct, # test\n                                     x1stuedexpct,                # if TRUE\n                                     x1paredexpct))               # if FALSE\n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1         NA\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1         NA\n 8  10008            8            6        1          8\n 9  10009           NA           NA        3         NA\n10  10010            8            6        1          8\n# … with 23,493 more rows\n\n\nDoing a quick “ocular test” of our first few rows, it seems like our new variable is correct…EXCEPT…it doesn’t look like we handled NA values correctly. Look at student 10002 in the second row: while the student doesn’t have an expectation (or said “I don’t know”), the parent does. However, our new variable records NA. Let’s fix it with this test:\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\ndf_tmp &lt;- mutate(df_tmp,\n                 ## step 1: compare with student's expectations\n                 high_expct = ifelse(is.na(high_expct) & !is.na(x1stuedexpct), \n                                     x1stuedexpct,                \n                                     high_expct),\n                 ## step 2: compare with parent's expectations\n                 high_expct = ifelse(is.na(high_expct) & !is.na(x1paredexpct), \n                                     x1paredexpct,                \n                                     high_expct))               \n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10009           NA           NA        3         NA\n10  10010            8            6        1          8\n# … with 23,493 more rows\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue. Looking at rows 7 and 9, it seems like those situations are correctly handled as well.\nTo be clear, there were other ways we could have handled fixing our missing values and creating our new variable. For example, we could have left our missing values as negative numbers (and converting 11 to a negative value) so that our comparison would have worked the first time. We could have used more sophisticated tests in our ifelse() statements. However, these paths weren’t clear until we’d already worked a bit. The point to keep in mind that the process is often iterative (two steps forward, one step back…) and that there’s seldom an single correct way.\n\nQuick exercise\nWhat happens when the student and parent expectations are the same, either a value or NA? Does our ifelse() statement account for those situations? If so, how?"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#filter-observations-rows",
    "href": "03-Data-Wrangling-I.html#filter-observations-rows",
    "title": "I: Enter the tidyverse",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\nLet’s check the counts of our new variable:\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ncount(df_tmp, high_expct)\n\n# A tibble: 11 × 2\n   high_expct     n\n        &lt;dbl&gt; &lt;int&gt;\n 1          1    71\n 2          2  2034\n 3          3   163\n 4          4  1282\n 5          5   132\n 6          6  4334\n 7          7   191\n 8          8  5087\n 9          9   168\n10         10  6578\n11         NA  3463\n\n\nSince we’re not going to use the missing values (we really can’t, even if we wanted to do so), we’ll drop those observations from our data frame using filter().\nAn important point about filter() that often trips people up at first: use it to filter in what you want. This is the opposite of the more common usage of filters, which are about removing things (e.g., air filters, water filters, etc).\n\n## filter out missing values\ndf_tmp &lt;- filter(df_tmp, !is.na(high_expct))\n\n## show\ndf_tmp\n\n# A tibble: 20,040 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10010            8            6        1          8\n10  10011            8            6        3          8\n# … with 20,030 more rows\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values). Since we haven’t removed rows until now, we can compare the number of rows in the original data frame, df, to what we have now.\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column. While not a formal test, it does support what we expected (in other words, if the number were different, we’d definitely want to go back and investigate)."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#summarize-data",
    "href": "03-Data-Wrangling-I.html#summarize-data",
    "title": "I: Enter the tidyverse",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. The summarize() command will allow us to apply a summary measure, like mean(), to a column of our data. (NOTE that if we wanted another summary measure, like the median or standard deviation, there are other functions like median() and sd()…if you need a particular stat, there’s likely a function for it!).\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nsummarize(df_tmp, high_expct_mean = mean(high_expct))\n\n# A tibble: 1 × 1\n  high_expct_mean\n            &lt;dbl&gt;\n1            7.27\n\n\nOverall, we can see that students and parents have high postsecondary expectations on average: to earn some graduate credential beyond a bachelor’s degree. However, this isn’t what we want. We want the values across census regions.\n\n## check our census regions\ncount(df_tmp, x1region)\n\n# A tibble: 4 × 2\n  x1region     n\n     &lt;dbl&gt; &lt;int&gt;\n1        1  3128\n2        2  5312\n3        3  8177\n4        4  3423\n\n\nWe’re not missing any census data, which is good. To calculate our average expectations, we need to use the group_by() function. This function allows to set groups and perform other dplyr operations within those groups. Right now, we’ll use it to get our summary.\n\n## get expectations average within region\ndf_tmp &lt;- group_by(df_tmp, x1region)\n\n## show grouping\ndf_tmp\n\n# A tibble: 20,040 × 5\n# Groups:   x1region [4]\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10010            8            6        1          8\n10  10011            8            6        3          8\n# … with 20,030 more rows\n\n\nNotice the extra row at the second line now? Groups:  x1region [4] tells us that our data set is now grouped.\n\nQuick exercise\nWhat does the [4] mean?\n\nNow that our groups are set, we can get the summary we really wanted\n\n## get average (assigning this time)\ndf_tmp &lt;- summarize(df_tmp, high_expct_mean = mean(high_expct))\n\n## show\ndf_tmp\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        2            7.17\n3        3            7.36\n4        4            7.13\n\n\nSuccess! Expectations are similar across the country, but not the same by region.\nNB: The reason we didn’t assign the first ungrouped summarize() function back to df_tmp is that summarize() fundamentally changes our data frame, from one of many observations to one that represents their summary (as we asked!). Keep this in mind for your own analyses: the order of operations matters. If you select columns, then columns you didn’t selection won’t be available later; if you summarize your data, then you only have access to the summary."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#arrange-data",
    "href": "03-Data-Wrangling-I.html#arrange-data",
    "title": "I: Enter the tidyverse",
    "section": "Arrange data",
    "text": "Arrange data\nAs our final step, we’ll arrange our data frame from highest to lowest (descending). For this, we’ll use arrange() and a special operator, desc() which is short for descending.\n\n## -----------------\n## arrange\n## -----------------\n\n## arrange from highest expectations (first row) to lowest\ndf_tmp &lt;- arrange(df_tmp, desc(high_expct_mean))\n\n## show\ndf_tmp\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        3            7.36\n3        2            7.17\n4        4            7.13\n\n\n\nQuick exercise\nWhat happens when you don’t include desc() around high_expct_mean?"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#write-out-updated-data",
    "href": "03-Data-Wrangling-I.html#write-out-updated-data",
    "title": "I: Enter the tidyverse",
    "section": "Write out updated data",
    "text": "Write out updated data\nWe can use this new data frame as a table in its own right or to make a figure. For now, however, we’ll simply save it using the opposite of read_csv() — write_csv() — which works like writeRDS() we’ve used before.\n\n## write with useful name\nwrite_csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\nAnd with that, we’ve met our task: we can show average educational expectations by region. To be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region. This caveat doesn’t necessarily make our analysis less useful, but rather sets its scope. Furthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#pipes",
    "href": "03-Data-Wrangling-I.html#pipes",
    "title": "I: Enter the tidyverse",
    "section": "Pipes (%>%)",
    "text": "Pipes (%&gt;%)\nAbove, we’ve performed each step of our analysis piecemeal, saving new objects or overwriting along the way. This is fine, but a huge benefit of the tidyverse is that it allows users to chain commands together using pipes.\nTidyverse pipes, %&gt;%, come from the magrittr package.\n  \nPipes take output from the left side and pipe it to the input of the right side. So mean(x) can be rewritten as x %&gt;% mean: x outputs itself and the pipe, %&gt;%, makes it the input for mean().\n\nQuick exercise\nStore 1,000 random values in x: x &lt;- rnorm(1000). Now run mean(x) and x %&gt;% mean. Do you get the same thing?\n\nThis may be a silly example (why would you do that?), but pipes are powerful because they allow data wrangling processes to be chained together.\nNormally, functions (like select(), mutate(), etc), can be nested in R, but after too many, the code becomes difficult to parse since it has to be read from the inside out. For this reason, many analysts run one discrete function after another, saving output along the way. This is what we did above.\nPipes allow functions to come one after another in the order of the work being done, which is more legible. As a bonus, chaining functions together is sometimes faster due to behind-the-scenes processing.\nLet’s use Hadley’s canonical example to make the readability comparison between nested functions and piped functions clearer:\n## foo_foo is an instance of a little bunny function\nfoo_foo &lt;- little_bunny()\n\n## adventures in base R must be read from the middle up and backwards\nbop_on(\n    scoop_up(\n        hop_through(foo_foo, forest),\n        field_mouse\n    ),\n    head\n)\n\n## adventures w/ pipes start at the top and work down\nfoo_foo %&gt;%\n    hop_through(forest) %&gt;%\n    scoop_up(field_mouse) %&gt;%\n    bop_on(head)\nIn the first set, we have to read the story of little bunny foo foo from the inside out: “Little bunny foo_foo bopped on the head a field mouse that was scooped up while hopping through the forest.”\nWith pipes, we can read it more like the original rhyme: “Little bunny foo foo hopped through the forest, scooped up a field mouse, and bopped it on the head.”\nThe main thing to remember is with pipes and tidyverse is that because the output of the function goes into the next function, you don’t need to include the first argument, that is, the data frame object name.\nThis:\ndf_example &lt;- select(df, col1, col2)\nbecomes this:\ndf_example &lt;- df %&gt;% select(col1, cols2)\nIn the second example, the object df was piped into the first argument spot in select(). Since that’s already accounted for, we were able to just start with the column names we want."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#rewriting-our-analysis-using-pipes",
    "href": "03-Data-Wrangling-I.html#rewriting-our-analysis-using-pipes",
    "title": "I: Enter the tidyverse",
    "section": "Rewriting our analysis using pipes",
    "text": "Rewriting our analysis using pipes\nReturning to our analysis above, let’s rewrite all our of our steps in on piped chain of commands.\n\n## start with the original data frame...\ndf_tmp_chained &lt;- df %&gt;%\n    ## (df is piped in): select the columns we want\n    select(stu_id, x1stuedexpct, x1paredexpct, x1region) %&gt;%\n    ## (selected df is piped in): mutate our data, starting with missing\n    mutate(x1stuedexpct = ifelse(x1stuedexpct &lt; 0,   \n                                 NA,                 \n                                 x1stuedexpct),      \n           x1stuedexpct = ifelse(x1stuedexpct == 11, \n                                 NA,                 \n                                 x1stuedexpct),      \n           x1paredexpct = ifelse(x1paredexpct &lt; 0,   \n                                 NA,                 \n                                 x1paredexpct),       \n           x1paredexpct = ifelse(x1paredexpct == 11,\n                                 NA,\n                                 x1paredexpct),\n           ## create new column\n           high_expct = ifelse(x1stuedexpct &gt; x1paredexpct, \n                               x1stuedexpct,                \n                               x1paredexpct),\n           ## fix new column NAs\n           high_expct = ifelse(is.na(high_expct) & !is.na(x1stuedexpct), \n                               x1stuedexpct,                \n                               high_expct),\n           high_expct = ifelse(is.na(high_expct) & !is.na(x1paredexpct), \n                               x1paredexpct,                \n                               high_expct)) %&gt;%\n    ## (mutated df is piped in): filter in non-missing rows\n    filter(!is.na(high_expct)) %&gt;%\n    ## (filtered df is piped in): group by region\n    group_by(x1region) %&gt;%\n    ## (grouped df is piped in): summarize mean expectations\n    summarize(high_expct_mean = mean(high_expct)) %&gt;%\n    ## (summarized df is piped in): arrange average expectations hi --&gt; lo\n    arrange(desc(high_expct_mean))         \n\nNotice how I included comments along the way? Since R ignores commented lines (it’s as if they don’t exist), you can include within your piped chain of commands. This is a good habit that collaborators and future you will appreciate.\nTo be sure, let’s check: is the result the same as before?\n\n## show\ndf_tmp_chained\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        3            7.36\n3        2            7.17\n4        4            7.13\n\n## test using identical()\nidentical(df_tmp, df_tmp_chained)\n\n[1] TRUE\n\n\nSuccess!"
  },
  {
    "objectID": "03-Data-Wrangling-I.html#final-notes",
    "href": "03-Data-Wrangling-I.html#final-notes",
    "title": "I: Enter the tidyverse",
    "section": "Final notes",
    "text": "Final notes\nIn this lesson, you’ve converted a research question into a data analysis that started with reading in raw data and ended with your summary table. You’ve seen how to wrangle your data in multiple discrete steps as well as chained together using pipes.\nAs you start to apply these tools to your own analyses, the first questions should always be:\n\nWhat I am trying to do?\nWhat would my data or results need to look like in order to do that?\nWhat are the discrete steps I need to get from what I have (raw data) to what I need (wrangled data)?\n\nRemember, these are iterative questions, meaning you will almost certainly need to revisit and adjust during your analysis. But becoming a better quantitative researcher mostly means becoming a better translator: question –&gt; data/coding –&gt; answer."
  },
  {
    "objectID": "03-Data-Wrangling-I.html#questions",
    "href": "03-Data-Wrangling-I.html#questions",
    "title": "I: Enter the tidyverse",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average standardized math test score?\nWhat is the average standardized math test score by gender?\nIn what year and month were the oldest students in the data set born? The youngest?\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income (give the category and what that category reprents).\nOf the students who earned a high school credential (diploma or GED), what percentage earned a GED or equivalency? How does this differ by region?\nWhat percentage of students ever attended a postsecondary institution by February 2016? Give the cross tabulations for:\n\nfamily incomes less than or equal to $35,000 and greater than $35,000\n\nregion\n\nThis means you should have percentages for 8 groups: above/below $35k within each region.\nHINT You can group_by() on more than one group.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_3.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "14-Bens-Philosophy.html#organization",
    "href": "14-Bens-Philosophy.html#organization",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Organization",
    "text": "Organization\nFrom the organizing lesson, we’ve been very concerned about how our project is organized. This includes both the directory of folders and files on our machine and the scripts that run our analyses.\n\nProject directory\nWhile it is certainly easier on the front end to store all project files in a single folder\n./project\n|\n|--+ analysis.R\n|--+ clean.R\n|--+ clean_data.rds\n|--+ data1.csv\n|--+ data2.csv\n|--+ histogram_x.png\n|--+ density_z.png\nit’s better to separate files by type/purpose in your project directory (“a place for everything and everything in its place”):\n./project\n|\n|__/data\n|   |--+ clean_data.rds\n|   |--+ data1.csv\n|   |--+ data2.csv\n|__/figures\n|   |--+ histogram_x.png\n|   |--+ density_z.png\n|__/scripts \n    |--+ analysis.R\n    |--+ clean.R\nJust like there’s something to be said for visiting the library in order to scan titles in a particular section — it’s easier to get an idea of what’s available by looking at the shelf than by scanning computer search output — it’s useful to be able to quickly scan only the relevant files in a project. With well-named files (discussed below) and an organized directory structure, a replicator may be able to guess the purpose, flow, and output of your project without even running your code.\n\nQuick question\nWhen using a project directory instead of just a single folder, we need to use file paths. In particular, we use relative paths rather than fixed paths. Why?\n\n\n\nScript\nIn the lesson on organizing, I also shared a template R script. While you do not need to use my particular template — in fact, you should modify it to meet your needs — it does follow a few organizational rules that you should follow as well.\n\nClear header with information about the project, this file, and you.\nLoad libraries, set paths and macros, and write functions towards the top of the file.\nClear sections for reading in data, process, and output (ingredients, recipe, prepared dish).\n\n\nQuick question\nWhy put things like file paths or macros at the top of the file?"
  },
  {
    "objectID": "14-Bens-Philosophy.html#clarity",
    "href": "14-Bens-Philosophy.html#clarity",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Clarity",
    "text": "Clarity\nRemember: even though our scripts are instructions for the computer, they are also meant to be read by us humans. How well you comment your code and how well you name your objects, scripts, data, output, etc, determine the clarity of your intent.\n\nCommenting\nDon’t assume that your intent is clear. Particularly because so much working code comes as the result of many (…many…many…) revisions to non-working code, it’s very important that you comment liberally.\n\nWhat are you doing?\n## Creating a dummy variable for each state from current categorical variable\nWhy are you doing it this way?\n## Converting 1/2 indicator to 0/1 so 1 == variable name\nLinks to supporting documents/websites\n## see &lt;url&gt; for data codebook\nHat-tip (h/t) for borrowed code\n## h/t &lt;url&gt; for general code that I slightly modified \nFormula / logic behind method\n## log(xy) = log(x) + log(y)\n## using logarithms for more numerically stable calculations \n\nAll of these items are good uses for comments.\n\nQuick question\nIn which situation are comments not useful?\n\n\n\nNaming\nFor R, all the following objects are functionally equivalent:\n## v.1\nx &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.2\nstuff &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.3\ncities &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.4\nstates &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\nHowever, only the last object, states, makes logical sense to us based on its contents. So while R will work just as happily with x, stuff, cities, or states, a collaborator will appreciate states since it gives an idea of what it contains (or should contain) — even without running the code.\n\nQuick question\nWithout seeing the initial object assignment, what might you expect to see as output from the following code coming from a fellow higher education researcher? Why?\nfor (i in flagship_names) {\n    print(i)\n}\n\nWhen objects are well named, your code may become largely self-documenting, which is particularly nice since you don’t have to worry about drift between your comments and code over time."
  },
  {
    "objectID": "14-Bens-Philosophy.html#automation",
    "href": "14-Bens-Philosophy.html#automation",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Automation",
    "text": "Automation\nIn the functional programming lesson, we discussed the difference between Don’t Repeat Yourself programming and Write Every Time programming. As much as possible and within the dictates of organizing and clarity, it’s a good idea to automate as much as you can.\n\nPush-button replication\nIn general, the goal for an analysis script — in particular, a replication script — is that it will run from top to bottom with no errors by just pushing a button. What I mean by this is that I want to:\n\nDownload the project directory\nOpen the script in R Studio\n(If necessary, make sure I’m in the correct working directory)\nPush the “Run” button and have all the analyses run with all estimates/figures/tables being produced and sent to their proper project folders.\n\nThe dream!\nWhen projects are complicated or data isn’t easily shared (or can’t be shared), the push button replication may not be feasible. But your goal should always be to get as close to that as possible. Make your life a little harder so your collaborators, replicators — and future you! — have it a little easier.\nONE NOTE Like writing a paper, the process going from a blank script to a final product typically isn’t linear. You’ll have starts and stops, dead-ends and rewrites. You shouldn’t expect a polished paper from the first draft, and you shouldn’t expect to code up a fully replicable script from the get-go.\nThere’s something to be said for just getting your ideas down on paper, no matter how messy. Similarly, it may be better to just get your code to work first and then clean it up later. My only warning here is that while you have to clean up a paper eventually (someone’s going to read it), code still remains largely hidden from public view. There will be a temptation to get it to work and then move on since “no one will see it anyway, right…right?” Fight that urge!\nJust like real debt, the technical debt that you take out each time you settle for a hacky solution that’s good enough can grow exponentially. At some point, the bill will come due in the form of obviously (or even scarier, not-so-obviously) incorrect results. Tracking down the source of errors for large projects or those that reuse code from past projects can be maddening. Take a little extra time to write good code now and it will pay dividends in saved hours down the road, not to mention fewer 2 a.m. cold-sweat awakenings, Is that weird finding maybe because I constructed my main outcome variable wrong…wait, how did I code that…?\n\nQuick question\nIn a world in which not everyone knows how to use R, what’s another benefit of push-button automation?\n\n\n\nMultiple scripts\nWe’ve only worked with one script so far, but you can use the source() function in R to call other R scripts. For a very large project, you may decide to have multiple scripts. For example, you may have one script that downloads data, one that cleans the data, one that performs the analyses, and one that makes the figures.\nRather than telling a would-be replicator to “run the following files in the following order”\n\nget_data.R\nclean_data.R\ndo_analysis.R\nmake_figures.R\n\nyou could instead include a run_all.R script that only sources each script in order and tell the replicator to run that:\n## To run all analyses, set the working directory to \"scripts\" and\n## enter\n##\n## source(\"./run_all.R\")\n##\n## into the console. \n\n## download data\nsource(\"./get_data.R\")\n\n## clean data\nsource(\"./clean_data.R\")\n\n## run analyses\nsource(\"./do_analysis.R\")\n\n## make figures\nsource(\"./make_figures.R\")\nNot only is that easier for others to run, you now have a single script that makes your analytic workflow much clearer. Note that there are other ways to do something like this, such as using makefiles, though using only R works pretty well.\nHere are some replication examples for projects that I’ve done:\n\nSkinner, B. T., & Doyle, W. R. (2021). Do civic returns to higher education differ across subpopulations? An analysis using propensity forests. Journal of Education Finance, 46(4), 519–562. (Replication files)\nSkinner, B. T. (2019). Choosing College in the 2000s: An Updated Analysis Using the Conditional Logistic Choice Model. Research in Higher Education, 60(2), 153–183. (Replication files)\nSkinner, B. T. (2019). Making the connection: Broadband access and online course enrollment at public open admissions institutions. Research in Higher Education, 60(7), 960–999. (Replication files)\n\nYou can find the rest on my website as well as on my GitHub profile. As you complete projects and create replication scripts, be sure to make them accessible. Not only is it good for the field, it also signals strong transparency and credibility on your part as a researcher."
  },
  {
    "objectID": "14-Bens-Philosophy.html#testing",
    "href": "14-Bens-Philosophy.html#testing",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Testing",
    "text": "Testing\nAs you wrangle your data, you should take advantage of your computer’s power not only to perform the munging tasks you require, but also to test that inputs and outputs are as expected.\n\nExpected data values\nLet’s say you are investigating how college students use their time. You have data from a time use study in which 3,000 students recorded how they spent every hour for a week. The data come to you at the student-week level and grouped by activity type. This means you see that Student A spent 15 hours working, 20 hours studying/doing homework, etc. You create a new column that adds the time for all categories together and take the summary. You find:\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\ntotal_hrs\n167.99\n1.93\n152.48\n183.46\n\n\n\nSomething is wrong.\n\nQuick question\nWhat’s wrong?\n\nRather than going through an ad-hoc set of “ocular” checks, you can build in more formal checks. We’ve done a little of this when using identical() or all.equal() to compare data frames. You could build these into your scripts. For example:\n\nsumming activity hours in a week, check if any exceed the total number of hours that are possible\nreading in data with student GPAs, check if they are negative\ncombining data from 100 students at 5 schools, make sure your final data frame has 500 rows\ncreating a new percentage from two values, check that the value between 0 and 100 (or between 0 and 1 if a proportion)\n\nYou can use simple tests to check, and include a stop() command inside an if() statement:\n## halt if max number of months is greater than 12 since data are only\n## for one year\nif (max(length(months)) &gt; 12) {\n    stop(\"Reported months greater than 12! Check!\")\n}\nThis only works if you know your data. Knowing your data requires both domain knowledge (“Why are summer enrollments so much higher than fall enrollments…that doesn’t make sense.”) and particular knowledge about your data (“All values of x between 50 and 100, except these two, which are -1 and 20000…something’s up”).\n\n\nExpected output from functions\nProgrammers are big on unit testing their functions. This simply means giving their functions or applications very simple input that should correspond to straightforward output and confirming that that is the case.\nFor example, if their function could have three types of output depending on the input, they run at least three tests, one for each type of output they expect to see. In practice, they might run many more tests for all kinds of conditions — even those that should “break” the code to make sure the proper errors or warnings are returned.\nAs data analysts, you are unlikely to need to conduct a full testing regime for each project. But if you write your own functions, you should definitely stress test them a bit. This testing shouldn’t be in your primary script but rather in another script, perhaps called tests.R. These can be informal tests such as\n## should return TRUE\n(function_to_return_1() == 1)\nor more sophisticated tests using R’s testthat library.\nAgain, the point isn’t that you spend all your time writing unit tests. But spending just a bit of time checking that your functions do what you think they do will go a long way.\n\nQuick question\n\n\nImagine you’ve written your own version of scale(), which lets you normalize a set of numbers to N(0,1):\nmy_scale &lt;- function(x) {\n    ## get mean\n    mean_x &lt;- mean(x)\n    ## set sd\n    sd_x &lt;- sd(x)\n    ## return scaled values\n    ##             x - mean_x\n    ## scaled_x = ------------\n    ##                sd_x\n    return((x - mean_x) / sd_x)\n}\nWhat kind of tests do you think you should run, i.e., what kind of inputs should you give it and what should you expect?\n\n\n\nFail loudly and awkwardly\nThere is a notion in software development that a program should fail gracefully, which means that when a problem occurs, the program should at the very least give the user a helpful error message rather than just stopping without note or looping forever until the machine halts and catches fire.\nAs data analysts, I want your scripts to do the opposite: I want your code to exit loudly and awkwardly if there’s a problem. You aren’t writing a program but rather an analysis. Therefore, the UX I care about is the reader of your final results and your future replicators. Rather than moving forward silently when something strange happens — which is lovely on the front end because everything keeps running but deadly on the back end — your code should stop and yell at you when something unexpected happens. It’s annoying, but that’s the point."
  },
  {
    "objectID": "14-Bens-Philosophy.html#four-stages-of-data-a-taxonomy",
    "href": "14-Bens-Philosophy.html#four-stages-of-data-a-taxonomy",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Four stages of data: a taxonomy",
    "text": "Four stages of data: a taxonomy\nIn the lessons so far, we’ve generally read in raw data, done some processing that we printed to the console, and then finished. This is fine for our small lessons and assignments, but in your own work, you will often want to save your changes to the data along the way — that is, save updated data files at discrete points in your data analysis. This is particularly true for large projects in which wrangling your data is distinct from its analysis and you don’t want to run each part every time.\nBroadly, there are four stages of data:\n\nRaw data\nBuilt data (objective changes)\nClean data (subjective changes)\nAnalysis data\n\nFor small projects, you may not end up with data files for each stage. That said, your workflow should be organized in terms of moving from one stage to the next.\nWe’ll talk about each below.\nACKNOWLEDGMENT I want to give a quick hat tip to two people who’ve really helped me clarify my thinking about these stages of data, Will Doyle and Sally Hudson: Will for first showing the importance of keeping raw data untouched; and Sally for showing how useful it is to make a distinction between objective and subjective data processing, particularly in large projects. Thanks and credit to both.\n\nRaw data\nRaw data, that is, the data you scrape, download, receive from an advisor or research partner, etc, are sacrosanct. Do not change them! They are to be read into R (or whatever software you use) and that is all.\nYou will be tempted from time to time to just fix this one thing that is wonky about the data. Maybe you have a *.csv file that has a bunch of junk notes written above the actual data table (not uncommon). The temptation will be to delete those rows so you have a clean file. NO! There are ways to skip rows in a file like this, for example. R is particularly good at reading in a large number of data types with a large number of special situations.\nIf you are at all concerned that you or a collaborator may accidentally save over your raw files, your operating system will allow you to change the file permissions such that they are read-only files (Dropbox will let you protect files in this way, too). Particularly if working with a number of collaborators, protecting your raw data files in this way is a good idea.\n\n\nBuilt data (objective changes)\nAs a first step in processing your data, you should only make objective modifications. What do I mean by objective? They are those modifications that:\n\nAre required to get your data in good working order\nAre modifications that will always be needed across projects using the same data (in the context of multiple projects)\n\nTo the first point, let’s say you receive student-level administrative data that includes a column, state, that is the student’s home state. However, values in the column include:\n\n\"Kentucky\"\n\"KY\"\n\"Ken\"\n\"21\" (state FIPS code)\n\nAnother example would be a date column, month, in which you see:\n\n\"February\"\n\"Feb\"\n\"2\"\n\nIn both cases, it’s clear that you want a consistent format for the data. Whatever you pick, these are objective changes that you would make at this time.\nTo the second point, let’s say this same administrative data are going to be used across multiple projects. In each project, you need to compute the proportion of in-state vs. out-of-state students. The data column that indicates this is called residency and is coded (with no data labels) as\n\n1 := in-state\n2 := out-of-state\n\nComing back to the data later and taking a look at the first few rows in residency, will it be clear\n\nwhat residency means?\nwhat the values of 1 and 2 represent?\n\nNo.\nAn objective change that you can use across analyses will be to make a new column called instate in which\n\n0 := out-of-state\n1 := in-state\n\nThe benefits are three-fold:\n\n0 and 1 are logically coded as FALSE and TRUE\ninstate gives a direction to the 0s and 1s: 1 means in-state\ntaking the mean of instate will give you the proportion of in-state students in your data\n\nIf your data or project (or number of projects) is large, you many want to save data files after your objective wrangling. These can be reused across projects, which will make sure that objective data wrangling decisions are consistent.\n\n\nClean data (subjective changes)\nWhatever changes are left over after you’ve make objective changes are subjective changes. These changes are project and analysis specific.\nFor example, let’s say your student-level administrative data have information about the student’s expected family contribution or EFC. In your data, you have the actual values. However, you want to compare your data to other published research in which EFC is binned into discrete categories. This means that in order to make a comparison, you have to bin your data the same way. This is a subjective task for two reasons:\n\nthere’s not an objective reason to bin your data in this manner, i.e. to make the data consistent or clearer\nit’s unlikely you’ll need this new column of data for other projects\n\nSome other subjective changes you might make to a data set:\n\ncreate an indicator variable that equals 1 when a student’s family income is below $35,000/year (why $30k? why not $25k or $35k?)\nconvert institutional enrollments to the log scale (do you always need to do this?)\njoin in data on the unemployment rate in the student’s county (do you always need this information at the county level? at all?)\n\nBut wait! Maybe you do want to use the binned value of EFC in multiple projects — perhaps the binned categories are standard across the literature — or the other examples appear always useful and otherwise standard. Isn’t that objective?\nMaybe!\nThe thing about determining the difference between objective vs subjective cleaning is that on the margins, well, it’s subjective! The point is not to have hard and fast rules, but rather to do your best to clearly separate those data wrangling tasks that must be done for the sake of consistency and clarity versus those that may change depending on your research question(s).\n\nQuick question\nWhat might be some other subjective data wrangling tasks? Think about other research you’ve conducted or seen. Or think about what our lessons and assignments so far: what have been subjective tasks?\n\n\n\nAnalysis data\nFinally, analysis data are data that are properly structured for the analysis you want to run (or the table or figure you want to make). In many cases, your analysis data set will be the same as your clean data set:\n\nit’s in the shape you want (rectangular, with observations in rows and variables in columns)\nit has all the variables you need to compute summary statistics or run regression models\n\nMost R statistical routines (e.g., mean(), sd(), lm()) do a fair amount of data wrangling under the hood to make sure it can do what you ask. For example, you may have missing values (NA) in your clean data set, but R’s linear model function, lm(), will just drop observations with missing values by default. If you are okay with that in your analysis, R can handle it.\nHowever, for some special models or figures, you may need to reshape or pre-process your clean data in one last step before running the specific analysis you want. For example, your clean data frame may have a column of parental occupational field (parocc) as a categorical variable in which the first few categories are\n\n11 := Management Occupations\n13 := Business and Financial Operations Occupations\n15 := Computer and Mathematical Occupations\n17 := Architecture and Engineering Occupations\n\n…and so on. You want to run an analysis in which you can’t use this categorical variable but instead need a vector of dummy variables, that is, converting this single column into a bunch of columns that only take on 0/1 values if parocc equals that occupation:\n\nmanage_occ == 1 if parocc == 11 else manage_occ == 0\nbusfin_occ == 1 if parocc == 13 else busfin_occ == 0\ncommat_occ == 1 if parocc == 15 else commat_occ == 0\narceng_occ == 1 if parocc == 17 else arceng_occ == 0\n\n…and so on. Particularly if\n\nyour data are large\nthe analysis takes a while to run\nyou want to use the same data set up for multiple (but not all) models or analytic tasks\n\nyou may want to save this particular analysis data set.\nAs with the difference between objective and subjective data, the difference between subjective and analysis data does not align to a set of hard and fast rules. That’s okay! The main point is that as a data analyst, you make data wrangling decisions thinking about how you will go from raw to objective to subjective to analysis data in a logical and clear manner. Collaborators, replicators, and future you will appreciate it!"
  },
  {
    "objectID": "14-Bens-Philosophy.html#recommended-texts",
    "href": "14-Bens-Philosophy.html#recommended-texts",
    "title": "Dr. Skinner’s Philosophy",
    "section": "Recommended texts",
    "text": "Recommended texts\nThere are a number of great texts that talk about the process of data wrangling. Two that have particularly informed this lecture and are worth looking at more fully:\n\nThe Pragmatic Programmer: From Journeyman to Master by Andrew Hunt and David Thomas\nCode and Data for the Social Sciences: A Practitioner’s Guide by Matthew Gentzkow and Jesse Shapiro"
  },
  {
    "objectID": "00-Syllabus.html",
    "href": "00-Syllabus.html",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-G\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2031\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit\nStudents also need to sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account\n\n\n\n\n\n\n\nClass Schedule\n\n\n\n\n\n\n\nWeek\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan 9th\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan 16th\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan 23rd\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan 30th\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb 6th\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb 13th\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb 20th\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb 27th\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar 5th\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n10\nMar 12th\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar 19th\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n12\nMar 26th\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr 2nd\nMevlin’s Week (Matt In/Out)\n\nTBD\n\n\nAssignment 10\n\n\n\n14\nApr 9th\n\nReproducible Report: Lab Time\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr 16th\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr 23rd\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4.5/5\n\n\nThis grading system is designed to encourage you revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 30 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for per- sonal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participa- tion, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, trans- mit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code."
  },
  {
    "objectID": "00-Syllabus.html#spring-2024",
    "href": "00-Syllabus.html#spring-2024",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-G\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2031\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit\nStudents also need to sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account"
  },
  {
    "objectID": "00-Syllabus.html#schedule",
    "href": "00-Syllabus.html#schedule",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Class Schedule\n\n\n\n\n\n\n\nWeek\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan 9th\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan 16th\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan 23rd\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan 30th\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb 6th\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb 13th\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb 20th\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb 27th\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar 5th\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n10\nMar 12th\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar 19th\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n12\nMar 26th\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr 2nd\nMevlin’s Week (Matt In/Out)\n\nTBD\n\n\nAssignment 10\n\n\n\n14\nApr 9th\n\nReproducible Report: Lab Time\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr 16th\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr 23rd\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4.5/5\n\n\nThis grading system is designed to encourage you revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 30 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for per- sonal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participa- tion, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, trans- mit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#organizing-a-project-directory",
    "href": "02-Setup-Reading-Data.html#organizing-a-project-directory",
    "title": "II: Reading Data & IPEDS",
    "section": "Organizing a project directory",
    "text": "Organizing a project directory\nWe’ll begin with how to organize your course and project files.\n\nA place for everything and everything in its place\nEvery data analysis project should have its own set of organized folders. Just like you might organize a kitchen so that ingredients, cookbooks, and prepared food all have a specific cabinet or shelf, so too should you organize your project. We’ll organize our course directory in a similar fashion.\nBut computers are pretty good at finding files, you say: you can use your machine’s search feature to look for what you need. If you don’t have that many files to look through, you might not be too bad at quickly scanning to find what you want either. If this is the case, then why bother organizing a project directory? Why not just dump everything — scripts, data, figures, tables, notes, etc — into a single folder (My downloads folder works just fine, thank you…)? If you need something, the computer can definitely find it.\nSo what’s the big deal?\nThe big deal is that you are thinking from your computer’s perspective when you should be thinking from the perspective of you, your collaborators (which includes your future self), and future replicators (which also includes yourself). Search features are nice, but there’s no substitute for being able to look through a project’s files just by looking through the project folders. When a project is well organized, it’s much easier to understand how everything — each input, process, and output — fits together.\n\n\nA common directory structure\nAs a reminder, here’s basic directory structure for this class:\nstudent_skinner/\n|\n|__ assignments/\n|__ data/\n|__ figures/\n|__ final_project/\n|__ lessons/\n|__ scripts/\n|__ working/\nAs you can see, we have a main directory (or folder — same thing) for the course called student_skinner. Your directory has a similar name, but with your last name: student_&lt;last name&gt;. That said, it could be named anything useful. This can live on your computer wherever you want to put it: on your Desktop, in your home directory, in another folder where you store materials for your other classes — wherever makes sense for you.\nInside the main course directory, there are subdirectories (or subfolders — again, same thing) for the different types of files you’ll collect or create this term. These subdirectories have self-explanatory names: PDFs for assignments go into assignments, PDFs for lessons into lessons and so on.\nNote that this type of structure works well with research projects. Of course, you’re unlike to have assignments or lessons subfolders within a research project directory, but you almost certainly will have subfolders for your scripts, data, and figures as well as a working folder (which some people call scratch, like a scratch pad) where you can store odds and ends or practice new ideas.\nYou may ask: why these folders in particular or, why should I have separate data, scripts, and figures in my project directory?\nThink about it this way. Following our kitchen analogy from before, we have:\n\nIngredients (Inputs) - data\nCookbooks (Processes) - scripts\nPrepared food (Outputs) - figures\n\nParticular projects may require particular folders (for example, you may find it useful to have a special subfolder for tables or one for regression output called estimates). But in almost all cases, your project directory should have separate subfolders for your data, your analysis scripts, and any output you produce.\n\nGreat! How to I set this up?\nYou can create new directories using your operating system. For both MacOS and Windows, one of the easiest ways to make a new folder is to right-click on your Desktop and choose to create a new folder. You can then open this folder and continue right-click creating subfolders until you have what you need.\nYou can also use the RStudio Files tab (lower right facet) to create new folders. Even though we already have our course directory from GitHub, let’s practice creating a directory structure from scratch, so you can make your own in the future.\nLet’s say I want to create a directory on my Desktop."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#nces-surveys",
    "href": "02-Setup-Reading-Data.html#nces-surveys",
    "title": "II: Reading Data & IPEDS",
    "section": "NCES Surveys",
    "text": "NCES Surveys\nThe National Center for Education Statistics (NCES) is part of the Department of Education’s Institute of Education Sciences (IES). NCES offers a large number of resources for researchers interested in higher education.\n\nAmong these are longitudinal surveys that follow different cohorts of students from high school through college and beyond.\n\nSpecifically, we’ll go through the steps to download the Education Longitudinal Study of 2002 (ELS). The good news is that the process is the same for the other surveys.\nTo get to the online code book you’ll use to download the raw data files, head to the NCES homepage (nces.ed.gov) and click in the following order: 1. Menu 1. Data & Tools 1. Downloads Microdata/Raw Data 1. EDAT\n\nYou may see a couple of popups — just agree. Once you’ve clicked through those, you should see the code book home screen. From here, you can access a number of data sets. We’ll focus on ELS, but as I said above, the process is the same: just choose another data set from the code book homepage if you’d rather use that data.\n\nWhen you choose ELS, you’ll see the online code book. You can (and should) use this to learn about variables — their definitions, how they’re constructed, missing values, etc. You can also use this tool to only select a few variables for download. Don’t do that! You should plan to download the full data set and do any filtering or subsetting in your analytic code.\nClick the Downloads button to get the data.\n\nYou’ll be presented with a number of file types. Because you are using R, you could read in all these data types — either with standard functions or functions from the tidyverse haven package.\nMy recommendation:\n\nIf you don’t care about labels: Download the CSV version for maximum portability\nIf you’d like labeled data: Download the STATA version and use haven::read_dta() to input the data\n\nI generally like labels, so we’ll choose the STATA version\n\nAfter choosing your file version, you can finally download the files. Go ahead and click each box to download all the files."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#nls",
    "href": "02-Setup-Reading-Data.html#nls",
    "title": "II: Reading Data & IPEDS",
    "section": "NLS",
    "text": "NLS\nThe Bureau of Labor Statistics (aside from a lot of other useful information) has a number of National Longitudinal Surveys. These are similar to those from the NCES, but much more expansive. They include:\n\nNational Longitudinal Survey of Youth 1997 (NLSY97)\nNational Longitudinal Survey of Youth 1979 (NLSY79)\nNLSY79 Children and Young Adults (NLSCYA)\nNational Longitudinal Surveys of Young Women and Mature Women (NLSW)\nNational Longitudinal Surveys of Young Men and Older Men (NLSM)\n\nIf you decide to use one of these surveys in your work, it will probably be the NLSY97, which began following a cohort of high schools students in 1997.\n\n\nInvestigator\nScrolling down on the NLS97 page, you’ll see a section for Accessing the data via the Investigator. Because the NLSY is so large, you may choose to go this route.\n\nYou’ll be shown a new external link. Click it to go to the data investigator.\n\nYou can create a log in, which is nice if you come back often since you can save tag sets (variable groups you want to download), or just log in as a guest.\n\nOnce inside, the investigator will allow you to choose which NLS data you want to access. So even though we got here via the NLSY97 page, we can still look at NLSY79 data if we want. Whichever you choose, go ahead a choose to look at all data rounds.\n\nNow you can explore the data via the menu tree on the left side of the page. When you find a variable or set of variables you want to download, be sure to click the box next to the variable name. When you are finished, click the Save/Download tab.\n\nOn the next screen, you’ll be able to save your tag set (meaning, not download the data, but name and keep the variable list you’ve chosen for a later date) or download.\nChoose the Advanced Download tab. Within that tab, choose which file type you want to download (I’ve chosen just plain CSV here) and what you want to call the download. When you’re ready, click the download button.\n\nAfter your data set is prepared, you can download it to your computer.\n\n\n\nDirect download of full NLS data sets\nAlternately, you can directly download the full NLS data files at www.nlsinfo.org/accessing-data-cohorts. I would recommend this approach if you think you’re going to want a large number of variables. Also, you’ll eventually find it easier to do your variable selection in R rather than via the Investigator."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#ipeds",
    "href": "02-Setup-Reading-Data.html#ipeds",
    "title": "II: Reading Data & IPEDS",
    "section": "IPEDS",
    "text": "IPEDS\nFor institution-level information, the Integrated Postsecondary Education Data System (IPEDS) will likely be your first stop. While the IPEDS site (nces.ed.gov/ipeds) will let you explore individual institutions or use a portal to select particular variables (like the BLS investigator), you’ll want to just download the raw files. Begin by selecting Use the Data.\n\nOn the next page, look in the right column for the section Survey Data. From the drop down menu, choose Complete data files.\nNB: If you know how to work with databases, the Access databases may be useful for you. But to use these, you either need a Microsoft Access license or a program to convert them to another format (like SQLite).\n\nYou may see a popup window — if so, just agree. You’ll now see a pretty lonely page. If you have a specific file or year you know you want, use the two drop down menus to filter your search. Otherwise, just click the Continue button to see your options.\n\nClick the links in the Data File column to get zipped versions of the CSV files. If you want a Stata data file instead, choose the link from the Stata Data File column. You will probably want to grab the Dictionary file while you’re at it.\nHow do I know which file I need?, you might be asking. If you are unsure, you may want to download the dictionary file first and check for the data element(s) you think you need. After a while, you’ll get better at knowing (or reasonably guessing) which file is the one you need based on the names.\n\n\nDownload all of IPEDS via R\nIf you don’t want to bother with the portal, I’ve written an R script that will download the entirety of IPEDS to your computer (a little over 1 GB if you only want one type of data file). See github.com/btskinner/downloadipeds for the script and information on how to use it."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#college-scorecard",
    "href": "02-Setup-Reading-Data.html#college-scorecard",
    "title": "II: Reading Data & IPEDS",
    "section": "College Scorecard",
    "text": "College Scorecard\nThough it’s intended to give students and their families better information about their college options, the College Scorecard offers data that’s useful for research. In particular, you can find earnings data linked to schools and programs that you can’t find anywhere else.\n\nDirect\nIf you go to the College Scorecard homepage (collegescorecard.ed.gov), you’ll see the portal that students use. Scroll to the bottom of the page.\n\nAt the bottom of the page, you’ll see a link to download the data files that power the Scorecard.\n\nOn the data page, you can download the full set of files or just the latest data. Unless you have a good reason to do otherwise, I would recommend getting all the data. You may also want to follow the Documentation tab to get the data documentation.\n\n\n\nrscorecard\nYou can also download College Scorecard data directly from R using the rscorecard package, which accesses Scorecard data via an API. See btskinner.io/rscorecard for more information and examples."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#american-community-survey-acs",
    "href": "02-Setup-Reading-Data.html#american-community-survey-acs",
    "title": "II: Reading Data & IPEDS",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\nThe American Community Survey (ACS) is part of the U.S. Census that, unlike the decennial census, collects data each year. While it has information on education that you may want to use directly, the ACS is also a great source for place-based data that you can merge with other data sets (student-level data, for example, if you know where they live). The ACS homepage is here: census.gov/programs-surveys/acs.\nThere are a few ways to access ACS data. I will show you how to get the public use micro sample (PUMS) data. From the ACS home page, click on the Data link on the left.\n\nOn the next screen, click on the PUMS link which again is on the left.\n\nThere are a couple of ways to get PUMS data: from the old FTP site or the newer data.census.gov site. Though it’s less pretty, I’ll show you the FTP version (if you have used FTP applications before to access data, you can use those here).\n\nYou’ll notice the FTP page looks like a file system. That’s basically what it is. Click on the file name you want. For more information on whether you want 1-, 3-, or 5-year estimates, check out this page: census.gov/programs-surveys/acs/guidance/estimates.html.\n\nOn the final page, you can choose data at the state level. There are two basic types of files, each pertaining to sections of the survey:\n\nHousing: These files start with h after the underscore\nPerson: These files start with p after the underscore\n\nFor more information about which file to use or PUMS more generally, visit census.gov/programs-surveys/acs/technical-documentation/pums/about.html."
  },
  {
    "objectID": "02-Setup-Reading-Data.html#other",
    "href": "02-Setup-Reading-Data.html#other",
    "title": "II: Reading Data & IPEDS",
    "section": "Other",
    "text": "Other\nBelow are some other data sources you may find useful, either on their own or joined with the data sets above.\n\nCurrent Population Survey (www.census.gov/programs-surveys/cps.html)\nTIGER/Line Shapefiles (www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)\nBureau of Labor Statistics (www.bls.gov/data/)\nDelta Cost Project (deltacostproject.org/delta-cost-data)\nUrban Institute Data Explorer (educationdata.urban.org/data-explorer/)\nPISA (www.oecd.org/pisa/data/)"
  },
  {
    "objectID": "02-Setup-Reading-Data.html#creating-a-new-directory",
    "href": "02-Setup-Reading-Data.html#creating-a-new-directory",
    "title": "II: Reading Data & IPEDS",
    "section": "Creating a new directory",
    "text": "Creating a new directory\n\nCreate a new top-level subdirectory in your course directory (i.e., the same level as scripts, data, and figures) called tables.\nCreate a Markdown file called README.md and place it in your new tables directory. In the tables/README.md file, add a header line that looks like the following, and save.\n# Tables"
  },
  {
    "objectID": "02-Setup-Reading-Data.html#creating-your-first-script",
    "href": "02-Setup-Reading-Data.html#creating-your-first-script",
    "title": "II: Reading Data & IPEDS",
    "section": "Creating your first script",
    "text": "Creating your first script\nUsing template.R (and organizing.R for help), create a script that does the following tasks — be sure your script is well organized:\n\nMake a copy of template.R and rename it to &lt;lastname&gt;_assignment_2.R. Make sure it is in your scripts folder if its not already there.\nFill in all relevant header information about the script.\nLoad the tidyverse library\nCreate objects/macros with the paths to the following directories:\n\ndata\nfigures\ntables\n\nInclude the old_to_new_score_ratio macro, but change it to a new value.\nInclude the old_to_new_score() function from class as is (just cut and paste).\nRead in the data set, test_scores.RDS.\nCreate a new column called test_scores_new_2 that converts the original test scores to updated values using your new ratio and the old_to_new_score() function.\nSave the updated data file in your data directory with a new name. You should now have three files: the original, the updated one from the organizing lesson, and the one you just made.\n\nNOTE When all is said and done, your new script should look much like the organizing.R script, but with your changes.\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_2.R) in your scripts directory (NOTE the different location) and your new README.md file in your new tables/ directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session."
  },
  {
    "objectID": "Extra-06-Purrr.html",
    "href": "Extra-06-Purrr.html",
    "title": "Extra: Functional Programming I: Purrr Edition",
    "section": "",
    "text": "LessonAssignment"
  }
]