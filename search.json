[
  {
    "objectID": "x-04-wrangle-phil.html#organization",
    "href": "x-04-wrangle-phil.html#organization",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Organization",
    "text": "Organization\nFrom the organizing lesson, we’ve been very concerned about how our project is organized. This includes both the directory of folders and files on our machine and the scripts that run our analyses.\n\nProject directory\n\nNote from Matt: Recall we discussed the difference between Dr. Skinner’s “pristine kitchen” approach (tidier) and our approach (easier) with scripts left “out on the counter” early in the class. Take some time to read Dr. Skinner’s opinion on that here and decide which you agree with.\n\nWhile it is certainly easier on the front end to store all project files in a single folder\n./project\n|\n|--+ analysis.R\n|--+ clean.R\n|--+ clean_data.rds\n|--+ data1.csv\n|--+ data2.csv\n|--+ histogram_x.png\n|--+ density_z.png\nit’s better to separate files by type/purpose in your project directory (“a place for everything and everything in its place”):\n./project\n|\n|__/data\n|   |--+ clean_data.rds\n|   |--+ data1.csv\n|   |--+ data2.csv\n|__/figures\n|   |--+ histogram_x.png\n|   |--+ density_z.png\n|__/scripts \n    |--+ analysis.R\n    |--+ clean.R\nJust like there’s something to be said for visiting the library in order to scan titles in a particular section — it’s easier to get an idea of what’s available by looking at the shelf than by scanning computer search output — it’s useful to be able to quickly scan only the relevant files in a project. With well-named files (discussed below) and an organized directory structure, a replicator may be able to guess the purpose, flow, and output of your project without even running your code.\n\nQuick question\nWhen using a project directory instead of just a single folder, we need to use file paths. In particular, we use relative paths rather than fixed paths. Why?\n\n\n\nScript\nIn the lesson on organizing, I also shared a template R script. While you do not need to use my particular template — in fact, you should modify it to meet your needs — it does follow a few organizational rules that you should follow as well.\n\nClear header with information about the project, this file, and you.\nLoad libraries, set paths and macros, and write functions towards the top of the file.\nClear sections for reading in data, process, and output (ingredients, recipe, prepared dish).\n\n\nQuick question\nWhy put things like file paths or macros at the top of the file?",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-wrangle-phil.html#clarity",
    "href": "x-04-wrangle-phil.html#clarity",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Clarity",
    "text": "Clarity\nRemember: even though our scripts are instructions for the computer, they are also meant to be read by us humans. How well you comment your code and how well you name your objects, scripts, data, output, etc, determine the clarity of your intent.\n\nCommenting\nDon’t assume that your intent is clear. Particularly because so much working code comes as the result of many (…many…many…) revisions to non-working code, it’s very important that you comment liberally.\n\nWhat are you doing?\n## Creating a dummy variable for each state from current categorical variable\nWhy are you doing it this way?\n## Converting 1/2 indicator to 0/1 so 1 == variable name\nLinks to supporting documents/websites\n## see &lt;url&gt; for data codebook\nHat-tip (h/t) for borrowed code\n## h/t &lt;url&gt; for general code that I slightly modified \nFormula / logic behind method\n## log(xy) = log(x) + log(y)\n## using logarithms for more numerically stable calculations \n\nAll of these items are good uses for comments.\n\nQuick question\nIn which situation are comments not useful?\n\n\n\nNaming\nFor R, all the following objects are functionally equivalent:\n## v.1\nx &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.2\nstuff &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.3\ncities &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.4\nstates &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\nHowever, only the last object, states, makes logical sense to us based on its contents. So while R will work just as happily with x, stuff, cities, or states, a collaborator will appreciate states since it gives an idea of what it contains (or should contain) — even without running the code.\n\nQuick question\nWithout seeing the initial object assignment, what might you expect to see as output from the following code coming from a fellow higher education researcher? Why?\nfor (i in flagship_names) {\n    print(i)\n}\n\nWhen objects are well named, your code may become largely self-documenting, which is particularly nice since you don’t have to worry about drift between your comments and code over time.",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-wrangle-phil.html#automation",
    "href": "x-04-wrangle-phil.html#automation",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Automation",
    "text": "Automation\nIn the functional programming lesson, we discussed the difference between Don’t Repeat Yourself programming and Write Every Time programming. As much as possible and within the dictates of organizing and clarity, it’s a good idea to automate as much as you can.\n\nPush-button replication\nIn general, the goal for an analysis script — in particular, a replication script — is that it will run from top to bottom with no errors by just pushing a button. What I mean by this is that I want to:\n\nDownload the project directory\nOpen the script in R Studio\n(If necessary, make sure I’m in the correct working directory)\nPush the “Run” button and have all the analyses run with all estimates/figures/tables being produced and sent to their proper project folders.\n\nThe dream!\nWhen projects are complicated or data isn’t easily shared (or can’t be shared), the push button replication may not be feasible. But your goal should always be to get as close to that as possible. Make your life a little harder so your collaborators, replicators — and future you! — have it a little easier.\nONE NOTE Like writing a paper, the process going from a blank script to a final product typically isn’t linear. You’ll have starts and stops, dead-ends and rewrites. You shouldn’t expect a polished paper from the first draft, and you shouldn’t expect to code up a fully replicable script from the get-go.\nThere’s something to be said for just getting your ideas down on paper, no matter how messy. Similarly, it may be better to just get your code to work first and then clean it up later. My only warning here is that while you have to clean up a paper eventually (someone’s going to read it), code still remains largely hidden from public view. There will be a temptation to get it to work and then move on since “no one will see it anyway, right…right?” Fight that urge!\nJust like real debt, the technical debt that you take out each time you settle for a hacky solution that’s good enough can grow exponentially. At some point, the bill will come due in the form of obviously (or even scarier, not-so-obviously) incorrect results. Tracking down the source of errors for large projects or those that reuse code from past projects can be maddening. Take a little extra time to write good code now and it will pay dividends in saved hours down the road, not to mention fewer 2 a.m. cold-sweat awakenings, Is that weird finding maybe because I constructed my main outcome variable wrong…wait, how did I code that…?\n\nQuick question\nIn a world in which not everyone knows how to use R, what’s another benefit of push-button automation?\n\n\n\nMultiple scripts\nWe’ve only worked with one script so far, but you can use the source() function in R to call other R scripts. For a very large project, you may decide to have multiple scripts. For example, you may have one script that downloads data, one that cleans the data, one that performs the analyses, and one that makes the figures.\nRather than telling a would-be replicator to “run the following files in the following order”\n\nget_data.R\nclean_data.R\ndo_analysis.R\nmake_figures.R\n\nyou could instead include a run_all.R script that only sources each script in order and tell the replicator to run that:\n## To run all analyses, set the working directory to \"scripts\" and\n## enter\n##\n## source(\"./run_all.R\")\n##\n## into the console. \n\n## download data\nsource(\"./get_data.R\")\n\n## clean data\nsource(\"./clean_data.R\")\n\n## run analyses\nsource(\"./do_analysis.R\")\n\n## make figures\nsource(\"./make_figures.R\")\nNot only is that easier for others to run, you now have a single script that makes your analytic workflow much clearer. Note that there are other ways to do something like this, such as using makefiles, though using only R works pretty well.\nHere are some replication examples for projects that I’ve done:\n\nSkinner, B. T., & Doyle, W. R. (2021). Do civic returns to higher education differ across subpopulations? An analysis using propensity forests. Journal of Education Finance, 46(4), 519–562. (Replication files)\nSkinner, B. T. (2019). Choosing College in the 2000s: An Updated Analysis Using the Conditional Logistic Choice Model. Research in Higher Education, 60(2), 153–183. (Replication files)\nSkinner, B. T. (2019). Making the connection: Broadband access and online course enrollment at public open admissions institutions. Research in Higher Education, 60(7), 960–999. (Replication files)\n\nYou can find the rest on my website as well as on my GitHub profile. As you complete projects and create replication scripts, be sure to make them accessible. Not only is it good for the field, it also signals strong transparency and credibility on your part as a researcher.",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-wrangle-phil.html#testing",
    "href": "x-04-wrangle-phil.html#testing",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Testing",
    "text": "Testing\nAs you wrangle your data, you should take advantage of your computer’s power not only to perform the munging tasks you require, but also to test that inputs and outputs are as expected.\n\nExpected data values\nLet’s say you are investigating how college students use their time. You have data from a time use study in which 3,000 students recorded how they spent every hour for a week. The data come to you at the student-week level and grouped by activity type. This means you see that Student A spent 15 hours working, 20 hours studying/doing homework, etc. You create a new column that adds the time for all categories together and take the summary. You find:\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\ntotal_hrs\n167.99\n1.93\n152.48\n183.46\n\n\n\nSomething is wrong.\n\nQuick question\nWhat’s wrong?\n\nRather than going through an ad-hoc set of “ocular” checks, you can build in more formal checks. We’ve done a little of this when using identical() or all.equal() to compare data frames. You could build these into your scripts. For example:\n\nsumming activity hours in a week, check if any exceed the total number of hours that are possible\nreading in data with student GPAs, check if they are negative\ncombining data from 100 students at 5 schools, make sure your final data frame has 500 rows\ncreating a new percentage from two values, check that the value between 0 and 100 (or between 0 and 1 if a proportion)\n\nYou can use simple tests to check, and include a stop() command inside an if() statement:\n## halt if max number of months is greater than 12 since data are only\n## for one year\nif (max(length(months)) &gt; 12) {\n    stop(\"Reported months greater than 12! Check!\")\n}\nThis only works if you know your data. Knowing your data requires both domain knowledge (“Why are summer enrollments so much higher than fall enrollments…that doesn’t make sense.”) and particular knowledge about your data (“All values of x between 50 and 100, except these two, which are -1 and 20000…something’s up”).\n\n\nExpected output from functions\nProgrammers are big on unit testing their functions. This simply means giving their functions or applications very simple input that should correspond to straightforward output and confirming that that is the case.\nFor example, if their function could have three types of output depending on the input, they run at least three tests, one for each type of output they expect to see. In practice, they might run many more tests for all kinds of conditions — even those that should “break” the code to make sure the proper errors or warnings are returned.\nAs data analysts, you are unlikely to need to conduct a full testing regime for each project. But if you write your own functions, you should definitely stress test them a bit. This testing shouldn’t be in your primary script but rather in another script, perhaps called tests.R. These can be informal tests such as\n## should return TRUE\n(function_to_return_1() == 1)\nor more sophisticated tests using R’s testthat library.\nAgain, the point isn’t that you spend all your time writing unit tests. But spending just a bit of time checking that your functions do what you think they do will go a long way.\n\nQuick question\n\n\nImagine you’ve written your own version of scale(), which lets you normalize a set of numbers to N(0,1):\nmy_scale &lt;- function(x) {\n    ## get mean\n    mean_x &lt;- mean(x)\n    ## set sd\n    sd_x &lt;- sd(x)\n    ## return scaled values\n    ##             x - mean_x\n    ## scaled_x = ------------\n    ##                sd_x\n    return((x - mean_x) / sd_x)\n}\nWhat kind of tests do you think you should run, i.e., what kind of inputs should you give it and what should you expect?\n\n\n\nFail loudly and awkwardly\nThere is a notion in software development that a program should fail gracefully, which means that when a problem occurs, the program should at the very least give the user a helpful error message rather than just stopping without note or looping forever until the machine halts and catches fire.\nAs data analysts, I want your scripts to do the opposite: I want your code to exit loudly and awkwardly if there’s a problem. You aren’t writing a program but rather an analysis. Therefore, the UX I care about is the reader of your final results and your future replicators. Rather than moving forward silently when something strange happens — which is lovely on the front end because everything keeps running but deadly on the back end — your code should stop and yell at you when something unexpected happens. It’s annoying, but that’s the point.",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-wrangle-phil.html#four-stages-of-data-a-taxonomy",
    "href": "x-04-wrangle-phil.html#four-stages-of-data-a-taxonomy",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Four stages of data: a taxonomy",
    "text": "Four stages of data: a taxonomy\nIn the lessons so far, we’ve generally read in raw data, done some processing that we printed to the console, and then finished. This is fine for our small lessons and assignments, but in your own work, you will often want to save your changes to the data along the way — that is, save updated data files at discrete points in your data analysis. This is particularly true for large projects in which wrangling your data is distinct from its analysis and you don’t want to run each part every time.\nBroadly, there are four stages of data:\n\nRaw data\nBuilt data (objective changes)\nClean data (subjective changes)\nAnalysis data\n\nFor small projects, you may not end up with data files for each stage. That said, your workflow should be organized in terms of moving from one stage to the next.\nWe’ll talk about each below.\nACKNOWLEDGMENT I want to give a quick hat tip to two people who’ve really helped me clarify my thinking about these stages of data, Will Doyle and Sally Hudson: Will for first showing the importance of keeping raw data untouched; and Sally for showing how useful it is to make a distinction between objective and subjective data processing, particularly in large projects. Thanks and credit to both.\n\nRaw data\nRaw data, that is, the data you scrape, download, receive from an advisor or research partner, etc, are sacrosanct. Do not change them! They are to be read into R (or whatever software you use) and that is all.\nYou will be tempted from time to time to just fix this one thing that is wonky about the data. Maybe you have a *.csv file that has a bunch of junk notes written above the actual data table (not uncommon). The temptation will be to delete those rows so you have a clean file. NO! There are ways to skip rows in a file like this, for example. R is particularly good at reading in a large number of data types with a large number of special situations.\nIf you are at all concerned that you or a collaborator may accidentally save over your raw files, your operating system will allow you to change the file permissions such that they are read-only files (Dropbox will let you protect files in this way, too). Particularly if working with a number of collaborators, protecting your raw data files in this way is a good idea.\n\n\nBuilt data (objective changes)\nAs a first step in processing your data, you should only make objective modifications. What do I mean by objective? They are those modifications that:\n\nAre required to get your data in good working order\nAre modifications that will always be needed across projects using the same data (in the context of multiple projects)\n\nTo the first point, let’s say you receive student-level administrative data that includes a column, state, that is the student’s home state. However, values in the column include:\n\n\"Kentucky\"\n\"KY\"\n\"Ken\"\n\"21\" (state FIPS code)\n\nAnother example would be a date column, month, in which you see:\n\n\"February\"\n\"Feb\"\n\"2\"\n\nIn both cases, it’s clear that you want a consistent format for the data. Whatever you pick, these are objective changes that you would make at this time.\nTo the second point, let’s say this same administrative data are going to be used across multiple projects. In each project, you need to compute the proportion of in-state vs. out-of-state students. The data column that indicates this is called residency and is coded (with no data labels) as\n\n1 := in-state\n2 := out-of-state\n\nComing back to the data later and taking a look at the first few rows in residency, will it be clear\n\nwhat residency means?\nwhat the values of 1 and 2 represent?\n\nNo.\nAn objective change that you can use across analyses will be to make a new column called instate in which\n\n0 := out-of-state\n1 := in-state\n\nThe benefits are three-fold:\n\n0 and 1 are logically coded as FALSE and TRUE\ninstate gives a direction to the 0s and 1s: 1 means in-state\ntaking the mean of instate will give you the proportion of in-state students in your data\n\nIf your data or project (or number of projects) is large, you many want to save data files after your objective wrangling. These can be reused across projects, which will make sure that objective data wrangling decisions are consistent.\n\n\nClean data (subjective changes)\nWhatever changes are left over after you’ve make objective changes are subjective changes. These changes are project and analysis specific.\nFor example, let’s say your student-level administrative data have information about the student’s expected family contribution or EFC. In your data, you have the actual values. However, you want to compare your data to other published research in which EFC is binned into discrete categories. This means that in order to make a comparison, you have to bin your data the same way. This is a subjective task for two reasons:\n\nthere’s not an objective reason to bin your data in this manner, i.e. to make the data consistent or clearer\nit’s unlikely you’ll need this new column of data for other projects\n\nSome other subjective changes you might make to a data set:\n\ncreate an indicator variable that equals 1 when a student’s family income is below $35,000/year (why $30k? why not $25k or $35k?)\nconvert institutional enrollments to the log scale (do you always need to do this?)\njoin in data on the unemployment rate in the student’s county (do you always need this information at the county level? at all?)\n\nBut wait! Maybe you do want to use the binned value of EFC in multiple projects — perhaps the binned categories are standard across the literature — or the other examples appear always useful and otherwise standard. Isn’t that objective?\nMaybe!\nThe thing about determining the difference between objective vs subjective cleaning is that on the margins, well, it’s subjective! The point is not to have hard and fast rules, but rather to do your best to clearly separate those data wrangling tasks that must be done for the sake of consistency and clarity versus those that may change depending on your research question(s).\n\nQuick question\nWhat might be some other subjective data wrangling tasks? Think about other research you’ve conducted or seen. Or think about what our lessons and assignments so far: what have been subjective tasks?\n\n\n\nAnalysis data\nFinally, analysis data are data that are properly structured for the analysis you want to run (or the table or figure you want to make). In many cases, your analysis data set will be the same as your clean data set:\n\nit’s in the shape you want (rectangular, with observations in rows and variables in columns)\nit has all the variables you need to compute summary statistics or run regression models\n\nMost R statistical routines (e.g., mean(), sd(), lm()) do a fair amount of data wrangling under the hood to make sure it can do what you ask. For example, you may have missing values (NA) in your clean data set, but R’s linear model function, lm(), will just drop observations with missing values by default. If you are okay with that in your analysis, R can handle it.\nHowever, for some special models or figures, you may need to reshape or pre-process your clean data in one last step before running the specific analysis you want. For example, your clean data frame may have a column of parental occupational field (parocc) as a categorical variable in which the first few categories are\n\n11 := Management Occupations\n13 := Business and Financial Operations Occupations\n15 := Computer and Mathematical Occupations\n17 := Architecture and Engineering Occupations\n\n…and so on. You want to run an analysis in which you can’t use this categorical variable but instead need a vector of dummy variables, that is, converting this single column into a bunch of columns that only take on 0/1 values if parocc equals that occupation:\n\nmanage_occ == 1 if parocc == 11 else manage_occ == 0\nbusfin_occ == 1 if parocc == 13 else busfin_occ == 0\ncommat_occ == 1 if parocc == 15 else commat_occ == 0\narceng_occ == 1 if parocc == 17 else arceng_occ == 0\n\n…and so on. Particularly if\n\nyour data are large\nthe analysis takes a while to run\nyou want to use the same data set up for multiple (but not all) models or analytic tasks\n\nyou may want to save this particular analysis data set.\nAs with the difference between objective and subjective data, the difference between subjective and analysis data does not align to a set of hard and fast rules. That’s okay! The main point is that as a data analyst, you make data wrangling decisions thinking about how you will go from raw to objective to subjective to analysis data in a logical and clear manner. Collaborators, replicators, and future you will appreciate it!",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-wrangle-phil.html#recommended-texts",
    "href": "x-04-wrangle-phil.html#recommended-texts",
    "title": "Extra: Skinner’s Philosophy of Data Wrangling",
    "section": "Recommended texts",
    "text": "Recommended texts\nThere are a number of great texts that talk about the process of data wrangling. Two that have particularly informed this lecture and are worth looking at more fully:\n\nThe Pragmatic Programmer: From Journeyman to Master by Andrew Hunt and David Thomas\nCode and Data for the Social Sciences: A Practitioner’s Guide by Matthew Gentzkow and Jesse Shapiro",
    "crumbs": [
      "Data Wrangling",
      "Extra: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-02-quarto-dload-ipeds.html#download-ipeds-by-dr.-skinner",
    "href": "x-02-quarto-dload-ipeds.html#download-ipeds-by-dr.-skinner",
    "title": "Extra: Using Dr. Skinner’s Download IPEDS",
    "section": "Download IPEDS by Dr. Skinner",
    "text": "Download IPEDS by Dr. Skinner\n\nDr. Skinner, the previous instructor of the class, wrote an R script that downloads IPEDS files directly to a project folder\n\nI actually used this for my final project when I took the class with him\nDoing this enables true “push button reproducibility” of your report\n\nAs in, all I need to do to replicate your report is hit “render”, no pre-downloading of the data required\n\n\nDownload IPEDS is an R-script available on Dr. Skinner’s personal website, here\nThat’s all the instruction you get for this lesson, Dr. Skinner’s site should be enough to work out the rest. Turn to the assignment tab to see how to earn the extra credit points\n\nThe purpose of this limited instruction is to push you a little into playing with reasources you find in the wild!",
    "crumbs": [
      "Quarto",
      "Extra: Using Dr. Skinner's Download IPEDS"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\n\nEmail: melvinjtanner@ufl.edu\nOffice Hours: Wednesday 12pm - 1pm and by appointment\nOffice Location: Tigert Hall 355\n\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-G\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2033\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit students also can sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account\n\n\n\n\n\n\n\nClass Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan-09\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-16\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan-23\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan-30\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-6\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-13\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-20\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb-27\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-5\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n10\nMar-12\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar-19\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n12\nMar-26\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr-2\n\nMethods & Applications\n\n\nAssignment 10\n\n\n\n14\nApr-9\n\nModeling Basics\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr-16\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4/5\n\n0/5 not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment, get 2.5/5 with a chance to improve to 4/5 than miss the deadline\n\n\n\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nTwo or more unaccetpable absences could negatively affect your participation grade. Acceptable reasons for absences include illness, serious family emergency, professional conferences, severe weather conditions, religious holidays, and other university-approved reasons. Please let us know if you unable to make class for any reason. Due to the nature of the class and the need for trouble-shooting on your computer, Zoom attendance is not typically an option.\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#spring-2024",
    "href": "syllabus.html#spring-2024",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\n\nEmail: melvinjtanner@ufl.edu\nOffice Hours: Wednesday 12pm - 1pm and by appointment\nOffice Location: Tigert Hall 355\n\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-G\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2033\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit students also can sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Class Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan-09\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-16\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan-23\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan-30\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-6\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-13\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-20\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb-27\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-5\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n10\nMar-12\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar-19\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n12\nMar-26\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr-2\n\nMethods & Applications\n\n\nAssignment 10\n\n\n\n14\nApr-9\n\nModeling Basics\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr-16\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4/5\n\n0/5 not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment, get 2.5/5 with a chance to improve to 4/5 than miss the deadline\n\n\n\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nTwo or more unaccetpable absences could negatively affect your participation grade. Acceptable reasons for absences include illness, serious family emergency, professional conferences, severe weather conditions, religious holidays, and other university-approved reasons. Please let us know if you unable to make class for any reason. Due to the nature of the class and the need for trouble-shooting on your computer, Zoom attendance is not typically an option.\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "99-final.html",
    "href": "99-final.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project for this class is to create a truly “reproducible report” on a topic of your choosing related to higher education\n\nThe topic can really be almost anything of interest related to higher ed, so long as you can find public data to use\n\nYour report should be 3-5 pages including multiple graphs and visual elements (i.e., not too much text). Your goal is something like what you might hand a senior administrator at your university to summarize a trend/issue/topic\n\nYou will likely only have a handful of citations\nYou should devote around half your page space to data visualizations and tables\n\nThe primary focus of this report is reproducibility\n\nYour data must be publicly available with no IRB restrictions, as you will not submit it. Instead either;\n\nYour data will be downloaded automatically as part of your report rendering process (ideal)\nYou will provide clear step by step instructions for me to\n\nGo and download your data\nSave it so that I can replicate your report\n\n\n\nAll you will submit is a .qmd file and some accompanying .R scripts (as needed if source()-ed). The final report document (either .docx or .pdf) will be generated by me as part of the grading process.\n\n\nProposal (5 points)Initial Analyses (10 points)Draft Report (For Feedback Only)Presentation (5 Points)Final Report (20 Points)Rubric\n\n\nThis assignment should be submitted as a text entry directly on Canvas consisting of;\n\nA paragraph describing your project:\n\nWhat will you be predicting?\nWhy is it interesting?\n\nA description of where you will find this data.\nA few lines describing your dependent variable in detail. What is the nature of this outcome as a variable\n\nThis should be submitted to Canvas by the due date listed.\n\n\nFor your initial analyses, I need the following in either a cleanly formatted R (.R) script or Quarto (.qmd) file:\n\nCode that:\n\nReads in the data set that contains your dependent variable\n[If appropriate] Converts missing values to NA\n[If appropriate] Reshapes data\n[If appropriate] Joins data\n\n3 of the 4 following plots:\n\nA univariate graphic describing the dependent variable (e.g. histogram)\nThe conditional mean of your dependent variable at levels of at least one independent variable (e.g. box-and-whisker)\nThe distribution of your dependent variable at levels of at least one independent variable (e.g. grouped density plot, faceted density plot)\nA bivariate graphic that compares your dependent variable with at least one other independent variable (e.g. scatter plot)\n\n\nThis should be submitted to Canvas by the due date listed.\n\n\nFor your draft report, I need the following in a cleanly formatted Quarto (.qmd) with (if needed) accompanying .R scripts that are source()-ed as discussed in the Quarto Lesson:\n\nWell commented code that:\n\nReads in your raw data set\nPerforms all necessary data wrangling tasks to clean, join, and reshape your data as necessary for your project\nPerforms all necessary analyses\n\nPlots (as necessary, but required) to show the results of your analyses\nTables (as necessary, but not required) to show the results of your analyses\nFirst draft of the written elements of your report; these elements should:\n\nMotivate your analyses\nDescribe your data\nDescribe your methodological approach, in high-level, audience-appropriate manner (think your boss or colleagues, not me specifically)\nDescribe your findings\nAppropriately contextualize your results (e.g. limitations, validity) and make appropriate recommendations\n[As necessary] Include citations\nBe well organized\n\n\nIf your data is not automatically downloaded as part of your project, I also need clear step by step instructions (provided as Canvas text entry) for me to 1. Go and download your data 2. Save it so that I can replicate your report\n\n\n\nYou will present the results of your report in class during the penultimate week of the semester (see date in Canvas)\nThe presentation format is up to you, previous students have simply presented an image of one intricate figure they created, others have created a short PowerPoint presentation, others have created presentations using Quarto/RMarkdown/LaTeX. It’s up to you!\nThe primary rule for this presentation is that it is to be 3-5 mins long (read min 3 mins, max 5 mins, ideal 4 mins)\n\nAs this is meant to replicate the you presenting your report to senior administrators, this is a hard time-limit, you will be stopped if you go over 5 mins\n\n\n\n\nFor your final report, I need:\n\nA clearly formatted Quarto (.qmd) file that will build your report and any accompanying .R scripts that are source()-ed as needed\nEither:\n\nYour data will be downloaded directly as part of the project\nYou will provide clear step by step instructions (provided as Canvas text entry) for me to\n\nGo and download your data\nSave it so that I can replicate your report\n\n\n\nYour report will be graded according to the final report rubric.\n\n\nThere are five major areas to the final project:\n\nData Analysis\nGraphical/Tabular Presentation\nWritten Description\nOrganization, Clarity, and Formatting\nCoding\n\n\nData Analysis\n\nA strong final project will have a data analysis that cleanly wrangles raw data into an analysis-ready data set; correctly performs all descriptive and statistical analyses necessary to answer the research question; and well presents the results. The analyses do not necessarily have to be complex, but they should represent the best reasonable approach.\nAn acceptable final project will have a data analysis that includes some some mistakes or inaccuracies in how the data are cleaned, analyses are run, and/or results are presented. The analyses may not be quite appropriate for the analytic task.\nA weak final project data analysis will not correctly wrangle raw data into a clean data set, will not well describe relationships between predictors and the outcome, will not use appropriate analytic tools, and will not present results well or at all. In general, the analysis will be messy and unable to provide insights into the research questions/problems motivated in the report.\n\n\n\nGraphical/Tabular Presentation\n\nA strong final project will include nicely labeled, easy to understand graphics that describe exactly what is happening with the patterns in the data. The graphics may be simple or complex, but they clearly connect to the analysis (e.g. not just a figure for the sake of a figure). The response could include (but doesn’t have to include) interactive graphics. A table or two may be included, but only sparingly and in a clear format.\nAn acceptable final project will include graphics, but these figures may not be easy to read, may not be sufficiently detailed, or may not represent the most appropriate way to show the relationships in the data. A table or two may be included, but not appropriately formatted or without a clear rationale for its inclusion (i.e. why a table and not a figure).\nA weak final project will include graphics and/or tables that are poorly labeled and don’t make much sense.\n\n\n\nWritten Description\n\nA strong final project will include clear and concise written sections that are easily understandable by an interested layperson. Assume that your audience is your boss or a colleague—not me.\nAn acceptable final project will be written generally well, but technical details may be poorly described or not described at all, and sentences will be hard to follow.\nA weak final project will be poorly written, with many mistakes regarding both the analysis and good writing practices.\n\n\n\nOrganization, Clarity, Formatting\n\nA strong final project will have an .qmd file that generates a very nicely formatted document, suitable for professional presentation. What kind of report would you want to give to a supervisor or have given to you? That’s what I want back from you. The organization should be very clear and easy to understand.\nAn acceptable final project will have some formatting problems and may not look very nice.\nA weak final project will include code chunks in the output, poor formatting, and in general will just be messy.\n\n\n\nCoding\n\nA strong final project will have code that can generate results from the raw data in an easy to understand way. The code will be commented and will run on my computer without my having to tweak it in any way. I.e., be fully reproducible. (NOTE: An easy test on your end is transfer your files to a new location with the appropriate directory structure and attempt to knit the document)\nAn acceptable final project will have code that is relatively clear, but that may not be commented in ways that make sense and that has some problems that require debugging on my end.\nA weak final project will have code that is messy, hard to understand and not commented. It will not run on my computer, and cannot be easily debugged.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "11-pro-functions.html#dry-vs-wet-programming",
    "href": "11-pro-functions.html#dry-vs-wet-programming",
    "title": "I: Functions & Loops",
    "section": "DRY vs WET programming",
    "text": "DRY vs WET programming\nThe watchwords for this lesson are DRY vs WET:\n\nDRY: Don’t repeat yourself\nWET: Write every time\n\nLet’s say you have a three-step analysis process for 20 files (read, lower names, add a column). Under a WET programming paradigm in which each command gets its own line of code, that’s 60 lines of code. If the number of your files grows to 50, that’s now 150 lines of code — for just three tasks! When you write every time, you not only make your code longer and harder to parse, you also increase the likelihood that your code will contain bugs while simultaneously decreasing its scalability.\nIf you need to repeat an analytic task (which may be a set of commands), then it’s better to have one statement of that process that you repeat, perhaps in a loop or in a function. Don’t repeat yourself — say it once and have R repeat it for you!\nThe goal of DRY programming is not abstraction or slickness for its own sake. That runs counter to the clarity and replicability we’ve been working toward. Instead, we aspire to DRY code since it is more scalable and less buggy than WET code. To be clear, a function or loop can still have bugs, but the bugs it introduces are often the same across repetitions and fixed at a single point of error. That is, it’s typically easier to debug when the bug has a single root cause than when it could be anywhere in 150 similar but slightly different lines of code.\nAs we work through the lesson examples, keep in the back of your mind:\n\nWhat would this code look like if I wrote everything twice (WET)?\nHow does this DRY process not only reduce the number of lines of code, but also make my intent clearer?",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#setup",
    "href": "11-pro-functions.html#setup",
    "title": "I: Functions & Loops",
    "section": "Setup",
    "text": "Setup\nWe’ll use a combination of nonce data and the school test score data we’ve used in a past lesson. We won’t read in the school test score data until the last section, but we’ll continue following our good organizational practice by setting the directory paths at the top of our script.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#part-1-control-flow",
    "href": "11-pro-functions.html#part-1-control-flow",
    "title": "I: Functions & Loops",
    "section": "Part 1: Control flow",
    "text": "Part 1: Control flow\nAs stated above, by control flow, I simply mean the functions that help you change how your script is read. Repeating commands often involves a loop, which is what it sounds like: upon reaching a loop, R will repeat the code inside the loop (looping back up the beginning of the section) for a certain number of times or until some condition is met. Once completed, R will go back to reading each line in order like normal.\nIf you google around, you may find that loops have a bad reputation in R, mostly for being slow. But they aren’t that slow and they are easy to write and understand.\n\nfor\nThe for() function allows you to build the aptly named for loops. There are few ways to use for(), but its construction is the same: for (variable in sequence).\nReading it backwards, the sequence is just the set of numbers or objects that we’re going to work through. The variable is a new variable that will temporarily hold a value from the sequence in each run through the loop. When the sequence is finished, so is the loop.\nFirst, let’s loop through a sequence of 10 numbers, printing each one at a time as we work through the loop.\n\n## make vector of numbers between 1 and 10\nnum_sequence &lt;- 1:10\n\n## loop through, printing each num_sequence value, one at a time\nfor (i in num_sequence) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice the braces {} that come after for(). This is the code in the loop that will be repeated as long as the loop is run. With each loop, i takes on the next value in the num_sequence. This is why we see 1 through 10 printed to the console.\nLet’s do it again, but this time with characters.\n\n## character vector using letters object from R base\nchr_sequence &lt;- letters[1:10]\n\n## loop through, printing each chr_sequence value, one at a time\nfor (i in chr_sequence) {\n    print(i)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nOnce more, with each loop, i takes on each chr_sequence value in turn and print() prints it to the console.\n\nQuick exercise\nCan you modify the above loop so that it works through both the num_sequence and chr_sequence in the same loop? (HINT: how might you combine/concatenate the two sequences?)\n\nAnother way to make a for loop is work through a vector by its indices, that is, instead of pulling out each item directly and storing it in i, we can use i as an index counter and then call items based on their index: chr_sequence[i]. Here’s an example.\n\n## for loop by indices\nfor (i in 1:length(chr_sequence)) {\n    print(chr_sequence[i])\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nTo understand what’s happening, let’s break the code into pieces to make it clearer.\nInside the for() parentheses, we have i in 1:length(chr_sequence). We know what i in means since it’s like what we’ve seen before. What’s 1:length(chr_sequence)? First, it looks like the colon construction we’ve seen before, &lt;start&gt;:&lt;end&gt;, which means give the full sequence of values from &lt;start&gt; through &lt;end&gt;.\nSince we made it above, we know that there are ten letters in chr_sequence. We could just use 1:10. However, this violates our no magic numbers rule (what happens if we want to add or subtract letters from the list later?).\nWe can get around this issue by using the base-R function, length(), which will return the number of items in a one-dimensional object. Since we know that length(chr_sequence) is equal to 10, that means that 1:length(chr_sequence) is the same thing as saying 1:10. It’s just another more flexible way to get the end number of our sequence. We can show this by just printing i again.\n\n## for loop by indices (just show indices)\nfor (i in 1:length(chr_sequence)) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nBack to the original function, we see that inside the braces ({}), we have print(chr_sequence[i]). If you’ve taken a look at the supplemental lesson for base R, you know that brackets ([]) are way of pulling out specific values from a vector. For example, if you have a vector of 3 items, e.g., x &lt;- c(\"a\", \"b\", \"c\"), you can select the 2nd item by using square brackets and an index value: x[2]. While an index value is a number (think of it like a street address or room number in a hall), we can also use variables that represent numbers, which is useful for programming. Here’s a non-looped test:\n\n## confirm that we can use variables as indices\ni &lt;- 1                     # set i == 1\nchr_sequence[i]      \n\n[1] \"a\"\n\ni &lt;- 2                     # now set i == 2\nchr_sequence[i]            # notice that code is exactly the same here\n\n[1] \"b\"\n\n\nWe know that i is going to take on values 1 through 10 in the loop, which means the print() function will get chr_sequence[1], chr_sequence[2], and so on. Because of the brackets, these will turn into…a, b, and so on. We should get the same thing as before! Let’s put all the pieces together and run again:\n\n## for loop by indices (once again)\nfor (i in 1:length(chr_sequence)) {\n    print(chr_sequence[i])\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n[1] \"f\"\n[1] \"g\"\n[1] \"h\"\n[1] \"i\"\n[1] \"j\"\n\n\nSuccess!\nWhether you decide to loop using actual values from the sequence or indices will usually depend on the code you want to run in the loop. Sometimes one way works better and other times the other. Just do whatever works best for you at that time.\n\nQuick exercise\nAdd another print statement to the last loop that shows the value of i with each loop.\n\n\n\nwhile\nThe while() function is similar to for() except that it doesn’t have a predetermined stopping point. As long the expression inside the parentheses is TRUE, the loop will keep going. Only when it becomes FALSE will it stop.\nOne way to use a while() loop is to set up a counter. When the counter reaches some value, the expression inside the while() parentheses is no longer true and the loop stops.\n\n## set up a counter\ni &lt;- 1\n## with each loop, add one to i\nwhile(i &lt; 11) {\n    print(i)\n    i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nUsing a while() loop with a counter is often the same as using a for() loop with a sequence. If that’s the case, it’s probably better just to use a for() loop.\nwhile() loops are most useful when it’s not clear from the start when the loop should stop. Imagine you have an algorithm that should only stop when a certain number is reached. If the time it takes to reach the number changes depending on the input, then a for() loop probably won’t work, but a while() loop will.\nYou have to careful, however, with while() loops. If you forget to increment the counter (like I did the first time I set up this example), the loop won’t ever stop because i will never get larger and will always be less than 11! If your while() loop will only stop when a certain condition is met, it’s still a good idea to build in a pre-specified number of trials. If your loop has tried X times to meet the condition and still hasn’t done so, it should stop with an error or return what it has so far (depending on your needs).\nYou have been warned!\n\n\nif\nWe’ve already used a version of if, ifelse(), quite a bit. We can use if statements in our script to decide whether a section of code should be run or skipped. We can also use if() inside a for() loop to set a condition that changes behavior on some iterations of the loop.\n\n## only print if number is not 5\nfor (i in num_sequence) {\n    if (i != 5) {\n        print(i)\n    }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice how 5 wasn’t printed to the console. It worked!\n\nQuick exercise\nChange the condition to print only numbers below 3 and above 7.\n\nWe can add one or more else if() / else() partners to if() if we need, for example, option B to happen if option A does not.\n\n## if/else loop\nfor (i in num_sequence) {\n    if (i != 3 & i != 5) {\n        print(i)\n    } else if (i == 3) {\n        print('three')\n    } else {\n        print('five')\n    }\n}\n\n[1] 1\n[1] 2\n[1] \"three\"\n[1] 4\n[1] \"five\"\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nAs with dplyr verbs, the small number of control functions (and there are some others) can be combined in an infinite number of ways to help you control how your script is read. If you find, however, that your script keeps getting more complex, with multiple nested loops and ifelse() exceptions, it might be time to either rethink your approach or to write your own functions that can handle expectations in a clearer way.",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#part-2-writing-functions",
    "href": "11-pro-functions.html#part-2-writing-functions",
    "title": "I: Functions & Loops",
    "section": "Part 2: Writing functions",
    "text": "Part 2: Writing functions\nYou can write your own functions in R and should! They don’t need to be complex. In fact, they tend to be best when kept simple. Mostly, you want a function to do one thing really well.\nTo make a function, you use the function() function. Put the code that you want your function to run in the braces {}. Any arguments that you want your function to take should be in the parentheses () right after the word function. The name of your function is the name of the object you assign it to.\nLet’s make one. The function below, say_hi(), doesn’t take any arguments and prints a simple string when called. After you’ve built it, call your function using its name, not forgetting to include the parentheses.\n\n## function to say hi!\nsay_hi &lt;- function() {\n    print(\"Hi!\")\n}\n\n## call it\nsay_hi()\n\n[1] \"Hi!\"\n\n\nLet’s make another one with an argument so that it’s more flexible. Let’s have our function take a name and print it. We’ll use the base-R function, paste0(), to combine all the string parts into one string. paste0() is like paste(), but it assumes we don’t want any space between the pieces. That works for us here because we’ll add our spaces manually.\n\n## function to say hi!\nsay_hi &lt;- function(name) {\n    ## combine (notice we add space after comma)\n    out_string &lt;- paste0(\"Hi, \", name, \"!\")\n    ## print output string\n    print(out_string)\n}\n\n## call it\nsay_hi(\"Leo\")\n\n[1] \"Hi, Leo!\"\n\n\nThis time, we want it to print out a sequence of numbers, but we want to be able to change the number each time we call it.\n\n## new function to print sequence of numbers\nprint_nums &lt;- function(num_vector) {\n    ## this code looks familiar...\n    for (i in num_vector) {\n        print(i)\n    }\n}\n\n## try it out!\nprint_nums(1:10)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice how the variable num_vector is repeated in both the main function argument and inside the for parentheses. The for() function sees num_vector and looks for it in the main function. It finds it because the num_vector you give the main function, print_nums(), is passed through to the code inside. Now for() can see it and use it! Let’s try a few more inputs:\n\n## v1\nprint_nums(1)\n\n[1] 1\n\n## v2\nprint_nums(1:5)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n## v3\nprint_nums(seq(1, 20, by = 2))\n\n[1] 1\n[1] 3\n[1] 5\n[1] 7\n[1] 9\n[1] 11\n[1] 13\n[1] 15\n[1] 17\n[1] 19\n\n\n\nQuick exercise\nWhat happens if you forget to put an argument in your new function? How do you think you might set a default argument for num_vector? Could you set it equal to something?\n\nOne last thing to keep in mind about functions is that they follow Vegas rules: what happens inside the function, stays inside the function. A more technical way of stating this is that the function has its own scope and objects created / processes run within that scope stay within that scope. That’s why even though we created an object called out_string in our say_hi() function, we can’t call it directly from the console. It only exists while the function is running and goes away once the function completes.\nEven if we repeat an object name that exists in our global (working) environment inside our function, the value we give it inside the function will override the global value within the function. But once the function is finished, the global value will again take precedence. Don’t worry to much about scoping (here’s more information if you are interested), but just be aware that it’s generally good practice that if you want an object for use inside your function, you should either create it there or make it an argument.",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#part-3-practical-examples",
    "href": "11-pro-functions.html#part-3-practical-examples",
    "title": "I: Functions & Loops",
    "section": "Part 3: Practical examples",
    "text": "Part 3: Practical examples\n\nExample 1: missing data\nNow that we’ve seen some control flow and programming methods, let’s move to a more realistic use case. In this first example, we’ll make a function that fills in missing values, a common task we’ve had. First, we’ll generate some fake data with missing values.\nNote that since we’re using R’s sample() function, your data will look a little different from mine due to randomness in the sample, but everything will work the same.\n\n## create a data frame with around 10% missing values (-97,-98,-99) in\n## three columns\ndf &lt;- tibble(\"id\" = 1:100,\n             \"age\" = sample(c(seq(11,20,1), -97),\n                            size = 100,\n                            replace = TRUE,\n                            prob = c(rep(.09, 10), .1)),\n             \"sibage\" = sample(c(seq(5,12,1), -98),\n                               size = 100,\n                               replace = TRUE,\n                               prob = c(rep(.115, 8), .08)),\n             \"parage\" = sample(c(seq(45,55,1), c(-98,-99)),\n                               size = 100,\n                               replace = TRUE,\n                               prob = c(rep(.085, 11), c(.12, .12)))\n             ) \n## show\ndf\n\n# A tibble: 100 × 4\n      id   age sibage parage\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    13      5     50\n 2     2    19      8     53\n 3     3    12      9    -98\n 4     4    19     10     49\n 5     5    15      8     50\n 6     6   -97      5    -98\n 7     7    17    -98     52\n 8     8    18      6     50\n 9     9    13      7     47\n10    10    18      7     49\n# ℹ 90 more rows\n\n\nWe could fix these manually like we have been in past lessons and assignments, but it would be nice have a shorthand function. The function needs to flexible though, because the missing data values are coded differently in each column.\n\n## function to fix missing values\nfix_missing &lt;- function(x, miss_val) {\n    ## use ifelse(&lt; test &gt;, &lt; do this if TRUE &gt;, &lt; do that if FALSE &gt;)\n    x &lt;- ifelse(x %in% miss_val,        # is x == any value in miss_val?\n                NA,                     # TRUE: replace with NA\n                x)                      # FALSE: return original value as is\n    ## return corrected x\n    return(x)\n}\n\nOur fix_missing() function should meet our needs. It takes the same ifelse() function we’ve used before, but instead of using the name of the object (like df), uses an argument name x that we can set each time. It does the same for miss_val. Instead of choosing a hard-coded value (a magic number), we can change it each time we call the function. Let’s try it out.\n\n## check\ndf |&gt;\n    count(age)\n\n# A tibble: 11 × 2\n     age     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   -97    13\n 2    11     7\n 3    12     5\n 4    13     9\n 5    14     8\n 6    15     7\n 7    16    10\n 8    17     8\n 9    18    12\n10    19    13\n11    20     8\n\n## missing values in age are coded as -97\ndf &lt;- df |&gt;\n    mutate(age = fix_missing(age, -97))\n\n## recheck\ndf |&gt;\n    count(age)\n\n# A tibble: 11 × 2\n     age     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    11     7\n 2    12     5\n 3    13     9\n 4    14     8\n 5    15     7\n 6    16    10\n 7    17     8\n 8    18    12\n 9    19    13\n10    20     8\n11    NA    13\n\n\nIt worked! All the values that were -97 before, are now in the NA table column. Importantly, none of the other values changed.\n\nQuick exercise\nCorrect the missing values in the other columns using our new function. NOTE that parage has missing values of -98 and -99.\n\n\n\nExample 2: batch read files\n\nRead in all files in folder\nIn our lesson on appending, joining, and merging, we read in a few administrative test score files. In that lesson, we read in each file individually and then appended them.\n\n## read in all Bend Gate test score files\ndf_1 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1980.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_2 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1981.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_3 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1982.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_4 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1983.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_5 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1984.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_6 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1985.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## append\ndf &lt;- bind_rows(df_1, df_2, df_3, df_4, df_5, df_6)\n\n## show\ndf\n\n# A tibble: 6 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n4 Bend Gate  1983   491   276     793\n5 Bend Gate  1984   502   310     788\n6 Bend Gate  1985   488   280     789\n\n\nThat was fine then, but when you see that much repeated code, you should immediately consider using a more functional programming approach. What if we wanted to read in all the files for the other schools as well? That would be a total of 24 very similar lines of code — ripe for introducing bugs.\nLet’s combine a number of the skills we’ve learned to this point — functions, regular expressions, and pipes — to read and bind all the test score files in a more functional (and less buggy) manner.\nFirst, we’ll store the names of the files in an object using list.files(). We’ve used the list.files() function a few times in class when checking that we were in the correct working directory. In those situations, we’ve just printed the output to the console. But as with all things R, we can save that output in an object instead and put it to good use.\n\n## get names (with full path) of all school test score files\n\nfiles &lt;- list.files(file.path(\"data\", \"sch-test\", \"by-school\"), full.names = TRUE)\n\n## show\nfiles\n\n [1] \"data/sch-test/by-school/bend-gate-1980.csv\"   \n [2] \"data/sch-test/by-school/bend-gate-1981.csv\"   \n [3] \"data/sch-test/by-school/bend-gate-1982.csv\"   \n [4] \"data/sch-test/by-school/bend-gate-1983.csv\"   \n [5] \"data/sch-test/by-school/bend-gate-1984.csv\"   \n [6] \"data/sch-test/by-school/bend-gate-1985.csv\"   \n [7] \"data/sch-test/by-school/east-heights-1980.csv\"\n [8] \"data/sch-test/by-school/east-heights-1981.csv\"\n [9] \"data/sch-test/by-school/east-heights-1982.csv\"\n[10] \"data/sch-test/by-school/east-heights-1983.csv\"\n[11] \"data/sch-test/by-school/east-heights-1984.csv\"\n[12] \"data/sch-test/by-school/east-heights-1985.csv\"\n[13] \"data/sch-test/by-school/niagara-1980.csv\"     \n[14] \"data/sch-test/by-school/niagara-1981.csv\"     \n[15] \"data/sch-test/by-school/niagara-1982.csv\"     \n[16] \"data/sch-test/by-school/niagara-1983.csv\"     \n[17] \"data/sch-test/by-school/niagara-1984.csv\"     \n[18] \"data/sch-test/by-school/niagara-1985.csv\"     \n[19] \"data/sch-test/by-school/spottsville-1980.csv\" \n[20] \"data/sch-test/by-school/spottsville-1981.csv\" \n[21] \"data/sch-test/by-school/spottsville-1982.csv\" \n[22] \"data/sch-test/by-school/spottsville-1983.csv\" \n[23] \"data/sch-test/by-school/spottsville-1984.csv\" \n[24] \"data/sch-test/by-school/spottsville-1985.csv\" \n\n\n\nQuick exercise\nRerun list.files() again, but set the full.names argument to the default value of FALSE. What does the output look like? Why is this a problem? (Once finished, be sure to set full.names = TRUE again and rerun.)\n\nNow that we have an object, we can read in the files using a loop. We’ll need something to store them in along the way. We’ll use a blank list. We’ve not used lists before, but think of them as special vectors, which we have used.\nOnce we have a list, we’ll read in each file, but instead of storing it in an object like df, we’ll put it in the list. Since we’ll be reading in a large number of data files in the next few steps, we’ll also include the argument show_col_types = FALSE so that read_csv() doesn’t print information for each file (our assumption is that we’ve already spot checked a few files and know what we’re getting).\n\n## init list\ndf_list &lt;- list()\n\n## use loop to read in files\nfor (i in 1:length(files)) {\n    ## read in file (f) and store in list (note double brackets for list)\n    df_list[[i]] &lt;- read_csv(files[i], show_col_types = FALSE)    \n}\n\n## show first 3 items\ndf_list[1:3]\n\n[[1]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\n[[2]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\n[[3]]\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nNow that the items are in a list, we can again use bind_rows(), which, in addition to individual objects, can take a single list of objects.\n\n## bind our list to single data frame\ndf &lt;- df_list |&gt;\n    bind_rows()\n\n## show\ndf\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\nAside from being more aesthetically pleasing, this code is better because it:\n\ndoesn’t rely on repeated lines of code with small changes\nis flexible\n\nImagine you receive three more years of data for each school: 1986, 1987, 1988. As long as the files are in the same format, all you need to do is put the same directory as the other files and rerun your code. It will add them to the files vector and your loop will run just as before.\n\n\nRead in only some files\nWhat if we only want files for Spottsville? Overall, the code is exactly the same except that we want our list to only contain the spottsville_*.csv files. How can we do that? With regular expressions!\nNotice that the second argument in list.files() is pattern with a default value of NULL. This means it doesn’t do any filtering if we leave it out. But if we want to filter which files are stored in files, we should use it.\n\n## filter files to be read in using pattern\nfiles_sp &lt;- list.files(file.path(\"data\", \"sch-test\", \"by-school\"), pattern = \"spottsville\", full.names = TRUE)\n\n## check\nfiles_sp\n\n[1] \"data/sch-test/by-school/spottsville-1980.csv\"\n[2] \"data/sch-test/by-school/spottsville-1981.csv\"\n[3] \"data/sch-test/by-school/spottsville-1982.csv\"\n[4] \"data/sch-test/by-school/spottsville-1983.csv\"\n[5] \"data/sch-test/by-school/spottsville-1984.csv\"\n[6] \"data/sch-test/by-school/spottsville-1985.csv\"\n\n\nLuckily, our regular expression pattern can simply be \"spottsville\". Were our files less consistently named, we might have had trouble (name those files well!).\nThe rest of the code should be as it was before (with the small addition of _sp in various names to keep distinct from our first attempt).\n\n## init list\ndf_sp_list &lt;- list()\n\n## use loop to read in files\nfor (i in 1:length(files_sp)) {\n    ## read in file (f) and store in list\n    df_sp_list[[i]] &lt;- read_csv(files_sp[i], show_col_types = FALSE)    \n}\n\n## bind our list to single data frame\ndf_sp &lt;- df_sp_list |&gt;\n    bind_rows()\n\n## show\ndf_sp\n\n# A tibble: 6 × 5\n  school       year  math  read science\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Spottsville  1980   498   288     813\n2 Spottsville  1981   494   270     765\n3 Spottsville  1982   507   289     801\n4 Spottsville  1983   515   288     775\n5 Spottsville  1984   475   289     779\n6 Spottsville  1985   515   285     784",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#questions",
    "href": "11-pro-functions.html#questions",
    "title": "I: Functions & Loops",
    "section": "Questions",
    "text": "Questions\n\nUsing a loop, do the following:\n\nRead in each of the individual school test files for Bend Gate and Niagara only. (HINT The vertical pipe operator, |, means OR in regular expression patterns.)\n\nWithin each iteration of the loop, add a column to the data frame that is called relative_path and contains the string relative path to the data file you just read in (e.g., if the file is located at data/sch_test/by_school/bend_gate_1980.csv, then relative_path ==    \"data/schools/by_test/bend_gate_1980.csv\" in that row).\n\nBind all the data sets together.\n\nRead in hsls_small.csv and do the following:\n\nUsing the user-written function fix_missing(), convert missing values in x1ses to NA.\n\nSubset the full data frame to the first 50 rows and pull out the test scores into a vector using the following code:\nr       test_scr &lt;- df |&gt;           filter(row_number() &lt;= 50) |&gt;           pull(x1txmtscor)\nUsing a for() loop, print out the index of the missing values (when test_scr equals -8).\nRepeat the same code, but add an else() companion to the initial if() statement that prints the value if non-missing.\nAdd an else if() between the initial if() and final else() in your loop that prints \"Flag: low score\" if the score is less than 40. Also, change your first if() statement to print \"Flag: missing value\" instead of the index if the value is missing.\nWrite your own function to compare two values and return the higher of the two. It should be called return_higher(), take two arguments, and return the higher of two values.\n\nOnce you’ve created it, use it in a dplyr chain to create a new column in the data frame called high_expct that is the represents the higher of x1stuedexpct and x1paredexpct. Don’t forget to account for missing values!\nHINT If stuck on what the inside of your function should look like, go back to the lesson in which we did this already — can you repurpose that code in some way?\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_9.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html",
    "href": "09-wrangle-iv.html",
    "title": "IV: Tidyverse Tricks & SQL",
    "section": "",
    "text": "LessonAssignment\n\n\nYou can download R  here.",
    "crumbs": [
      "Data Wrangling",
      "IV: Tidyverse Tricks & SQL"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#quarto-visual-editor",
    "href": "07-quarto-intro.html#quarto-visual-editor",
    "title": "Reproducable Reports with Quarto",
    "section": "Quarto Visual Editor",
    "text": "Quarto Visual Editor\n\nOne of the coolest features of Quarto vs plain RMarkdown is the visual editor\n\nThis functions like a very basic MS Word\n\nIt is a new feature, so sometimes can be buggy\n\n\nThere are still times the source editor works better\n\nI find that for writing extended periods of text, the visual editor is really nice (especially with the Zotero integration covered in the Quarto-Zotero extra credit) However, for writing short amounts with code chunks, for hyperlinks, or for bullet point lists, the source editor is easier for me\n\nAs you learn markdown, the visual editor will probably be helpful in all situations, once you’re more comfortable with the syntax, you’ll likely prefer the source editor for some things",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#sourcing-scripts",
    "href": "07-quarto-intro.html#sourcing-scripts",
    "title": "Reproducable Reports with Quarto",
    "section": "Sourcing scripts",
    "text": "Sourcing scripts\nIn addition to using R in this course, you will also learn to use Markdown, a plain text markup language created by John Gruber in 2004. If you’re thinking, no, not another language — rest assured that Markdown is very easy to learn and use. In fact, it’s possible you’ve already used Markdown (or a version of it) if you’ve ever used Slack or a lightweight writing tool like iAWriter.\n\nBut why not just use MS Word?\nMost people who do any writing on a computer are familiar with MS Word. Even people who don’t use MS Word likely write with a MS Word-like program. Since we’re all on the same page, why introduce a new way to write — why Markdown?\nThe primary reason to bother is that Markdown files are easier to share and better support replicability in a quantitative research workflow. Understanding why this it the case requires first understanding the difference in how each of these writing tools handles a document’s content and formatting.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#wysiwyg-vs-wysiwym",
    "href": "07-quarto-intro.html#wysiwyg-vs-wysiwym",
    "title": "Reproducable Reports with Quarto",
    "section": "WYSIWYG vs WYSIWYM",
    "text": "WYSIWYG vs WYSIWYM\n\n“What you see is what you get”\nWYSIWYG (pronounced how you might guess - “whizzy-whig”) stands for what you see is what you get. MS Word and similar programs are WYSIWYG writing tools. Want some text bolded? You highlight the text, click on the bold text button (or hit Control/Command-B) and the text becomes bold. Want 1.05” margins to increase the page count (teachers always know, by the way…), then you adjust the margins and watch the text squeeze a little and the page count increase.\nThe point is that as you write, you control the content and how it’s formatted at the same time. This is really powerful. You can see your document (literally) taking shape and when you’re done writing, you’re mostly done formatting, too.\nBut one problem from a research perspective is that WYSIWYG document preparation programs don’t always share well, meaning that the formatting isn’t always preserved across computers or operating systems. Sometimes equations don’t open correctly; the font you selected doesn’t exist on another computer. While programs may be able to open each other’s files (e.g. OpenOffice can open a .docx file), that’s not always the case. They open different-program files with many errors or even not at all.\nThe second problem is they don’t work well with reproducible workflows. Let’s say you’ve done some data analysis and make 10 tables and 10 figures. You’ve carefully placed and formatted in your MS Word report. Perfect! But before giving to your supervisor or submitting to a journal, you get some new data and need to rerun everything…ah! You’ll have to go through the whole transfer and formatting process again, increasing the likelihood of introducing errors.\n\n\n“What you see is what you mean”\nWYSIWIM (“whizzy-whim”), on the other hand, separates formatting from content. Rather than making bold text bold, you instead add a bit of markup — some special syntax — to the text you want to be bold. Only when the document is finally compiled into the final form will the text be bold.\nMarkdown syntax uses plain text characters to indicate formatting. This lesson was written in Markdown, so to show you an example, here’s the prior paragraph, in plain Markdown syntax:\n_WYSIWIM_ (\"whizzy-whim\"), on the other hand, separates formatting\nfrom content. Rather than making bold text bold, you instead add a bit\nof markup --- some special syntax --- to the text you want to be\nbold. Only when the document is finally _compiled_ into the final\nform will the **text be bold**. \nMarkdown solves the two problems noted above. First, it’s written in plain text, which means that it can be opened on any computer running any operating system. Even if the end user doesn’t have a way to compile the raw Markdown syntax into the nice-looking final form, the text is still very legible. In fact, this feature — ability to read uncompiled — was a motivating force behind the development of Markdown:\n\nThe idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. (John Gruber, Markdown website)\n\nSecond, because it’s plain text, it integrates well with scripting languages like R. Remember our example from before? If your final report was in Markdown instead of MS Word, you could rerun your analyses with the updated data and then recompile your final report — tables and figures updated automatically!\nIn fact, R and Markdown work so well together that you can combine them in a single — appropriately named — RMarkdown document that takes a combined file ending:\n\nR script: analysis.R\nMarkdown script: write_up.md\nRMarkdown script: report.Rmd\n\nor\nanalysis.R + write_up.md = report.Rmd\nWe’ll keep our R and Markdown scripts separate at first, but know that you’ll submit an RMarkdown file for your final project.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#markdown-syntax",
    "href": "07-quarto-intro.html#markdown-syntax",
    "title": "Reproducable Reports with Quarto",
    "section": "Markdown syntax",
    "text": "Markdown syntax\nRather than list Markdown syntax here, I’ll direct you to an excellent resource: The Markdown Guide. On this site, you find example of both basic syntax (headers, italics, bold, links) and more advanced syntax (tables and footnotes).\nHere’s an example from the Markdown Guide basic syntax page for making headers:\n\n\n\nMarkdown Guide\n\n\nOn the left you have the Markdown syntax. To make a header, just put a pound sign / hash (#) in front of the line. As you want smaller headers, just keep adding pound signs. The middle column shows you the underlying HTML (web markup language) code. This isn’t that important for us. The last column, however, shows the text as it will render in your final document.\nThe site is also nice in that it shows you a few different ways, when they exist, of doing the same thing. Take some time to go through the site — it won’t take long — and keep it in mind as a reference for the future.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#other-references",
    "href": "07-quarto-intro.html#other-references",
    "title": "Reproducable Reports with Quarto",
    "section": "Other references",
    "text": "Other references\n\nThe Plain Person’s Guide to Plain Text Social Science, by Kieran Healy (very short, but the first chapters make a strong case for using plain text when doing quantitative social science)\n\nIn this lesson, we’ll combine many of the pieces we’ve already covered — reading in data, cleaning data, making figures — into a single RMarkdown document. We’ll purposefully keep it simple at first by reusing some code we’ve seen before.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#getting-started",
    "href": "07-quarto-intro.html#getting-started",
    "title": "Reproducable Reports with Quarto",
    "section": "Getting started",
    "text": "Getting started\nTo properly compile a PDF documents from Markdown, you’ll need some version of LaTeX, a typesetting system best known for being able to nicely render mathematical notation but that is really useful for making reproducible documents. You should have already downloaded this at the start of the course. If you didn’t, visit the software page for information on how to get it.\nIf you are unable to get LaTeX to install properly or cannot get the document to compile as PDF, you should be able to compile to HTML instead.\nYou will also need the R knitr and rmarkdown libraries. You should have rmarkdown already, but if you haven’t already installed either, type\ninstall.packages(c(\"knitr\",\"rmarkdown\"))\ninto your R console. NOTE that even if you’ve already installed rmarkdown, install.packages() will just quickly reinstall it.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#what-is-rmarkdown",
    "href": "07-quarto-intro.html#what-is-rmarkdown",
    "title": "Reproducable Reports with Quarto",
    "section": "What is RMarkdown?",
    "text": "What is RMarkdown?\nFrom the RStudio website:\n\nR Markdown is a file format for making dynamic documents with R. An R Markdown document is written in markdown (an easy-to-write plain text format) and contains chunks of embedded R code…\n\nIn other words, an RMarkdown (hereafter RMD) document has two basic components:\n\nR code (in code chunks)\nMarkdown text (most everything else outside of the code chunks)\n\nRMD documents use the file ending, *.Rmd, which makes sense as they combine R code with md text. To compile an RMD file, meaning to\n\nconvert the plain Markdown text into formatted text\nrun R code, producing all output along the way\ncombine the Markdown text plus R output into a finished document\n\nYou will use the rmarkdown render() function, which in turn uses the knitr knit() function under the hood. It can be a bit confusing how all the pieces work together, but luckily, you can use RStudio’s point-and-click interface to knit your documents.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#starting-a-new-document",
    "href": "07-quarto-intro.html#starting-a-new-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Starting a new document",
    "text": "Starting a new document\nWhen you open a blank RMarkdown document, RStudio will by default fill it with some example text that looks like this. You can change this in RStudio’s settings, but I think it’s helpful to see the skeleton of an RMarkdown document (plus, it’s not a big deal to just erase the parts you don’t need).\n\nExample text in new file started via RStudio (default)\n---\ntitle: \"Document Title\"\nauthor: \"Benjamin Skinner\"\ndate: \"1/30/21\"\noutput: pdf_document\n---\n\n\nR Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#including-plots",
    "href": "07-quarto-intro.html#including-plots",
    "title": "Reproducable Reports with Quarto",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\nRather than going through this example text to learn about RMarkdown, we’ll use our own example document, test_scores.Rmd, which is linked above.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#compile-an-rmarkdown-document",
    "href": "07-quarto-intro.html#compile-an-rmarkdown-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Compile an RMarkdown document",
    "text": "Compile an RMarkdown document\nWe’ll go through the main sections our example RMD document below. So you can follow along better, you should compile test_scores.Rmd as your first step.\nOnce you’ve downloaded the file and data (if you don’t have it already), place the RMD file in the scripts directory and the unzipped data in the data directory.\nWhen you open test_scores.Rmd in RStudio, you should see a button in the upper left facet that says Knit with a ball of string icon.\n\nIf you have the working directory correctly set to scripts and have placed the sch_test/ data folder inside data, you should be able to click Knit and have the document compile into a PDF. By default, RStudio will open a PDF viewer window or show you the file in the lower right facet in the Viewer tab.\nNOTE that if you’ve had trouble with LaTeX, you should be able to compile into an HTML file, which you can open in your browser.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#sections-of-our-document",
    "href": "07-quarto-intro.html#sections-of-our-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Sections of our document",
    "text": "Sections of our document",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#yaml-header",
    "href": "07-quarto-intro.html#yaml-header",
    "title": "Reproducable Reports with Quarto",
    "section": "YAML header",
    "text": "YAML header\nYAML, which stands for “YAML Ain’t a Markup Language”, is a common way to configure dynamic documents like RMD documents. It’s the first thing you see at the top of an RMarkdown file. The YAML header is this piece of code:\nNotice the opening and closing three hyphens (---). This is how R knows that this section of code is special. The YAML can become complex, as you add document options, but for now we keep it simple:\n\ntitle: the document title (printed)\nauthor: the document author (printed)\ndate: manually set date (printed)\n\nleave date out of YAML and the date on which the document is compiled will be added automatically, or you can use the LaTeX macro \\today, which will also print today’s date\nset date to \"\" (empty string) for no printed date\n\noutput: document output\n\npdf_output: for PDF (uses LaTeX)\nhtml_output: for web page output (open in browser)\nword_output: for MSWord output (uses MSWord)\n\n\nWe’re using pdf_output but you can either change this setting or override it when compiling the final document.\nNOTE that the colon (:) is a special character in YAML. Notice that I don’t necessarily have to use quotation marks for strings with spaces — I do for the title, but not for my name. That said, if your title includes a colon, you need to wrap the entire title string in double quotation marks (\") — otherwise the document won’t compile.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#code-chunks",
    "href": "07-quarto-intro.html#code-chunks",
    "title": "Reproducable Reports with Quarto",
    "section": "Code chunks",
    "text": "Code chunks\nIn general, an RMD code chunk looks like a markdown code chunk. The key difference between the two is that while a plain markdown code chunk is purely about formatting, the RMD code chunk will by default try to run the code and print any output:\n\nMarkdown code chunk\n## this is just a representation \n## when compiled: nothing happens, only code is printed\nx &lt;- rnorm(1000)\nx\n\n\nRMarkdown code chunk\n\n## this is active R code\n## when compiled: the R code is run, and both code and results are printed\nx &lt;- rnorm(1000)\nx\n\n   [1]  0.8614986278  1.9566984695 -0.0145348441 -0.5432555951  1.4193547964\n   [6] -1.2937519840  2.3488479528  1.1040798907  0.4202046929 -0.5775631567\n  [11] -0.8633097641 -1.2450186078  0.7219204914  0.9114048543  1.1288243492\n  [16]  0.8548178366  0.5451892770  1.7327727799 -0.8885403559  1.9539173304\n  [21] -0.0086142865  0.7703488298 -0.7357779432  0.6594660677  0.9495323683\n  [26] -0.0254788007 -1.0387937088  1.2737686407 -2.2048896615 -1.2570821965\n  [31] -0.9693220183  1.8438153461  0.4766026742 -0.5083777019 -2.0135106252\n  [36]  0.6036639004 -0.9363236527  2.2174986565 -1.2662284598  1.1901746428\n  [41]  0.7224146412  2.0665564840  0.6682408668 -0.9764950118  0.4884789379\n  [46] -0.7203863771 -1.3368255421 -0.3640207933  1.7948684272  0.6049392373\n  [51] -1.4856733844 -0.2429913740  1.6363785805  0.4106506131 -0.4321054360\n  [56]  0.1707175643  0.0407857008 -0.0729334995  0.1583748797  0.3684853025\n  [61] -0.0603582226 -0.5284030253  1.2560105064 -1.1422345264 -0.5973128510\n  [66] -0.6140722577 -2.0220654262  1.2311174227 -1.2171132347 -0.6616153054\n  [71]  2.5827662735 -1.5301871344  0.1952187046 -1.6306011228  0.7757931044\n  [76] -0.0207587794  0.3449217196  1.1309485604  0.3610543154 -1.4555419475\n  [81]  0.2765139718 -0.1368126828 -0.0374978881  0.7064330626  1.0132107759\n  [86] -1.1805506819 -0.6935817499  0.6423476634 -1.2044347570  1.0187475632\n  [91] -0.4164789528  1.3910801701  0.5834354608  0.5984306458  0.2785249741\n  [96]  1.2206153781  0.2054933051 -1.6565862213  2.7197848051  0.5172901351\n [101] -0.5626852019 -1.0158204940  1.8038932300 -0.6946990793 -1.4117740138\n [106]  0.2387991253 -0.0806013775  0.1220009020  0.6080757371  0.2637496570\n [111] -0.4248032518  0.6459824288 -1.6855386183 -0.2744472503  0.2005748380\n [116]  0.2936505490  0.4325436134 -0.3036561429 -2.9233050553 -0.4347197248\n [121] -0.4226788458  1.2506389912  1.7325359476 -0.6101985072  0.7283070320\n [126] -1.4686834016 -0.5256694498 -0.5561242737  0.8208238760 -0.1995292206\n [131] -1.2927041602 -0.9875202465 -1.2475114237  1.0714163210  0.3633609142\n [136] -1.2497260472  1.5230910823  0.6988935984  0.8859433523 -0.7872027787\n [141] -0.6904642039 -0.2752764307 -0.4864043573  0.7584386842  0.5945985267\n [146]  0.4714338448 -0.3542423383 -0.8975537674  0.4278959563 -1.2757090059\n [151]  0.6149906002  0.4804148643 -1.6648207834 -1.3306711255 -0.7685859062\n [156]  0.9182391662 -1.7373613292  0.3351939322  0.2013342675  0.7790140144\n [161]  0.4833572082  0.4701053850  1.4284729268  0.0725028650 -0.5037753212\n [166]  0.5453578114  0.5375697199 -0.7232418472 -0.8216334479 -0.1958374853\n [171]  0.2673931141 -0.9807395753 -0.2639890681  0.1925737098 -0.3302207463\n [176]  0.5377699602  1.3593859423 -1.9436276049 -0.5597564504 -1.1316525051\n [181]  0.0894381941  0.3596523916 -0.9784642695  0.1387670972 -0.1652797569\n [186]  0.0775865583 -0.0270094222 -0.1896360598  0.2696821249 -0.1633892715\n [191]  0.2404281619 -0.4473843611 -0.4465925780 -0.6258477406 -0.0369924802\n [196] -1.5079682381 -0.2011252890 -0.5642303280 -1.3347330102 -1.7057999581\n [201]  1.4931240414 -0.7154678674  1.1561787016 -0.6787247289  0.5172295677\n [206]  0.8407601576  2.3457733871 -0.2604132098 -1.0962614650  0.5081787134\n [211]  1.8735205166 -0.6158694072 -0.7471215408 -0.8525721220 -0.6887783080\n [216]  0.5651734491  0.7807749141  0.8793550301  2.0631377510 -0.3108070104\n [221] -0.5875475894  0.5028220507  1.2434395929 -0.5309003856  2.3060894739\n [226] -0.1836425400  0.9358193027 -0.3646027583 -1.6511545526 -1.1759077902\n [231]  0.0178754214  1.2070216605 -0.7108774436  0.2109644666  2.7816070587\n [236] -0.9957340899 -1.0578418894  0.7052768683 -0.5716623335 -0.7216576612\n [241] -0.3509945020  1.0589219894 -2.0238821180  1.0287088734 -1.7123011003\n [246] -0.1730286317 -0.1433674027 -1.3556663797 -0.1247410626 -0.4352451947\n [251]  1.0792029608 -2.2402215872 -1.2331230294 -1.4978134614  0.0968050810\n [256] -0.0300963864  0.7246958020  0.5753269255  0.8732125253 -0.4088576729\n [261] -2.0267696114  1.2144259481 -1.8052604893 -0.4308129775  3.0130822203\n [266] -0.6099244234 -1.3588632531  0.7667644899 -0.8183312152 -1.3100822476\n [271] -0.1899722905 -1.4392166291  0.8688420195 -0.5625487725  0.8318310071\n [276]  0.5725134531  0.0294375884  0.4867121786 -0.6749196552  0.1726161112\n [281]  0.6554454294 -1.8075136851 -1.2335031546  0.3884976867 -0.3659357330\n [286] -0.3280154085 -1.0982725572 -0.1200626530  0.0510244831 -0.9768877481\n [291]  1.5285914840 -1.2002279251 -0.8765243697 -0.3091305279  0.3716276646\n [296]  2.4363987846  1.8301108214 -1.2558064580  0.5515288188 -0.3941749074\n [301] -0.8938733166  1.1350827951  0.0248953490 -0.3098577041  0.3511730877\n [306] -0.9856736764  1.0124856349 -0.4510186641 -0.2861913641 -0.8075502834\n [311]  0.7926746975 -0.2697766657  0.6813043294 -0.3426878229  0.1818207472\n [316]  1.9214756082  2.1554839748  0.6064410759 -2.8662171792  0.9552261839\n [321] -1.7251682826 -0.8583489171 -1.6087776936 -1.7280656548 -0.0113488788\n [326]  1.3465338872 -0.5315597870 -0.9135838327 -1.3026413671 -0.6101795573\n [331]  0.0230150497 -1.0024780256 -0.6555656534  0.3095014112  1.2570119362\n [336] -0.3224868953  1.1044764314 -1.0908288957 -0.3256519001  0.1860912204\n [341]  0.8906811342 -2.4659136328  0.0824204363  0.3340233496 -0.8253248313\n [346]  0.2615468587  0.0336828396 -1.0985744376 -0.2880330048 -0.6325482268\n [351]  0.0522070004 -1.0694114600  1.1262254578  0.8682822160  0.5667877493\n [356] -0.9612696367 -0.9511632420  0.7826140559 -1.6406655183 -0.0800699320\n [361] -0.1197862817  0.5151773010  0.7825559308 -0.4328061829  1.1551446518\n [366] -0.5421892330  1.3633039325 -0.5780697176 -0.4935159090  0.4105643286\n [371] -2.2123847137 -2.4352555310 -0.7840313132  0.4110724862 -1.0338540245\n [376] -1.2135054016 -0.8461352064  1.2078627041  1.9491641400 -0.1914678988\n [381] -1.3640066307 -0.3745195723  0.9706035732 -0.5214582526  0.0231370744\n [386]  1.0522070964 -0.3178505853 -2.9388430945 -0.7612339499  1.0534782519\n [391]  0.9743762357  1.2935783586  1.1411680336 -0.1047813165  0.5145935699\n [396]  0.6356915298 -2.8202312917  0.5025786484  0.8124883795  1.9918504272\n [401] -0.2936789846 -0.0957105439 -0.3896095951 -1.2277830455  1.1193909545\n [406] -0.6484397307 -0.5867516589  0.5562539079 -0.7744677884  0.0964371073\n [411]  1.7825146775 -0.3114926181 -0.1137202838 -2.5395297420  0.8020369182\n [416] -0.7881290643 -0.0868958701 -1.6028974738 -1.0186353523  1.0975602272\n [421]  0.5107681987 -0.6808345153 -0.5932965850  0.8527518959 -0.1113687008\n [426]  0.0766944926  0.1456942697  0.0444330768  0.0575984809 -2.1760513784\n [431] -0.2859410440 -2.2164544781  0.4989634223  1.2905439806  1.1193529779\n [436] -1.0756947430 -0.8256620942 -1.1051163119 -1.6495920277  0.4530768398\n [441] -0.6033640269 -0.1718295926  1.0082271097 -1.1662591042  0.0559285674\n [446] -1.6110291951  2.4534926817 -1.6704854941 -1.5225368442 -0.2351183553\n [451] -1.6774170028 -0.9146799162 -0.9412464284 -0.4488550384 -2.2701861669\n [456]  1.0004200307 -0.9307549669 -1.3121469120  1.7148063546  1.3733290478\n [461] -0.0483241846 -0.0008001175  0.8719075957  0.9681333508  1.3298219984\n [466] -0.3168681513  0.9579780423 -0.6366060365 -0.0423719409  0.1072067359\n [471]  0.9758518587  1.3869132569  0.9216392690 -1.0327098264 -0.0359606351\n [476]  0.4804335859  1.0471956273 -0.4993101406  0.7948315038  0.0904149089\n [481]  0.8861191874  0.5120015981  0.8409731982 -0.3933432445 -0.4221695844\n [486] -2.6681723241  0.9098489306  0.3695335272 -0.1248809375  0.4999468592\n [491] -0.9563365106  0.8091846854  0.6738776290  0.5387706438  0.6555448912\n [496]  0.0069296988 -0.9650859513  1.4389422577 -1.2428755892 -0.9310051313\n [501]  0.0381866281 -0.6086629583  1.1008644818 -1.2565829030 -0.0646352541\n [506] -0.4017452781 -0.1549543516 -0.1361754467  0.7165970604  2.9854799722\n [511] -1.1031016169  0.1008355153 -0.6068185751  0.9259086615  0.2714337893\n [516] -0.4584675332  0.5798986506 -1.1603249090 -0.4613089793  1.7155663396\n [521] -0.0329615691  0.0225875314  0.1491706276 -0.2658131020  1.4721828053\n [526] -0.3913682483 -1.1658956341 -0.6289033151 -0.2595385688  1.0732594361\n [531] -1.6350135667 -0.2941671575 -0.4721585306 -1.0550076527 -0.4922237125\n [536] -0.0168353908  1.6410029503  2.2906999374  0.3828335748  0.7639805438\n [541]  0.8042497272  0.8871190568  0.4646425262 -0.4660715045  1.4624536043\n [546] -0.6356170235  0.2009716379  0.7919057957  0.4762963853  0.1340782288\n [551] -0.3360757199  0.5824404675 -0.2551589676  1.4331178349 -0.8945516731\n [556] -0.3400248185 -0.6881902946 -1.7791942292  0.6248379318  2.2286105308\n [561] -0.8442329964 -2.3472678388  1.1604051937 -0.9471461262 -0.6211444271\n [566] -0.1856749263 -0.9812538691 -1.4133021260 -1.7738386632 -2.0295068882\n [571]  0.2264325321 -0.0720305343 -0.7626699195  0.5539670502 -0.9953913257\n [576]  0.0378394811  1.2448364744 -0.0955631064  0.5754202519 -0.2382753506\n [581]  0.2303351669 -0.1631054883 -0.5824405455  1.5781058455 -0.9970500836\n [586] -0.4820598036 -0.0546721885  0.6777636823 -1.5419216102  0.6499219838\n [591]  0.6519328245 -0.7815620135  1.1188954462  1.1087726069 -0.5398287207\n [596]  2.2939593869  0.0291747018  0.1215881858 -0.9840805897  0.2335793560\n [601]  0.8112338781 -1.2548992081  0.7716208969 -0.7102115263  0.3030728032\n [606] -0.0719669270  0.4264278888 -2.1712217269  2.0426518522  1.3369102928\n [611] -0.7150050257 -0.3231292124  1.0985597189  1.0735765539 -0.6469930852\n [616]  0.2061791348  1.5669524689 -1.0419161555 -1.6769139789 -0.9615987190\n [621]  0.7953702689 -0.9963396407  0.0729676760  0.8276774033  0.8790368409\n [626]  0.9118513248 -2.1808591654 -0.3218654852  0.7398957799 -0.6162816591\n [631] -1.3227647455 -0.0167722088 -1.1498056899 -1.9592836899 -1.5348552286\n [636] -0.2093528606  1.0877832914 -0.3209951540 -0.6787818134  0.0366060431\n [641] -1.3339403346  3.0546613281  0.2361451299 -0.3443451139  0.8336547687\n [646]  0.6512684310 -0.0368474947 -0.4563187213  0.2465225165 -0.8411609073\n [651]  0.2203328697 -0.5422861529  0.1941254011 -0.1108748494  0.4266565841\n [656] -1.3055327497 -2.3876920137 -1.6888105889 -1.3835223663  0.9672288958\n [661]  0.6653146277  0.0113089212 -0.5523037132  1.0188974047 -0.2259576722\n [666]  0.1273138192 -1.2371894960 -1.9741588986  1.3873398486 -0.3575952682\n [671] -0.2411214127  1.5556753043 -0.5675225007 -0.8165607403  0.3591391020\n [676]  1.7072374856  0.6706325086  0.9512419132 -0.2450716781 -0.4947695107\n [681]  0.7819588622 -1.6268485038 -0.0354752061  0.5114016462 -1.5316268542\n [686] -1.6969097401 -0.9448727565  0.4259888019  0.8784906755  0.2361332137\n [691] -0.4076053874  0.8641923858  0.4485397013 -0.9193411425  0.8586193015\n [696]  0.7466095993  0.5387565343  2.0748317581  0.4806345113 -0.4816257576\n [701]  0.3442907201  0.8838236714  0.4637828577 -0.4502007071  0.2543650323\n [706]  0.5754344256  0.5519560798 -0.5260719310 -0.2270335689  0.9515118938\n [711]  0.5675470054  1.9697287642  0.6158094078 -0.9965986577 -1.0355453826\n [716] -0.3600829354 -0.1645562660 -0.4159505041 -0.3897031338  1.2145181855\n [721]  1.1790236500  1.5456446144 -0.1934965459 -0.5829626981 -1.4142416334\n [726] -2.1738055086  1.3868493170  0.5751572356  1.0694387757 -1.0525236591\n [731] -1.5797631874 -1.0790290032  0.1781568694 -0.9855522011 -0.7886900090\n [736]  0.1823273442  0.2548385535 -0.3984222659  0.8086290337  0.4494643179\n [741]  0.5505269972  0.6357215182 -0.0987915210  1.2273565160  1.9225543234\n [746] -0.0255381178  0.4265694754  1.4627357920 -0.9724854999 -0.0598876937\n [751]  0.7575033307 -1.1609072911  0.2185336994 -1.1704153933  0.5093661351\n [756] -0.0967016652  1.4602091078 -0.7169349101 -1.1268814174  1.8097290547\n [761] -1.2710401802 -0.1393903261  1.1729281560  0.2309728336  1.6058248488\n [766] -1.1291758239  2.1129340916  1.7826791582  0.7430866799 -0.8253941411\n [771] -0.6591205949  1.2768974283  1.8599505569  0.3194592734  0.2786081339\n [776]  0.5381598237 -0.8324299839 -0.2783403310 -0.5997070844 -0.2650738836\n [781] -1.0847933656  0.6615217737  0.4143818323 -0.6589567162  0.6709519153\n [786]  1.9695983380 -0.9375701309 -0.5618998503 -0.2828064328 -0.7901797584\n [791]  0.7533844516  0.2435434130  1.5213963307 -0.1566801753 -0.9973110226\n [796]  0.8056927087  1.1775603279 -0.0323934695 -0.4010859887  0.0418607739\n [801]  1.9580991747  0.1097239450  0.7327799822 -0.4727044867 -1.8394891031\n [806]  0.4017489155 -0.1872154597  0.0691733022  0.6716978227  0.3106073709\n [811]  0.3241580751 -0.3823100336  0.1922618497  1.0095648472 -0.4449254891\n [816] -0.0696772338 -1.4756150623 -1.2457798698  0.1697021499 -1.0196880732\n [821]  0.0631137159 -1.0206648724  0.7751262467  0.2964885540 -0.1406508741\n [826]  0.1584503294  0.6310702915 -0.6829887699  0.1950507147  0.8839629220\n [831]  0.7139407875 -0.1552366354  1.1560124430 -0.2109748465 -0.7713091486\n [836] -0.0046734999 -1.1477402601 -1.1485113922 -0.6295214789 -1.2879038960\n [841]  0.4007127558 -1.6942883469  1.9321747733  0.6344868564  1.0023186490\n [846] -1.9241143531  0.0743880791  0.3131832654  0.9138858621 -0.3770558704\n [851]  0.2093081152 -0.0137569819  1.6535245553 -1.2131743778  1.5891851381\n [856]  0.1468413421 -1.5007414375 -0.1131214055 -0.1297834864 -0.4630327371\n [861]  1.9202462284  0.2047994589  0.7424810697  0.9154592929  0.9554906356\n [866] -0.2960565985  0.4375206275  1.8628489184 -0.7613875235 -1.5994906794\n [871]  0.6594747292  0.7730742805 -0.7842414754  1.3594801752 -1.0353121798\n [876] -1.2527067111  0.2679831362  0.6024957667 -0.1356313762 -1.9896474522\n [881] -0.4762004696 -3.1138228110  0.1157595565 -0.5409597289  0.6151936568\n [886] -0.0598299189 -1.8470601328  0.1849867990 -0.4943209434 -0.5919981837\n [891] -0.7935354211  0.5569240960 -0.2235815320  0.8302782600  0.7003531470\n [896]  0.6484500627 -2.0709274966 -0.7831114477  0.8767684835  0.6130454324\n [901]  0.1195208951  0.8714646638 -0.4554497266  1.3463628681  0.7167207883\n [906]  0.8090087193  0.5741336562  0.6517677665  0.1350809869  2.0908620745\n [911]  0.6894353683  0.3769174561 -0.5461101662  0.3755413259 -0.1959746474\n [916]  0.0776918470  0.1526354294  0.5439289887 -0.5736578319 -0.0908506334\n [921] -1.0154913710  0.0938663952  1.5403991915  1.0996456034 -0.0587689914\n [926] -0.2020864081  1.3341063600  0.8333417046  0.3606725237  0.1785125813\n [931] -0.8371061619  0.7054921630 -1.4704387284  1.5560357039 -0.2569012594\n [936] -1.1483010924 -0.0176125135 -1.2862003529  0.1549718851 -0.6854393845\n [941] -0.8361634481 -0.0695341856 -0.6291895731  0.6073686829  0.4773526820\n [946] -0.7931968844 -0.8390375031  0.2987617478 -0.9512877809 -0.8182305699\n [951] -1.1410273609 -0.6950102773 -0.1481613836  0.0939974571  0.2270088671\n [956]  1.8778947071  0.4319414267 -1.2800700753  1.0664868709 -0.1529305484\n [961]  1.2001664500  0.6549199885  0.1859376826  0.1726076912 -0.2635891343\n [966] -0.7069522412  0.6554307633 -0.1042290523 -0.2909642231  0.2928348854\n [971]  1.7557364943  1.1876672057  0.5403322483 -0.8605104300  1.6149404085\n [976] -0.2047024508 -0.3872343280 -1.1909211664  0.3468866607  0.1181553735\n [981]  1.5473815109 -0.8571059701 -0.1005949058 -0.0357884827  2.3757281715\n [986]  0.0674967576  0.0961917182  0.5953164200 -0.9060161874 -0.7397378844\n [991] -1.3616045723 -0.8989769618 -1.0847044955 -0.5523419473 -0.8678555122\n [996]  0.2993580123 -0.1673696786  0.4606786160  0.0838811704 -0.2073994213\n\n\nSee the difference? It’s subtle, but notice that the RMD chunk places braces around the r after the tick marks: {r}. In a normal markdown document, the braces won’t mean anything. But in an RMD document, it’s the difference between just printing the code and running the code before printing the code and its output.\n\n\nCode chunk options\nIn our first code chunk, notice how we still load our libraries and set our file paths. For the libraries, we need to load knitr with library(knitr) in addition to whichever libraries we need for our analysis. As usual, we also load the tidyverse.\nIn addition to our normal analysis setup, notice that we add knitr-specific options in two places.\n\nLocal code chunk options (only affect this code chunk)\nFirst, we can set local code chunk options within the braces that start the code chunk. These options will only affect this particular code chunk.\nAfter r, the first word is the name of the chunk. I’ve called it setup, since that’s what this chunk is doing, but you can name it anything you want. It’s not strictly necessary to name your chunks, but it can come in handy as your documents become more complex: if you get an error, it’s much easier to find data_input chunk than unnamed_chunk_38. NOTE that all named chunks need to be unique or your document will not compile. If really like a particular chunk and want to reuse it, you can always add a number at the end: data_input_1, data_input_2, data_input_3, etc.\nThere are a lot of options you can set for your chunks. Here we set the following:\n\necho=F (FALSE): don’t repeat this code in output\ninclude=F (FALSE): run code, but don’t include output (unless a plot)\nmessage=F (FALSE): don’t output any messages\nwarning=F (FALSE): don’t output any warnings\nerror=F (FALSE): don’t output any errors\n\nAs of knitr 1.35, you can also include chunk options in rows below the opening line using the #| symbol pair. We could rewrite our example options using:\nHowever you choose to include them, these options keep our chunk from echoing the input code into our document and prevents any output. Basically, silence. Sometimes we want our code to echo; sometimes we want output. But since we are making a report, we generally want the underlying code to remain hidden. Readers of our report should only see the write up and any relevant tables and figures — but not all the hard coding we did to make them!\n\n\nGlobal code chunk options (affect all code chunks)\nAfter this first chunk, we can save some typing by setting these options for the rest of the document using knitr::opt_chunks$set(). Notice that we include the same settings as above plus a few more:\n\nfig.path: path + prefix for all figures (put them in our /figures folder and add \"ts-\" to the name)\ndpi (dots per inch): the print quality of our figures; 300 dpi is a nice standard for print (72dpi is sufficient for most web output)\nout.width: our figures should fill the line width; if it’s an 8.5 by 11 inch page with 1 inch margins, then a width of 7.5 inches\ncomment: if we return code output, don’t prepend with # or anything — just the output.\n\nThere are other options we can use. We can also override these setting as necessary for individual code chunks using local settings like we did in the first chunk (as you’ll see below). The main idea with the set up code chunk is to get our document settings as close as possible to the way we generally want them.\n\n\n\nChunk to chunk\nBelow, you see two code chunks with some Markdown text in the middle.\nAn important thing to remember is that your coding environment carries from chunk to chunk, meaning that if you read in data in the code chunk (named input) as df, then df will still be available to you in the next chunk (named table_all) after writing some Markdown text. This means that you can still organize your RMD scripts like your R scripts (no need to do everything at once in a single huge chunk).\n\n\nMake a nice(r) table with kable()\nSince we want to show all of our data (which isn’t very big in this case) and because the data frame df already is organized in the way we want to show the data (school by year with different columns for each test), we can just print out the data frame. To be clear, often our data will be too big to do this, but in this instance, we are okay.\nWe could just print the data frame by calling df in a chunk. But to make it look nicer with a better format, we use kable() which is part of knitr.\nEven using mostly default options, kable() will make a nice looking table for us. We add digits = 0 to make sure that we only show whole numbers and we change our column names to something nicer (leaving a blank \"\" for the school name column, which is obvious). Notice that in the chunk braces we add include = T so that the output — our table — for just this one code chunk will be printed. This is an example of using local code chunk options to override global chunk options.\n\n\nInline code\nYou can also call R code inline, that is, R code that sits outside of code chunks proper and instead is mixed in with your Markdown text.\nInside the code chunk called table_averages we do three things:\n\nUse dplyr to munge our data to get averages\nStore names and scores for high test scores in distinct well named objects:\n\nhi_&lt;test&gt;_sch: school name with highest average math/read/science score\nhi_&lt;test&gt;_scr: highest average math/read/science score\n\nMake/print table using kable()\n\nIn the Markdown text below this code chunk, we call the values using the inline code method\n`r `\nthat is, single back tick, an r, the code we want, then closing back tick. We also pipe the object value to the round() function so that we don’t return averages with extra and unnecessary decimal points. We could have simply run all the code inline (included what we did in step 2 of the code chunk above), but that would have made for extra messy code.\nWhy do this? One reason is that being able to incorporate data-driven values directly in your test is very powerful. Imagine you need to reproduce the same report on a monthly or quarterly basis when data are updated. Part of the written report includes values directly taken or calculated from the data. Rather than update these “magic numbers” each time (potentially missing some), you can use inline R code like we’ve done here. All you need to do then is update the data and recompile the report. Voila! Everything is properly updated.\nTaking it a step further, you can include all kinds of ifelse() logic to make complex dynamic documents. If the value of X is equal to or greater than the value of Y, then print “equals or exceeds”; else print “remains less than”. Be aware, however, that your document text still needs to make sense. It can be difficult enough writing one clear sentence; having to write a sentence that will remain coherent despite variable inputs can be very tough!\n\n\nFigures\nFinally, making figures is pretty much the same as making tables:\nHaving reshaped our original data frame long (df_long), we make a figure just as we’ve done in the past — with some formatting improvements to make it nicer looking. While it’s not strictly necessary to store the figure in an object (p) that we then call, it works just fine.\nNotice that again added include = T to the chunk brace. Because we added figure options to opt_chunks$set() in the setup code chunk, this figure (as well as the next one) is sized so that it fills up the page width (with height determined as a ratio of that width) and printed at 300 dpi quality. If you look in the figures folder, you’ll see the figure named ts-fig_unadjusted, which is the prefix we set above with the name of the code chunk.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#text",
    "href": "07-quarto-intro.html#text",
    "title": "Reproducable Reports with Quarto",
    "section": "Text",
    "text": "Text\nThroughout our RMD file, we’ve include Markdown text. This text lives outside of the code chunks and is always printed in the final document. It follows normal Markdown text rules, but can have R code placed inline, as we saw above.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#writing-an-rmd-document",
    "href": "07-quarto-intro.html#writing-an-rmd-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Writing an RMD document",
    "text": "Writing an RMD document\nJust as when you write a plain R script, your progress from initial RMD draft to final product will be iterative. While you can run R code from inside code chunks just as you’ve been all semester, you may find it useful to start your analyses in plain R files first and only add them to an RMD document later.\nFor big projects, such as dissertation, it also doesn’t make much sense to put everything — data reading, cleaning, analysis, table/figure making — inside a single RMD document. You have to redo your entire workflow each time you compile! For large projects, it might make sense to do all the heavy lifting in separate R scripts — saving cleaned up data sets, tables, and figures along the way — and putting all the pre-establish pieces together at the end. But for small projects, such a descriptive policy report, a single RMD document might suffice.",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#installing-latex",
    "href": "07-quarto-intro.html#installing-latex",
    "title": "Reproducable Reports with Quarto",
    "section": "Installing LaTeX",
    "text": "Installing LaTeX\nLaTeX is a document typesetting system/language. While it’s probably best known for its ability to nicely typeset mathematical equations, LaTeX works really well quantitative research workflows. That said, it can be difficult to install and work with.\nWe’ll use LaTeX later in the semester so that you can make nice PDF reports. The good news is that you won’t really need to interact with LaTeX at all to do so — other than to install it now.\nSince you don’t need a full TeX distribution on your computer, you can most likely get by using the TinyTeX distribution that we can install directly from R. If you want a full version of TeX on your computer (NOTE: It’s very large), then skip to the full installation for your computer.\n\nTinyTex\nOnce you’ve installed R and RStudio, open RStudio and type the following in the Console:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\nThis will install the tinytex R package and then install the TinyTeX distribution (it may take a minute or two).",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "05-viz-i.html#setup",
    "href": "05-viz-i.html#setup",
    "title": "I: Basics",
    "section": "Setup",
    "text": "Setup\nWe’re using two libraries today:\n\nggplot2\nhaven\n\nThe ggplot2 library is part of the tidyverse, so we don’t need to load it separately (we can just use library(tidyverse) as always).\nWe’re also going to use haven, which allows us to read in data files from other software such as SPSS, SAS, and Stata. We’ll use it to read in a Stata (*.dta) version of the small HSLS data we’ve used before. The Stata version, unlike the plain CSV version, has labels for the variables and values. These will be useful when plotting.\nThough haven is part of the tidyverse (and should have been installed when you installed tidyverse), we’ll have to explicitly call it.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(haven)\n\nNote that since we have two data files this lesson, we’ll give them unique names instead of the normal df:\n\ndf_hs := hsls_small.dta\ndf_ts := all_schools.csv\n\n\n## ---------------------------\n## input data\n## ---------------------------\n\n## read_dta() ==&gt; read in Stata (*.dta) files\n## read_csv() ==&gt; read in comma separated value (*.csv) files\ndf_hs &lt;- read_dta(file.path(\"data\", \"hsls-small.dta\"))\ndf_ts &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-base-r",
    "href": "05-viz-i.html#plots-using-base-r",
    "title": "I: Basics",
    "section": "Plots using base R",
    "text": "Plots using base R\nEven though new graphics libraries have been developed, the base R graphics system remains powerful. The base system is also very easy to use in a pinch. When I want a quick visual of a data distribution that’s just for me, I often use base R.\nNote that for the next few plots, I’m not much concerned with how they look. Specifically, the axis labels won’t look very nice. We could spend time learning to make really nice base R plots for publication, but I’d rather we spend that time with ggplot2 graphics.\nAlso note that we’ll switch to using the base R data frame $ notation to pull out the columns we want. If you need some more information on using $ notation, check out the supplemental lesson on data wrangling with base R.\n\nHistogram\nFor continuous variables, a histogram is a useful plot. Though the hist() function has many options to adjust how it looks, the defaults work really well if you just want a quick look at the distribution.\n\n## histogram of math scores (which should be normal by design)\nhist(df_hs$x1txmtscor)\n\n\n\n\n\n\n\n\n\nQuick exercise\nCheck the distribution of the students’ socioeconomic score (SES).\n\n\n\nDensity\nDensity plots are also really helpful. R doesn’t have single density plot function, but you can get a density plot in one of two ways, each of which will give a slightly different result.\nFirst, you can adjust the hist() function to add the freq = FALSE argument. It looks like the first histogram above, but notice that the y-axis now represents density rather than counts.\n\n## density plot of math scores with hist() function\nhist(df_hs$x1txmtscor, freq = FALSE)\n\n\n\n\n\n\n\n\nSecond, you can plot() the density() of a continuous variable. Unlike hist(), however, density() doesn’t automatically ignore missing values, so we have to tell it to remove NAs using the na.rm = TRUE argument (a common argument for base R functions that’s useful to remember).\n\n## density plot of math scores\n## read inside out: get density value, then plot values\nplot(density(df_hs$x1txmtscor, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nQuick exercise\nPlot the density of SES. Next, add the col argument in plot() to change the color of the line to \"red\": plot(density(df_hs$x1txtmscor, na.rm = TRUE), col = \"red\").\n\n\n\nBox plot\nA box plot will let you see the distribution of a continuous variable at specific values of another discrete variable. For example, test scores ranges at each student expectation level.\nCall a box plot using the boxplot() function. This one is a little trickier because it uses the R formula construction to set the continuous variable against the discrete variable. The formula uses a tilde, ~, and should be constructed like this:\n\n&lt;continuous var&gt; ~ &lt;discrete var&gt;\n\nNotice how we can use the data = df_hs argument instead of adding df_hs$ in front of the variable names. This saves us some typing.\n\n## box plot of math scores against student expectations\nboxplot(x1txmtscor ~ x1stuedexpct, data = df_hs)\n\n\n\n\n\n\n\n\nFrom the boxplot, we can see that math test scores tend to increase as students’ educational expectations increase (remember that 11 means “I don’t know [how far I’ll go in school]”), though there’s quite a bit of overlap in the marginal distributions.\n\n\nScatter\nPlot two continuous variables against one another using the base plot() function. There are two primary ways to make a scatter plot using plot():\n\nplot(x, y)\nplot(y ~ x)\n\nWith both, x is the variable that will go on the x-axis and y the one that will go on the y-axis. It’s really a matter of which makes sense to you. We’ll use the first.\n\n## scatter plot of math against SES\nplot(df_hs$x1ses, df_hs$x1txmtscor)\n\n\n\n\n\n\n\n\nWhile the data seem to show a positive correlation between socioeconomic status and math test score, there’s also quite a bit of variation in that association (notice that the cloud-like nature of the circles).\n\nQuick exercise\nRerun the above plot, but this time store it in an object, plot_1. Next, make the same plot, but this time use the second formula construction (~) — store it in an object, plot_2. Visually compare the two, but for a more formal test, use identical(plot_1, plot_2) on the two plot objects to prove they are the same.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-ggplot2",
    "href": "05-viz-i.html#plots-using-ggplot2",
    "title": "I: Basics",
    "section": "Plots using ggplot2",
    "text": "Plots using ggplot2\nggplot2 is my — and many R users’ — primary system for making plots. It is based on the idea of a grammar of graphics. Just as we can use finite rules of a language grammar to construct an endless number of unique sentences, so too can we use a few graphical grammatical rules to make an endless number of unique figures.\nThe ggplot2 system is too involved to cover in all of its details, but that’s kind of the point of the grammar of graphics: once you see how it’s put together, you can anticipate the commands you need to build your plot.\nWe’ll start by covering the same plots as above.\n\nHistogram\nAs the main help site says, all ggplot2 plots need three things:\n\n[data]: The source of the variables you want to plot\n[aesthetics]: How variables in the data map onto the plot (e.g., what’s on the x-axis? what’s on the y-axis?)\n[geom]: The geometry of the figure or the kind of figure you want to make (e.g., what do you want to do with those data and mappings? A line graph? A box plot?…)\n\nWe’ll start by making a histogram again. To help make these pieces clearer, I’ll use the argument names when possible. The first function, which initializes the plot is ggplot(). Its first argument is the data.\nThe aesthetic mappings, that is, which variables go where or how they function on the plot, go inside the aes() function. Since we only have one variable, x1txmtscor, it is assigned to x.\nIf we assign this first part to an object, p, and print by calling the object…\n\n## init ggplot \np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor))\np\n\n\n\n\n\n\n\n\n…nothing! Well, not nothing, but no histogram. That’s because the plot object p knows the data and the key variable mapping but doesn’t know what do with them. What do we want?\nSince we want a histogram, we add the geom_histogram() function to the existing plot object with a plus sign(+). Once we do that, we’ll try to print the plot again…\n\n## add histogram instruction (notice we can add pieces using +)\np &lt;- p + geom_histogram()\np\n\n\n\n\n\n\n\n\nSuccess!\nLet’s repeat it the whole process, but without the middle step:\n\n## create histogram using ggplot\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram()\np\n\n\n\n\n\n\n\n\nAs you can see, the code to make a ggplot2 figure looks a lot like what we’ve seen with other tidyverse libraries, e.g. dplyr. The key difference between ggplot2 and dplyr, however, is that while dplyr uses the pipe (|&gt;) to connect different functions, ggplot2 uses a plus sign (+).\nIt may help you remember the difference:\n\ndplyr moves output from left to the input in the right and so needs a pipe (|&gt;)\nggplot2 adds layer upon layer to build up the final figure and so needs a plus sign (+)\n\n\n\nDensity\nUnlike the base R graphics system, ggplot2 does have a density plotting command, geom_density(). Instead of building up the figure piecemeal, we’ll go ahead and chain the geom to the first command and print.\nNotice how the function chain is the mostly the same as above, but (1) written in a single linked chain and (2) using a different geom_*() command at the end to indicate that we want something different.\n\n## density\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_density()\np\n\n\n\n\n\n\n\n\n\nQuick exercise\nMake a density plot of SES.\n\nIf we want to superimpose the density plot over the histogram, we only need chain the two commands together with a slight modification in how the histogram is made. This way, the histogram and the density will be on the same scale.\nThe change happens in the geom_histogram() function, where we add a new mapping: aes(y = ..density..). (NOTE: this is similar to what we did above in base R to make a histogram on a density scale.)\n\n## histogram with density plot overlapping\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram(mapping = aes(y = ..density..)) +\n    geom_density()\np\n\n\n\n\n\n\n\n\nIt worked, but it’s not the greatest visual since the colors are the same and the density plot is thin with no fill.\nAdding to what came before, the geom_histogram() and geom_density() both take on new arguments that change the defaults. Now the resulting plot should look nicer and be easier to read.\n\n## histogram with density plot overlapping (add color to see better)\np &lt;- ggplot(data = df_hs, mapping = aes(x = x1txmtscor)) +\n    geom_histogram(mapping = aes(y = ..density..),\n                   color = \"black\",\n                   fill = \"white\") +\n    geom_density(fill = \"red\", alpha = 0.2)\np\n\n\n\n\n\n\n\n\n\nQuick exercise\nTry changing some of the arguments in the last plot. What happens when you change alpha (keep the value between 0 and 1)? What does the color argument change? And fill? What happens if you switch the geom_*() functions, call geom_histogram() after you call geom_density()?\n\nA key thing to note about arguments is that when the are outside of the aes(), they apply uniformly to the whole geom (e.g. all the histogram bars are white with a black outline, the density is light red). When you want some aesthetic of the figure to change as a function of the data, you need to put it inside aes(). We’ll see this in the next plot.\n\n\nTwo-way\nPlotting the difference in a continuous distribution across groups is a common task. Let’s see the difference between student math scores between students with parents who have any postsecondary degree and those without.\nSince we’re using data that was labeled in Stata, we’ll see the labels when we use count()\n\n## see the counts for each group\ndf_hs |&gt; count(x1paredu)\n\n# A tibble: 7 × 2\n  x1paredu                                         n\n  &lt;dbl+lbl&gt;                                    &lt;int&gt;\n1  1 [Less than high school]                    1010\n2  2 [High school diploma or GED]               5909\n3  3 [Associate's degree]                       2549\n4  4 [Bachelor's degree]                        4102\n5  5 [Master's degree]                          2116\n6  7 [Ph.D/M.D/Law/other high lvl prof degree]  1096\n7 NA                                            6721\n\n\nWe can see that all values of x1paredu greater than 2 represent parents with some college credential. Since we want only two distinct groups, we can use the operator &gt;= to make a new 0/1 binary variable. If a value of x1paredu is above 3, then the new indicator pared_coll will be 1; if not, 0.\nNOTE that in the Stata version of hsls_small, all the missing values, which are normally negative numbers, have already been properly converted to NA values. That’s why we see a count column for NA and not labels for missingness that we might have expected based on prior lessons.\nThe ggplot() function doesn’t need to use our full data. In fact, our data needs to be set up a bit differently to make this plot. We’ll make a new temporary data object that only has the data we need.\n\n## need to set up data\nplot_df &lt;- df_hs |&gt;\n    ## select the columns we need\n    select(x1paredu, x1txmtscor) |&gt;\n    ## can't plot NA so will drop\n    drop_na() |&gt;\n    ## create new variable that == 1 if parents have any college\n    mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0)) |&gt;\n    ## drop (using negative sign) the original variable we don't need now\n    select(-x1paredu) \n\n## show\nhead(plot_df)\n\n# A tibble: 6 × 2\n  x1txmtscor pared_coll\n  &lt;dbl+lbl&gt;       &lt;dbl&gt;\n1 59.4                1\n2 47.7                1\n3 64.2                1\n4 49.3                1\n5 62.6                1\n6 58.1                1\n\n\nTo plot against the two groups we’ve made, we need to add it to the aesthetic feature, aes(). The math score, x1txmtscor, is still mapped to x, but since we want two side-by-side histograms, we set the fill aesthetic to our new indicator variable. So the function knows that it’s a group (and not just a continuous number with only two values), we wrap it in the factor() function.\nFinally, we add some changes to the geom_histogram() function so that each group is on the same scale.\n\n## two way histogram\np &lt;- ggplot(data = plot_df,\n            aes(x = x1txmtscor, fill = factor(pared_coll))) +\n    geom_histogram(alpha = 0.5, stat = \"density\", position = \"identity\")\np\n\n\n\n\n\n\n\n\nBy assigning pared_coll to the fill aesthetic, we can see a difference in the distribution of math test scores between students whose parents have at least some college and those whose parents do not.\n\nQuick exercise\nRemove some of the new arguments in geom_histogram(). How does the resulting plot change? Remove the factor() function from around pared_coll: what happens?\n\n\n\nBox plot\nBy this point, you’re hopefully seeing the pattern in how ggplot2 figures are put together. To make a box plot, we need to add a y mapping to the aes() in addition to the x mapping. We’ve also added the same variable to fill as we did to x. We do this so that in addition to having different box and whisker plots along the x-axis, each plot is given its own color.\n\n## box plot using both factor() and as_factor()\np &lt;- ggplot(data = df_hs,\n            mapping = aes(x = factor(x1paredu),\n                          y = x1txmtscor,\n                          fill = as_factor(x1paredu))) +\n    geom_boxplot()\np\n\n\n\n\n\n\n\n\nIn a way, this plot is similar to the dual histogram above. But since we want to see the distribution of math scores across finer-grained levels of parental education, the box and whisker plot is clearer than trying to overlap seven histograms.\n\nQuick exercise\nChange the as_factor() and factor() functions above. How does the plot change?\n\n\n\nScatter\nTo make a scatter plot, make sure that the aes() has mappings for the x axis and y axis and then use geom_point() to plot. To make things easier to see (remembering the cloud from the base R plot above), we’ll reduce the data to 10% of the full sample using sample_frac() from dplyr. We’ll also limit our 10% to those who aren’t missing information about student education expectations\n\n## sample 10% to make figure clearer\ndf_hs_10 &lt;- df_hs |&gt;\n    ## drop observations with missing values for x1stuedexpct\n    drop_na(x1stuedexpct) |&gt;\n    ## sample\n    sample_frac(0.1)\n\n## scatter\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point()\np\n\n\n\n\n\n\n\n\nNow that we have our scatter plot, let’s say that we want to add a third dimension. Specifically, we want to change the color of each point based on whether a student plans to earn a Bachelor’s degree or higher. That means we need a new dummy variable that is 1 for those with BA/BS plans and 0 for others.\nWe can look at the student base year expectations with count():\n\n## see student base year plans\ndf_hs |&gt;\n    count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct                                     n\n   &lt;dbl+lbl&gt;                                    &lt;int&gt;\n 1  1 [Less than high school]                      93\n 2  2 [High school diploma or GED]               2619\n 3  3 [Start an Associate's degree]               140\n 4  4 [Complete an Associate's degree]           1195\n 5  5 [Start a Bachelor's degree]                 115\n 6  6 [Complete a Bachelor's degree]             3505\n 7  7 [Start a Master's degree]                   231\n 8  8 [Complete a Master's degree]               4278\n 9  9 [Start Ph.D/M.D/Law/other prof degree]      176\n10 10 [Complete Ph.D/M.D/Law/other prof degree]  4461\n11 11 [Don't know]                               4631\n12 NA                                            2059\n\n\nWe see that x1stuedexpct &gt;= 6 means a student plans to earn a Bachelor’s degree or higher. But since we need to account for the fact that 11 means “I don’t know”, we need to make sure our test includes x1stuedexpct &lt; 11. Remember from a prior lesson that we can connect these two statements together with the operator &. Let’s create our new variable.\n\n## create variable for students who plan to graduate from college\ndf_hs_10 &lt;- df_hs_10 |&gt;\n    mutate(plan_col_grad = ifelse(x1stuedexpct &gt;= 6 & x1stuedexpct &lt; 11,\n                                  1,        # if T: 1\n                                  0))       # if F: 0\n\nNow that we have our new variable plan_col_grad, we can add it the color aesthetic, aes() in geom_point(). Don’t forget to use factor() so that ggplot knows to treat it like a group!\n\n## scatter\np &lt;- ggplot(data = df_hs_10,\n            mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5)\np\n\n\n\n\n\n\n\n\n\nQuick exercise\nChange how you make plan_col_grad so that instead of 1 and 0, you use ‘yes’ and ‘no’. Make your figure again. What changes?\n\n\n\nFitted lines\nIt’s often helpful to plot fitted lines against a scatter plot to help see the underlying trend. There are a number of ways to do this with the geom_smooth() function.\n\nLinear fit\nSetting method = lm in geom_smooth() will fit a simple straight line of best fit with 95% confidence interval shaded around it.\n\n## add fitted line with linear fit\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = lm)\np\n\n\n\n\n\n\n\n\n\n\nLinear fit with polynomials\nIn addition to the method, we can add a formula to allow the fitted line to take a non-linear shape. Using the aes() values of x and y, the argument below uses an R formula, y ~ x, but with the addition of the poly() function. Setting the second argument in poly() to 2 gives the line an extra quadratic term, which allows it to take a more curved shape.\n\n## add fitted line with polynomial linear fit\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = lm, formula = y ~ poly(x,2))\np\n\n\n\n\n\n\n\n\n\nQuick exercise\nChange the value in poly() to higher numbers. How does the line change?\n\n\n\nLoess\nFinally, we can skip trying to adjust a linear line and just fit a LOESS curve, which is a smooth line produced by fitting a large number of local polynomial regressions on subsets of the data.\n\n## add fitted line with loess\np &lt;- ggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n    geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n    geom_smooth(method = loess)\np\n\n\n\n\n\n\n\n\nTo be clear, these semi-automated lines of best fit should not be used to draw final conclusions about the relationships in your data. You will want to do much more analytic work to make sure any correlations you observe aren’t simply spurious and that fitted lines are telling you something useful. That said, fitted lines via ggplot2 can be useful when first trying to understand your data or to more clearly show observed trends.\n\n\n\nLine graph\nWhen you want to show changes in one variable as a function of another variable, e.g., changes in test scores over time, then a line graph is often a good choice. Since our hsls_small data is cross-sectional, we’ll shift to using our school test score data. Remember that the test score data show three sets of test scores (math, science, and reading) for four schools over a period of six years. This data frame is long in year, but wide in test type. It looks like this:\n\n## show test score data\ndf_ts\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\nTo keep it simple for our first line plot, we’ll filter our plot data to keep only scores for one school. Notice how we can do that directly with pipes inside the ggplot() function. We want to see changes in test scores over time, so we’ll map year to the x axis and, for now, math to the y axis. To see a line graph, we add geom_line().\n\n## line graph\np &lt;- ggplot(data = df_ts |&gt; filter(school == \"Spottsville\"),\n            mapping = aes(x = year, y = math)) +\n    geom_line()\np\n\n\n\n\n\n\n\n\n\nQUICK EXERCISE\nChange the school in filter() to “East Heights” and then “Bend Gate”.\n\nEasy enough, but let’s say that we want to add a third dimension — to show math scores for each school in the same plot area. To do this, we can map a third aesthetic to school. Looking at the help file for geom_line(), we see that lines (a version of a path) can take colour, which means we can change line color based on a variable.\nThe code below is almost exactly the same as before less two things:\n\nWe don’t filter df_ts this time, because we want all schools\nWe add colour = school inside aes()\n\n\n## line graph for math scores at every school over time\np &lt;- ggplot(data = df_ts,\n            mapping = aes(x = year, y = math, colour = school)) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nThis is nice (though maybe a little messy at the moment) because it allows us to compare math scores across time across schools. But we have two more test types — reading and science — that we would like to include as well. One approach that will let us add yet another dimension is to use facets.\n\n\nFacets\nWith facets, we can put multiple plots together, each showing some subset of the data. For example, instead of plotting changes in math scores across schools over time on the same plot area (only changing the color), we can use facet_wrap() to give each school its own little plot. You might hear me or other refer to plots like this a showing small multiples or as small multiples figures.\nCompared to the code just above, notice how we’ve removed colour = school from aes() and included facet_wrap(~ school). The tilde (~) works like the tilde in plot(y ~ x) above: it means “plot against or by X”. In this case, we are plotting math test scores over time by each school.\n\n## facet line graph\np &lt;- ggplot(data = df_ts,\n            mapping = aes(x = year, y = math)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nIs this faceted plot better than the color line plot before it? To my eyes, it’s a little clearer, but not so much so that I couldn’t be convinced to use the first one. Whether you use the first or the second would largely depend on your specific data and presentation needs.\nFaceting has a clearer advantage, however, when you want to include the fourth level of comparison: (1) scores across (2) time across (3) schools from (4) different tests. To make this comparison, we first need to reshape our data, which is only long in year, to be long in test, too. As we’ve already seen in a past lesson, we’ll use pivot_longer() to place each test type in its own column (test) with the score next to it.\n\n## reshape data long\ndf_ts_long &lt;- df_ts |&gt;\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"), # cols to pivot long\n                 names_to = \"test\",                 # where col names go\n                 values_to = \"score\")               # where col values go\n\n## show\ndf_ts_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQUICK EXERCISE\nIf we have 4 schools, 6 years, and 3 tests, how many observations should df_ts_long have in total? Does it?\n\nWith our reshaped data frame, we now reintroduce colour into the aes(), this time set to test. We make one other change: y = score now, since that’s the column for test scores in our reshaped data. All else is the same.\n\n## facet line graph, with colour = test and ~school\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nWell, it worked…we can see each school’s different test score trends over time, with each school in its own facet and test scores set to a different color. But the result is a bit underwhelming. Because the different test types are such different scales (even though they are normed within themselves), within-test changes seem rather flat over time.\nLet’s try something different: in the next figure, we’ll swap the variables we give to colour and within facet_wrap(). This means that each test should have its own facet and each line will represent a different school.\n\n## facet line graph, now with colour = school and ~test\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = school)) +\n    facet_wrap(~ test) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nOkay. New problem. While it’s maybe a little easier to see same-test differences across schools over time, the different scales of the tests still make the figure less useful than we might hope. It’s not that the students are way better at science than reading; it’s just that the tests are scaled differently. Someone quickly reading this figure, however, might make that incorrect interpretation.\nOne thing we can do is change the y-axis for each facet. The default is to keep the y-axis scale the same. By adding scales = \"free_y\" to facet_wrap(), we’ll let each test have its own y-axis scale.\nHaving different axis scales side-by-side can be confusing, however (this is why the default is to keep them the same). To mitigate that confusion, we’ll also rearrange the facets so they stack rather than sit side by side. To do this, we’ll add ncol = 1 to facet_wrap(). This says our facets have to stick to one column, effectively meaning they will stack vertically.\n\n## facet line graph, with one column so they stack\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score, colour = school)) +\n    facet_wrap(~ test, ncol = 1, scales = \"free_y\") +\n    geom_line()\np\n\n\n\n\n\n\n\n\nThat looks better! But we can do even better than that…\nCurrently, each test score is on its own normed scale. While our new figure allows us to make comparisons across schools over time within test, it’s more difficult to make a good comparison between tests. For example, East Heights has a little over 20 point drop in reading scores from 1981 to 1982 and about the same drop in science scores from 1982 to 1983. How should we think about these drops? Are they about the same or is one drop relatively bigger than the other?\nTo better answer this question, we could re-standardize each test score so that it is centered at 0 and a one unit change is equal to 1 standard deviation difference in score. We’ll use mutate() to create a new variable score_std. Because we group_by() test, score_std will be standardized within test.\n\n## rescale test scores\ndf_ts_long &lt;- df_ts_long |&gt;\n    group_by(test) |&gt;\n    mutate(score_std = (score - mean(score)) / sd(score)) |&gt;\n    ungroup()\n\nWe’ll repeat the same code as before, but this time substitute y = score_std. Because all tests are on the same standardized scale, we can also drop scales = \"free_y\".\n\n## facet line graph with standardized test scores\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std, colour = school)) +\n    facet_wrap(~ test, ncol = 1) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nNotice the lines look the same relative to one another, but now we have a consistent scale to help judge changes. To answer our question from before, it seems that the drop in reading scores (1981 to 1982) and science scores (1982 to 1983) were each about 1.5 standard deviations. We could test more formally, but we have a clearer idea now that all tests are on the same scale.\n\nQUICK EXERCISE\nWhat happens if you use the argument scales = \"free_y\" in the last bit of code? Why might you not use that once we’ve scaled the test scores?\n\nAs a quick change, we can go back to having each school in its own facet and test scores within.\n\n## facet line graph\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\n\n\n\n\n\nQUICK EXERCISE\nWhy did we drop ncol = 1 from facet_wrap()? What happens if you keep it?\n\nOur plot is looking better, but it still may not contain the information we want. We’ve standardized the test scores over this time window, but maybe what we really want to know is how they’ve changed relative to the beginning of the sample period. You can imagine a superintendent who took over in 1981 would be keen to know how scores have changed during their tenure.\nThis means that while we still want to standardize the scores, we should zero them not at the overall mean, but at the value in the first year. We can do that by grouping by school and test, arranging in year order, making a new variable that is the first() score (within test, within school) and using that rather than the mean test score to make our new variable, score_std_sch.\n\n## standardize test scores within school to first year\ndf_ts_long &lt;- df_ts_long |&gt;\n    group_by(test, school) |&gt;\n    arrange(year) |&gt; \n    mutate(score_year_one = first(score),\n           ## note that we're using score_year_one instead of mean(score)\n           score_std_sch = (score - score_year_one) / sd(score)) |&gt;\n    ungroup()\n\nNow we’ll plot using our new variable score_std_sch.\n\n## facet line graph\np &lt;- ggplot(data = df_ts_long,\n            mapping = aes(x = year, y = score_std_sch, colour = test)) +\n    facet_wrap(~ school) +\n    geom_line()\np\n\n\n\n\n\n\n\n\nWith this final graph, we can see relative changes across schools, across times, across tests. Notice that line shapes within each facet are the same as before, just shifted up or down so that the first value for each test in 1981 is 0.\nIs this the best version of this figure (minus making the axis and legend labels look nicer)? Again, it depends on what you want to show. Remember that figures don’t speak for themselves: it’s up to you to explain to your reader (include your future self) what they mean. That said, a well crafted figure will make that job much easier.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#questions",
    "href": "05-viz-i.html#questions",
    "title": "I: Basics",
    "section": "Questions",
    "text": "Questions\n\nWhat is the distribution of household size among students in the sample?\nHow does student socioeconomic status differ between students who ever attended college and those who did not?\nHow do parental educational expectations differ across region?\nHow does the relationship between socioeconomic status and math test score differ across region (use a smoothing line to help show any relationship)?\nAmong students who ever attended college, how does socioeconomic status differ between those who delayed postsecondary enrollment and those who did not delay, when delay is defined as:\n\nmore than 6 months between high school graduation and postsecondary enrollment?\nmore than 12 months?\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_5.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#core-processes-in-a-data-analysis",
    "href": "03-wrangle-i.html#core-processes-in-a-data-analysis",
    "title": "I: Enter the tidyverse",
    "section": "Core processes in a data analysis",
    "text": "Core processes in a data analysis\nLarge or small, a typical data analysis will involve most — if not all — of the following steps:\n\nRead in data\nSelect variables (columns)\nMutate data into new forms\nFilter observations (rows)\nSummarize data\nArrange data\nWrite out updated data\n\nReturning to our cooking metaphor from the organizing lesson: if the first and last steps represent our raw ingredients and finished dish, respectively, then the middle steps are the core processes we use to prepare the meal.\nAs you can see, there aren’t that many core processes to use. Their power comes from the infinite ways they can be ordered and combined. There are, of course, many specialized tools for specialized data wrangling tasks — too many to cover in this course (Google is your friend here!). But I call these processes “core processes” for a reason — they will be at the center of most of your data analytic work.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#tidyverse",
    "href": "03-wrangle-i.html#tidyverse",
    "title": "I: Enter the tidyverse",
    "section": "Tidyverse",
    "text": "Tidyverse\nThe tidyverse is shorthand for a number of packages that are built to work well together and can be used in place of base R functions. A few of the tidyverse packages that you will often use are:\n\ndplyr for data manipulation\n\ntidyr for making data tidy\n\nreadr for flat file I/O\n\nreadxl for Excel file I/O\n\nhaven for other file format I/O\n\nggplot2 for making graphics\n\nstringr for working with strings\n\nlubridate for working with dates\n\npurrr for working with functions\n\nMany R users find functions from these libraries to be more intuitive than base R functions. In some cases, tidyverse functions are faster than base R, which is an added benefit when working with large data sets.\nToday we will primarily use functions from the dplyr and readr libraries. We’ll also use some common base R functions as necessary.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#check-working-directory",
    "href": "03-wrangle-i.html#check-working-directory",
    "title": "I: Enter the tidyverse",
    "section": "Check working directory",
    "text": "Check working directory\nThis script — like the one from the organizing lesson — assumes that the scripts subdirectory is the working directory, that the required data file is in the data subdirectory, and that both subdirectories are at the same level in the course directory. Like this:\nstudent_skinner/         &lt;--- Top-level\n|\n|__/data                 &lt;--- Sub-level 1\n|    |--+ hsls_small.csv\n|\n|__/scripts              &lt;--- Sub-level 1 (Working directory)\n     |--+ dw_one.R\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that I’m not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, I tell you where you need to be (a common landmark), let you get there, and then rely on relative paths afterwards.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#example-data-analysis-task",
    "href": "03-wrangle-i.html#example-data-analysis-task",
    "title": "I: Enter the tidyverse",
    "section": "Example data analysis task",
    "text": "Example data analysis task\nLet’s imagine we’ve been given the following data analysis task with the HSLS09 data:\n\nFigure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R. Instead, it’s up to you to (1) translate a research question into the discrete steps coding steps necessary to provide an answer, and then (2) translate the answer such that everyone understands what you’ve found.\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nArrange in order so it’s easier to rank and share\nWrite out the results to a file so we have it for later\n\nLet’s do it!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#read-in-data",
    "href": "03-wrangle-i.html#read-in-data",
    "title": "I: Enter the tidyverse",
    "section": "Read in data",
    "text": "Read in data\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES / NCES data set that features:\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link (after a little navigation).\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read_csv() from the readr library\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUnlike the readRDS() function we’ve used before, read_csv() prints out information about the data just read in. Nothing is wrong! The read_csv() function, like many other functions in the tidyverse, assumes you’d rather have more rather than less information and acts accordingly. We can see that all the columns were read in as doubles (dbl(16)), which is just a type of number that the computer understands in a special way (a distinction that’s not important for us in this case). For other data (or if we had told read_csv() how to parse the columns), we might see other column types like:\n\nint(): another type of number (again, an important distinction for the computer, but not usually for us)\nchr(): strings (e.g., \"Ben\" or \"1\" [notice the quotes])\nlgl(): Boolean values of TRUE or FALSE\n\n\nQuick exercise\nread_csv() is special version of read_delim(), which can read various delimited file types, that is, tabular data in which data cells are separated by a special character. What’s the special character used to separate CSV files? Once you figure it out, re-read in the data using read_delim(), being sure to set the delim argument to the correct character.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#select-variables-columns",
    "href": "03-wrangle-i.html#select-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\nTo choose variables, either when making a new data frame or dropping them, use select(). Like the other dplyr functions we’ll use, the first argument select() takes is the data frame (or tibble) object. After that, we list the column names we want to keep.\nSome pseudocode for using select() is:\n## pseudocode (not to be run)\nselect(&lt; df object &gt;, column_1_name, column_2_name)\nBecause we don’t want to overwrite our original data in memory, we’ll assign (&lt;-) the output to a new object called df_tmp.\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- select(df, stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n# ℹ 23,493 more rows",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#mutate-data-into-new-forms",
    "href": "03-wrangle-i.html#mutate-data-into-new-forms",
    "title": "I: Enter the tidyverse",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\nTo add variables and change existing ones, use the mutate() function.\nJust like select(), the mutate() function takes the data frame as the first argument, followed by variable name, new or old, that is created/modified by some function:\n## pseudocode (not to be run)\nmutate(&lt; df object &gt;, column_1_name = function(...))\nIn this case, the function(...) (or, stuff we want to do) is add a new column that is the larger of x1stuedexpct and x1paredexpct.\n\nUnderstanding our data\nFirst things first, however, we need to check the code book to see what the numerical values for our two education expectation variables represent. To save time, I’ve copied them here:\n\nx1stuedexpct\nHow far in school 9th grader thinks he/she will get\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n\n\n\nx1paredexpct\nHow far in school parent thinks 9th grader will go\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n-9\nMissing\n\n\n\nThe good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression. The bad news is that we have three values — -8, -9, and 11 — that we need to deal with so that the averages we compute later represent what we mean.\nLet’s see how many observations are affected by these values using count() (notice that we don’t assign to a new object; this means we’ll see the result in the console, but nothing in our data or object will change):\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ncount(df_tmp, x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -8  2059\n 2            1    93\n 3            2  2619\n 4            3   140\n 5            4  1195\n 6            5   115\n 7            6  3505\n 8            7   231\n 9            8  4278\n10            9   176\n11           10  4461\n12           11  4631\n\n## see unique values for parental expectation\ncount(df_tmp, x1paredexpct)\n\n# A tibble: 13 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -9    32\n 2           -8  6715\n 3            1    55\n 4            2  1293\n 5            3   149\n 6            4  1199\n 7            5   133\n 8            6  4952\n 9            7    76\n10            8  3355\n11            9    37\n12           10  3782\n13           11  1725\n\n\nDealing with -8 and -9 is straightforward — we’ll convert it missing. In R, missing values are technically stored as NA. Not all statistical software uses the same values to represent missing values (for example, Stata uses a dot .). Likely because they want to be software agnostic, NCES has decided to represent missing values as a limited number of negative values. In this case, -8 and -9 represent missing values.\nHow to handle missing values is a very important topic, one we could spend all semester discussing. For now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results.\nDeciding what to do with 11 is a little trickier. While it’s not a missing value per se, it also doesn’t make much sense in its current ordering, that is, to be “higher” than completing a professional degree. We’ll make a decision to convert these to NA as well, effectively deciding that an answer of “I don’t know” is the same as missing an answer.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#changing-existing-variables-columns",
    "href": "03-wrangle-i.html#changing-existing-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Changing existing variables (columns)",
    "text": "Changing existing variables (columns)\nSo first step: convert -8, -9, and 11 in both variables to NA. We can do this by overwriting cells in each variable with NA when they equal one of these two values. For this, we’ll use the ifelse() function, which has three parts:\n## pseudo code (not to be run)\nifelse(&lt; test &gt;, &lt; return this if TRUE &gt;, &lt; return this if FALSE &gt;)\nifelse() works by asking: if the &lt; test &gt; is TRUE do this else (i.e. the &lt; test &gt; is FALSE) do that. By test, I mean a code statement that evaluates to either TRUE or FALSE. For example:\n\n1 == 1 (TRUE)\n1 == 2 (FALSE)\n1 + 1 == 2 (TRUE)\n\nNOTE When checking whether something equals something else, use a double equals sign (==); a single equals sign typically means assignment = or the same thing as the arrow, &lt;-.\nWhat we want is to go row by row (that is, observation by observation) through both x1stuedexpct and x1paredexpct and test whether the value is either -8, -9, or 11 — if it is, then replace with NA, otherwise, just replace it with the value it found (i.e. leave it alone).\nWe could develop a sophisticated test that looked for any of these conditions, but we can also just do it with multiple ifelse() functions. Notice, however, that we can test for both -8 and -9 in the same test since all the categories we want are positive.\n\n## use case_when to overwrite -8 and 11 with NA in our two expectation variables\ndf_tmp &lt;- mutate(df_tmp,\n                 ## correct student expectations \n                 x1stuedexpct = ifelse(x1stuedexpct &lt; 0,   # is value &lt; 0?\n                                       NA,                 # T: replace with NA\n                                       x1stuedexpct),      # F: replace with self\n                 x1stuedexpct = ifelse(x1stuedexpct == 11, # is value == 11?\n                                       NA,                 # T: replace with NA\n                                       x1stuedexpct),      # F: replace with self\n                 ## correct parental expectations\n                 x1paredexpct = ifelse(x1paredexpct &lt; 0,   # (same as above...)\n                                       NA,                 \n                                       x1paredexpct),       \n                 x1paredexpct = ifelse(x1paredexpct == 11,\n                                       NA,\n                                       x1paredexpct))\n\nNotice that we used df_tmp rather than df. That’s because we want to carry through the work we did with select() before. If we used df instead, then we’d be back to the original data object — not what we want.\nLet’s confirm that our code worked as we planned by using count() again.\n\n## again see unique values for student expectation\ncount(df_tmp, x1stuedexpct)\n\n# A tibble: 11 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    93\n 2            2  2619\n 3            3   140\n 4            4  1195\n 5            5   115\n 6            6  3505\n 7            7   231\n 8            8  4278\n 9            9   176\n10           10  4461\n11           NA  6690\n\n## again see unique values for parental expectation\ncount(df_tmp, x1paredexpct)\n\n# A tibble: 11 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    55\n 2            2  1293\n 3            3   149\n 4            4  1199\n 5            5   133\n 6            6  4952\n 7            7    76\n 8            8  3355\n 9            9    37\n10           10  3782\n11           NA  8472",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#adding-new-variables-columns",
    "href": "03-wrangle-i.html#adding-new-variables-columns",
    "title": "I: Enter the tidyverse",
    "section": "Adding new variables (columns)",
    "text": "Adding new variables (columns)\nAdding a new variable to our data frame is just like modifying an existing column. The only difference is that instead of putting an existing column name on the LHS of the = sign in mutate(), we’ll make up a new name. This tells R to make a new column in our data frame that contains the results from the the RHS function(s).\n## pseudocode (not to be run)\nmutate(&lt; df object &gt;, new_column_name = function(...))\nNow that we’ve corrected our expectation variables, we create a new variable that is the higher of the two (per our initial instructions to choose the higher of the two if both existed).\nUsing ifelse() again, we can test whether each student’s degree expectation is higher than that of their parent; if true, we’ll put the student’s value into the new variable — if false, we’ll put the parent’s value into the new variable.\n\n## mutate (notice that we use df_tmp now)\ndf_tmp &lt;- mutate(df_tmp,\n                 high_expct = ifelse(x1stuedexpct &gt; x1paredexpct, # test\n                                     x1stuedexpct,                # if TRUE\n                                     x1paredexpct))               # if FALSE\n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1         NA\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1         NA\n 8  10008            8            6        1          8\n 9  10009           NA           NA        3         NA\n10  10010            8            6        1          8\n# ℹ 23,493 more rows\n\n\nDoing a quick “ocular test” of our first few rows, it seems like our new variable is correct…EXCEPT…it doesn’t look like we handled NA values correctly. Look at student 10002 in the second row: while the student doesn’t have an expectation (or said “I don’t know”), the parent does. However, our new variable records NA. Let’s fix it with this test:\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\ndf_tmp &lt;- mutate(df_tmp,\n                 ## step 1: compare with student's expectations\n                 high_expct = ifelse(is.na(high_expct) & !is.na(x1stuedexpct), \n                                     x1stuedexpct,                \n                                     high_expct),\n                 ## step 2: compare with parent's expectations\n                 high_expct = ifelse(is.na(high_expct) & !is.na(x1paredexpct), \n                                     x1paredexpct,                \n                                     high_expct))               \n\n## show\ndf_tmp\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10009           NA           NA        3         NA\n10  10010            8            6        1          8\n# ℹ 23,493 more rows\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue. Looking at rows 7 and 9, it seems like those situations are correctly handled as well.\nTo be clear, there were other ways we could have handled fixing our missing values and creating our new variable. For example, we could have left our missing values as negative numbers (and converting 11 to a negative value) so that our comparison would have worked the first time. We could have used more sophisticated tests in our ifelse() statements. However, these paths weren’t clear until we’d already worked a bit. The point to keep in mind that the process is often iterative (two steps forward, one step back…) and that there’s seldom an single correct way.\n\nQuick exercise\nWhat happens when the student and parent expectations are the same, either a value or NA? Does our ifelse() statement account for those situations? If so, how?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#filter-observations-rows",
    "href": "03-wrangle-i.html#filter-observations-rows",
    "title": "I: Enter the tidyverse",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\nLet’s check the counts of our new variable:\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ncount(df_tmp, high_expct)\n\n# A tibble: 11 × 2\n   high_expct     n\n        &lt;dbl&gt; &lt;int&gt;\n 1          1    71\n 2          2  2034\n 3          3   163\n 4          4  1282\n 5          5   132\n 6          6  4334\n 7          7   191\n 8          8  5087\n 9          9   168\n10         10  6578\n11         NA  3463\n\n\nSince we’re not going to use the missing values (we really can’t, even if we wanted to do so), we’ll drop those observations from our data frame using filter().\nAn important point about filter() that often trips people up at first: use it to filter in what you want. This is the opposite of the more common usage of filters, which are about removing things (e.g., air filters, water filters, etc).\n\n## filter out missing values\ndf_tmp &lt;- filter(df_tmp, !is.na(high_expct))\n\n## show\ndf_tmp\n\n# A tibble: 20,040 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10010            8            6        1          8\n10  10011            8            6        3          8\n# ℹ 20,030 more rows\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values). Since we haven’t removed rows until now, we can compare the number of rows in the original data frame, df, to what we have now.\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column. While not a formal test, it does support what we expected (in other words, if the number were different, we’d definitely want to go back and investigate).",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#summarize-data",
    "href": "03-wrangle-i.html#summarize-data",
    "title": "I: Enter the tidyverse",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. The summarize() command will allow us to apply a summary measure, like mean(), to a column of our data. (NOTE that if we wanted another summary measure, like the median or standard deviation, there are other functions like median() and sd()…if you need a particular stat, there’s likely a function for it!).\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nsummarize(df_tmp, high_expct_mean = mean(high_expct))\n\n# A tibble: 1 × 1\n  high_expct_mean\n            &lt;dbl&gt;\n1            7.27\n\n\nOverall, we can see that students and parents have high postsecondary expectations on average: to earn some graduate credential beyond a bachelor’s degree. However, this isn’t what we want. We want the values across census regions.\n\n## check our census regions\ncount(df_tmp, x1region)\n\n# A tibble: 4 × 2\n  x1region     n\n     &lt;dbl&gt; &lt;int&gt;\n1        1  3128\n2        2  5312\n3        3  8177\n4        4  3423\n\n\nWe’re not missing any census data, which is good. To calculate our average expectations, we need to use the group_by() function. This function allows to set groups and perform other dplyr operations within those groups. Right now, we’ll use it to get our summary.\n\n## get expectations average within region\ndf_tmp &lt;- group_by(df_tmp, x1region)\n\n## show grouping\ndf_tmp\n\n# A tibble: 20,040 × 5\n# Groups:   x1region [4]\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  10001            8            6        2          8\n 2  10002           NA            6        1          6\n 3  10003           10           10        4         10\n 4  10004           10           10        3         10\n 5  10005            6           10        3         10\n 6  10006           10            8        3         10\n 7  10007            8           NA        1          8\n 8  10008            8            6        1          8\n 9  10010            8            6        1          8\n10  10011            8            6        3          8\n# ℹ 20,030 more rows\n\n\nNotice the extra row at the second line now? Groups:  x1region [4] tells us that our data set is now grouped.\n\nQuick exercise\nWhat does the [4] mean?\n\nNow that our groups are set, we can get the summary we really wanted\n\n## get average (assigning this time)\ndf_tmp &lt;- summarize(df_tmp, high_expct_mean = mean(high_expct))\n\n## show\ndf_tmp\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        2            7.17\n3        3            7.36\n4        4            7.13\n\n\nSuccess! Expectations are similar across the country, but not the same by region.\nNB: The reason we didn’t assign the first ungrouped summarize() function back to df_tmp is that summarize() fundamentally changes our data frame, from one of many observations to one that represents their summary (as we asked!). Keep this in mind for your own analyses: the order of operations matters. If you select columns, then columns you didn’t selection won’t be available later; if you summarize your data, then you only have access to the summary.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#arrange-data",
    "href": "03-wrangle-i.html#arrange-data",
    "title": "I: Enter the tidyverse",
    "section": "Arrange data",
    "text": "Arrange data\nAs our final step, we’ll arrange our data frame from highest to lowest (descending). For this, we’ll use arrange() and a special operator, desc() which is short for descending.\n\n## -----------------\n## arrange\n## -----------------\n\n## arrange from highest expectations (first row) to lowest\ndf_tmp &lt;- arrange(df_tmp, desc(high_expct_mean))\n\n## show\ndf_tmp\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        3            7.36\n3        2            7.17\n4        4            7.13\n\n\n\nQuick exercise\nWhat happens when you don’t include desc() around high_expct_mean?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#write-out-updated-data",
    "href": "03-wrangle-i.html#write-out-updated-data",
    "title": "I: Enter the tidyverse",
    "section": "Write out updated data",
    "text": "Write out updated data\nWe can use this new data frame as a table in its own right or to make a figure. For now, however, we’ll simply save it using the opposite of read_csv() — write_csv() — which works like writeRDS() we’ve used before.\n\n## write with useful name\nwrite_csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\nAnd with that, we’ve met our task: we can show average educational expectations by region. To be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region. This caveat doesn’t necessarily make our analysis less useful, but rather sets its scope. Furthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#pipes",
    "href": "03-wrangle-i.html#pipes",
    "title": "I: Enter the tidyverse",
    "section": "Pipes (|>)",
    "text": "Pipes (|&gt;)\nAbove, we’ve performed each step of our analysis piecemeal, saving new objects or overwriting along the way. This is fine, but a huge benefit of the tidyverse is that it allows users to chain commands together using pipes.\nTidyverse pipes, |&gt;, come from the magrittr package.\n  \nPipes take output from the left side and pipe it to the input of the right side. So mean(x) can be rewritten as x |&gt; mean: x outputs itself and the pipe, |&gt;, makes it the input for mean().\n\nQuick exercise\nStore 1,000 random values in x: x &lt;- rnorm(1000). Now run mean(x) and x |&gt; mean. Do you get the same thing?\n\nThis may be a silly example (why would you do that?), but pipes are powerful because they allow data wrangling processes to be chained together.\nNormally, functions (like select(), mutate(), etc), can be nested in R, but after too many, the code becomes difficult to parse since it has to be read from the inside out. For this reason, many analysts run one discrete function after another, saving output along the way. This is what we did above.\nPipes allow functions to come one after another in the order of the work being done, which is more legible. As a bonus, chaining functions together is sometimes faster due to behind-the-scenes processing.\nLet’s use Hadley’s canonical example to make the readability comparison between nested functions and piped functions clearer:\n## foo_foo is an instance of a little bunny function\nfoo_foo &lt;- little_bunny()\n\n## adventures in base R must be read from the middle up and backwards\nbop_on(\n    scoop_up(\n        hop_through(foo_foo, forest),\n        field_mouse\n    ),\n    head\n)\n\n## adventures w/ pipes start at the top and work down\nfoo_foo |&gt;\n    hop_through(forest) |&gt;\n    scoop_up(field_mouse) |&gt;\n    bop_on(head)\nIn the first set, we have to read the story of little bunny foo foo from the inside out: “Little bunny foo_foo bopped on the head a field mouse that was scooped up while hopping through the forest.”\nWith pipes, we can read it more like the original rhyme: “Little bunny foo foo hopped through the forest, scooped up a field mouse, and bopped it on the head.”\nThe main thing to remember is with pipes and tidyverse is that because the output of the function goes into the next function, you don’t need to include the first argument, that is, the data frame object name.\nThis:\ndf_example &lt;- select(df, col1, col2)\nbecomes this:\ndf_example &lt;- df |&gt; select(col1, cols2)\nIn the second example, the object df was piped into the first argument spot in select(). Since that’s already accounted for, we were able to just start with the column names we want.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#rewriting-our-analysis-using-pipes",
    "href": "03-wrangle-i.html#rewriting-our-analysis-using-pipes",
    "title": "I: Enter the tidyverse",
    "section": "Rewriting our analysis using pipes",
    "text": "Rewriting our analysis using pipes\nReturning to our analysis above, let’s rewrite all our of our steps in on piped chain of commands.\n\n## start with the original data frame...\ndf_tmp_chained &lt;- df |&gt;\n    ## (df is piped in): select the columns we want\n    select(stu_id, x1stuedexpct, x1paredexpct, x1region) |&gt;\n    ## (selected df is piped in): mutate our data, starting with missing\n    mutate(x1stuedexpct = ifelse(x1stuedexpct &lt; 0,   \n                                 NA,                 \n                                 x1stuedexpct),      \n           x1stuedexpct = ifelse(x1stuedexpct == 11, \n                                 NA,                 \n                                 x1stuedexpct),      \n           x1paredexpct = ifelse(x1paredexpct &lt; 0,   \n                                 NA,                 \n                                 x1paredexpct),       \n           x1paredexpct = ifelse(x1paredexpct == 11,\n                                 NA,\n                                 x1paredexpct),\n           ## create new column\n           high_expct = ifelse(x1stuedexpct &gt; x1paredexpct, \n                               x1stuedexpct,                \n                               x1paredexpct),\n           ## fix new column NAs\n           high_expct = ifelse(is.na(high_expct) & !is.na(x1stuedexpct), \n                               x1stuedexpct,                \n                               high_expct),\n           high_expct = ifelse(is.na(high_expct) & !is.na(x1paredexpct), \n                               x1paredexpct,                \n                               high_expct)) |&gt;\n    ## (mutated df is piped in): filter in non-missing rows\n    filter(!is.na(high_expct)) |&gt;\n    ## (filtered df is piped in): group by region\n    group_by(x1region) |&gt;\n    ## (grouped df is piped in): summarize mean expectations\n    summarize(high_expct_mean = mean(high_expct)) |&gt;\n    ## (summarized df is piped in): arrange average expectations hi --&gt; lo\n    arrange(desc(high_expct_mean))         \n\nNotice how I included comments along the way? Since R ignores commented lines (it’s as if they don’t exist), you can include within your piped chain of commands. This is a good habit that collaborators and future you will appreciate.\nTo be sure, let’s check: is the result the same as before?\n\n## show\ndf_tmp_chained\n\n# A tibble: 4 × 2\n  x1region high_expct_mean\n     &lt;dbl&gt;           &lt;dbl&gt;\n1        1            7.39\n2        3            7.36\n3        2            7.17\n4        4            7.13\n\n## test using identical()\nidentical(df_tmp, df_tmp_chained)\n\n[1] TRUE\n\n\nSuccess!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#final-notes",
    "href": "03-wrangle-i.html#final-notes",
    "title": "I: Enter the tidyverse",
    "section": "Final notes",
    "text": "Final notes\nIn this lesson, you’ve converted a research question into a data analysis that started with reading in raw data and ended with your summary table. You’ve seen how to wrangle your data in multiple discrete steps as well as chained together using pipes.\nAs you start to apply these tools to your own analyses, the first questions should always be:\n\nWhat I am trying to do?\nWhat would my data or results need to look like in order to do that?\nWhat are the discrete steps I need to get from what I have (raw data) to what I need (wrangled data)?\n\nRemember, these are iterative questions, meaning you will almost certainly need to revisit and adjust during your analysis. But becoming a better quantitative researcher mostly means becoming a better translator: question –&gt; data/coding –&gt; answer.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#questions",
    "href": "03-wrangle-i.html#questions",
    "title": "I: Enter the tidyverse",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average standardized math test score?\nWhat is the average standardized math test score by gender?\nIn what year and month were the oldest students in the data set born? The youngest?\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income (give the category and what that category reprents).\nOf the students who earned a high school credential (diploma or GED), what percentage earned a GED or equivalency? How does this differ by region?\nWhat percentage of students ever attended a postsecondary institution by February 2016? Give the cross tabulations for:\n\nfamily incomes less than or equal to $35,000 and greater than $35,000\n\nregion\n\nThis means you should have percentages for 8 groups: above/below $35k within each region.\nHINT You can group_by() on more than one group.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_3.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "01-set-install.html#getting-started",
    "href": "01-set-install.html#getting-started",
    "title": "I: Installing R & RStudio",
    "section": "Getting started",
    "text": "Getting started\nThe primary pieces of software you are going to need for this class are\n\nR\nRStudio\nMicrosoft Office\n\nI am going to assume you already have this, but we can meet to install it if not, it is free for UF students\n\n\nThere are also a few optional pieces of software you’ll need for extra credit lessons, but we will cover when needed.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-r",
    "href": "01-set-install.html#installing-r",
    "title": "I: Installing R & RStudio",
    "section": "Installing R",
    "text": "Installing R\n\nR is fantastic (hopefully you will see that throughout the course), but sometimes it can make things seem more complicated than they need to\n\nThe first time it does this is when trying to install it, there’s a bunch of options called “mirrors”\n\nThese are basically to reduce strain on the servers that you download from by using the closest location\n\nThe good news, however, is that a URL that automates this whole process for you came out recently\nThe even better news is that I’ve set up a little portal to that URL here, so you can download it without leaving this page\n\n\n\n\nClick the option for the OS you have (Windows/Mac/Linux)\nThen under the “latest release”\n\n\n\nFor windows users, select “base” then “Download R…” (the top options)\nFor mac users, select either apple silicon or intel options depending on how new your mac is - If you need to check which kind your mac is, hit the apple logo in the top left of your screen, then “About This Mac” then under “Processor” it will either intel or silicon. I can also help you with this in class if needed.\n\n\n\nR will then download, double click on the download when it’s finished and then follow the on-screen prompts\nR is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-rstudio",
    "href": "01-set-install.html#installing-rstudio",
    "title": "I: Installing R & RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nTechnically, R is all you need to do all of our analyses. However, to make it accessible and usable, we also need a “development environment”\n\nThe reason of this, unlike a computer program like Stata or SAS, R is a programming language (same as Python, C++, etc.), that’s what we just installed\nThe easiest way to use programming languages is through a “development environment”\n\nThere are multiple “development environments” you can use for R. VSCode is a great option by Microsoft for using a variety of languages, but, the best option for R is RStudio and it is purpose built for the language (it also works with Python too)\n\n\nI did try to create another little portal below, but, sadly Posit (the company who created RStudio) have disabled the way I do that. Instead…\n\n\nGo to this site and click the “Download RStudio Desktop for…” button underneath “2: Install RStudio” (we already did step 1)\n\n\nThis is simpler than installing R, Posit have a more sophisticated website which will automatically download the right version for your computer\n\n\nRStudio will then download, double click on the download when it’s finished and then follow the on-screen prompts\n\n\nFor mac users, this will just be drag n’ drop RStudio into your Applications folder\n\n\nRStudio is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#lets-see-what-we-just-installed",
    "href": "01-set-install.html#lets-see-what-we-just-installed",
    "title": "I: Installing R & RStudio",
    "section": "Let’s See What We Just Installed",
    "text": "Let’s See What We Just Installed\n\nHopefully, you should now be able to open RStudio on your computer (it should be the same place all your software is kept)\n\nGo ahead and open it up!\n\n\nBy default, RStudio has 3-4 main frames:\n\nTop left: Script window (will be closed at first if you don’t have any scripts open)\nBottom left: Console\nTop right: Environment / History / Connections\nBottom right: Files / Plots / Packages / Help / Viewer\n\nLet’s go ahead and open up our first R script 01-Setup-Installing-R.R",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#basic-r-commands",
    "href": "01-set-install.html#basic-r-commands",
    "title": "I: Installing R & RStudio",
    "section": "Basic R Commands",
    "text": "Basic R Commands\nFirst, let’s try the traditional first command!\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nWe can also use R like a basic calculator\n\n1 + 1\n\n[1] 2\n\n\n\nIf we want to, we could actually put these commands directly into the console (try copying 1 + 1 into the console)\n\nThis is actually all running a script does\n\nWe use scripts to save a record of what we did",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#assignment",
    "href": "01-set-install.html#assignment",
    "title": "I: Installing R & RStudio",
    "section": "Assignment",
    "text": "Assignment\n\nThe first two commands we ran simply spat the output out in the console\n\nThis can be useful if you want to check something quickly or if we have our final output\n\nMore often, though, we want to save the output to our R Environment (top right panel)\nTo do this, we need to assign the output to an object”\n\nR is a type of object-oriented programming environment. This means that R thinks of things in its world as objects, which are like virtual boxes in which we can put things: data, functions, and even other objects.\n\nIn R (for quirky reasons), the primary means of assignment is the arrow, &lt;-, which is a less than symbol, &lt;, followed by a hyphen, -.\n\nYou can use = (which is more common across other programming languages), and you may see this “in the wild”\nBut R traditionalists prefer &lt;- for clarity and readability\nWhich you use is up to you! (I generally use &lt;-)\n\n\n\n## assign value to object x using &lt;-\nx &lt;- 1\n\n\nBut’s where’s the output?\n\nCheck out the “Environment” tab on the top left panel\n\nWe see something called x has a value of 1\n\nNow let’s call that object\n\n\n\n\n## what's in x?\nx\n\n[1] 1\n\n\nNote: the [1] is just the index (order) number, if we had more than 1 thing in our object, that would be more useful\n\nQuick exercise\nUsing the arrow, assign the output of 1 + 1 to x. Next subtract 1 from x and reassign the result to x. Show the value in x.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#comments",
    "href": "01-set-install.html#comments",
    "title": "I: Installing R & RStudio",
    "section": "Comments",
    "text": "Comments\n\nYou may have noticed already, but comments in R are set off using the hash or pound character at the beginning of the line: #\nThe comment character tells R to ignore the line\n\n\nQuick exercise\nType the phrase “This is a comment” directly into the R console both with and without a leading “#”. What happens each time?\n\n\nYou may notice I (Ben, and therefore, now me) use two hashes\n\nYou can use only a single # for your comments if you like, R treats them all the same\nIf you’re typing longer comments ##' (two hashes and an apostrophe) is really useful in RStudio, as it automatically comments the next line (although this can be annoying at times too)\nIf you want to take your comments to the next level ##' RStudio also enables some fun color coding options with @, [], **, and :\n\nThese can be useful in longer scripts to draw attention to specific points\nNote: these only show up in RStudio\n\n\n\n\n##' @Matt needs to work on this  \n##' Matt needs to work on [this]\n##' Matt *needs* to work on this\n##' [Matt: needs to work on this]\n\n\nLastly, RStudio can comment/uncomment multiple lines of code you’ve already written\n\nOn the top menu bar select “Code” then “Commment/Uncomment Lines”\n\nAlso see the keyboard shortcut next to that option!\n\n\nThis is a big time saver!\n\n\n## Try commenting/uncommenting the below line\n\n# Matt &lt;- \"Hi\"",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#data-types-and-structures",
    "href": "01-set-install.html#data-types-and-structures",
    "title": "I: Installing R & RStudio",
    "section": "Data types and structures",
    "text": "Data types and structures\nR uses variety of data types and structures to represent and work with data. There are many, but the major ones that you’ll use most often are:\n\nlogical\nnumeric (integer & double)\ncharacter\nvector\nmatrix\nlist\ndataframe\n\nUnderstanding the nuanced differences between data types is not important right now. Just know that they exist and that you’ll gain an intuitive understanding of them as you become better aquainted with R.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#packages",
    "href": "01-set-install.html#packages",
    "title": "I: Installing R & RStudio",
    "section": "Packages",
    "text": "Packages\n\nUser-submitted packages are a huge part of what makes R great\nYou may hear me use the phrases “base R” during class\n\nWhat I mean by this is the R that comes as you download it with no packages loaded (sometimes also called “vanilla R”)\nWhile it’s powerful in and of itself — you can do everything you need with base R — most of your scripts will make use of one of more contributed packages = These will make your data analytic life much nicer. We’ll lean heavily on the tidyverse suite of packages this semester.\n\n\n\nInstalling packages from CRAN\n\nMany contributed packages are hosted on the CRAN package repository. - What’s really nice about CRAN is that packages have to go through quite a few checks in order for CRAN to approve and host them. Checks include;\n\nMaking sure the package has documentation\nWorks on a variety of systems\nDoesn’t try to do odd things to your computer\n\nThe upshot is that you should feel okay downloading these packages from CRAN\n\nTo download a package from CRAN, use:\n\ninstall.packages(\"&lt;package name&gt;\")\n\nNOTE Throughout this course, if you see something in triangle brackets (&lt;...&gt;), that means it’s a placeholder for you to change accordingly.\nMany packages rely on other packages to function properly. When you use install.packages(), the default option is to install all dependencies. By default, R will check how you installed R and download the right operating system file type.\n\nQuick exercise\nInstall the tidyverse package, which is really a suite of packages that we’ll use throughout the semester. Don’t forget to use double quotation marks around the package name: install.packages(\"tidyverse\")\n\n\n\nLoading package libraries\nPackage libraries can loaded in a number of ways, but the easiest it to write:\n\nlibrary(\"&lt;library name&gt;\")\n\nwhere \"&lt;library name&gt;\" is the name of the package/library. You will need to load these before you can use their functions in your scripts. Typically, they are placed at the top of the script file.\nFor example, let’s load the tidyverse library we just installed:\n\n## load library (note quirk that you don't need quotes here)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that when you load the tidyverse (which, again, is actually loading a number of other libraries), you see a lot of output. Not all packages are this noisy, but the information is useful here because it shows all the libraries that are now loaded and ready for you to use.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#help",
    "href": "01-set-install.html#help",
    "title": "I: Installing R & RStudio",
    "section": "Help",
    "text": "Help\nI don’t have every R function and nuance memorized, so I certainly don’t expect that you will. With all the user-written packages, it would be difficult to keep up if I tried! When stuck, there are a few ways to get help.\n\nHelp files\nIn the console, typing a function name immediately after a question mark will bring up that function’s help file (in RStudio, you should see in the bottom right panel):\n\n## get help file for function\n?median\n\nTwo question marks will search for the command name in CRAN packages (again, in the bottom right facet):\n\n## search for function in CRAN\n??median\n\nAt first, using help files may feel like trying to use a dictionary to see how to spell a word — if you knew how to spell it, you wouldn’t need the dictionary! Similarly, if you knew what you needed, you wouldn’t need the help file. But over time, they will become more useful, particularly when you want to figure out an obscure option that will give you exactly what you need.\n\n\nPackage Website\n\nWhile all R packages have to have help files, not all R packages have nice webpages. However, a lot of the main ones do\n\nI find them much nicer than the CRAN helpfiles\n\n\nFor example, here’s another magic portal to the tidyverse’s dplyr website (you may spent a good amount of time here this semester)\n\nUsually I Google something like “&lt;package name&gt; R” and the website comes up\nYou can find links to all the tidyverse packages here\n\n\nGoogle it!\nGoogle is a coder’s best friend. If you are having a problem, odds are a 1,000+ other people have too and at least one of them has been brave enough (people can be mean on the internet) to ask about it in a forum like StackOverflow, CrossValidated, or R-help mailing list.\nIf you are lucky, you’ll find the exact answer to your question. More likely, you’ll find a partial answer that you’ll need to modify for your needs. Sometimes, you’ll find multiple partial answers that, in combination, help you figure out a solution. It can feel overwhelming at first, particularly if it’s a way of problem-solving that’s different from what you’re used to. But it does become easier with practice.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#useful-packages",
    "href": "01-set-install.html#useful-packages",
    "title": "I: Installing R & RStudio",
    "section": "Useful packages",
    "text": "Useful packages\nWe’re going to use a number of packages this semester. While we may need more than this list — and you almost certainly will in your own future work — let’s install these to get us started.\n\nQuick exercise\nInstall the following packages using the install.packages() function:\n\n\n\ndevtools\nknitr\nquarto",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "02-set-data.html#organizing-a-project-folder",
    "href": "02-set-data.html#organizing-a-project-folder",
    "title": "II: Reading Data & IPEDS",
    "section": "Organizing a Project Folder",
    "text": "Organizing a Project Folder\nWe’ll begin with how to organize your course and project files.\n\nWe are now arriving at one of the first small differences from previous versions of the course; how the sub-folders are organized.\n\n\nBen’s kitchen example\n“Every data analysis project should have its own set of organized folders. Just like you might organize a kitchen so that ingredients, cookbooks, and prepared food all have a specific cabinet or shelf, so too should you organize your project. We’ll organize our course directory in a similar fashion.”\n\nIn this world, think of pristine kitchen where NOTHING sits out on the counter.\nHowever, we are going to have kitchen where some things are stored away in cupboards, but what we actually work with (the scripts) sits out on the counter\n\nThe kitchen metaphor also works to explain why you might take these approaches\n\nIt’s definitely tidier to keep everything in your kitchen stored away, but, it adds an extra step whenever you want to cook a meal. The same is true here.\nWhy? We will get to that later…\n\n\n\n\n\nEDH 7916 Folder Setup\n\nWith this kitchen metaphor in mind, let’s set up our folder for the class\nI’ve made an EDH-7916 example folder you can download here which is also available on the class homepage.\n\nIn here, you will see\n\nAn .R script template\nA set of numbered .R scripts (these are our lesson scripts)\nA data folder\nA reproducible-report fo (with it’s own data sub-folder)\n\n\nDownload and save this folder wherever you usually keep class folders\n\nI personally use my Desktop, but you can use your Documents etc. if you wish\nYou can rename the folder if you’d like (please don’t rename the internal folders)\n\nSee naming guidelines below on how best to name files\n\n\nThroughout the class (and especially in your final project) you may feel the need for other sub-folders for other items (such as one to keep graphs in), but this should be fine for now\n\n\n\nR Project Setup\n\nRStudio has some really helpful features, one of which is creating R Projects easily\n\nAt their very simplest, these can be ways of keeping your RStudio environment saved (especially helpful switching between projects), but also enable more feature like using git (see extra credit lesson)\n\nIt’s pretty easy to set up a project now we have our class folder set up\n\nIn the top right corner of RStudio you’ll see a blue cube with “none” next to it\nClick there, then “new project”\nThen click “from existing directory”\nFind the class folder we just created, select it, and we’re done!\n\nThis is really useful for keeping track of multiple projects, but if this is all you use it for, it will be helpful to keep working directory correct!\n\n\n\nNaming Guidelines\n\nYour class scripts and data files are already named, but there will be numerous files you need to create through the class (assignment scripts, everything for your file report, etc.). So it’s best we get on the same page\nAlways name your files something logical\n\nThe file name should always tell something about the purpose of that file or folder\n\nScript numbering\nI am personally a big fan of Hadley Wickham (the founder of RStudio)’s script numbering\n\nBasically start all your script names with the number that they should be run in\n\n01-data-reading\n02-data-cleaning\n03-data-analysis\n\nI think this is especially helpful if you’re keeping scripts in the top level of the project directory to keep things organized\n\nGenerally, a good programming tip is to avoid spaces at all costs, use dashes or underscores instead\n\nI prefer dashes as they format better on macs, so you will see all the files I give you will have dashes\n\nIt’s also good to be consistent with capitalization, most traditional programmers will avoid it completely, but if you do it, do it consistently throughout that project\n\nI used no capitalization through this class, but I often will use title case for aesthetic reasons in other situations\n\nWhatever you do, never (ever, ever) have different versions of files with the same name but different capitalization\n\nMost modern computers won’t let you have both at the same time, but even changing it can cause problems with case sensitive systems like git (I found this out the hard way…)\n\n\n\nLastly, try to keep files names as short as possible\n\nLater on we will be be in situations where we have to type out file names, so if you go too long, it can become frustrating\n\nYou may think the names of your class scripts are a little odd, so let me explain those as an example 1.Hadley Wickham’s script numbering corresponding to the order of the lessons\n\nset, wrangle, viz, quarto, or pro indicate which group of lessons it belongs to (it helps me automatically create the groups on the website sidebar)\nAnything else is just descriptive, roman numerals for the lesson series, or a descriptive word\n\nWith our class folder now set up, it’s time to go over some other key organization principles\n\n\n\nFile Paths\n\nWhen we are working with R, we (most of the time) need to bring in other items, such as data\n\nIn order to do that, our computer has to find these items, and there are two ways it can do that\n\n\n\nAbsolute Paths\n\nAbsolute paths are directions to what you’re looking for starting from the root of your computer, and list out exactly where a file is. For example, as I’m writing this lesson, the absolute path of this Quarto script is\n\n\"/Users/Matt/Desktop/7916/02-set-data.qmd\"\n\nThis is perfectly fine, assuming two things\n\nI don’t move the project directory\nIt only needs to run on my computer\n\nOftentimes, we cannot rely on both these assumptions being true\n\nPlus, if we start with these absolute paths and then need to change, it will then become a real pain to update everything\n\nSo, we should ALWAYS use relative paths instead (this one of the only strict rules for assignments)\n\n\n\n\n\nRelative Paths\n\nImagine I am giving you directions to a College of Education cookout, but, I give you directions from my house. That’s only any use if you know where my house is…\n\nInstead, you really want directions from somewhere we all know, like Norman Hall\n\nThat is (basically) how relative paths work, we give directions to to our data from a common point\n\n\nRelative paths are directions to what you’re looking for from where you are right now (a.k.a your “working directory”)\n\n\n\nThe file.path() Function\n\nOne last thing, you see how file paths are typically written with / or \\? (which depends on your computer)\n\nR has a nice function that means we don’t have to\n\n\nworry about which way around the slash should be\n\n\navoid issues with different computers expecting different slashes\n\n\nfile.path()\n\nInside that, we just type the name of each folder/file in “quotes”\n\n\"Users/Matt/Desktop/7916\" into\nfile.path(\"Users\", \"Matt\", \"Desktop\", \"7916\")\nThis may look longer, but, it’s more compatible and easier to remember”\n\n\n\n\n\n\nWhat If I Need to Go Back a Level?\n\nSometimes we are in a folder, but want to go back a level, i.e. not the folder our current folder is in\n\nThis is very common if with we were using the “pristine kitchen” approach\n\nTo do so is easy, we just add a \"..\" to our file.path()\n\nSo, if we are in my class folder on my desktop, and we want to go to another folder on the desktop\n\nfile.path(\"..\", \"&lt;folder we want&gt;\")\n\n\n\n\n\n\nWorking Directory\n\nThe working directory is almost certainly the most common cause of issues in this class, and continues to be something I get tripped up by from time to time, so this may take a minute to get your head around\nAs a general rule, no matter how you have your folders organized in the future, you usually want your working directory set to where your script is\n\nThat way, you’re always giving directions from the common point of “where we are we are right now”\n\nThis will then be the same if I move the project folder on my computer, or run it on someone else’s computer\n\n\nBy default, when we open a project in RStudio, RStudio helpfully sets our working directory to the project folder\n\nThis is why we are keeping our scripts out on the counter top so to speak, the default working directory should be the correct one\n\nThat said, there will be times when you need to change your working directory, so, let’s go over the basics of that quickly\n\nFor instance, if you forget to open a project, RStudio will often the leave working directory as your root folder\n\nYou can see the currently working directory path next to the little R icon and version at the top of your console panel\nIf it’s wrong, there are a few ways to change it\n\nFind “session” on the top drop-down menu\n\n\nThen “set working directory”\nThen “To source file location”\n\nThis should be the same as “To project directory” as our scripts are stored at the top level of the project folder\n\n\n\nInstall the this.path package (recall how to do that from last week)\n\n\nWith that installed, call this.path::here() |&gt; setwd() at the top of the script\n\nNote 1: We will cover what |&gt; means next week, but effectively we took the output of this.path::here() and passed it into setwd()\nNote 2: this.path::here() is the same as doing library(this.path) followed by here() but is more efficient if you only want one thing from a package\nAssuming you want the working directory to be the script location, this never hurts to always leave at the top\n\n\n\nNavigate to the desired folder in the files pane (bottom right)\n\n\nSelect the cog symbol\n\nSelect “Set as working directory”\n\nNote: “Go to working directory” can be useful to see what’s in the folder if you navigate away\n\n\n\n\nThe old school vanilla R way setwd(\"&lt;path to your script&gt;\")\n\n\nBut, this really isn’t usually the most efficient\nNote: getwd() is sometimes still useful to check what your current working directory is set to\n\n\n\nAs I say, if we organize our folder as outlined in this lesson, and use an R project, we shouldn’t need to change this much, but it’s inevitable you will every now and then",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#script-template",
    "href": "02-set-data.html#script-template",
    "title": "II: Reading Data & IPEDS",
    "section": "Script Template",
    "text": "Script Template\n\nIn our shiny new class folder, you’ll see an r-script-template.R file. This is the template I use in my work, closely based on Dr. Skinner’s R script template (with a little bit of added color).\n\nThis a resource for you to use for assignments and other work, feel free to change it to suit your needs\n\nGenerally I just “Save As” the template everytime I make a new script\n\n\nThe script header block is a useful way to keep more info than a file name can\nThe main reason to use a template is to keep your work organized into sections\n\nThis template has\n\nLibraries to load needed packages\nInput to load data\nPrep to clean the data\nAnalysis to run our analyses\nOutput to save our modified data\n\nHowever, these will not always be the sections you need\n\nIn bigger projects, you might have a whole script for data cleaning\nIn other projects, you might want a section or script just for making plots\nIn your assignments, you’ll likely want a section for each question\n\nThe main point is to ensure you have some kind of sections in your scripts\n\nScripts can be really hard to navigate if you don’t!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#reading-in-data",
    "href": "02-set-data.html#reading-in-data",
    "title": "II: Reading Data & IPEDS",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nNext, let’s apply some of this thrilling knowledge about file paths and working directories to read in some data from IPEDS\nTo do this, open 02-set-data.R from your class folder\nFirst up, check your working directory by either\n\nLooking at the top of your console or\nTyping getwd() into the console\n\nThis should be your class folder, but if not, we need to set it there\n\nOn the top drop-down menu, select “Session”, “Set Working Directory,”To Source File Location”\nQuick Question: Without scrolling up, who can remember the other ways of doing this?\n\nOkay, with this set, it’s time to read in our first dataset!\n\nQuick Question 1: Where is our data?\nQuick Question 2: Who remembers how we assign something in R?\n\nWith those questions answered, we have everything we need\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_ipeds &lt;- read_csv(file.path(\"data\", \"hd2007.csv\"))\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSuccess!\nWe will cover other ways of reading in data over the course of the class (we can download data directly to R somtimes), but this is most common way, so we are ready for some analysis next week!\nThat’s it for R today, phew!\nNow let’s go and explore IPEDS Data Center and see where a lot of contemporary higher education research data comes from!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "href": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "title": "II: Reading Data & IPEDS",
    "section": "IPEDS Exploration Key Points (for review)",
    "text": "IPEDS Exploration Key Points (for review)\n\nIPEDS is an annual federally mandated data collection process (and compliance is a significant portion of many Institutional Researcher jobs)\nThere are a few ways of downloading IPEDS data, if you click through the website you may well find a point-and-click way of selecting specifc variables\n\nThis is NOT reproducible and therefore NOT the best practice for research\n\nWe want to use the IPEDS data center to access the complete data files then select and join variables to get our desired data set\n\nFear not, we will go over how to do those things in the first two data wrangling lessons!\n\n\n\n.csv or .dta (STATA data)\n\nMost NCES datasets have two options to download, plain old .csv files, and .dta STATA data files\n\nThe actual data is identical, but, STATA data is labeled (i.e., short descriptions attached to the variables beyond the variable name and codebook)\n“But this is an R class, not a STATA class”\n\nTrue, but R can read labeled STATA data easily using the tidyverse\nGenerally, labeled data is nice, but .csv data is easier to open in other programs (say you wanted to explore it in Excel first). So it’s up to you!\n\n\n\n\n\nUsing Codebooks\n\nTo be able to use most of these big data sets, you need to be able to understand the code. Let’s look together at the codebook for EFFY (headcount enrollment) for 2021\n\nFor IPEDS, the codebook is called the dictionary, and is always an Excel file (.xlsx). Other data sources will look different but the general principle will be the same",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "href": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "title": "II: Reading Data & IPEDS",
    "section": "Other common data sources for higher education research",
    "text": "Other common data sources for higher education research\n\nNational Center for Educational Statistics (also the owner of IPEDS)\n\nLongitudinal Surveys (HSLS, ECLES-K, etc.)\nOne-off data collection\nK-12 equivilant of IPEDS\nNCES has a good amount of publicly available data, but they also have a LOT of restricted data\n\nTypically publicly available data will be either institution (school, college, university wide) or fully anonomized. Meanwhile restricted data will often be student level and have some more detailed information.\n\nGetting restricted data is tough, but not impossible\n\nYou will need a clear purpose of your study and to know exactly what data you want access to (see available data here)\nYou’ll then need to take this idea to your advisor\n\nFor your project in this class, your data MUST be publicly available\n\nThis means we must be able to go and download it ourselves, you won’t submit data with your final project submission\n\n\n\n\nCollege Scorecard\n\nDesigned more as a tool for potential college students, college scorecard has data points of interest to this audience, but some things useful for our research too, in particular graduate earning levels\n\nSimilarly to IPEDS, if using College Scorecard, we want to avoid the point-and-click interface and download the entire data files available here\n\n\nNational Bureau of Labor Statistics\n\nLongitudinal surveys similar to NCES, but some more extensive\nNational Longitudinal Survey of Youth (NLSY) is one of the most used\n\nCensus & American Community Survey\n\nUseful for population statistics, not student specific\n\nCommon variables for higher ed research include education and income levels of population\nWe will actually do some cool stuff to download ACS data directly to R later in Data Viz III\n\n\nMany, many more, have fun exploring!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#data",
    "href": "04-wrangle-ii.html#data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Data",
    "text": "Data\nAfter you download and unzip the data for today’s lesson, move the full folder, sch_test, into the data subdirectory. It should look something like this:\n|__ data/\n    |-- ...\n    |__ sch_test/\n        |-- all_schools.csv\n        |-- all_schools_wide.csv\n        |__ by_school/\n            |-- bend_gate_1980.csv\n            |-- bend_gate_1981.csv\n            |...\n            |-- spottsville_1985.csv\nThese fake data represent test scores across three subjects — math, reading, and science — across four schools over six years. Each school has a file for each year in the by_school subdirectory. The two files in sch_test directory, all_schools.csv and all_schools_wide.csv, combine the individual files but in different formats. We’ll use these data sets to practice appending, joining, and reshaping.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#setup",
    "href": "04-wrangle-ii.html#setup",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Setup",
    "text": "Setup\nAs always, we begin by reading in the tidyverse library and assigning our paths to macros we can reuse below.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAs we did in the past lesson, we run this script assuming that our working directory is set to the scripts directory. Notice that we also include macros for our subdirectories within the data directory. Since they are nested, we can use the previous macros to set new macros.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#appending-data",
    "href": "04-wrangle-ii.html#appending-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Appending data",
    "text": "Appending data\nOur first task is the most straightforward. When appending data, we simply add similarly structured rows to an exiting data frame. What do I mean by similarly structured? Imagine you have a data frame that looks like this:\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\n\nNow, assume you are given data that look like this:\n\n\n\nid\nyear\nscore\n\n\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\nThese data are similarly structured: same column names in the same order. If we know that the data came from the same process (e.g., ids represent students in the same classroom with each file representing a different test day), then we can safely append the second to the first:\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\nData that are the result of the exact same data collecting process across locations or time may be appended. In education research, administrative data are often recorded each term or year, meaning you can build a panel data set by appending. The NCES IPEDS data files generally work like this.\nHowever, it’s incumbent upon you as the researcher to understand your data. Just because you are able to append (R will try to make it work for you) doesn’t mean you always should. What if the score column in our data weren’t on the same scale? What if the test date mattered but isn’t included in the file? What if the files actually represent scores from different grades or schools? It’s possible that we can account for each of these issues as we clean our data, but it won’t happen automatically — append with care!\n\n\n\n\n\nExample\nLet’s practice with an example. First, we’ll read in three data files from the by_school directory.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data, storing in df_*, where * is a unique number\ndf_1 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1980.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_2 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1981.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_3 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1982.csv\"))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLooking at each, we can see that they are similarly structured, with the following columns in the same order: school, year, math, read, science:\n\n## ---------------------------\n## process\n## ---------------------------\n\n## show each\ndf_1\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\ndf_2\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\ndf_3\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nFrom the dplyr library, we use the bind_rows() function to append the second and third data frames to the first.\n\n## append files\ndf &lt;- bind_rows(df_1, df_2, df_3)\n\n## show\ndf\n\n# A tibble: 3 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n\n\nThat’s it!\n\nQuick exercise\nRead in the rest of the files for Bend Gate and append them to the current data frame.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#joining-data",
    "href": "04-wrangle-ii.html#joining-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Joining data",
    "text": "Joining data\nMore often than appending your data files, however, you will need to merge or join them. With a join, you add to your data frame new columns (new variables) that come from a second data frame. The key difference between joining and appending is that a join requires a key, that is, a variable or index common to each data frame that uniquely identifies observations. It’s this key that’s used to line everything up.\nFor example, say you have these two data sets,\n\n\n\nid\nsch\nyear\nscore\n\n\n\n\nA\n1\n2020\n98\n\n\nB\n1\n2020\n95\n\n\nC\n2\n2020\n85\n\n\nD\n3\n2020\n94\n\n\n\n\n\n\nsch\ntype\n\n\n\n\n1\nelementary\n\n\n2\nmiddle\n\n\n3\nhigh\n\n\n\nand you want to add the school type to the first data set. You can do this because you have a common key between each set: sch. A pseudocode description of this join would be:\n\nAdd a column to the first data frame called type\nFill in each row of the new column with the type value that corresponds to the matching sch value in both data frames:\n\nsch == 1 --&gt; elementary\nsch == 2 --&gt; middle\nsch == 3 --&gt; high\n\n\nThe end result would then look like this:\n\n\n\nid\nsch\nyear\nscore\ntype\n\n\n\n\nA\n1\n2020\n98\nelementary\n\n\nB\n1\n2020\n95\nelementary\n\n\nC\n2\n2020\n85\nmiddle\n\n\nD\n3\n2020\n94\nhigh\n\n\n\n\nExample\nA common join task in education research involves adding group-level aggregate statistics to individual observations: for example, adding school-level average test scores to each student’s row. With a panel data set (observations across time), we might want within-year averages added to each unit-by-time period row. Let’s do the second, adding within-year across school average test scores to each school-by-year observation.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in all_schools data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLooking at the data, we see that it’s similar to what we’ve seen above, with additional schools.\n\n## show\ndf\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\nOur task is two-fold:\n\nGet the average of each test score (math, reading, science) across all schools within each year and save the summary data frame in an object.\nJoin the new summary data frame to the original data frame.\n\n\n1. Get summary\n\n## ---------------------------\n## process\n## ---------------------------\n\n## get test score summary \ndf_sum &lt;- df |&gt;\n    ## grouping by year so average within each year\n    group_by(year) |&gt;\n    ## get mean(&lt;score&gt;) for each test\n    summarize(math_m = mean(math),\n              read_m = mean(read),\n              science_m = mean(science))\n\n## show\ndf_sum\n\n# A tibble: 6 × 4\n   year math_m read_m science_m\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  1980   507    295.      798.\n2  1981   496.   293.      788.\n3  1982   506    302.      802.\n4  1983   500    293.      794.\n5  1984   490    300.      792.\n6  1985   500.   290.      794.\n\n\n\nQuick exercise\nThinking ahead, why do you think we created new names for the summarized columns? Why the _m ending?\n\n\n\n2. Join\nWhile one can merge using base R, dplyr uses the SQL language of joins, which can be conceptually clearer (particularly for those who already have experience with relational database structures). Here are the most common joins you will use:\n\nleft_join(x, y): keep all x, drop unmatched y\nright_join(x, y): keep all y, drop unmatched x\ninner_join(x, y): keep only matching\nfull_join(x, y): keep everything\n\n\n\n\nIn the Venn diagrams above, blue represents the observations that are kept, white the observations that are dropped.\n\n\nFor example, the result of a left join between data frame X and data frame Y will include all observations in X and those in Y that are also in X.\nX\n\n\n\nid\ncol_A\ncol_B\n\n\n\n\n001\na\n1\n\n\n002\nb\n2\n\n\n003\na\n3\n\n\n\nY\n\n\n\nid\ncol_C\ncol_D\n\n\n\n\n001\nT\n9\n\n\n002\nT\n9\n\n\n004\nF\n9\n\n\n\nXY (result of left join)\n\n\n\nid\ncol_A\ncol_B\ncol_C\ncol_D\n\n\n\n\n001\na\n1\nT\n9\n\n\n002\nb\n2\nT\n9\n\n\n003\na\n3\nNA\nNA\n\n\n\nObservations in both X and Y (001 and 002, above), will have data for the columns that were separately in X and Y before. Those in X only (003), will have missing values in the new columns that came from Y because they didn’t exist there. Observations in Y but not X (004) are dropped entirely.\nBack to our example…\nSince we want to join a smaller aggregated data frame, df_sum, to the original data frame, df, we’ll use a left_join(). The join functions will try to guess the joining variable (and tell you what it picked) if you don’t supply one, but we’ll specify one to be clear.\n\n## start with data frame...\ndf_joined &lt;- df |&gt;\n    ## pipe into left_join to join with df_sum using \"year\" as key\n    left_join(df_sum, by = \"year\")\n\n## show\ndf_joined\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nQuick exercise\nLook at the first 10 rows of df_joined. What do you notice about the new summary columns we added?",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#reshaping-data",
    "href": "04-wrangle-ii.html#reshaping-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Reshaping data",
    "text": "Reshaping data\nReshaping data is a common data wrangling task. Whether going from wide to long format or long to wide, it can be a painful process. But with a little practice, the ability to reshape data will become a powerful tool in your toolbox.\n\nDefinitions\nWhile there are various definitions of tabular data structure, the two you will most often come across are wide and long. Wide data are data structures in which all variable/values are columns. At the extreme end, every id will only have a single row:\n\n\n\n\n\n\n\n\n\n\nid\nmath_score_2019\nread_score_2019\nmath_score_2020\nread_score_2020\n\n\n\n\nA\n93\n88\n92\n98\n\n\nB\n99\n92\n97\n95\n\n\nC\n89\n88\n84\n85\n\n\n\nNotice how each particular score (by year) has its own column? Compare this to long data in which each observational unit (id test score within a given year) will have a row:\n\n\n\nid\nyear\ntest\nscore\n\n\n\n\nA\n2019\nmath\n93\n\n\nA\n2019\nread\n88\n\n\nA\n2020\nmath\n92\n\n\nA\n2020\nread\n98\n\n\nB\n2019\nmath\n99\n\n\nB\n2019\nread\n92\n\n\nB\n2020\nmath\n97\n\n\nB\n2020\nread\n95\n\n\nC\n2019\nmath\n89\n\n\nC\n2019\nread\n88\n\n\nC\n2020\nmath\n84\n\n\nC\n2020\nread\n85\n\n\n\nThe first wide and second long table present the same information in a different format. So why bother reshaping? The short answer is that you sometimes need one format and sometimes the other due to the demands of the analysis you want to run, the figure you want to plot, or the table you want to make.\nNB: Data in the wild are often some combination of these two types: wide-ish or long-ish. For an example, see our all_schools.csv data below, which is wide in some variables (test), but long in others (year). The point of defining long vs wide is not to have a testable definition, but rather to have a framework for thinking about how your data are structured and if that structure will work for your data analysis needs.\n\n\nExample: wide –&gt; long\nTo start, we’ll go back to the all_schools.csv file.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## reading again just to be sure we have the original data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): school\ndbl (4): year, math, read, science\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice how the data are wide in test: each school has one row per year, but each test gets its own column. While this setup can be efficient for storage, it’s not always the best for analysis or even just browsing. What we want is for the data to be long.\nInstead of each test having its own column, we would like to make the data look like our long data example above, with each row representing a single school, year, test, score:\n\n\n\n\n\n\n\n\n\nschool\nyear\ntest\nscore\n\n\n\n\nBend Gate\n1980\nmath\n515\n\n\nBend Gate\n1980\nread\n281\n\n\nBend Gate\n1980\nscience\n808\n\n\n…\n…\n…\n…\n\n\n\nAs with joins, you can reshape data frames using base R commands. But again, we’ll use tidyverse functions in the tidyr library. Specifically, we’ll rely on the tidyr pivot_longer() and pivot_wider() commands.\n\npivot_longer()\nThe pivot_longer() function can take a number of arguments, but the core things it needs to know are:\n\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\ncols: the names of the columns that you want to pivot into values of a single new column (thereby making the data frame “longer”)\nnames_to: the name of the new column that will contain the names of the cols you just listed\nvalues_to: the name of the column where the values in the cols you listed will go\n\nIn our current situation, our cols to pivot are \"math\", \"read\", and \"science\". Since they are test types, we’ll call our names_to column \"test\" and our values_to column \"score\".\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df |&gt;\n    ## cols: current test columns\n    ## names_to: where \"math\", \"read\", and \"science\" will go\n    ## values_to: where the values in cols will go\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"),\n                 names_to = \"test\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQuick (ocular test) exercise\nHow many rows did our initial data frame df have? How many unique tests did we have in each year? When reshaping from wide to long, how many rows should we expect our new data frame to have? Does our new data frame have that many rows?\n\n\n\n\nExample: long –&gt; wide\n\npivot_wider()\nNow that we have our long data, let’s reshape it back to wide format using pivot_wider(). In this case, we’re doing just the opposite from before — here are the main arguments you need to attend to:\n\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\nnames_from: the name of the column that contains the values which will become new column names\nvalues_from: the name of the column that contains the values associated with the values in names_from column; these will go into the new columns.\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## long to wide\ndf_wide &lt;- df_long |&gt;\n    ## names_from: values in this column will become new column names\n    ## values_from: values in this column will become values in new cols\n    pivot_wider(names_from = \"test\",\n                values_from = \"score\")\n\n## show\ndf_wide\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\nQuick exercise\nIn this case, our new wide data frame, df_wide, should be the same as our initial data frame. Is it? How can you tell?\n\n\n\n\nExample: wide –&gt; long with corrections\nUnfortunately, it’s not always so clear cut to reshape data. In this second example, we’ll again reshape from wide to long, but we’ll have to munge our data a bit after the reshape to make it analysis ready.\nFirst, we’ll read in a second file all_schools_wide.csv. This file contains the same information as before, but in a very wide format: each school has only one row and each test by year value gets its own column in the form &lt;test&gt;_&lt;year&gt;.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in very wide test score data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools-wide.csv\"))\n\nRows: 4 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): school\ndbl (18): math_1980, read_1980, science_1980, math_1981, read_1981, science_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## show\ndf\n\n# A tibble: 4 × 19\n  school       math_1980 read_1980 science_1980 math_1981 read_1981 science_1981\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Bend Gate          515       281          808       503       312          814\n2 East Heights       501       318          782       487       323          813\n3 Niagara            514       292          787       499       268          762\n4 Spottsville        498       288          813       494       270          765\n# ℹ 12 more variables: math_1982 &lt;dbl&gt;, read_1982 &lt;dbl&gt;, science_1982 &lt;dbl&gt;,\n#   math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;, science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;,\n#   read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;, math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;,\n#   science_1985 &lt;dbl&gt;\n\n\nSecond, we can pivot_longer() as we did before using the following values for our key arguments:\n\ndata : df (but piped in using |&gt;)\ncols : use special tidyselect helper function contains() to select all test by year columns\nnames_to: test_year\nvalues_to: score\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df |&gt;\n    ## NB: contains() looks for \"19\" in name: if there, it adds it to cols\n    pivot_longer(cols = contains(\"19\"),\n                 names_to = \"test_year\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 3\n   school    test_year    score\n   &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n 1 Bend Gate math_1980      515\n 2 Bend Gate read_1980      281\n 3 Bend Gate science_1980   808\n 4 Bend Gate math_1981      503\n 5 Bend Gate read_1981      312\n 6 Bend Gate science_1981   814\n 7 Bend Gate math_1982      514\n 8 Bend Gate read_1982      316\n 9 Bend Gate science_1982   816\n10 Bend Gate math_1983      491\n# ℹ 62 more rows\n\n\n\nQuick exercise\nWhy did we use “19” as our value in the contains() function? HINT: use the names() function to return a list of the original data frame (df) column names.\n\nThis mostly worked to get our data long, but now we have this weird combined test_year column. What we really want are two columns, one for the year and one for the test type. We can fix this using tidyr separate() function with the following arguments:\n\ndata: our df_long object, piped in using |&gt;\ncol: the column we want to split (test_year)\ninto: the names of the new columns to create from col (test and year)\nsep: the name of the character that splits the values in col, so R knows how to fill each of the into columns (\"_\")\n\n\n## separate test_year into two columns, filling appropriately\ndf_long_fix &lt;- df_long |&gt;\n    ## col: the column to split\n    ## into: names of resulting splits\n    ## sep: the split point --&gt; left to \"test\", right to \"year\"\n    separate(col = \"test_year\",\n             into = c(\"test\", \"year\"),\n             sep = \"_\")\n\n## show\ndf_long_fix\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# ℹ 62 more rows\n\n\n\nQuick exercise\nRedo the last few steps in a single combined chain using pipes. That is, start with df (which contains all_schools_wide.csv), reshape long, and fix so that you end up with four columns — all in a single piped chain.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#final-note",
    "href": "04-wrangle-ii.html#final-note",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Final note",
    "text": "Final note\nJust as all data sets are unique, so too are the particular steps you may need to take to append, join, or reshape your data. Even experienced coders rarely get all the steps correct the first try. Be prepared to spend time getting to know your data and figuring out, through trial and error, how to wrangle it so that it meets your analytic needs. Code books, institutional/domain knowledge, and patience are your friends here!",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#questions",
    "href": "04-wrangle-ii.html#questions",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Questions",
    "text": "Questions\n\nCompute the average test score by region and join back into the full data frame. Next, compute the difference between each student’s test score and that of the region. Finally, return the mean of these differences by region.\nCompute the average test score by region and family income level. Join back to the full data frame. HINT You can join on more than one key.\nSelect the following variables from the full data set:\n\nstu_id\nx1stuedexpct\nx1paredexpct\nx4evratndclg\n\nFrom this reduced data frame, reshape the data frame so that it is long in educational expectations, meaning that each observation should have two rows, one for each educational expectation type.\ne.g. (your column names and values may be different)\n\n\n\nstu_id\nexpect_type\nexpectation\nx4evratndclg\n\n\n\n\n0001\nx1stuedexpct\n6\n1\n\n\n0001\nx1paredexpct\n7\n1\n\n\n0002\nx1stuedexpct\n5\n1\n\n\n0002\nx1paredexpct\n5\n1\n\n\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_4.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "06-viz-ii.html#libraries-functions-and-paths",
    "href": "06-viz-ii.html#libraries-functions-and-paths",
    "title": "II: Customization",
    "section": "Libraries, functions, and paths",
    "text": "Libraries, functions, and paths\nIn addition to tidyverse, we’ll add a new library, patchwork, that we’ll use toward the end of the lesson. If you haven’t already downloaded it, be sure to install it first using install.packages(\"patchwork\").\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\nWe’ll need to convert and then replace some missing values in the lesson, so we’ll include our user-written function, fix_missing(), that we first used in the programming lesson.\n\n## ---------------------------\n## functions\n## ---------------------------\n\n## utility function to convert values to NA\nfix_missing &lt;- function(x, miss_val) {\n  x &lt;- ifelse(x %in% miss_val, NA, x)\n  return(x)\n}\n\nFinally, we’ll load the data file we’ll be using, hsls_small.csv. Since we already know about the structure of hsls_small.csv, we’ll use the read_csv() argument show_col_types = FALSE to prevent all the extra console output when we read in the data file.\n\n## ---------------------------\n## input data\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"), show_col_types = FALSE)",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#initial-plot-with-no-formatting",
    "href": "06-viz-ii.html#initial-plot-with-no-formatting",
    "title": "II: Customization",
    "section": "Initial plot with no formatting",
    "text": "Initial plot with no formatting\nRather than make a variety of plots, we’ll focus on making and incrementally improving a single figure (with some slight variations along the way). In general, we’ll be looking at math test scores via the x1txtmscor data column.\nLet’s start with the most basic histogram we can make. But first, we need to fix our variable of interest. As you may recall from an earlier lesson, x1txmtscor is a normed variable with a mean of 50 and standard deviation of 10. That means the negative values need to be converted to NA values and, for our plotting purposes, dropped.\n\n## -----------------------------------------------------------------------------\n## initial plain plot\n## -----------------------------------------------------------------------------\n\n## fix missing values for text score and then drop missing\ndf &lt;- df |&gt;\n  mutate(math_test = fix_missing(x1txmtscor, -8)) |&gt;\n  drop_na(math_test)\n\nNow we can make our histogram with no extra settings:\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram()\n\n## show\np\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nSo there it is. Let’s make it better.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#titles-and-captions",
    "href": "06-viz-ii.html#titles-and-captions",
    "title": "II: Customization",
    "section": "Titles and captions",
    "text": "Titles and captions\nThe easiest things to improve on a figure are the title, subtitle, axis labels, and caption. As with a lot of ggplot2 commands, there are a few different ways to set these labels, but the most straightforward way is to use the labs() function as part of the ggplot() chain. Notice that we’ve added it to the end. (Also notice that we’ve set the bins = 30 argument within geom_histogram(), which is the default and will prevent a message from popping up each time.)\n\n## -----------------------------------------------------------------------------\n## titles and captions\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot, showing placeholder titles/labels/captions\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Title\",\n       subtitle = \"Subtitle\",\n       caption = \"Caption\",\n       x = \"X axis label\",\n       y = \"Y axis label\")\n\n## show \np\n\n\n\n\n\n\n\n\nRather than accurately labeling the figure, I’ve repeated the arguments in strings so that it’s clearer where every piece goes. The title is of course on top, with the subtitle in a smaller font just below. The x and y axis labels go with their respective axes and the caption is right-aligned below the figure. You don’t have to use all of these options for every figure. If you don’t want to use one, you have a couple of options:\n\nIf the argument would otherwise be blank (title, subtitle, and caption), you can just leave the argument out of labs()\nIf the argument will be filled, as is the case on the axes (ggplot will use the variable name by default), you can use NULL\n\nTo make our figure nicer, we’ll add a title, axis labels, and caption describing the data source. We don’t really need a subtitle and since there’s no default value, we’ll just leave it out.\n\n## ---------------------------\n## titles and captions: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nThat looks better. Now we’ll move to improving the axis scales.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#axis-formatting",
    "href": "06-viz-ii.html#axis-formatting",
    "title": "II: Customization",
    "section": "Axis formatting",
    "text": "Axis formatting\nIn general, the default tick mark spacing and accompanying labels are pretty good. But sometimes we want to change them, sometimes to have fewer ticks and sometimes to have more. For this figure, we could use more ticks on the x axis to make differences in math test score clearer. While we’re at, we’ll increase the number of tick marks on the y axis too.\nTo change these values, we need to use scale_&lt; aesthetic &gt;_&lt; distribution &gt; function. These may seem strange at first, but they follow a logic. Specifically:\n\n&lt; aesthetic &gt;: x, y, fill, colour, etc (what is being changed?)\n&lt; distribution &gt;: is the underlying variable continuous, discrete, or do you want to make a manual change?\n\nTo change our x and y tick marks we use:\n\nscale_x_continuous()\nscale_y_continuous()\n\nWe use x and y because those are the aesthetics being adjusted and we use continuous in both cases because math_test on the x axis and the histogram counts on the y axis are both continuous variables.\nThere are a number of options within the scale_*() family of functions — and they can change depending on which scale_*() function you use — but we’ll focus on using two:\n\nbreaks: where the major lines are going (they get numbers on the axis)\nminor_breaks: where the minor lines are going (they don’t get numbers on the axis)\n\nBoth breaks and minor_breaks take a vector of numbers. We can put each number in manually using c() (e.g., c(0, 10, 20, 30, 40)), but a better way is to use R’s seq() function: seq(start, end, by). Notice that within each scale_*() function, we use the same start and stop arguments for each seq(). We only change the by argument. This will give us axis numbers at spaced intervals with thinner, unnumbered lines between.\n\n## -----------------------------------------------------------------------------\n## axis formatting\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 100),\n                     minor_breaks = seq(from = 0, to = 2500, by = 50)) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nWe certainly have more lines now. Maybe too many on the y axis, which is a sort of low-information axis (do we need really that much detail for histogram counts?). Let’s keep what we have for the x axis and increase the by values of the y axis.\n\n## ---------------------------\n## axis formatting: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  geom_histogram(bins = 30) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nThat seems like a better balance. We’ll stick with that and move on to legend labels.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#legend-labels",
    "href": "06-viz-ii.html#legend-labels",
    "title": "II: Customization",
    "section": "Legend labels",
    "text": "Legend labels\nLet’s make our histogram a little more complex by separating math scores by parental education. Specifically, we’ll use a binary variable that represents, did either parent attend college? First, we need to create a new variable, pared_coll, from the ordinal variable, x1paredu. You can check the discussion of why we create the variable this way from the first plotting lesson.\n\n## -----------------------------------------------------------------------------\n## legend labels\n## -----------------------------------------------------------------------------\n\n## add indicator that == 1 if either parent has any college\ndf &lt;- df |&gt;\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0))\n\nNow we’ll make our same histogram, but add the fill aesthetic. As we’ve done in the past, we’ll wrap our new binary variable in as_factor() so ggplot understands that 0/1 are discrete values. We’ll also modify geom_histogram() to use smaller bins, a new \"identity\" position, and make the fill colors semi-transparent with alpha.\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test, fill = as_factor(pared_coll))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nExcept for our labels and tick mark adjustments, this looks similar to what we’ve made before. The problem with this figure is two-fold:\n\nThe legend title is not very nice — it’s just the variable name wrapped in the as_factor() function\nThe legend itself isn’t very informative: what do 0 and 1 mean?\n\nTo fix this, we’ll switch from using as_factor() to factor(), which has more options. We’ll add the following function to aes() in the initial ggplot() function:\nfill = factor(pared_coll,\n              levels = c(0,1),\n              labels = c(\"No college\",\"College\"))\nWith factor(), we first say which variable should be converted to a factor, pared_coll. Next, we manually set the levels of the factor. That’s easy here because we only have two levels, 0 and 1, which we can set using levels = c(0,1). Finally, we can add labels to the levels. The main thing to make sure of is that the order of our labels match the order of the levels. Since\n\n0 := no parental college\n1 := at least one parent went to college\n\nwe use labels = c(\"No college\",\"College\") which match the c(0,1) order in levels. Other than that, everything else is the same.\nNOTE: we could have made pared_coll a factor when we initially created it. In general, that is easier if we want the variable to always be a factor and we’re making a large number of figures. But for our purposes at the moment, we just convert it on the fly inside ggplot.\n\n## ---------------------------\n## legend labels: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test,\n                          fill = factor(pared_coll,\n                                        levels = c(0,1),\n                                        labels = c(\"No college\",\n                                                   \"College\")))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nCloser, but not quite! The 0/1 have been given proper labels, but the legend title is even worse! Not only is it not nice to look at it, it’s now so long that it squishes our plot. What we need to add is a scale_*() function to fix it. Since we’re working with fill and a discrete variable (remember: the factor only takes on countable values, two in this case), then we’ll use scale_fill_discrete(). We don’t really need to do anything other than give the legend that goes with the fill aesthetic a name, so that’s the argument we use: name.\nLet’s add that to the chain just below our other scale_*() functions before labs().\n\n## ---------------------------\n## legend labels: ver 3\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test,\n                          fill = factor(pared_coll,\n                                        levels = c(0,1),\n                                        labels = c(\"No college\",\n                                                   \"College\")))) +\n  geom_histogram(bins = 50, alpha = 0.5, position = \"identity\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  scale_fill_discrete(name = \"Parental education\") +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#facet-labels",
    "href": "06-viz-ii.html#facet-labels",
    "title": "II: Customization",
    "section": "Facet labels",
    "text": "Facet labels\nNow that we’ve done the hard work of setting a factor, we can use the same bit of code to more properly label facets. Instead of splitting the test score histogram by color within the same plot area like we do above, let’s say we use facet_wrap() instead. This will give us discrete plot areas for each value of pared_coll.\nTo convert to a facetted figure, we’ll just move the factor(...) information from fill to facet_wrap(). Since we don’t have color changes based on fill, we can remove alpha and position from geom_histogram().\n\n## -----------------------------------------------------------------------------\n## facet labels\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\nNotice how each facet has a proper label. Easy enough! Note that there is another way to fix facet labels using the labeller() function, but setting the labels using factor() will work for most situations.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#themes",
    "href": "06-viz-ii.html#themes",
    "title": "II: Customization",
    "section": "Themes",
    "text": "Themes\nNow that we’ve largely set our various labels, we can adjust the overall look of the figure. If you did the mapping lesson you may have noticed the we called theme_void() on all of our maps, which completely removed all the plotting structure: titles, labels, ticks, axes, etc. That’s the extreme end of adjusting the theme!\nLet’s start with simply removing the gray area of the figure. To do this, we use the theme() function at the end of our ggplot chain. Specifically, we’ll call the argument panel.background and remove it using element_blank().\n\n## -----------------------------------------------------------------------------\n## themes\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank())\n\n## show \np\n\n\n\n\n\n\n\n\nSo we removed the panel, but since our grid lines were white to offset the gray, we don’t have grid lines any more. These would be helpful! We can add them back in, but make them gray using panel.grid.major and panel.grid.minor (notice the similar construction of the names) and setting them with element_line(colour = \"gray\").\n\n## ---------------------------\n## themes: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major = element_line(colour = \"gray\"),\n        panel.grid.minor = element_line(colour = \"gray\"))\n\n## show \np\n\n\n\n\n\n\n\n\nThat returned our lines, but let’s say that we don’t really care about the horizontal lines. Rather than have the reader focus on counts, we really just want them to focus on the distribution around the math score. If we want to adjust the panel grids one axis at a time, we use the same stub and add *.x and *.y as necessary. Notice how for the x panel grids we use the old code, but for the y panel grids, return to using element_blank().\n\n## ---------------------------\n## themes: ver 3\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        panel.grid.minor.x = element_line(colour = \"grey\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n## show \np\n\n\n\n\n\n\n\n\nGreat! Now we only have vertical grid lines. Of course, we don’t really need the y axis ticks and labels now. We can ditch them by setting axis.title.y, axis.text.y, and axis.ticks.y to element_blank(). Notice that since we call this after labs(), our label for y is ignored.\n\n## ---------------------------\n## themes: ver 4\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme(panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        panel.grid.minor.x = element_line(colour = \"grey\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n## show \np\n\n\n\n\n\n\n\n\nOkay! We have what we set out to get.\nRemember, all the elements of a ggplot figure can be adjusted. That said, there are some shortcut theme_*() functions we can use that will save some typing. For example, theme_bw() will give something very similar to what we built before removing the horizontal lines.\n\n## ---------------------------\n## themes: ver 5\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw()\n\n## show \np\n\n\n\n\n\n\n\n\nThere are other complete themes you might find useful in your work. If you want to make manual changes, here’s the full list of arguments and here are options for theme elements. Check them out!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#multiple-plots-with-patchwork",
    "href": "06-viz-ii.html#multiple-plots-with-patchwork",
    "title": "II: Customization",
    "section": "Multiple plots with patchwork",
    "text": "Multiple plots with patchwork\nIn this final section, we’ll practice putting multiple figures together. All the plots we’ve made so far have used the same underlying data. Even when we’ve used facet_wrap() to make multiple plot areas, they were related in some way. But what if we want to neatly paste different unrelated plots into a single figure, laid out exactly the way we want?\nWe use the patchwork library!\nWe’ll start by making a new figure. Rather than splitting math scores by parental education, we’ll split by whether the student is below or above 185% of the federal poverty level. As before, we’ll remove missing values from the variable, x1poverty185, and create a new variable, pov185, that takes a binary 0 (below) / 1 (above) set of values.\n\n## -----------------------------------------------------------------------------\n## multiple plots with patchwork\n## -----------------------------------------------------------------------------\n\n## remove missing values\ndf &lt;- df |&gt;\n  mutate(pov185 = fix_missing(x1poverty185, c(-8,-9))) |&gt;\n  drop_na(pov185)\n\n## make histogram\np2 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pov185,\n                      levels = c(0,1),\n                      labels = c(\"Below 185% poverty line\",\n                                 \"Above 185% poverty line\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by poverty level\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw()\n\n## show\np2\n\n\n\n\n\n\n\n\nNow that we have our new figure, let’s paste it side by side (left-right) with our first figure. Once we’ve loaded the patchwork library (like we already did at the top of the script), we can use a + sign between out two ggplot objects: p + p2. We’ll store that in a new object, pp, and then call that.\n\n## ---------------------------\n## patchwork: side by side\n## ---------------------------\n\n## use plus sign for side by side\npp &lt;- p + p2\n\n## show\npp\n\n\n\n\n\n\n\n\nDefinitely works, but it’s a little squished. Rather than side by side, let’s stack them this time. To stack two plots with patchwork, use a forward slash, /.\n\n## ---------------------------\n## patchwork: stack\n## ---------------------------\n\n## use forward slash to stack\npp &lt;- p / p2\n\n## show\npp\n\n\n\n\n\n\n\n\nThat looks better!\nPatchwork is sufficiently flexible that you can arrange many figures. Let’s create yet another figure: test score by socioeconomic status. After cleaning up that variable, we make a new plot.\n\n## ---------------------------\n## patchwork: 2 over 1\n## ---------------------------\n\n## drop missing SES values\ndf &lt;- df |&gt;\n  mutate(ses = fix_missing(x1ses, -8)) |&gt;\n  drop_na(ses)\n\n## create third histogram of just SES\np3 &lt;- ggplot(data = df,\n             mapping = aes(x = x1ses)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = -5, to = 5, by = 1),\n                     minor_breaks = seq(from = -5, to = 5, by = 0.5)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1000, by = 100),\n                     minor_breaks = seq(from = 0, to = 1000, by = 25)) +\n  labs(title = \"Socioeconomic score\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"SES\",\n       y = \"Count\") +\n  theme_bw()\n\n## show\np3\n\n\n\n\n\n\n\n\nNow that we have this new plot, let’s paste it to the other figures in a 2 over 1 pattern. To make that clear to patchwork, we use parentheses just like we might in algebra (remember PEMDAS?) to set priority. The parentheses paste the first two figures side by side and then stack this new combined plot above the new plot.\n\n## use parentheses to put figures together (like in algebra)\npp &lt;- (p + p2) / p3\n\n## show\npp\n\n\n\n\n\n\n\n\nBecause of the new structure, the side by side of the first two figures doesn’t look quite as squished as before. That said, labels and titles still overlap. We also have redundant information. Do we really need that data caption three times?\nLet’s do some clean up to make a nice final figure. The easiest thing will be to remake the figures. This time we’ll:\n\nremove the caption argument from labels (we’ll add it in later)\nuse theme_bw(base_size = 8) to change the overall size of the font. This should help with all the overlapping text.\n\n\n## ---------------------------\n## patchwork: clean up\n## ---------------------------\n\n## Redo the above plots so that:\n## - remove some redundant captions\n## - change base_size so font is smaller\n\n## test score by parental education\np1 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pared_coll,\n                      levels = c(0,1),\n                      labels = c(\"No college\",\"College\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by parental education\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw(base_size = 8)\n\n## test score by poverty level\np2 &lt;- ggplot(data = df,\n            mapping = aes(x = math_test)) +\n  facet_wrap(~ factor(pov185,\n                      levels = c(0,1),\n                      labels = c(\"Below 185% poverty line\",\n                                 \"Above 185% poverty line\"))) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 1500, by = 100),\n                     minor_breaks = seq(from = 0, to = 1500, by = 25)) +\n  labs(title = \"Math test scores by poverty level\",\n       x = \"Math score\",\n       y = \"Count\") +\n  theme_bw(base_size = 8)\n\n## create third histogram of just SES\np3 &lt;- ggplot(data = df,\n             mapping = aes(x = x1ses, y = math_test)) + \n  geom_point() +\n  scale_x_continuous(breaks = seq(from = -5, to = 5, by = 1),\n                     minor_breaks = seq(from = -5, to = 5, by = 0.5)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 100, by = 10),\n                     minor_breaks = seq(from = 0, to = 100, by = 5)) +\n  labs(title = \"Math test scores by socioeconomic status\",\n       x = \"SES\",\n       y = \"Math score\") +\n  theme_bw(base_size = 8)\n\n## use parentheses to put figures together (like in algebra)\npp &lt;- (p1 + p2) / p3\n\nWe’ve remade our figures and used patchwork to put them together. But as a final step, we’ll use patchwork’s plot_annotation() argument to add:\n\noverall title\na single caption\nplot-specific tags that are useful for referencing certain plots (i.e., you can say “plot / facet A” rather than “the top left plot / facet”)\n\nWe add plot_annotation() using a + sign just like with a normal ggplot chain.\n\n## add annotations\npp &lt;- pp + plot_annotation(\n  title = \"Math scores across various factors\",\n  caption = \"Data: High School Longitudinal Study, 2009\",\n  tag_levels = \"A\"\n)\n\n## show\npp\n\n\n\n\n\n\n\n\nDone and looking pretty good! Well, the blob of in plot C maybe isn’t that useful…\nWe can always do more, of course, but remember that a figure doesn’t need to be complicated to be good. In fact, simpler is often better. The main thing is that it is clean and clear and tells the story you want the reader to hear. What exactly that looks like is up to you and your project!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#question",
    "href": "06-viz-ii.html#question",
    "title": "II: Customization",
    "section": "Question",
    "text": "Question\n\nMake 3-4 different figures showing relationships between variables in hsls_small. You can remake some figures we made in prior lesson, but whatever you do, make sure that data are clean, everything is properly labeled, tick marks are appropriately spaced and numbered — just that the figures look nice. Once done, put them together in a nice arrangement using patchwork. This may mean making some adjustments so that there’s no redundant information in the final figure.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_6.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#setup",
    "href": "08-wrangle-iii.html#setup",
    "title": "III: Working with strings & dates",
    "section": "Setup",
    "text": "Setup\nAs before, we’ll continue working within the tidyverse. We’ll focus, however, on using two specific libraries:\n\nstringr for strings\nlubridate for dates\n\nYou may have noticed already that when we load the tidyverse library with library(tidyverse), the stringr library is already loaded. The lubridate library, though part of the tidyverse, is not. We need to load it separately.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NB: The stringr library is loaded with tidyverse, but\n## lubridate is not, so we need to load it separately\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\n\nNB: As we have done in the past few lessons, we’ll run this script assuming that our working directory is set to the scripts directory.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-1-working-with-strings",
    "href": "08-wrangle-iii.html#part-1-working-with-strings",
    "title": "III: Working with strings & dates",
    "section": "Part 1: Working with strings",
    "text": "Part 1: Working with strings\nTo practice working with strings, we’ll use data from Integrated Postsecondary Education Data System (IPEDS):\n\nThe National Center for Education Statistics (NCES) administers the Integrated Postsecondary Education Data System (IPEDS), which is a large-scale survey that collects institution-level data from postsecondary institutions in the United States (50 states and the District of Columbia) and other U.S. jurisdictions. IPEDS defines a postsecondary institution as an organization that is open to the public and has the provision of postsecondary education or training beyond the high school level as one of its primary missions. This definition includes institutions that offer academic, vocational and continuing professional education programs and excludes institutions that offer only avocational (leisure) and adult basic education programs. Definitions for other terms used in this report may be found in the IPEDS online glossary.\nNCES annually releases national-level statistics on postsecondary institutions based on the IPEDS data. National statistics include tuition and fees, number and types of degrees and certificates conferred, number of students applying and enrolled, number of employees, financial statistics, graduation rates, student outcomes, student financial aid, and academic libraries.\n\nYou can find more information about IPEDS here. As higher education scholars, IPEDS data are a valuable resource that you may often turn to (I do).\nWe’ll use one file (which can be found here), that covers institutional characteristics for one year:\n\nDirectory information, 2007 (hd2007.csv)\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data and lower all names using rename_all(tolower)\ndf &lt;- read_csv(file.path(\"data\", \"hd2007.csv\")) |&gt;\n    rename_all(tolower)\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#finding-str_detect",
    "href": "08-wrangle-iii.html#finding-str_detect",
    "title": "III: Working with strings & dates",
    "section": "Finding: str_detect()",
    "text": "Finding: str_detect()\nSo far, we’ve filtered data using dplyr’s filter() verb. When matching a string, we have used == (or != for negative match). For example, if we wanted to limit our data to only those institutions in Florida, we could filter using the stabbr column:\n\n## filter using state abbreviation (not saving, just viewing)\ndf |&gt;\n    filter(stabbr == \"FL\")\n\n# A tibble: 316 × 59\n   unitid instnm    addr  city  stabbr zip    fips obereg chfnm chftitle gentele\n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 132268 Wyotech-… 470 … Ormo… FL     32174    12      5 Stev… Preside… 3.86e12\n 2 132338 The Art … 1799… Fort… FL     3331…    12      5 Char… Preside… 9.54e13\n 3 132374 Atlantic… 4700… Coco… FL     3306…    12      5 Robe… Director 7.54e 9\n 4 132408 The Bapt… 5400… Grac… FL     32440    12      5 Thom… Preside… 8.50e 9\n 5 132471 Barry Un… 1130… Miami FL     3316…    12      5 Sist… Preside… 8.01e 9\n 6 132523 Gooding … 615 … Pana… FL     32401    12      5 Dr. … CRNA Ph… 8.51e 9\n 7 132602 Bethune-… 640 … Dayt… FL     3211…    12      5 Dr T… Preside… 3.86e 9\n 8 132657 Lynn Uni… 3601… Boca… FL     3343…    12      5 Kevi… Preside… 5.61e 9\n 9 132666 Bradento… 5505… Brad… FL     34209    12      5 A. P… CEO      9.42e 9\n10 132675 Bradford… 609 … Star… FL     32091    12      5 Rand… Director 9.05e 9\n# ℹ 306 more rows\n# ℹ 48 more variables: ein &lt;chr&gt;, opeid &lt;chr&gt;, opeflag &lt;dbl&gt;, webaddr &lt;chr&gt;,\n#   adminurl &lt;chr&gt;, faidurl &lt;chr&gt;, applurl &lt;chr&gt;, sector &lt;dbl&gt;, iclevel &lt;dbl&gt;,\n#   control &lt;dbl&gt;, hloffer &lt;dbl&gt;, ugoffer &lt;dbl&gt;, groffer &lt;dbl&gt;, fpoffer &lt;dbl&gt;,\n#   hdegoffr &lt;dbl&gt;, deggrant &lt;dbl&gt;, hbcu &lt;dbl&gt;, hospital &lt;dbl&gt;, medical &lt;dbl&gt;,\n#   tribal &lt;dbl&gt;, locale &lt;dbl&gt;, openpubl &lt;dbl&gt;, act &lt;chr&gt;, newid &lt;dbl&gt;,\n#   deathyr &lt;dbl&gt;, closedat &lt;chr&gt;, cyactive &lt;dbl&gt;, postsec &lt;dbl&gt;, …\n\n\nThis works well because the stabbr column, even though it uses strings, is regular. But what happens when the strings aren’t so regular? For example, let’s look the different titles chief college administrators take.\n\n## see first few rows of distinct chief titles\ndf |&gt;\n    distinct(chftitle)\n\n# A tibble: 556 × 1\n   chftitle          \n   &lt;chr&gt;             \n 1 Commandant        \n 2 President         \n 3 Chancellor        \n 4 Interim President \n 5 CEO               \n 6 Acting President  \n 7 Director          \n 8 President/CEO     \n 9 Interim Chancellor\n10 President/COO     \n# ℹ 546 more rows\n\n\nWe find over 500 unique titles. Just looking at the first 10 rows, we see that some titles are pretty similar — President vs. CEO vs. President/CEO — but not exactly the same. Let’s look again, but this time get counts of each distinct title and arrange from most common to least.\n\n## return the most common titles\ndf |&gt;\n    ## get counts of each type\n    count(chftitle) |&gt;\n    ## arrange in descending order so we see most popular at top\n    arrange(desc(n))\n\n# A tibble: 556 × 2\n   chftitle               n\n   &lt;chr&gt;              &lt;int&gt;\n 1 President           3840\n 2 Director             560\n 3 Chancellor           265\n 4 Executive Director   209\n 5 Owner                164\n 6 Campus President     116\n 7 Superintendent       105\n 8 CEO                   90\n 9 &lt;NA&gt;                  85\n10 Interim President     75\n# ℹ 546 more rows\n\n\n\nQuick exercise\nWhat do you notice about the data frames returned by distinct() and count()? What’s the same? What does count() do that distinct() does not?\n\nGetting our counts and arranging, we can see that President is by far the most common title. That said, we also see Campus President and Interim President (and before we saw Acting President as well).\nIf your research question asked, how many chief administrators use the title of “President”? regardless the various iterations, you can’t really use a simple == filter any more. In theory, you could inspect your data, find the unique versions, get counts of each of those using ==, and then sum them up — but that’s a lot of work and likely to be error prone!\nInstead, we can use the stringr function str_detect(), which looks for a pattern in a vector of strings:\nstr_detect(&lt; vector of strings &gt;, &lt; pattern &gt;)\nGoing item by item in the vector, it compares what it sees to the pattern. If it matches, then it returns TRUE; it not, then FALSE. Here’s a toy example:\n\n## string vector example\nfruits &lt;- c(\"green apple\", \"banana\", \"red apple\")\n\n## search for \"apple\", which should be true for the first and third item\nstr_detect(fruits, \"apple\")\n\n[1]  TRUE FALSE  TRUE\n\n\nWe can use str_detect() inside filter() to select only certain rows in our data frame. In our case, we want only those observations in which the title \"President\" occurs in the chftitle column. Because we’re only detecting, as long as \"President\" occurs anywhere in the title, we’ll get that row back.\n\n## how many use some form of the title president?\ndf |&gt;\n    ## still starting with our count\n    count(chftitle) |&gt;\n    ## ...but keeping only those titles that contain \"President\"\n    filter(str_detect(chftitle, \"President\")) |&gt;\n    ## arranging as before\n    arrange(desc(n))\n\n# A tibble: 173 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 163 more rows\n\n\nNow we’re seeing many more versions. We can even more clearly see a few titles that are almost certainly the same title, but were just inputted differently — President/CEO vs. President and CEO vs. President & CEO.\n\nQuick exercise\nIgnoring the sub-counts of the various versions, how many chief administrators have the word “President” in their title?\n\nSeeing the different versions of basically the same title should have us stopping to think: since it seems that this data column contains free form input (e.g. Input chief administrator title:), maybe we should allow for typos? The easiest: Is there any reason to assume that “President” will be capitalized?\n\nQuick exercise\nWhat happens if we search for “president” with a lowercase “p”?\n\nAh! We find a few stragglers. How can we restructure our filter so that we get these, too? There are at least two solutions.\n\n1. Use regular expressions\nRegular expressions (aka regex) are strings that use a special syntax to create patterns that can be used to match other strings. They are very useful when you need to match strings that have some general form, but may differ in specifics.\nWe already used this technique in the a prior lesson when we matched columns in the all_schools_wide.csv with contains(\"19\") so that we could pivot_longer(). Instead of naming all the columns specifically, we recognized that each column took the form of &lt;test&gt;_19&lt;YY&gt;. This is a type of regular expression.\nIn the tidyverse some of the stringr and tidyselect helper functions abstract-away some of the nitty-gritty behind regular expressions. Knowing a little about regular expression syntax, particularly how it is used in R, can go a long way.\nIn our first case, we can match strings that have a capital P President or lowercase p president using square brackets ([]). If we want either “P” or “p”, then we can use the regex, [Pp], in place of the first character: \"[Pp]resident\". This will match either \"President\" or \"president\".\n\n## solution 1: look for either P or p\ndf |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(chftitle, \"[Pp]resident\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 175 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 165 more rows\n\n\nThough we don’t see the new observations in the abbreviated output, we note that the number of rows has increased by two. This means that there are at least two title formats in which \"president\" is lowercase and that we weren’t picking up when we only used the uppercase version of \"President\" before.\n\n\n2. Put everything in the same case and match with that case\nAnother solution, which is probably much easier in this particular case, is to set all potential values in chftitle to the same case and then match using that case. In many situations, this is preferable since you don’t need to guess cases up front.\nWe won’t change the values in chftitle permanently — only while filtering. To compare apples to apples (rather than \"Apples\" to \"apples\"), we’ll wrap our column name with the function str_to_lower(), which will make character lowercase, and match using lowercase \"president\".\n\n## solution 2: make everything lowercase so that case doesn't matter\ndf |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(str_to_lower(chftitle), \"president\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 177 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 167 more rows\n\n\nWe recover another two titles when using this second solution. Clearly, our first solution didn’t account for other cases (perhaps “PRESIDENT\"?).\nIn general, I find it’s a good idea to try a solution like the second one before a more complicated one like the first. But because every problem is different, so too are the solutions. You may find yourself using a combination of the two.\n\nNot-so-quick exercise\nAnother chief title that was high on the list was “Owner.” How many institutions have an “Owner” as their chief administrator? Of these, how many are private, for-profit institutions (control == 3)? How many have the word “Beauty” in their name?\n\n\n\nReplace using string position: str_sub()\nIn addition to filtering data, we sometimes need to create new variables from pieces of exiting variables. For example, let’s look at the zip code values that are included in the file.\n\n## show first few zip code values\ndf |&gt;\n    select(unitid, zip)\n\n# A tibble: 7,052 × 2\n   unitid zip       \n    &lt;dbl&gt; &lt;chr&gt;     \n 1 100636 36112-6613\n 2 100654 35762     \n 3 100663 35294-0110\n 4 100690 36117-3553\n 5 100706 35899     \n 6 100724 36101-0271\n 7 100733 35401     \n 8 100751 35487-0166\n 9 100760 35010     \n10 100812 35611     \n# ℹ 7,042 more rows\n\n\nWe can see that we have both regular 5 digit zip codes as well as those that include the extra 4 digits (ZIP+4). Let’s say we don’t need those last four digits for our analysis (particularly because not every school uses them anyway). Our task is to create a new column that pulls out only the main part of the zip code. It is has to work both for zip values that include the additional hyphen and 4 digits as well as those that only have the primary 5 digits to begin with.\nOne solution in this case is to take advantage of the fact that zip codes — minus the sometimes extra 4 digits — should be regular: 5 digits. If want the sub-part of a string and that sub-part is always in the same spot, we can use the function, str_sub(), which takes a string or column name first, and has arguments for the starting and ending character that mark the sub-string of interest.\nIn our case, we want the first 5 digits so we should start == 1 and end == 5:\n\n## pull out first 5 digits of zip code\ndf &lt;- df |&gt;\n    mutate(zip5 = str_sub(zip, start = 1, end = 5))\n\n## show (use select() to subset so we can set new columns)\ndf |&gt;\n    select(unitid, zip, zip5)\n\n# A tibble: 7,052 × 3\n   unitid zip        zip5 \n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 100636 36112-6613 36112\n 2 100654 35762      35762\n 3 100663 35294-0110 35294\n 4 100690 36117-3553 36117\n 5 100706 35899      35899\n 6 100724 36101-0271 36101\n 7 100733 35401      35401\n 8 100751 35487-0166 35487\n 9 100760 35010      35010\n10 100812 35611      35611\n# ℹ 7,042 more rows\n\n\nA quick visual inspection of the first few rows shows that our str_sub() function performed as expected (for a real analysis, you’ll want to do more formal checks).\n\n\nReplace using regular expressions: str_replace()\nWe can also use a more sophisticated regex pattern with the function str_replace(). The pieces of our regex pattern, \"([0-9]+)(-[0-9]+)?\", are translated as this:\n\n[0-9] := any digit, 0 1 2 3 4 5 6 7 8 9\n+ := match the preceding one or more times\n? := match the preceding 0 or more times\n() := subexpression\n\nPut together, we have:\n\n([0-9]+) := first, look for 1 or more digits\n(-[0-9]+)? := second, look for a hyphen and one or more digits, but you may not find any of that\n\nBecause we used parentheses, (), to separate our subexpressions, we can call them using their numbers (in order) in the last argument of str_replace():\n\n\"\\\\1\" := return the first subexpression\n\nSo what’s happening? If given a zip code that is \"32605\", the regex pattern will collect each digit — \"3\" \"2\" \"6\" \"0\" \"5\" — into the first subexpression because it never sees a hyphen. That first subexpression, \"\\\\1\", is returned: \"32605\". That’s what we want.\nIf given \"32605-1234\", it will collect the first 5 digits in the first subexpression, but will stop adding characters there when it sees the hyphen. From then on out, it adds everything it sees the second subexpression: \"-\" \"1\" \"2\" \"3\" \"4\". But because str_replace() only returns the first subexpression, we still get the same answer: \"32605\". This is what we want.\nLet’s try it on the data.\n\n## drop last four digits of extended zip code if they exist\ndf &lt;- df |&gt;\n    mutate(zip5_v2 = str_replace(zip, \"([0-9]+)(-[0-9]+)?\", \"\\\\1\"))\n\n## show (use select() to subset so we can set new columns)\ndf |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 7,052 × 4\n   unitid zip        zip5  zip5_v2\n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  \n 1 100636 36112-6613 36112 36112  \n 2 100654 35762      35762 35762  \n 3 100663 35294-0110 35294 35294  \n 4 100690 36117-3553 36117 36117  \n 5 100706 35899      35899 35899  \n 6 100724 36101-0271 36101 36101  \n 7 100733 35401      35401 35401  \n 8 100751 35487-0166 35487 35487  \n 9 100760 35010      35010 35010  \n10 100812 35611      35611 35611  \n# ℹ 7,042 more rows\n\n\n\nQuick exercise\nWhat if you wanted to the get the last 4 digits (after the hyphen)? What bit of two bits of code above would you change so that you can store the last 4 digits without including the hyphen? Make a new variable called zip_plus4 and store these values. HINT Look at the help file for str_replace().\n\nLet’s compare our two versions: do we get the same results?\n\n## check if both versions of new zip column are equal\nidentical(df |&gt; select(zip5), df |&gt; select(zip5_v2))\n\n[1] FALSE\n\n\nNo! Let’s see where they are different:\n\n## filter to rows where zip5 != zip5_v2 (not storing...just looking)\ndf |&gt;\n    filter(zip5 != zip5_v2) |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 4 × 4\n  unitid zip        zip5  zip5_v2   \n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     \n1 108199 90015--350 90015 90015--350\n2 113953 92113--191 92113 92113--191\n3 431707 06360--709 06360 06360--709\n4 435240 551012595  55101 551012595 \n\n\n\nQuick exercise\nWhat happened? In this scenario, which string subsetting technique worked better?\n\nDepending on the task, regular expressions can either feel like a blessing or a curse. To be honest, I’ve spent more time cursing than thanking them. That said, regular expressions are often the only way to perform a data wrangling task on unstructured string data. They are also a cornerstone of natural language processing techniques, which are increasingly of interest to education researchers.\nWe’ve only scratched the surface of what regular expressions can do. If you face string data in the future, taking a little time to craft a regular expression can be well worth it.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-2-working-with-dates",
    "href": "08-wrangle-iii.html#part-2-working-with-dates",
    "title": "III: Working with strings & dates",
    "section": "Part 2: Working with dates",
    "text": "Part 2: Working with dates\nIn opening section, we’ve seen that dates often come in many different formats. While you can format and clean them using regular expressions, you may also want to format them such that R knows they are dates.\nWhy?\nWhen dealing with something straightforward like years, it’s easy enough to store the years a regular numbers and then subtract the recent year from the past year to get a duration: 2020 - 2002 equals 18 years.\nBut what if you have daily data for the school year and you want to know how many days a student had between a first and second test? What if the differences were more than a month of days and every student took the first and second tests on different days? What if you had a panel data set, with students across years, some of which were leap years? You can see how calculating the exact number days between tests for each student could quickly become difficult if trying to do it using regular numerical values.\nR makes this easier by having special time-based data types that will keep track of these issues for us and allow us to work with dates almost as we do with regular numbers.\nIn our IPEDS data set, we can see that few institutions closed in 2007 and 2008. We’ll limit our next analyses to these institutions.\n\n## subset to schools who closed during this period\ndf &lt;- df |&gt;\n    filter(closedat != -2)\n\n## show first few rows\ndf |&gt; select(unitid, instnm, closedat)\n\n# A tibble: 83 × 3\n   unitid instnm                                                  closedat\n    &lt;dbl&gt; &lt;chr&gt;                                                   &lt;chr&gt;   \n 1 103440 Sheldon Jackson College                                 6/29/07 \n 2 104522 DeVoe College of Beauty                                 3/29/08 \n 3 105242 Mundus Institute                                        Sep-07  \n 4 105880 Long Technical College-East Valley                      3/31/07 \n 5 119711 New College of California                               Jan-08  \n 6 136996 Ross Medical Education Center                           7/31/07 \n 7 137625 Suncoast II the Tampa Bay School of Massage Therapy LLC 5/31/08 \n 8 141583 Hawaii Business College                                 Sep-07  \n 9 150127 Ball Memorial Hospital School of Radiologic Technology  May-07  \n10 160144 Pat Goins Shreveport Beauty School                      3/1/08  \n# ℹ 73 more rows\n\n\nWe can see that closedat is stored as a string. Based on our domain knowledge and context clues, we know that the dates are generally in a MM/DD/YYYY (American) format.\nWe can use the lubridate command mdy() to make a new variable that contains the same information, but in a format that R recognizes as a date.\n\n## create a new close date column \ndf &lt;- df |&gt;\n    mutate(closedat_dt = mdy(closedat))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `closedat_dt = mdy(closedat)`.\nCaused by warning:\n!  35 failed to parse.\n\n## show\ndf |&gt; select(starts_with(\"close\"))\n\n# A tibble: 83 × 2\n   closedat closedat_dt\n   &lt;chr&gt;    &lt;date&gt;     \n 1 6/29/07  2007-06-29 \n 2 3/29/08  2008-03-29 \n 3 Sep-07   NA         \n 4 3/31/07  2007-03-31 \n 5 Jan-08   NA         \n 6 7/31/07  2007-07-31 \n 7 5/31/08  2008-05-31 \n 8 Sep-07   NA         \n 9 May-07   NA         \n10 3/1/08   2008-03-01 \n# ℹ 73 more rows\n\n\nWell, we are part of the way there. It seems that mdy() didn’t really work with dates like Sep-2007. What can we do?\nOne solution is to add in a fake day for the ones that didn’t parse and then convert using mdy(). We’ll use regular expressions with an str_replace().\n\n## convert MON-YYYY to MON-01-YYYY\ndf &lt;- df |&gt;\n    mutate(closedat_fix = str_replace(closedat, \"-\", \"-01-\"),\n           closedat_fix_dt = mdy(closedat_fix))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `closedat_fix_dt = mdy(closedat_fix)`.\nCaused by warning:\n!  7 failed to parse.\n\n## show\ndf |&gt; select(starts_with(\"close\"))                                \n\n# A tibble: 83 × 4\n   closedat closedat_dt closedat_fix closedat_fix_dt\n   &lt;chr&gt;    &lt;date&gt;      &lt;chr&gt;        &lt;date&gt;         \n 1 6/29/07  2007-06-29  6/29/07      2007-06-29     \n 2 3/29/08  2008-03-29  3/29/08      2008-03-29     \n 3 Sep-07   NA          Sep-01-07    2007-09-01     \n 4 3/31/07  2007-03-31  3/31/07      2007-03-31     \n 5 Jan-08   NA          Jan-01-08    2008-01-01     \n 6 7/31/07  2007-07-31  7/31/07      2007-07-31     \n 7 5/31/08  2008-05-31  5/31/08      2008-05-31     \n 8 Sep-07   NA          Sep-01-07    2007-09-01     \n 9 May-07   NA          May-01-07    2007-05-01     \n10 3/1/08   2008-03-01  3/1/08       2008-03-01     \n# ℹ 73 more rows\n\n\n\nQuick exercise\nWe had 7 parsing errors. Can you figure out which rows failed to parse and guess why? HINT if mdy() failed to parse closedat, then the subsequent new columns are likely missing values.\n\nNow that we’ve successfully converted the string date into a proper date type, it’s easy to pull out the pieces of that date, including:\n\nyear with year()\nmonth with month()\nday with day()\nday of week with wday()\n\n\n## add columns for\n## - year\n## - month\n## - day\n## - day of week (dow)\ndf &lt;- df |&gt;\n    mutate(close_year = year(closedat_fix_dt),\n           close_month = month(closedat_fix_dt),\n           close_day = day(closedat_fix_dt),\n           close_dow = wday(closedat_fix_dt, label = TRUE))\n## show\ndf |&gt;\n    select(closedat_fix_dt, close_year, close_month, close_day, close_dow)\n\n# A tibble: 83 × 5\n   closedat_fix_dt close_year close_month close_day close_dow\n   &lt;date&gt;               &lt;dbl&gt;       &lt;dbl&gt;     &lt;int&gt; &lt;ord&gt;    \n 1 2007-06-29            2007           6        29 Fri      \n 2 2008-03-29            2008           3        29 Sat      \n 3 2007-09-01            2007           9         1 Sat      \n 4 2007-03-31            2007           3        31 Sat      \n 5 2008-01-01            2008           1         1 Tue      \n 6 2007-07-31            2007           7        31 Tue      \n 7 2008-05-31            2008           5        31 Sat      \n 8 2007-09-01            2007           9         1 Sat      \n 9 2007-05-01            2007           5         1 Tue      \n10 2008-03-01            2008           3         1 Sat      \n# ℹ 73 more rows\n\n\n\nQuick exercise\nCan we trust our close_dow variable? Why?\n\nIt’s also easy to calculate differences of time. We can use normal arithmetic — future date - past date — and R will take care of all the underlying calendar calculations for us (e.g., days in a given month, leap years, etc).\n\n## how long since the institution closed\n## - as of 1 January 2020\n## - as of today\ndf &lt;- df |&gt;\n    mutate(time_since_close_jan = ymd(\"2020-01-01\") - closedat_fix_dt,\n           time_since_close_now = today() - closedat_fix_dt)\n\n## show\ndf |&gt; select(starts_with(\"time_since_close\"))\n\n# A tibble: 83 × 2\n   time_since_close_jan time_since_close_now\n   &lt;drtn&gt;               &lt;drtn&gt;              \n 1 4569 days            6037 days           \n 2 4295 days            5763 days           \n 3 4505 days            5973 days           \n 4 4659 days            6127 days           \n 5 4383 days            5851 days           \n 6 4537 days            6005 days           \n 7 4232 days            5700 days           \n 8 4505 days            5973 days           \n 9 4628 days            6096 days           \n10 4323 days            5791 days           \n# ℹ 73 more rows\n\n\nAs with strings and regular expressions, we’ve only scratched the surface of working with dates in R. For example, you can also work with times (hours, minutes, seconds, etc). Now that you’ve been introduced, however, you should have a starting point for working with panel and administrative data that includes strings and dates that you need to process before conducting your analyses.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#questions",
    "href": "08-wrangle-iii.html#questions",
    "title": "III: Working with strings & dates",
    "section": "Questions",
    "text": "Questions\nNB To answer the questions, you will need to join the two IPEDS data sets using the common unitid key. Note that column names in hd2007.csv are uppercase (UNITID) while those in ic2007mission.csv are lowercase (unitid). There are a few ways to join when the keys don’t exactly match. One is to set all column names to the same case. If you want to use left_join() starting with hd2007.csv, you can first use the the dplyr verb rename_all(tolower) in your chain to lower all column names. See the help file for left_join() for other ways to join by different variable names.\n\nHow many chief administrator names start with “Dr.”?\nNB Many chief administrators are listed on more than one line due to branch campuses. Make sure to take this into account by keeping only distinct names.\nBONUS How many chief administrator names end with the title “PH.D.” or some variant?\nAmong those schools that give their mission statement:\n\nHow many repeat their institutional name in their mission statement?\n\nHow many use the word civic?\nWhich top 3 states have the most schools with mission statements that use the word future?\n\nWhich type of schools (public, private-non-profit, private-for-profit) are most likely to use the word skill in their mission statement?\n\nAmong the schools that closed in 2007 or 2008 and give a date with at least a month and year:\n\nWhich has been closed for the longest time? How many months has it been from its close date to the beginning of this current month (1 February 2020)?\n\nHow many days were there between the first school to close and the last?\n\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_8.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data",
    "href": "10-viz-iii.html#reading-in-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in Data",
    "text": "Reading in Data\nOne of the great advantages of using APIs for mapping is that we download spatial data directly into R instead of having to download and handle these quite large files through our computer. If we weren’t using an API or wanted to plot some spatial data not available through one, we would need to find and download a shapefile folder containing a selection of files, then read in the one ending in *.shp — something like this below:\n\n## ---------------------------\n## example of shapefile read\n## ---------------------------\n\n## pseudo code (won't run!)\ndf &lt;- read_sf(file.path(\"&lt;Data-Folder&gt;\", \"&lt;Folder-You-Downloaded&gt;\", \"&lt;Shapefile-Name&gt;.shp\"))\n\nThese shapefiles can sometimes be hard to find, take up a lot of space on our computer (especially if they are overly detailed for our needs), and make it much harder to share our project with others for reproducibility. That is why we are going to use an API.",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#setting-up-apis-and-tidycensus",
    "href": "10-viz-iii.html#setting-up-apis-and-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Setting up APIs and tidycensus",
    "text": "Setting up APIs and tidycensus\nSo what exactly is an API? In short, think of it as a way of R going to a website/database and pulling data directly from the server-side or backend, without our having to ever interact with the website directly. (Note from BS: we avoid point-click at all costs!) We are going to use the API tidycensus today, but all APIs operate on the same basic idea.\nTidycensus is, in my opinion, one of the easiest APIs to get set up and use in R. Most APIs require that you use some kind of key that identifies you as an authorized user. Typically you need to set up the key the first time you use the API, but helpfully, it’s usually possible to store the key on your computer for all future use (think of the way we initially set up GitHub and then it worked without needing to go through that process again — the good news is that API keys are way easier to set up). Most keys are free to obtain and use. If you were using an API to access a private database such as Google Maps, you might need to pay for your key to have access or on a sliding scale depending on how much you use it. But because we are using Census data, which is freely available to the public, there’s no charge.\nHopefully, most of you were able to get your Census API key before class, but if anyone needs a reminder,\n\nsimply go here\nenter your organization name (University of Florida)\nenter your email.\n\nYou will quickly receive an email with your API key, which you will need below.\nTo set up tidycensus for the first time, we first need to set our API key. The tidycensus library makes this much easier than many APIs by having a built-in function that you can use to save your API key to your computer. Simply place your API key in the &lt;&gt; of the code below. The install option means it will save the API key for future use, so you will not need to worry about this step again.\n\n## ---------------------------\n## set API key\n## ---------------------------\n\n## you only need to do this once: replace everything between the\n## quotes with the key in the email you received\n##\n## eg. census_api_key(\"XXXXXXXXXXXXXXXXXX\", install = T)\ncensus_api_key(\"&lt;Your API Key Here&gt;\", install = T)\n\nNow that this is set up, we are ready to start using tidycensus — yay!",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "href": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in data with tidycensus",
    "text": "Reading in data with tidycensus\nThere are five main tidycensus functions that you can use to call in data, with each calling data from a different source operated by the US Census Bureau. For today’s lesson we are going to use get_acs(), which collects data from the American Community Survey (regular sampled surveys of demographic data across the US). There are a selection of other functions to collect data from different sources within the Census; the most useful ones for us start with get_. You can see more info here.\nWe are going to assign &lt;- the data we pull down into the object df_census:\n\n## ---------------------------\n## first data pull\n## ---------------------------\n\ndf_census &lt;- get_acs(geography = \"county\",\n                     state = \"FL\",\n                     year = 2021,\n                     variables = \"DP02_0065PE\", # Pop &gt;=25 with Bachelors\n                     output = \"wide\",\n                     geometry = TRUE)\n\nGetting data from the 2017-2021 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nUsing the ACS Data Profile\n\n\nLet’s walk through each element of this command in turn:\n\ngeography = \"county\" is telling the function to get estimates (and spatial data later) at the county level; this could also be \"state\", for example, to get state level data.\n\nstate = \"FL\" is telling the function to get data only for the state of Florida. You could put a group of states with c(), use full state names, or use FIPS codes — tidycensus is flexible. If you want a narrower set of data, you could also add county =, which works in a similar way. For example, if you added county = \"Alachua\", you would only get county-level data for Alachua County, Florida.\nyear = 2021 is telling the function to pull data for the survey year 2021. For ACS, this will be the survey set ending in that year. Keep in mind that some data are not available for every year. For example, data from the full decennial census are only available for 2010 or 2020.\nvariables = \"DP02_0065PE\" is telling the function to pull the variable coded \"DP02_0065PE\", which is the percentage of the population older than 25 with a Bachelor’s degree. This is the only tricky part of using tidycensus: understanding census API’s variable names. Let me breakdown what we are calling here:\n\nDP02_0065\n\nThis is the main variable code the census uses. You can call this by using the load_variables() command, but doing so creates a massive table in R that is hard to navigate through. An easier way is to go the census API’s list of variables for the dataset you are using, which for the 2021 ACS is here (change the years/data sources as needed for other surveys).\nIn here you can crtl-f or cmd-f search for the variable you are looking for. For this variable we could search “bachelor,” which will highlight all the variables that have “bachelor” in the title. Find the variable you want and copy the name.\n\nPE\n\nYou will notice there are multiple DP02_0065 variables, these are the same underlying variable, but in different forms. The common endings are E or PE, which stand for Estimate and Percentage Estimate. For our purposes, we are most often going to want the percentage estimate (PE), so we will select DP02_0065PE, the percent estimate of Bachelor’s degree attainment for those 25 years old and above, and DP02_0065PM which is the margin of error for the percentage (hence the M at the end). If you want the total count instead, select E.\n\n\noutput = \"wide\" is telling it we want the data in a wide format. Think back to Data Wrangling II: wide data means having a separate column for each variable whereas long data would be in two columns, one with the variable name and one with the variable value. For ease of plotting/mapping, we are going to want it in wide format.\ngeometry = T is telling the function we want to download geometry (a kind of spatial data) to go with our census data. This saves us having to deal with finding, loading, and joining a shapefile to make our map. We will discuss this more shortly.\n\nOkay, let see what the top of our new data looks like.\n\n## show header of census data\nhead(df_census)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -82.57599 ymin: 27.64324 xmax: -80.73292 ymax: 30.14312\nGeodetic CRS:  NAD83\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4\n                        geometry\n1 MULTIPOLYGON (((-81.65856 2...\n2 MULTIPOLYGON (((-82.57599 2...\n3 MULTIPOLYGON (((-81.95616 2...\n4 MULTIPOLYGON (((-81.6809 29...\n5 MULTIPOLYGON (((-82.10621 2...\n6 MULTIPOLYGON (((-82.31133 2...\n\n\nIt looks a bit different than a normal data frame. For now, let’s not worry too much about the first few lines which give a summary of the spatial aspects of the our downloaded data. If you look underneath those lines, from GEOID to DP02_0065PM, you’ll see something that looks more like the tibbles we are familiar with. Then, in the last column, we get to our spatial data in the geometry column. If you open df_census in the viewer, it looks like a normal data frame ending with this slightly different column called geometry.\nNote: I wouldn’t recommend often looking through the file in viewer as the the spatial data can make it slow/laggy. If you need to dig into the data that way, use st_drop_geometry() and assign it to a new object.\n\n## view data frame without geometry data (not assigning, just viewing)\ndf_census |&gt;\n  st_drop_geometry()\n\n   GEOID                         NAME DP02_0065PE DP02_0065PM\n1  12095       Orange County, Florida        23.0         0.6\n2  12125        Union County, Florida         7.6         2.1\n3  12069         Lake County, Florida        16.0         0.9\n4  12127      Volusia County, Florida        16.8         0.5\n5  12105         Polk County, Florida        14.0         0.5\n6  12119       Sumter County, Florida        19.4         1.4\n7  12073         Leon County, Florida        26.7         0.9\n8  12047     Hamilton County, Florida         6.9         1.6\n9  12093   Okeechobee County, Florida        11.3         1.4\n10 12071          Lee County, Florida        17.8         0.5\n11 12001      Alachua County, Florida        23.2         1.0\n12 12077      Liberty County, Florida         8.3         2.6\n13 12097      Osceola County, Florida        16.5         0.9\n14 12123       Taylor County, Florida         8.3         2.1\n15 12013      Calhoun County, Florida         7.5         2.0\n16 12037     Franklin County, Florida        12.2         2.5\n17 12029        Dixie County, Florida         5.9         1.9\n18 12133   Washington County, Florida         7.9         1.4\n19 12129      Wakulla County, Florida        12.2         2.2\n20 12131       Walton County, Florida        21.1         1.6\n21 12007     Bradford County, Florida         7.3         1.6\n22 12031        Duval County, Florida        21.0         0.6\n23 12033     Escambia County, Florida        17.4         0.8\n24 12089       Nassau County, Florida        20.5         1.4\n25 12009      Brevard County, Florida        19.5         0.6\n26 12086   Miami-Dade County, Florida        19.8         0.3\n27 12053     Hernando County, Florida        12.9         0.6\n28 12107       Putnam County, Florida         8.6         1.2\n29 12023     Columbia County, Florida        10.6         1.1\n30 12049       Hardee County, Florida         7.2         1.8\n31 12017       Citrus County, Florida        11.9         0.9\n32 12117     Seminole County, Florida        27.7         0.8\n33 12039      Gadsden County, Florida        12.8         1.4\n34 12045         Gulf County, Florida        14.6         2.8\n35 12121     Suwannee County, Florida         9.3         1.8\n36 12065    Jefferson County, Florida        14.5         2.2\n37 12075         Levy County, Florida        10.5         1.6\n38 12057 Hillsborough County, Florida        22.5         0.4\n39 12103     Pinellas County, Florida        22.0         0.5\n40 12083       Marion County, Florida        13.8         0.8\n41 12055    Highlands County, Florida        12.2         1.0\n42 12027       DeSoto County, Florida         8.8         1.3\n43 12113   Santa Rosa County, Florida        18.8         1.1\n44 12079      Madison County, Florida         8.8         1.8\n45 12041    Gilchrist County, Florida         9.4         1.7\n46 12087       Monroe County, Florida        21.7         1.3\n47 12111    St. Lucie County, Florida        15.9         0.8\n48 12109    St. Johns County, Florida        28.1         1.1\n49 12003        Baker County, Florida         9.2         2.2\n50 12035      Flagler County, Florida        17.8         1.1\n51 12051       Hendry County, Florida         5.8         1.4\n52 12091     Okaloosa County, Florida        20.7         1.0\n53 12005          Bay County, Florida        16.4         1.0\n54 12099   Palm Beach County, Florida        23.0         0.4\n55 12011      Broward County, Florida        21.2         0.4\n56 12021      Collier County, Florida        22.4         0.8\n57 12081      Manatee County, Florida        19.6         0.6\n58 12085       Martin County, Florida        21.9         1.1\n59 12043       Glades County, Florida         9.5         2.5\n60 12067    Lafayette County, Florida         5.8         2.9\n61 12063      Jackson County, Florida         7.8         1.1\n62 12015    Charlotte County, Florida        15.5         0.9\n63 12059       Holmes County, Florida         6.6         1.7\n64 12101        Pasco County, Florida        16.9         0.5\n65 12019         Clay County, Florida        18.2         1.1\n66 12115     Sarasota County, Florida        21.4         0.6\n67 12061 Indian River County, Florida        19.4         1.3",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "href": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "A (Very) Brief Overview of Spatial Data",
    "text": "A (Very) Brief Overview of Spatial Data\nWe do not have time to really get into all the complexities of spatial data in this class, so, unfortunately, it will have to remain a bit of black box for now. But below is a quick overview of how R handles it.\nAs we saw above, there is a column on the end of our data called geometry. This is not technically a column like we are used to; for example, you can’t select(geometry) or filter(geometry == x) like we do with other variables in our data frames. Instead, think of it as a special attachment R places on each observation.\n\nVectors vs Rasters\nWhen looking online for spatial data, you might see how spatial data can be either in vector or raster format. For our purposes, everything is going to be vector, which is kind of like a vector in R: collection of data points that represent something spatial. Raster data, on the other hand, is a grid with information assigned to each square, commonly used for satellite imagery analysis.\n\nVector: here’s instructions (e.g., a formula) to draw a line; great for animations and things like maps and scales well\nRaster: here’s a big paint-by-numbers grid and the line you see is where some squares are filled in; great for photographic images, but doesn’t scale well\n\nVector data are usually either as points (think dots on a map), lines (think a line connecting two points on a map), or polygons (think a collection of lines on a map that create a closed shape). In this lesson we are going to use both point and polygon data (you won’t use line data as much). If this sounds complicated, fear not! It is much simpler than it sounds right now!\nFor those interested, this is a nice intro to the types of spatial data.\n\n\nCoordinate Reference Systems (CRS)\nFor purposes of this lesson, the only internal workings of spatial data we need to be aware of is something called the Coordinate Reference System or CRS. Our earth is not flat, but rather is a curved three-dimensional object (NB from BS: this is most likely true). Since we don’t want to carry around globes, we take this 3D object and squish it into two dimensions on a map. This process necessarily involves some kind of transformation, compromise, or projection.\nIn a nutshell, this is a very simplified explanation of what a CRS decides: it’s how we are deciding to twist, pull, squish a 3D Earth surface into a flat surface. Turns out this matters a lot. Do you want your results to have the correct areas? Or maybe correct distances? Or maybe straight lines of bearing (particularly important if you are sailing and don’t want those trips to take any longer than necessary).\nHere’s a (somewhat old) pop culture look at this issue from one of my favorite shows…\n\nThis is a relatively complicated process we are not going to go into here. If you’re interested here’s a nice introduction to CRS by QGIS.\nFor our class we are going to use the CRS ESPG 4326, which is in essence a projection that makes east/west run straight left/right and north/south run straight up/down. All different CRS have their advantages and disadvantages. This is nice and simple for quick descriptive maps, but distorts shapes in ways that might be harmful, particularly if you are going to do any distance calculations, etc.\nNB from BS: If you are going to do spatial work in education research (other than just making maps for display), you really need to know what your projection is doing. Even if you are just making maps for display, some projections are, IMNSHO, more aesthetically pleasing that others in different situations. I personally will tell you if your map offends the dictates of good taste.\nKeep an eye out for crs = 4326 as we go through some examples plotting spatial data below.\nIn short, what you need to know about spatial data for this lesson is this:\n\nR stores spatial data in something called geometry attached to each observation/row\nTo handle spatial data, you can’t just filter() it like normal; instead you have to use functions from a spatial data package such as sf or tigris\nThe CRS (coordinate reference system) is how we choose to account for the earth being curved; what is most important for mapping is that everything we use on the plot is using the same CRS. Using crs   = 4326 will give a nice simple flat projection. This projection has drawbacks, but is easy to work with and so is what we will use for now.",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#lets-make-a-map-finally",
    "href": "10-viz-iii.html#lets-make-a-map-finally",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Let’s Make a Map (finally)!",
    "text": "Let’s Make a Map (finally)!\nIf we have made it this far, things are about to get much more interesting and hands-on!\nWe are going to make an education-focused map based on template I used for a real consulting project last summer as part of my GA-ship. This template is really adaptable for a lot the kind of maps we might want educational research and reports. So let’s get started.\nWe are going to have two layers, a base map with the census data we already downloaded, and a layer of points on top representing colleges.\n\nLayer One: Base Map\nBefore we plot anything, particularly since we are going to have multiple layers, we want to check our CRS\n\n## ---------------------------------------------------------\n## making a map\n## ---------------------------------------------------------\n## ---------------------------\n## layer one: base map\n## ---------------------------\n\n## show CRS for dataframe\nst_crs(df_census)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nThat isn’t our simple flat ESPG 4326, so we are going to st_transform() to set that.\n\n## transform the CRS to 4326\ndf_census &lt;- df_census |&gt;\n  st_transform(crs = 4326)\n\nThen we can check again…\n\n## show CRS again; notice how it changed from NAD93 to ESPG:4326\nst_crs(df_census) \n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nLooks good!\nOkay, with our CRS now set, let’s plot our base map. We actually use the familiar ggplot() to make our maps because there is a special geom_* that works with spatial data: geom_sf(). Everything works in a similar way to our normal plots, so this should be familiar. Luckily all the tricky spatial aspects are handled by ggplot for us.\nThe below code will make our base map, and store in an object called base_map.\n\n## create base map\nbase_map &lt;- ggplot() +\n  geom_sf(data = df_census,\n          aes(fill = DP02_0065PE),\n          color = \"black\",\n          size = 0.1) +\n  labs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) +\n  scale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\")\n\nLet’s go through each line of the geom_sf() as we did for get_acs() above:\n\ndata = df_census: all we need to do take make our spatial plot is call a data frame with a geometry attachment. geom_sf() will handle how to plot that for us.\naes(fill = DP02_0065PE): much like we would with a box plot, we are simply telling ggplot to fill the shapes (in our case, Florida’s counties) based on that variable. So here we are filling Florida’s counties based on the percent of the population over 25 with a Bachelor’s degree (the variable we chose from tidycensus)\ncolor = \"black\": remember since this is outside the aes() argument it will applied consistenly across the plot. We are telling it to make all the lines black.\nsize = 0.1: similarly, we are telling to make the lines 0.1 thickness (thinner than the default)\n\nThen we have added two visual alterations like we covered in the second plotting lesson. For a quick reminder:\n\nlabs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) is saying to give the legend for fill this title; a new function, str_wrap() says to make a newline (wrap) when there are more than 20 characters\nscale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\") is telling fill with a color gradient starting at with light slate blue and finishing with a dark slate blue; instead of colour names, we’re using hex color codes\n\nNow, let’s call our base_map object to see what this looks like\n\n## call base map by itself\nbase_map\n\n\n\n\n\n\n\n\nWe have made a map! But we are going to add one more layer.\n\n\nLayer Two: Institution Points\nA lot of education data comes with a latitude and longitude for the institution. Today we are going to use IPEDS, but you can certainly get these for K-12 schools and a whole lot more besides.\nWe are now going to read in some library data from IPEDS that I cleaned and merged earlier.\n\n## ---------------------------\n## layer two: institutions\n## ---------------------------\n\n## read in IPEDS data\ndf_ipeds &lt;- read_csv(file.path(\"data\", \"mapping-api-data.csv\"))\n\nRows: 3764 Columns: 78\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (35): INSTNM, STABBR, COUNTYNM, XLPBOOKS, XLEBOOKS, XLEDATAB, XLPMEDIA, ...\ndbl (43): UNITID, CONTROL, ICLEVEL, FIPS, COUNTYCD, LATITUDE, LONGITUD, LEXP...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our data\n\n## show IPEDS data\nhead(df_ipeds)\n\n# A tibble: 6 × 78\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LATITUDE LONGITUD\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089     34.8    -86.6\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073     33.5    -86.8\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101     32.4    -86.2\n4 100706 Unive…       1       1 AL         1 Madison…     1089     34.7    -86.6\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101     32.4    -86.3\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125     33.2    -87.5\n# ℹ 68 more variables: LEXP100K &lt;dbl&gt;, LCOLELYN &lt;dbl&gt;, XLPBOOKS &lt;chr&gt;,\n#   LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;, LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;,\n#   LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;, LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;,\n#   LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;, LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;,\n#   LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;, LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;,\n#   LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;, LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;,\n#   LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;, LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, …\n\n\nWe see a normal data frame for colleges with bunch of variables (use the IPEDS codebook to unpack the variable names), including latitude and longitude. Latitude and longitude represent something spatial, but they’re not quite spatial data like R knows. Let’s change that!\n\n## convert coordinates columns into a true geometry column; this is\n## much more reliable than simply plotting them as geom_points as it\n## ensures the CRS matches etc.\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_as_sf(coords = c(\"LONGITUD\", \"LATITUDE\"))\n\nAbove we call st_as_sf(), then tell it the coordinates, coords =, are in columns name LONGITUD and LATITUDE. If we aren’t using argument names (we aren’t) just remember that since longitude tells you were you are east/west on the globe, it translates to the x axis. Because latitude gives you north/south direction, it translates to the y axis.\nIf we look at our data again, we are going to see that spatial summary again as R has attached some point geometry to our college data based on the coordinates.\n\n## show IPEDS data again\nhead(df_ipeds)\n\nSimple feature collection with 6 features and 76 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -87.54598 ymin: 32.36261 xmax: -86.17401 ymax: 34.78337\nCRS:           NA\n# A tibble: 6 × 77\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LEXP100K LCOLELYN\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089        1        2\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073        1        2\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101        1        2\n4 100706 Unive…       1       1 AL         1 Madison…     1089        1        2\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101        1        2\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125        1        2\n# ℹ 67 more variables: XLPBOOKS &lt;chr&gt;, LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;,\n#   LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;, LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;,\n#   LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;, LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;,\n#   LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;, LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;,\n#   LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;, LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;,\n#   LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;, LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;,\n#   LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, LTCRCLT &lt;dbl&gt;, LILLDYN &lt;dbl&gt;, …\n\n\nBut then look at our CRS: it’s NA! This means R will not be able to turn that data into a map. Basically, R knows we have spatial data, but it doesn’t know how we want to put onto a 2D surface (how to project it). To be sure, let’s check the CRS directly:\n\n## check CRS for IPEDS data\nst_crs(df_ipeds)\n\nCoordinate Reference System: NA\n\n\nStill NA…\nLuckily the fix for this is similar to how we change the CRS for our earlier map.\n\n## add CRS to our IPEDS data\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_set_crs(4326) # When you first add coordinates to geometry, it doesn't know\n                   # what CRS to use, so we set to 4326 to match our base map data\n\nOkay, let’s have another look…\n\n## check CRS of IPEDS data again\nst_crs(df_ipeds)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAnd we see we have our nice CRS back!\nOkay, now the hard work is done, we just need to call our base_map, add a layer representing the colleges as points, and store it into a new object point_map:\n\npoint_map &lt;- base_map +\n  geom_sf(data = df_ipeds |&gt; filter(FIPS == 12), # Only want to plot colleges in FL\n          aes(size = LPBOOKS),\n          alpha = 0.8,\n          shape = 23, # Get the diamond shape which stands out nicely on the map\n          fill = \"white\", # This shape has a fill and color for the outline\n          color = \"black\") + # FYI 21 is a circle with both fill and color\n  labs(size = \"Number of Books in Library\")\n\nAs we have done all lesson, we can take a quick look through our second geom_sf() function line by line:\n\ndata = df_ipeds |&gt; filter(FIPS == 12): for this layer we are using our df_ipeds data, which covers the country, but since our base map is Florida, we only want colleges located in the Sunshine State (which is FIPS code 12).\naes(size = LPBOOKS) is saying we want to change the size of point based on LPBOOKS, which is the total number of books in the college’s library collection. More books, bigger dot!\nalpha = 0.5 is outside the aes() so we are making it all 50% transparent.\nThen we added labs(size = \"Number of Books in Library\") to change the legend title to “Number of Books in Library”\n\nPhew! Last thing, let’s call our new point_map object and take a look at what we created!\n\n## show new map\npoint_map\n\nWarning: Removed 14 rows containing missing values (`geom_sf()`)\n\n\n\n\n\n\n\n\n\nThere we go! We now have a map that shows us county bachelor’s degree attainment and the number of books in a college’s library. If you notice, UF has most books out of all Florida colleges, Go Gators!\nObviously, this may not be the most useful map in the world, but the template is very adaptable. Using tidycensus we can swap out the base map geography to have most common US geographies and/or swap out any variable available in from the Census Bureau. Equally, we can swap out the point data to represent anything we have coordinate points for and change the aesthetics to represent any data we have for those points.",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "href": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Supplemental Material: US transformations & tigris basics",
    "text": "Supplemental Material: US transformations & tigris basics\nFor the sake of time, I left this until the end as we don’t need it for the assignment. But it may be useful if you are looking to make any maps in your final assignment or the future.\ntigris is a package that offers a direct way of downloading US spatial data that is not tied to census data. (Note: it’s actually used by tidycensus behind the scenes to get your spatial data.) If you get spatial data from tigris it won’t come with any additional data to plot per say, but it comes with identifying variables you could use to pair up with external data using something like left_join().\nSomething we as educational researchers might be interested in plotting is school districts. While we could get these from tidycensus with geography = \"school district (unified)\", it may be the case that we have school district data rather than census data we want to plot. In that case, it might be easier to use tigris directly to get the blank shapefiles. The function names for tigris are really simple. school_districts() for example retrieves a shapefile for US school districts whereas states() retrieves boundries for the all US states and territories.\nLet’s take a quick look at the 50 states.\n\n## ---------------------------------------------------------\n## supplemental using tigris directly\n## ---------------------------------------------------------\n## ---------------------------\n## get states geometries\n## ---------------------------\ndf_st &lt;- states() |&gt;\n  filter(STATEFP &lt;= 56) # keeping only the 50 states plus D.C.\n\nRetrieving data for the year 2021\n\n\nLike we did before, let’s take a peak at our newly downloaded data.\n\n## look at head of state data\nhead(df_st)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -97.23909 ymin: 24.39631 xmax: -71.08857 ymax: 49.38448\nGeodetic CRS:  NAD83\n  REGION DIVISION STATEFP  STATENS GEOID STUSPS          NAME LSAD MTFCC\n1      3        5      54 01779805    54     WV West Virginia   00 G4000\n2      3        5      12 00294478    12     FL       Florida   00 G4000\n3      2        3      17 01779784    17     IL      Illinois   00 G4000\n4      2        4      27 00662849    27     MN     Minnesota   00 G4000\n5      3        5      24 01714934    24     MD      Maryland   00 G4000\n6      1        1      44 01219835    44     RI  Rhode Island   00 G4000\n  FUNCSTAT        ALAND      AWATER    INTPTLAT     INTPTLON\n1        A  62266298634   489204185 +38.6472854 -080.6183274\n2        A 138961722096 45972570361 +28.3989775 -082.5143005\n3        A 143778561906  6216493488 +40.1028754 -089.1526108\n4        A 206232627084 18949394733 +46.3159573 -094.1996043\n5        A  25151992308  6979074857 +38.9466584 -076.6744939\n6        A   2677763359  1323686988 +41.5964850 -071.5264901\n                        geometry\n1 MULTIPOLYGON (((-80.85847 3...\n2 MULTIPOLYGON (((-83.10874 2...\n3 MULTIPOLYGON (((-89.17208 3...\n4 MULTIPOLYGON (((-92.74568 4...\n5 MULTIPOLYGON (((-75.76659 3...\n6 MULTIPOLYGON (((-71.67881 4...\n\n\nSimilar to before, we have\n\na spatial summary at the top\na set of normal looking columns with different ID codes and names\nan attached geometry for each row\n\nIf we simply plot this with no aesthetics, we get the outline of all states, but there is something about it that makes it less ideal for a quick data visualization:\n\n## quick plot of states\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\nAs we can see, while the map is geographically accurate, there is a lot of open ocean on the map due to the geographic structure of the US. Often when we see maps of the US, Alaska and Hawaii are moved to make it easier to read. Tigris offers an easy way of doing this:\n\n## shift position of Hawaii and Alaska\ndf_st &lt;- df_st |&gt;\n  shift_geometry(position = \"below\")\n\nThe shift_geometry() should work on any spatial data with Alaska and Hawaii, not only data from tigris. Now let’s run that same plot again.\n\n## replotting with shifted Hawaii and Alaska\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\nAlthough not always a good idea (never do spatial analysis on data you’ve done this to, it will be severely off), if you’re looking to plot the 50 states in an easy-to-read manner, this can be a really useful tool.\nLastly, to re-illustrate what a CRS does, let’s plot this two more times, putting it onto our simple ESPG 4326 CRS, and then using the Peters Projection referenced in the video clip at the start of class.\n\n## change CRS to what we used for earlier map\ndf_st &lt;- df_st |&gt;\n  st_transform(crs = 4326)\n\n## make make\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\nSee how the line are now a perfect grid, but the shapes of states (look at Montana) are a little different? That’s the power of a CRS!\nFinally, let’s please the Organization of Cartographers for Social Equality and look at the Peters projection. Note: while this projection is great for showing comparably accurate area across the globe, it does that by other trade offs not acknowledged by Dr. Fallow from CSE, so it’s not universally better, it’s better for the task it was designed for. That’s the key with CRS, find the best one for the task you’re doing.\n\n## change CRS to requirements for Peters projection\n## h/t https://gis.stackexchange.com/questions/194295/getting-borders-as-svg-using-peters-projection\npp_crs &lt;- \"+proj=cea +lon_0=0 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\ndf_st &lt;- df_st |&gt;\n  st_transform(crs = pp_crs)\n\n## make mape\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\nSee how to the gap between 45 and 50 degrees north is much smaller than between 20 and 25 degrees north? That’s the projection at work (think about how this reflects how the globe is shaped).",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "12-pro-model.html#t-test",
    "href": "12-pro-model.html#t-test",
    "title": "II: Modeling Basics",
    "section": "t-test",
    "text": "t-test",
    "crumbs": [
      "Programming",
      "II: Modeling Basics"
    ]
  },
  {
    "objectID": "12-pro-model.html#lm",
    "href": "12-pro-model.html#lm",
    "title": "II: Modeling Basics",
    "section": "lm",
    "text": "lm\n\nextracting predictions\n\n\nchecking residuals\n\n\nassumption checking packages",
    "crumbs": [
      "Programming",
      "II: Modeling Basics"
    ]
  },
  {
    "objectID": "12-pro-model.html#formula-objects",
    "href": "12-pro-model.html#formula-objects",
    "title": "II: Modeling Basics",
    "section": "formula objects",
    "text": "formula objects",
    "crumbs": [
      "Programming",
      "II: Modeling Basics"
    ]
  },
  {
    "objectID": "12-pro-model.html#modeling-programatically-with-loops",
    "href": "12-pro-model.html#modeling-programatically-with-loops",
    "title": "II: Modeling Basics",
    "section": "modeling programatically with loops",
    "text": "modeling programatically with loops",
    "crumbs": [
      "Programming",
      "II: Modeling Basics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Example Class Folder\nR-Script Template",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#class-overview",
    "href": "index.html#class-overview",
    "title": "Homepage",
    "section": "Class Overview",
    "text": "Class Overview\nContemporary research in higher education (and other disciplines) should be both rigorous and reproducible. This is class will teach you the fundamentals of data management and quantitative research workflow with emphasis on rigor and reproducibility.\nOften referred to informally as “the R class”, you will get an introduction to coding using the R programming language, but many of the skills are directly transferable to future work in Python, Stata, or other software.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#credit-to-dr.-skinner",
    "href": "index.html#credit-to-dr.-skinner",
    "title": "Homepage",
    "section": "Credit to Dr. Skinner",
    "text": "Credit to Dr. Skinner\nFirst and foremost, credit for the structure and vast majority of the content on this site goes to Dr. Benjamin T. Skinner who designed this course before leaving UF to take a data scientist position at the National Endowment for the Humanities. I took this class with Dr. Skinner in Spring 2022, then was a Teaching Assistant for this class with him during Spring 2023. In building this class website, I’ve tried to keep the majority of the content consistent whilst making the class work for the format Dr. Tanner and I are going to teach it in.\nIf you’re interested in seeing Dr. Skinner’s versions of the class, here are links to his course site\n\nEDH 7916 Spring 2023\nEDH 7916 Spring 2022",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#a-note-on-uncertainty-tolerance",
    "href": "index.html#a-note-on-uncertainty-tolerance",
    "title": "Homepage",
    "section": "A Note on “Uncertainty Tolerance”",
    "text": "A Note on “Uncertainty Tolerance”\nOne of my close friends is full-time application developer for UF, and he likes to talk with his new hires about the need for “uncertainty tolerance”. In essence, this is the ability to be okay with not knowing if something is going to work as expected and having the patience to play around until it does. For some, this comes naturally, for others this is one of the biggest hurdles to overcome. Unfortunately, whichever category you fall into, to learn to code you will need to build up some level of “uncertainty tolerance”. There are going to be times you code doesn’t run or work as expected, and every single programmer I know solves these issues the same way, playing around until it works. At first this may be wildly frustrating, particularly for some of you, but it will come over time, just bear with it as best as you can!",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#the-im-stuck-workflow",
    "href": "index.html#the-im-stuck-workflow",
    "title": "Homepage",
    "section": "The “I’m Stuck” Workflow",
    "text": "The “I’m Stuck” Workflow\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nMatt’s office hours or email\n\nTrying the above steps first really helps me help you\n\nI’d probably start by going through them anyway\n\nI rarely will give direct answers, I just help you think through the issue\n\n\nNote: As one of the main purposes of this class is to teach you the basics of R programming, the use of AI-based coding tools (such as ChatGPT, GitHub Co-Pilot, Google Bard, etc.) is not permitted.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#useful-rstudio-keyboard-shortcuts",
    "href": "index.html#useful-rstudio-keyboard-shortcuts",
    "title": "Homepage",
    "section": "Useful RStudio Keyboard Shortcuts",
    "text": "Useful RStudio Keyboard Shortcuts\n\nRun selected code: Command Return (mac) Ctrl Enter (windows)\nComment/un-comment selected lines: Shift Command C (mac) Ctrl Shift C (windows)\nAuto-format code indentations: Command I (mac) Ctrl I (windows)\nNew code chunk (in .qmd files): Option Command I (mac) Ctrl Alt I\nToggle full screen code window: Control Shift 1 (mac) Ctrl Shift 1 (windows)\nMulti-line cursor: Alt then Drag-the-mouse\nTypical shortcuts such as Command/Ctrl C for copy also work\n\nSee more here",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Homepage",
    "section": "Useful Links",
    "text": "Useful Links\n Email Matt\n Email Melvin\n Stack Overflow R Questions\n See Examples of Matt’s Code on Github\n See Examples of Dr. Skinner’s Code on Github",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#good-luck-in-the-class",
    "href": "index.html#good-luck-in-the-class",
    "title": "Homepage",
    "section": "Good luck in the class!",
    "text": "Good luck in the class!\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "x-01-set-git.html#installing-git",
    "href": "x-01-set-git.html#installing-git",
    "title": "Extra: Git & GitHub",
    "section": "Installing git",
    "text": "Installing git\nThere are two things you need to do to use git/GitHub in our course: (1) have an installation of git and (2) have a GitHub account. Rather than reinventing the wheel, I suggest following the instructions from Jenny Bryan.\n\nGet a GitHub account\nInstall git on your computer\n\nNOTE As part of registering an account with GitHub, I recommend requesting an Education Discount so you can get free private repositories for future work.\nFrom the git website http://git-scm.com/\n\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#why-use-git",
    "href": "x-01-set-git.html#why-use-git",
    "title": "Extra: Git & GitHub",
    "section": "Why use git?",
    "text": "Why use git?\nWith so many other (read: easier!) ways to share and collaborate on documents, why use git? Isn’t it a bit overkill to learn an entirely new syntax? Why not just email files or use something like DropBox? Because it is very easy to end up with something like this:\n\nCredit: Jorge Chan\n\nAs complexity increases, so does the need for git\n\\[ Project = f(size, scope, collaborators) \\]\nAs any part of that function grows, so too does the need for a work flow that:\n\nAllows for many moving parts\nAllows for peaceful collaboration (no overwrites)\nAllows for timely collaboration (synchronous)\nAllows for distant collaboration (centralized file location)\nAllows for version control (so you can go back if necessary)\n\nGit was designed to do all these things, so it works better than other systems.\n\n\nWhat’s the difference between git and GitHub?\nYep, you guessed it. Git is the system/language that supports version control. GitHub is an online service that will host your remote repositories, for free if public or a small fee if private. (Students get an education discount and some free repos. Check out https://education.github.com/.)\nRStudio is nice because it provides an nice point-and-click interface for git. (Sourcetree and GitHub Desktop are also really nice GUIs.) If you want to run git at the command line, go for it! But using RStudio or another GUI is fine.\n\n\nHow does git/GitHub work?\n\nCredit: Lbhtw (Own work)\n\n\nPlain text and git\nGit works best with plain text files. This is because it notes the differences across two plain text files rather than just copying and re-copying the same file over and over again as it changes. When you’ve only changed one word, git’s method of version control is much more efficient than making a whole new copy.\nIt’s also useful when you need to merge two files. If file B is just file A with new section, file B can be easily merged into file A by inserting the new section — just like you would add a paragraph in a paper you’re writing. R scripts are plain text. Some data files, like *.csv, are plain text. This is why git works really well with data analysis workflows.\nOn the other hand, git does not work as well with binary files. These files are stored in a format closer to what your computer understands, which comes with benefits. Data files, like Stata’s .dta and R’s .Rdata, as well as MS Office files — .docx, .xlsx, *.pptx, etc — are binary files. Git will keep track of your MS Word document, but due to its underlying structure, you won’t be able to merge and every small change will just make a whole new copy of the file. This is why we generally don’t commit large binary data files to Git: your repo just becomes larger and larger with each small change to your data.\n\n\nSome notes on work flow (good habits)\n\nAlways pull from your local repo with your remote repo before starting any work. This makes sure you have the latest files. RStudio has a pull button in the Git tab.\nDon’t wait to push your commits. Just like you save your papers every few lines or paragraphs, you should push to your remote repo. This way, you’re less likely to lose work in the event of a computer crash. Also, should you want to return to a prior version, small changes make it easier to find what you want.\n\nAdd useful commit messages. RStudio will make you add something. Don’t say “stuff.” A version history of 95 “stuff”s is pretty non-helpful. Also, I would keep it clean. Even in a private repository, work as if someone will see what you write someday.\n\nDon’t freak out! If you accidentally push and overwrite or delete something on the remote, you can get it back. That’s the point of version control! Sometimes weird things like merges happen (two files with the same name are shoved together). They can be a pain to fix, but they can be fixed.\nAvoid tracking overly large files and pushing them to your remote repository. GitHub has a file size limit of 100 MB per file and 1 GB per repo. The history of large files compounds quickly, so think carefully before adding them. Usually this means that small data sets are okay; big ones are better backed up somewhere else and left on your machine.\nRemember: even in a private repository, your files are potentially viewable. If any of your data sets or files are restricted, do no push them remotely. Use .gitignore to make sure that git doesn’t track or push them.\n\nEvery class and work session should go like this:\n\npull from GitHub remote\ndo work\nadd or stage (if new file)\ncommit with message\npush back to GitHub remote\n\nIf you’d like you can also use terminal commands to accomplish the same things. Many people (including me) like to use the terminal commands directly.\n\nPull: git pull\nStage: git add &lt;filenames&gt;\nCommit: git commit -m \"&lt;your message here&gt;\"\nPush: git push\n\nThat said, you should use whatever works best for you.",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#getting-your-class-repo-and-linking-with-git",
    "href": "x-01-set-git.html#getting-your-class-repo-and-linking-with-git",
    "title": "Extra: Git & GitHub",
    "section": "Getting your class repo and linking with git",
    "text": "Getting your class repo and linking with git\nIn this class, you’ll work in and submit your work through a private git repository that only you and I will be able to access. I’ve already set that up in our course organization on GitHub. Let’s get your repo onto your computer.\n\nStep 0: Install git on your computer\nFollow these directions to install git on your computer, if you need it and haven’t already done so. If you use a Mac, there’s a good chance you already have git on your machine. You can check by opening the Terminal application and typing which git at the prompt. If you get a response like /usr/bin/git then you have git. It’s unlikely to be the newest version, but is probably good enough for our class. That said, it generally doesn’t hurt to install a newer version.\n\n\nStep 1: setting a personal access token or PAT\nBefore we try to use git through RStudio, we need to set up your GitHub user credentials on your computer. This is a security feature of GitHub that prevents others from accessing your account. Think of a personal access token or PAT as a special type of password that links your computer to GitHub and that can be set with different levels of permission on your account. It can be very limited, allowing the computer user only a very number of options or very broad, giving the computer user a many ways to interact with GitHub. It can also be set to expire if you want.\nTo set up your PAT, we’ll once again use Jenny Bryan’s GitHub guide. Specifically, we’ll use instructions found here. Repeating the instructions, they are:\nIn the RStudio console, type the following and press enter:\nusethis::create_github_token()\n\n\n\nRStudio gitcred prompt\n\n\nThis should open up a web page on GitHub to create a token with some presents. The presets are fine, though I would recommend setting the PAT to last at least until the course is over (so you don’t have to do all of this again in a month). In this example, I put this course id in the use Note box and set the expiration for middle of the summer after the course has ended. Once you’ve set these, scroll to the bottom and click the button to generate the PAT.\n\n\n\nGitHub PAT page\n\n\nYou see a screen that looks like this:\n\n\n\nGitHub PAT page\n\n\nI’ve blocked out the letters and numbers after gh_ (not that I’m using this PAT since it’s now public!), but you should see a full string. Keep this window open and head back to RStudio.\nIn the console type and press enter:\ngitcreds::gitcreds_set()\n\n\n\nGitHub PAT page\n\n\nIt will as you to add your password. Paste the gh_ PAT and press enter. If it worked, you should see the following:\n\n\n\nGitHub PAT page: finish\n\n\nIf you (or someone you share your computer with) has already set up a PAT in the past, you will be given the option to keep your current set up or replace. I’ll leave that to you. All else being equal, it’s probably better to replace the old PAT with the new PAT unless you have compelling reason not to do so.\nYou should now be set to use GitHub with class!\n\n\nStep 2: clone your GitHub repo to your computer\nNavigate to your GitHub repo at github.com. Once you’ve signed into your account, you should be able to navigate to it through GitHub’s interface. Because the repos are structured the same way, you can also use this link:\nhttps://github.com/edquant/&lt;...&gt;\nWhere &lt;...&gt; is your repo name. Unless otherwise said, it should take the form of student_&lt;last name&gt; (students with shared last names will have a slightly different repo name since repo names within the organization must be unique — ask me or check your email). For example, my repo is student_skinner and is found at:\nhttps://github.com/edquant/student_skinner\nIf you click on this an you aren’t me, you’ll get a 404 error page. That’s because it’s a private repository, meaning only those with access can see it — GitHub pretends it doesn’t exist otherwise. If you can’t find your repo, double check two things:\n\nthat you have the correct repo name (no typos)\nthat you are signed into GitHub\n\nIf you are signed in and have the right link, you should see your page. For example, mine looks like this:\n\n\n\nGitHub student repo\n\n\nJust above the list of files, click on the button that says clone (it’s green by default).\n\n\n\nGitHub clone\n\n\nYou’ll see a drop down. Confirm that HTTPS is bold and underlined. You’ll want to copy to the web address in the text box for the next step (you can use the overlapping box symbol just to the right of the address to copy it to your clipboard).\nNow, return to RStudio. In the upper right hand corner, you’ll see Project: (none) (if you’ve been using RStudio, you may have a project going, meaning it won’t say “none”, but that’s okay). Click on that to see a drop down list of options. You will click on the first option New Project.\n\n\n\nRStudio new project\n\n\nThis window will pop up.\n\n\n\nRStudio new project: version control\n\n\nChoose the option Version Control option. In the next window that opens, choose Git.\n\n\n\nRStudio new project: git\n\n\nIn the final window, paste the URL for your GitHub repo in the first text box. The middle text box should auto-fill with your repo name: student_&lt;last_name&gt;. The last text box is where you want to store your repo. On my computer, it shows the Desktop. If you’d rather place your course files in another place on your computer (e.g. a projects folder or maybe a class folder), then use the Browse button to select a new location. The main thing is to remember where you saved it! Once you’re satisfied, click the Create Project button.\n\n\n\nRStudio new project: create\n\n\nIf everything works, you’ll see a drop down window that looks like it’s doing some work with files. It will quickly close and RStudio will look like it is restarting, when it reopens, you should see your repo files in the bottom right pane, a new tab that says Git, and your repo name in the upper right as the new project name. You are now ready to work with your class files!\n\n\n\nRStudio new project: complete",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#stagingadding-committing-and-pushing-files-from-your-computer-to-github",
    "href": "x-01-set-git.html#stagingadding-committing-and-pushing-files-from-your-computer-to-github",
    "title": "Extra: Git & GitHub",
    "section": "Staging/adding, committing, and pushing files from your computer to GitHub",
    "text": "Staging/adding, committing, and pushing files from your computer to GitHub\nThere is nothing special about working in a git directory compared to a normal directory: you still open your files, edit them, and save as normal. The difference is that along the way, you take an extra step that involves:\n\nStaging/adding your files (telling git to pay attention to the changes you’ve made)\nCommitting your files (confirming that you want git to store your changes in your local repository)\nPushing your changes (copying changes to your local repository to your remote repository — GitHub)\n\nIf you click on the Git tab, you’ll a new file listed with some boxes next to it.\n\n\n\nRStudio new project: Git tab\n\n\nThis particular file is a settings file and not that important for us, but it will be good to use as an example. If you want to see information about the file, click the Diff button on the far left, just under the Environment tab.\nA new window will open. The bottom window shows the changes made to the file. Lines with additions are in green; lines with subtractions are in red. Because this is a new file, the entire text is green.\n\n\n\nRStudio new project: Git diff\n\n\nIf you click on the History button, you can see the history of changes to the repo. There’s not much right now, but over time, the history can be very helpful for finding deleted code or pinpointing when a bug was introduced.\n\n\n\nRStudio new project: Git history\n\n\nGoing back by clicking on Changes window, let’s stage/add our file, commit it, and push it.\n\nStage/add\nTo stage/add a file, click the box next to the file name. You’ll notice that the two boxes under the Status header become a single box with an “A” in it for Add.\n\n\n\nRStudio new project: Git add\n\n\n\n\nCommit\nNext, you need to Commit the changes to the local repository. Do this by writing a message in the box on the right. You can write whatever you want, but per the suggestions above, I recommend something short and informative. Once you’re done, click the Commit button.\n\n\n\nRStudio new project: Git commit\n\n\nYou’ll see a drop down menu telling you what has happened. At first this can seem like nothing, but over time, you’ll get the hang of the messages. This tells you that 1 file (our Rproj file changed with 13 new lines (if there were deletions, it would say that, too). You can close this out.\n\n\n\nRStudio new project: Git commit result\n\n\nAfter closing, notice that you don’t see the file name any more or the changes in the bottom panel. However, there is new line towards the top left that tells you “Your branch is ahead of origin/main by 1 commit.” origin/main is the git name of your remote repo: origin is the remote (GitHub) and main is the specific branch (we will only use the “main” branch in this course). In other words, your local repo has changes that are one step ahead of what’s on GitHub. Time to sync them up!\n\n\n\nRStudio new project: Git post commit\n\n\n\n\nPush\nPushing the changes to GitHub is simple: just click the Push button in the far upper right (up arrow). Once again, you’ll get a drop down window with messages from git. There’s no error (it’ll let you know), so you can close the window.\n\n\n\nRStudio new project: Git push\n\n\nWhen you go back to the main RStudio window, you’ll notice that file is no longer listed in the Git tab. Should you make changes to it, it will reappear. Similarly, any files in your local repo that you change, add, or delete will show up here.\n\n\n\nRStudio new project: Git post push\n\n\nIf you want to confirm that the push worked, go to your repo at GitHub. If you can see the new file and the commit message, then you know your changes have made it to your GitHub repo. Importantly for this course, if you see the changes on GitHub, that means I can see them. Conversely, if you don’t see the changes on GitHub, then I can’t see them. Double check!\n\n\n\nRStudio new project: GitHub post push",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#pulling-down-changes-from-github",
    "href": "x-01-set-git.html#pulling-down-changes-from-github",
    "title": "Extra: Git & GitHub",
    "section": "Pulling down changes from GitHub",
    "text": "Pulling down changes from GitHub\nClosing the loop, you also need to pull down changes from GitHub. In general, this is necessary when are you are using multiple computers or working with a collaborator. For this course, you’ll need to pull down updates and comments I make to your repo.\nTo pull, all you need to do is click the Pull button in the Git tab (with the down arrow).\n\n\n\nRStudio: pull\n\n\nIf there’s nothing new, you’ll get a message like this. If so, you’re good to go!\n\n\n\nRStudio: post pull\n\n\nIf there are changes, you’ll be informed of those and you’ll see them reflected in your local files. If there’s a new file, you’ll see it appear in your local directory. If a file is deleted, it will be removed from your local directory. If there were any changes in a file, you’ll see them when you next open the file.\n\nCommitting changes before pulling down\nOne thing that catches new (and experienced!) git users is forgetting to stage/add and commit changes before pulling down new changes from the remote repo. If you try to pull from GitHub and there’s a change to a file you have changed but haven’t add/stage + committed, the pull won’t work. It’s annoying, but it’s actually a nice feature: this prevents changes that you aren’t tracking and therefore can’t recover.\nIf this happens, stage/add and commit your local changes and then pull. If you have made changes that don’t align with the new changes you just pulled down, you’ll end up with a merged file. It can look messy, but it’s totally fixable. If this happens and you have trouble, let me know.",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#final-note-about-git-and-this-course",
    "href": "x-01-set-git.html#final-note-about-git-and-this-course",
    "title": "Extra: Git & GitHub",
    "section": "Final note about git and this course",
    "text": "Final note about git and this course\nBy construction of the scripts I use to build this course, I have the ability to overwrite files you use. That said, I will only overwrite stuff that I put in there, like lessons, assignments, class scripts, etc. I will never overwrite anything you add. Also, I will not overwrite anything you put in the working folder. That’s yours.\nIf you want to, for example, take notes in class scripts using comments (a very good idea), then simply rename the script. For example, you could rename the intro_r.R script to intro_r_notes.R. That way, when I update your repo, you won’t lose your notes when intro_r.R is reset.",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-set-git.html#complete-your-github-profile",
    "href": "x-01-set-git.html#complete-your-github-profile",
    "title": "Extra: Git & GitHub",
    "section": "Complete your GitHub profile",
    "text": "Complete your GitHub profile\nIf you haven’t already, please do the following things for your GitHub profile:\n\nAdd a picture (I don’t care whether you use a photo of yourself or something else more private, but please use something other than the default icon).\nCreate a profile README using these instructions. It doesn’t have to be fancy and can include minimal information that you are comfortable publicly sharing, but at the very least please have something. You can use the default prompts or create your own.",
    "crumbs": [
      "Getting Started",
      "Extra: Git & GitHub"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#example-data-analysis-task",
    "href": "x-03-pro-vanilla.html#example-data-analysis-task",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Example data analysis task",
    "text": "Example data analysis task\nLet’s imagine we’ve been given the following data analysis task with the HSLS09 data:\n\nFigure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R. Instead, it’s up to you to (1) translate a research question into the discrete steps coding steps necessary to provide an answer, and then (2) translate the answer such that everyone understands what you’ve found.\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nArrange in order so it’s easier to rank and share\nWrite out the results to a file so we have it for later\n\nLet’s do it!\nNOTE: Since we’re not using the tidyverse, we don’t need to call it this time. Even with non-tidyverse R, you may need to call libraries. This analysis, however, does not require any.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NONE",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#check-working-directory",
    "href": "x-03-pro-vanilla.html#check-working-directory",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Check working directory",
    "text": "Check working directory\nThis script — like the one from the organizing lesson — assumes that the scripts subdirectory is the working directory, that the required data file is in the data subdirectory, and that both subdirectories are at the same level in the course directory. Like this:\nstudent_skinner/         &lt;--- Top-level\n|\n|__/data                 &lt;--- Sub-level 1\n|    |--+ hsls_small.csv\n|\n|__/scripts              &lt;--- Sub-level 1 (Working directory)\n     |--+ dw_one_base_r.R\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that I’m not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, I tell you where you need to be (a common landmark), let you get there, and then rely on relative paths afterwards.",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#read-in-data",
    "href": "x-03-pro-vanilla.html#read-in-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Read in data",
    "text": "Read in data\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES / NCES data set that features:\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link (after a little navigation).\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read.csv(), which is base R function\ndf &lt;- read.csv(file.path(\"data\", \"hsls-small.csv\"))\n\nUnlike the read_csv() function we’ve used before, read.csv() (notice the difference: a . instead of an _) doesn’t print anything. So that we can see our data, well print to the console. BUT before we do that…\nread.csv() returns a base R data.frame() rather than the special data frame or tibble() that the tidyverse uses. It’s mostly the same, but one difference is that whereas R will only print the first 10 rows of a tibble, it will print the entire data.frame. We don’t need to see the whole thing, so we’ll use the head() function to print only the first 10 rows.\n\n## show first 10 rows\nhead(df, n = 10)\n\n   stu_id x1sex x1race x1stdob x1txmtscor x1paredu x1hhnumber x1famincome\n1   10001     1      8  199502    59.3710        5          3          10\n2   10002     2      8  199511    47.6821        3          6           3\n3   10003     2      3  199506    64.2431        7          3           6\n4   10004     2      8  199505    49.2690        4          2           5\n5   10005     1      8  199505    62.5897        4          4           9\n6   10006     2      8  199504    58.1268        3          6           5\n7   10007     2      8  199409    49.4960        2          2           4\n8   10008     1      8  199410    54.6249        7          3           7\n9   10009     1      8  199501    53.1875        2          3           4\n10  10010     2      8  199503    63.7986        3          4           4\n   x1poverty185   x1ses x1stuedexpct x1paredexpct x1region x4hscompstat\n1             0  1.5644            8            6        2            1\n2             1 -0.3699           11            6        1            1\n3             0  1.2741           10           10        4            1\n4             0  0.5498           10           10        3            1\n5             0  0.1495            6           10        3            1\n6             0  1.0639           10            8        3           -8\n7             0 -0.4300            8           11        1            1\n8             0  1.5144            8            6        1            1\n9             0 -0.3103           11           11        3            1\n10            0  0.0451            8            6        1           -8\n   x4evratndclg x4hs2psmos\n1             1          3\n2             1          3\n3             1          4\n4             0         -7\n5             0         -7\n6            -8         -8\n7             1          2\n8             1          3\n9             1          8\n10           -8         -8\n\n\n\nQuick exercise\nread.csv() is special version of read.table(), which can read various delimited file types, that is, tabular data in which data cells are separated by a special character. What’s the special character used to separate CSV files? Once you figure it out, re-read in the data using read.table(), being sure to set the sep argument to the correct character.\n\nYou’ll also notice that the data.frame doesn’t tell use the types of our columns. If we really want to know, we can use the class() function. So that we can see all columns, we can use the sapply() function, which will let us apply the class() function across all columns.\n\n## show column types\nsapply(df, class)\n\n      stu_id        x1sex       x1race      x1stdob   x1txmtscor     x1paredu \n   \"integer\"    \"integer\"    \"integer\"    \"integer\"    \"numeric\"    \"integer\" \n  x1hhnumber  x1famincome x1poverty185        x1ses x1stuedexpct x1paredexpct \n   \"integer\"    \"integer\"    \"integer\"    \"numeric\"    \"integer\"    \"integer\" \n    x1region x4hscompstat x4evratndclg   x4hs2psmos \n   \"integer\"    \"integer\"    \"integer\"    \"integer\"",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#select-variables-columns",
    "href": "x-03-pro-vanilla.html#select-variables-columns",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\nData frames are like special matrices. They have rows and columns. You can access these rows and columns using square bracket notation ([]). Because a matrix has two dimensions, you use a comma inside the square brackets to indicate what you mean ([,]):\n\ndf[&lt;rows&gt;,&lt;cols&gt;]\n\nAt it’s most basic, you can use numbers to represent the index of the cell or cells you’re interested in. For example, if you want to access the value of the cell in row 1, column 4, you can use:\n\n## show value at row 1, col 4\ndf[1, 4]\n\n[1] 199502\n\n\nBecause data frames have column names (the variable names in our data set), we can also refer to them by name. The fourth column is the student date of birth variable, x1stdob. Using that instead of 4 (notice the quotation marks \"\"):\n\n## show value at row 1, x1stdob column\ndf[1, \"x1stdob\"]\n\n[1] 199502\n\n\nIf we want to see more than one column, we can put the names in a concatenated vector using the c() function:\n\n## show values at row 1, stu_id & x1stdob column\ndf[1, c(\"stu_id\", \"x1stdob\")]\n\n  stu_id x1stdob\n1  10001  199502\n\n\nSo far, we’ve not assigned these results to anything, so they’ve just printed to the console. However, we can assign them to a new object. If we want to slice our data so that we only have selected columns, we can leave the rows section blank (meaning we want all rows) and include all the columns we want to keep in our new data frame object.\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- df[, c(\"stu_id\", \"x1stuedexpct\", \"x1paredexpct\", \"x1region\")]\n\n## show 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region\n1   10001            8            6        2\n2   10002           11            6        1\n3   10003           10           10        4\n4   10004           10           10        3\n5   10005            6           10        3\n6   10006           10            8        3\n7   10007            8           11        1\n8   10008            8            6        1\n9   10009           11           11        3\n10  10010            8            6        1",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#mutate-data-into-new-forms",
    "href": "x-03-pro-vanilla.html#mutate-data-into-new-forms",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\n\nChanging existing variables (columns)\nTo conditionally change a variable, we’ll once again use the bracket notation to target our changes. This time, however, we do a couple of things differently:\n\ninclude square brackets on the LHS of the assignment\nuse conditions in the &lt;rows&gt; part of the bracket\n\nAs before, we need to account for the fact that our two expectation variables, x1stuedexpct and x1paredexpct, have values that need to be converted to NA: -8, -9, and 11. See the first data wrangling lesson for the rationale behind these changes.\nFirst, let’s look at the unique values using the table() function. So that we see any missing values, we’ll include an extra argument:\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n  -8    1    2    3    4    5    6    7    8    9   10   11 \n2059   93 2619  140 1195  115 3505  231 4278  176 4461 4631 \n\n## see unique values for parental expectation\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n  -9   -8    1    2    3    4    5    6    7    8    9   10   11 \n  32 6715   55 1293  149 1199  133 4952   76 3355   37 3782 1725 \n\n\nNotice that we use a dollar sign, $, to call the column name directly. Unlike with the tidyverse, we cannot just use the column name. Base R will look for that column name not as a column in a data frame, but as its own object. It probably won’t find it (or worse, you’ll have another object in memory that it will find and you’ll get the wrong thing!).\nRemember how data.frames are special matrices? One of the special features is that you can use the $ sign with the column name as a short cut or double square brackets without the comma.\n\n## each version pulls the column of data for student expectations\n## TRUE == 1, so if the mean of all values == 1, then all are TRUE\nmean(df_tmp$x1stuedexpct == df_tmp[, \"x1stuedexpct\"]) == 1\n\n[1] TRUE\n\nmean(df_tmp$x1stuedexpct == df_tmp[[\"x1stuedexpct\"]]) == 1\n\n[1] TRUE\n\n\nBack to replacing our missing values with NA…\nThe conditions we care about are when df_tmp$x1stuedexpct == -8, for example. Using that condition in the row section of the square bracket, we can replace only what we want.\n\n## replace student expectation values\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8] &lt;- NA\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == 11] &lt;- NA\n\n## replace parent expectation values\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == -8] &lt;- NA\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == -9] &lt;- NA\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct == 11] &lt;- NA\n\nWhat each of these lines says is (first one for example): In the data.frame df_tmp, replace the value in x1stuedexpct — where the value of df_tmp$x1stuedexpct is -8 — with NA\nAs less convoluted way of saying this might be (more generally stated): Where the value in column A equals X, replace the value in column A with Y.\nLet’s confirm using table() again. The values that were in -8, -9, and 11 should now be summed under NA.\n\n## see unique values for student expectation (confirm changes)\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  93 2619  140 1195  115 3505  231 4278  176 4461 6690 \n\n## see unique values for parental expectation (confirm changes)\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  55 1293  149 1199  133 4952   76 3355   37 3782 8472 \n\n\n\n\nAdding new variables (columns)\nAdding a new variable to our data frame is just like modifying an existing column. The only difference is that instead of putting an existing column name after the first $ sign, we’ll make up a new name. This tells R to add a new column to our data frame.\nAs with the tidyverse version, we’ll use the ifelse() function to create a new variable that is the higher of student or parental expectations.\n\n## add new column\ndf_tmp$high_expct &lt;- ifelse(df_tmp$x1stuedexpct &gt; df_tmp$x1paredexpct, # test\n                            df_tmp$x1stuedexpct,                       # if TRUE\n                            df_tmp$x1paredexpct)                       # if FALSE\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1         NA\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1         NA\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\nAgain, our “ocular test” shows that this doesn’t handle NA values correctly. Look at student 10002 in the second row: while the student doesn’t have an expectation (or said “I don’t know”), the parent does. However, our new variable records NA. Let’s fix it with this test:\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\n\n## NB: We have to include [is.na(df_tmp$high_expct)] each time so that\n## everything lines up\n\n## step 1 student\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]), \n    ## if TRUE do this...\n    df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n## step 2 parent\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1paredexpct[is.na(df_tmp$high_expct)]),\n    ## if TRUE do this...\n    df_tmp$x1paredexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\nThat’s a lot of text! What’s happening is that we are trying to replace a vector of values with another vector of values, which need to line up and be the same length. That’s why we have\n## what we'll use to replace when TRUE\ndf_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]\nWhen our high_expct column has missing values, we want to replace with non-missing x1stuedexpct values in the same row. That means we also need to subset that column to only include values in rows that have missing high_expct values. Because we must do this each time, our script gets pretty long and unwieldy.\nLet’s check to make sure it worked as intended.\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue. Looking at rows 7 and 9, it seems like those situations are correctly handled as well.",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#filter-observations-rows",
    "href": "x-03-pro-vanilla.html#filter-observations-rows",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\nLet’s check the counts of our new variable:\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ntable(df_tmp$high_expct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  71 2034  163 1282  132 4334  191 5087  168 6578 3463 \n\n\nSince we’re not going to use the missing values (we really can’t, even if we wanted to do so), we’ll drop those observations from our data frame.\nAs when we selected columns above, we’ll use the square brackets notation. As with dplyr’s filter(), we want to filter in what we want. We set this condition before the comma in the square brackets. Because we want all the columns, we leave the space after the comma blank.\n\n## filter in values that aren't missing\ndf_tmp &lt;- df_tmp[!is.na(df_tmp$high_expct),]\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n10  10010            8            6        1          8\n11  10011            8            6        3          8\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values). Since we haven’t removed rows until now, we can compare the number of rows in the original data frame, df, to what we have now.\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column. While not a formal test, it does support what we expected (in other words, if the number were different, we’d definitely want to go back and investigate).",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#summarize-data",
    "href": "x-03-pro-vanilla.html#summarize-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. For an overall average, we can just use the mean() function.\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nmean(df_tmp$high_expct)\n\n[1] 7.272705\n\n\nOverall, we can see that students and parents have high postsecondary expectations on average: to earn some graduate credential beyond a bachelor’s degree. However, this isn’t what we want. We want the values across census regions.\n\n## check our census regions\ntable(df_tmp$x1region, useNA = \"ifany\")\n\n\n   1    2    3    4 \n3128 5312 8177 3423 \n\n\nWe’re not missing any census data, which is good. To calculate our average expectations, we need to use the aggregate function. This function allows to compute a FUNction by a group. We’ll use it to get our summary.\n\n## get average (assigning this time)\ndf_tmp &lt;- aggregate(df_tmp[\"high_expct\"],                # var of interest\n                    by = list(region = df_tmp$x1region), # by group\n                    FUN = mean)                          # function to run\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n2      2   7.168110\n3      3   7.357833\n4      4   7.125329\n\n\nSuccess! Expectations are similar across the country, but not the same by region.",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#arrange-data",
    "href": "x-03-pro-vanilla.html#arrange-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Arrange data",
    "text": "Arrange data\nAs our final step, we’ll arrange our data frame from highest to lowest (descending). For this, we’ll use sort() and the decreasing option.\n\n## -----------------\n## arrange\n## -----------------\n\n## arrange from highest expectations (first row) to lowest\ndf_tmp &lt;- df_tmp[order(df_tmp$high_expct, decreasing = TRUE),]\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n3      3   7.357833\n2      2   7.168110\n4      4   7.125329",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#write-out-updated-data",
    "href": "x-03-pro-vanilla.html#write-out-updated-data",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Write out updated data",
    "text": "Write out updated data\nWe can use this new data frame as a table in its own right or to make a figure. For now, however, we’ll simply save it using the opposite of read.csv() — write.csv() — which works like writeRDS() we’ve used before.\n\n## write with useful name\nwrite.csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\nAnd with that, we’ve met our task: we can show average educational expectations by region. To be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region. This caveat doesn’t necessarily make our analysis less useful, but rather sets its scope. Furthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference.",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  },
  {
    "objectID": "x-03-pro-vanilla.html#questions",
    "href": "x-03-pro-vanilla.html#questions",
    "title": "Extra: Vanilla R (Data Wrangling I Redux)",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average standardized math test score?\nWhat is the average standardized math test score by gender?\nIn what year and month were the oldest students in the data set born? The youngest?\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income (give the category and what that category reprents).\nOf the students who earned a high school credential (diploma or GED), what percentage earned a GED or equivalency? How does this differ by region?\nWhat percentage of students ever attended a postsecondary institution by February 2016? Give the cross tabulations for:\n\nfamily incomes less than or equal to $35,000 and greater than $35,000\n\nregion\n\nThis means you should have percentages for 8 groups: above/below $35k within each region.\n\n\nSubmission details\n\nSave your script (&lt;lastname&gt;_assignment_10.R) in your scripts directory.\nPush changes to your repo (the new script and new folder) to GitHub prior to the next class session.",
    "crumbs": [
      "Programming",
      "Extra: Vanilla R (Data Wrangling I Redux)"
    ]
  }
]