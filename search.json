[
  {
    "objectID": "01-set-install.html#getting-started",
    "href": "01-set-install.html#getting-started",
    "title": "I: Installing R & RStudio",
    "section": "Getting started",
    "text": "Getting started\nThe primary pieces of software you are going to need for this class are\n\nR\nRStudio\nMicrosoft Office\n\nAssuming you already have this, but we can meet to install it if not, it is free for UF students\n\n\nThere are also a few optional pieces of software you’ll need for extra credit lessons, but we will cover when needed.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-r",
    "href": "01-set-install.html#installing-r",
    "title": "I: Installing R & RStudio",
    "section": "Installing R",
    "text": "Installing R\n\nR is fantastic (hopefully you will see that throughout the course), but sometimes it can make things seem more complicated than they need to\n\nThe first time it does this is when trying to install it, there’s a bunch of options called “mirrors”\n\nThese are basically to reduce strain on the servers that you download from by using the closest location\n\nThe good news, however, is that a URL that automates this whole process for you came out recently\nThe even better news is that we’ve set up a little portal to that URL here, so you can download it without leaving this page\n\n\n\n\nClick the option for the OS you have (Windows/Mac/Linux)\nThen under the “latest release”\n\n\n\nFor windows users, select “base” then “Download R…” (the top options)\nFor mac users, select either apple silicon or intel options depending on how new your mac is\n\nIf you need to check which kind your mac is, hit the apple logo in the top left of your screen, then “About This Mac.” On Mac computers with Apple silicon, it will show an item labeled Chip, followed by the name of the chip. On Mac computers with an Intel processor, it will show an item labeled Processor, followed by the name of an Intel processor\n\n\n\n\nR will then download, double click on the download when it’s finished and then follow the on-screen prompts\nR is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-rstudio",
    "href": "01-set-install.html#installing-rstudio",
    "title": "I: Installing R & RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nTechnically, R is all you need to do all of our analyses. However, to make it accessible and usable, we also need a “development environment”\n\nThe reason of this, unlike a computer program like Stata or SAS, R is a programming language (same as Python, C++, etc.), that’s what we just installed\nThe easiest way to use programming languages is through a “development environment”\n\nThere are multiple “development environments” you can use for R. VSCode is a great option by Microsoft for using a variety of languages, but, the best option for R is RStudio as it is purpose built for the language (it also works with Python too)\n\n\nTo install RStudio, let’s do the following:\n\n\nGo to this site and click the “Download RStudio Desktop for…” button underneath “2: Install RStudio” (we already did step 1 Install R)\n\n\nThis is simpler than installing R, Posit have a more sophisticated website which will automatically download the right version for your computer\n\n\nRStudio will then download, double click on the download when it’s finished and then follow the on-screen prompts\n\n\nFor mac users, this will just be drag n’ drop RStudio into your Applications folder\n\n\nRStudio is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#lets-see-what-we-just-installed",
    "href": "01-set-install.html#lets-see-what-we-just-installed",
    "title": "I: Installing R & RStudio",
    "section": "Let’s See What We Just Installed",
    "text": "Let’s See What We Just Installed\n\nHopefully, you should now be able to open RStudio on your computer (it should be the same place all your software is kept)\n\nGo ahead and open it up!\n\n\nBy default, RStudio has 3-4 main frames:\n\nTop left: Script window (will be closed at first if you don’t have any scripts open)\nBottom left: Console\nTop right: Environment / History / Connections\nBottom right: Files / Plots / Packages / Help / Viewer\n\nFor today, we are mostly going to explore some basic features of R using the console, copying and pasting commands from the website rather than saving them in a script (which we will set up next week). All an .R script does is save your code and pass it line-by-line to the console. After today, anything we want to save will be done through a script, anything we just need to run one time will be done in console",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#basic-r-commands",
    "href": "01-set-install.html#basic-r-commands",
    "title": "I: Installing R & RStudio",
    "section": "Basic R Commands",
    "text": "Basic R Commands\nFirst, let’s try the traditional first command!\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nWe can also use R like a basic calculator\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#assignment",
    "href": "01-set-install.html#assignment",
    "title": "I: Installing R & RStudio",
    "section": "Assignment",
    "text": "Assignment\n\nThe first two commands we ran simply spat the output out in the console\n\nThis can be useful if you want to check something quickly or if we have our final output\n\nMore often, though, we want to save the output to our R Environment (top right panel)\nTo do this, we need to assign the output to an object\n\nR is a type of object-oriented programming environment. This means that R thinks of things in its world as objects, which are like virtual boxes in which we can put things: data, functions, and even other objects.\n\nIn R (for quirky reasons), the primary means of assignment is the arrow, &lt;-, which is a less than symbol, &lt;, followed by a hyphen, -.\n\nYou can use = (which is more common across other programming languages), and you may see this “in the wild”\nBut R traditionalists prefer &lt;- for clarity and readability, and let’s try to use &lt;- in this class\n\n\n\n## assign value to object x using &lt;-\nx &lt;- 1\n\n\nBut where’s the output?\n\nCheck out the “Environment” tab on the top left panel\n\nWe see something called x has a value of 1\n\nNow let’s call that object\n\n\n\n\n## what's in x?\nx\n\n[1] 1\n\n\nNote: the [1] is just the index (order) number, if we had more than 1 thing in our object, that would be more useful\n\nQuick exercise\nUsing the arrow, assign the output of 1 + 1 to x. Next subtract 1 from x and reassign the result to x. Show the value in x.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#comments",
    "href": "01-set-install.html#comments",
    "title": "I: Installing R & RStudio",
    "section": "Comments",
    "text": "Comments\nFor this section, let’s just open a blank .R script in RStudio (again, all these commands will be in a script in your class folder we set up next week)\n\nComments in R are set off using the hash or pound character at the beginning of the line: #\nThe comment character tells R to ignore the line\nComments are useful for explaining what your code is doing, why you’re doing it, or for temporarily removing code from your script without deleting it\nYou can also use comments to take notes for this class!\n\n\nQuick exercise\nType the phrase “This is a comment” directly into the R console both with and without a leading “#”. What happens each time?\n\n\nYou may notice sometimes we use two hashes\n\nYou can use only a single # for your comments if you like, R treats them all the same\nIf you’re typing longer comments ##' (two hashes and an apostrophe) is really useful in RStudio, as it automatically comments the next line (although this can be annoying at times too)\n\nLastly, RStudio can comment/uncomment multiple lines of code you’ve already written\n\nOn the top menu bar select “Code” then “Commment/Uncomment Lines”\n\nAlso see the keyboard shortcut next to that option!\n\n\nThis is a big time saver!\n\n\n## Try commenting/uncommenting the below line\n\n# EDH7916 &lt;- \"Hi\"",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#data-types-and-structures",
    "href": "01-set-install.html#data-types-and-structures",
    "title": "I: Installing R & RStudio",
    "section": "Data types and structures",
    "text": "Data types and structures\nR uses variety of data types and structures to represent and work with data. There are many, but the major ones that you’ll use most often are:\n\nlogical\nnumeric (integer & double)\ncharacter\nvector\nmatrix\nlist\ndataframe\n\nLet’s see what type of object x we created earlier is\n\ntypeof(x)\n\n[1] \"double\"\n\n\nWhat if we make it “1”?\n\nx &lt;- \"1\"\ntypeof(x)\n\n[1] \"character\"\n\n\nUnderstanding the nuanced differences between data types is not important right now. Just know that they exist and that you’ll gain an intuitive understanding of them as you become better acquainted with R.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#packages",
    "href": "01-set-install.html#packages",
    "title": "I: Installing R & RStudio",
    "section": "Packages",
    "text": "Packages\n\nUser-submitted packages are a huge part of what makes R great\nYou may hear the phrases “base R” or “vanilla R” during class\n\nThat is the R that comes as you download it with no packages loaded\nWhile it’s powerful in and of itself — you can do everything you need with base R — most of your scripts will make use of one of more contributed packages. These will make your data analytic life much nicer. We’ll lean heavily on the tidyverse suite of packages this semester.\n\n\n\nInstalling packages from CRAN\n\nMany contributed packages are hosted on the CRAN package repository. - What’s really nice about CRAN is that packages have to go through quite a few checks in order for CRAN to approve and host them. Checks include;\n\nMaking sure the package has documentation\nWorks on a variety of systems\nDoesn’t try to do odd things to your computer\n\nThe upshot is that you should feel okay downloading these packages from CRAN\n\nTo download a package from CRAN, use:\n\ninstall.packages(\"&lt;package name&gt;\")\n\nNOTE Throughout this course, if you see something in triangle brackets (&lt;...&gt;), that means it’s a placeholder for you to change accordingly.\nMany packages rely on other packages to function properly. When you use install.packages(), the default option is to install all dependencies. By default, R will check how you installed R and download the right operating system file type.\n\nQuick exercise\nInstall the tidyverse package, which is really a suite of packages that we’ll use throughout the semester. Don’t forget to use double quotation marks around the package name:\n\n\ninstall.packages(\"tidyverse\")\n\n\n\nInstalling packages using the top menu bar\n\nAlternatively, you can install packages by going to “Tools”, then “Install Packages”, then type in the package you want to install\n\n\n\nLoading package libraries\nPackage libraries can loaded in a number of ways, but the easiest it to write:\n\nlibrary(\"&lt;library name&gt;\")\n\nwhere \"&lt;library name&gt;\" is the name of the package/library. You will need to load these before you can use their functions in your scripts. Typically, they are placed at the top of the script file.\nFor example, let’s load the tidyverse library we just installed:\n\n## load library (note quirk that you don't need quotes here)\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that when you load the tidyverse (which, again, is actually loading a number of other libraries), you see a lot of output. Not all packages are this noisy, but the information is useful here because it shows all the libraries that are now loaded and ready for you to use.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#help",
    "href": "01-set-install.html#help",
    "title": "I: Installing R & RStudio",
    "section": "Help",
    "text": "Help\nIt’s almost impossible to have every R function and nuance memorized, and you don’t have to. With all the user-written packages, it would be difficult to keep up. When stuck, there are a few ways to get help.\n\nHelp files\nIn the console, typing a function name immediately after a question mark will bring up that function’s help file (in RStudio, you should see in the bottom right panel):\n\n## get help file for function\n?median\n\nTwo question marks will search for the command name in CRAN packages (again, in the bottom right facet):\n\n## search for function in CRAN\n??median\n\nAt first, using help files may feel like trying to use a dictionary to see how to spell a word — if you knew how to spell it, you wouldn’t need the dictionary! Similarly, if you knew what you needed, you wouldn’t need the help file. But over time, they will become more useful, particularly when you want to figure out an obscure option that will give you exactly what you need.\n\n\nPackage Website\n\nWhile all R packages have to have help files, not all R packages have nice webpages. However, a lot of the main ones do, and they are often much nicer than the CRAN helpfiles\n\nFor example, here’s another magic portal to the tidyverse’s dplyr website (you may spent a good amount of time here this semester)\n\nUsually if you Google something like “&lt;package name&gt; R,” and the website will come up\nYou can find links to all the tidyverse packages here\n\n\nGoogle it!\nGoogle is a coder’s best friend. If you are having a problem, odds are a 1,000+ other people have too and at least one of them has been brave enough (people can be mean on the internet) to ask about it in a forum like StackOverflow, CrossValidated, or R-help mailing list.\nIf you are lucky, you’ll find the exact answer to your question. More likely, you’ll find a partial answer that you’ll need to modify for your needs. Sometimes, you’ll find multiple partial answers that, in combination, help you figure out a solution. It can feel overwhelming at first, particularly if it’s a way of problem-solving that’s different from what you’re used to. But it does become easier with practice.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#useful-packages",
    "href": "01-set-install.html#useful-packages",
    "title": "I: Installing R & RStudio",
    "section": "Useful packages",
    "text": "Useful packages\nWe’re going to use a number of packages this semester. While we may need more than this list — and you almost certainly will in your own future work — let’s install these to get us started.\n\nQuick exercise\nInstall the following packages using the install.packages() function:\n\n\n\ndevtools\nknitr\n`rmarkdown`\nquarto",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#organization",
    "href": "x-04-skinner-phil.html#organization",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Organization",
    "text": "Organization\nFrom the organizing lesson, we’ve been very concerned about how our project is organized. This includes both the directory of folders and files on our machine and the scripts that run our analyses.\n\nProject directory\n\nNote from Matt: Recall we discussed the difference between Dr. Skinner’s “pristine kitchen” approach (tidier) and our approach (easier) with scripts left “out on the counter” early in the class. Take some time to read Dr. Skinner’s opinion on that here and decide which you agree with.\n\nWhile it is certainly easier on the front end to store all project files in a single folder\n./project\n|\n|--+ analysis.R\n|--+ clean.R\n|--+ clean_data.rds\n|--+ data1.csv\n|--+ data2.csv\n|--+ histogram_x.png\n|--+ density_z.png\nit’s better to separate files by type/purpose in your project directory (“a place for everything and everything in its place”):\n./project\n|\n|__/data\n|   |--+ clean_data.rds\n|   |--+ data1.csv\n|   |--+ data2.csv\n|__/figures\n|   |--+ histogram_x.png\n|   |--+ density_z.png\n|__/scripts \n    |--+ analysis.R\n    |--+ clean.R\nJust like there’s something to be said for visiting the library in order to scan titles in a particular section — it’s easier to get an idea of what’s available by looking at the shelf than by scanning computer search output — it’s useful to be able to quickly scan only the relevant files in a project. With well-named files (discussed below) and an organized directory structure, a replicator may be able to guess the purpose, flow, and output of your project without even running your code.\n\nQuick question\nWhen using a project directory instead of just a single folder, we need to use file paths. In particular, we use relative paths rather than fixed paths. Why?\n\n\n\nScript\nIn the lesson on organizing, I also shared a template R script. While you do not need to use my particular template — in fact, you should modify it to meet your needs — it does follow a few organizational rules that you should follow as well.\n\nClear header with information about the project, this file, and you.\nLoad libraries, set paths and macros, and write functions towards the top of the file.\nClear sections for reading in data, process, and output (ingredients, recipe, prepared dish).\n\n\nQuick question\nWhy put things like file paths or macros at the top of the file?",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#clarity",
    "href": "x-04-skinner-phil.html#clarity",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Clarity",
    "text": "Clarity\nRemember: even though our scripts are instructions for the computer, they are also meant to be read by us humans. How well you comment your code and how well you name your objects, scripts, data, output, etc, determine the clarity of your intent.\n\nCommenting\nDon’t assume that your intent is clear. Particularly because so much working code comes as the result of many (…many…many…) revisions to non-working code, it’s very important that you comment liberally.\n\nWhat are you doing?\n## Creating a dummy variable for each state from current categorical variable\nWhy are you doing it this way?\n## Converting 1/2 indicator to 0/1 so 1 == variable name\nLinks to supporting documents/websites\n## see &lt;url&gt; for data codebook\nHat-tip (h/t) for borrowed code\n## h/t &lt;url&gt; for general code that I slightly modified \nFormula / logic behind method\n## log(xy) = log(x) + log(y)\n## using logarithms for more numerically stable calculations \n\nAll of these items are good uses for comments.\n\nQuick question\nIn which situation are comments not useful?\n\n\n\nNaming\nFor R, all the following objects are functionally equivalent:\n## v.1\nx &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.2\nstuff &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.3\ncities &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.4\nstates &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\nHowever, only the last object, states, makes logical sense to us based on its contents. So while R will work just as happily with x, stuff, cities, or states, a collaborator will appreciate states since it gives an idea of what it contains (or should contain) — even without running the code.\n\nQuick question\nWithout seeing the initial object assignment, what might you expect to see as output from the following code coming from a fellow higher education researcher? Why?\nfor (i in flagship_names) {\n    print(i)\n}\n\nWhen objects are well named, your code may become largely self-documenting, which is particularly nice since you don’t have to worry about drift between your comments and code over time.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#automation",
    "href": "x-04-skinner-phil.html#automation",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Automation",
    "text": "Automation\nIn the functional programming lesson, we discussed the difference between Don’t Repeat Yourself programming and Write Every Time programming. As much as possible and within the dictates of organizing and clarity, it’s a good idea to automate as much as you can.\n\nPush-button replication\nIn general, the goal for an analysis script — in particular, a replication script — is that it will run from top to bottom with no errors by just pushing a button. What I mean by this is that I want to:\n\nDownload the project directory\nOpen the script in R Studio\n(If necessary, make sure I’m in the correct working directory)\nPush the “Run” button and have all the analyses run with all estimates/figures/tables being produced and sent to their proper project folders.\n\nThe dream!\nWhen projects are complicated or data isn’t easily shared (or can’t be shared), the push button replication may not be feasible. But your goal should always be to get as close to that as possible. Make your life a little harder so your collaborators, replicators — and future you! — have it a little easier.\nONE NOTE Like writing a paper, the process going from a blank script to a final product typically isn’t linear. You’ll have starts and stops, dead-ends and rewrites. You shouldn’t expect a polished paper from the first draft, and you shouldn’t expect to code up a fully replicable script from the get-go.\nThere’s something to be said for just getting your ideas down on paper, no matter how messy. Similarly, it may be better to just get your code to work first and then clean it up later. My only warning here is that while you have to clean up a paper eventually (someone’s going to read it), code still remains largely hidden from public view. There will be a temptation to get it to work and then move on since “no one will see it anyway, right…right?” Fight that urge!\nJust like real debt, the technical debt that you take out each time you settle for a hacky solution that’s good enough can grow exponentially. At some point, the bill will come due in the form of obviously (or even scarier, not-so-obviously) incorrect results. Tracking down the source of errors for large projects or those that reuse code from past projects can be maddening. Take a little extra time to write good code now and it will pay dividends in saved hours down the road, not to mention fewer 2 a.m. cold-sweat awakenings, Is that weird finding maybe because I constructed my main outcome variable wrong…wait, how did I code that…?\n\nQuick question\nIn a world in which not everyone knows how to use R, what’s another benefit of push-button automation?\n\n\n\nMultiple scripts\nWe’ve only worked with one script so far, but you can use the source() function in R to call other R scripts. For a very large project, you may decide to have multiple scripts. For example, you may have one script that downloads data, one that cleans the data, one that performs the analyses, and one that makes the figures.\nRather than telling a would-be replicator to “run the following files in the following order”\n\nget_data.R\nclean_data.R\ndo_analysis.R\nmake_figures.R\n\nyou could instead include a run_all.R script that only sources each script in order and tell the replicator to run that:\n## To run all analyses, set the working directory to \"scripts\" and\n## enter\n##\n## source(\"./run_all.R\")\n##\n## into the console. \n\n## download data\nsource(\"./get_data.R\")\n\n## clean data\nsource(\"./clean_data.R\")\n\n## run analyses\nsource(\"./do_analysis.R\")\n\n## make figures\nsource(\"./make_figures.R\")\nNot only is that easier for others to run, you now have a single script that makes your analytic workflow much clearer. Note that there are other ways to do something like this, such as using makefiles, though using only R works pretty well.\nHere are some replication examples for projects that I’ve done:\n\nSkinner, B. T., & Doyle, W. R. (2021). Do civic returns to higher education differ across subpopulations? An analysis using propensity forests. Journal of Education Finance, 46(4), 519–562. (Replication files)\nSkinner, B. T. (2019). Choosing College in the 2000s: An Updated Analysis Using the Conditional Logistic Choice Model. Research in Higher Education, 60(2), 153–183. (Replication files)\nSkinner, B. T. (2019). Making the connection: Broadband access and online course enrollment at public open admissions institutions. Research in Higher Education, 60(7), 960–999. (Replication files)\n\nYou can find the rest on my website as well as on my GitHub profile. As you complete projects and create replication scripts, be sure to make them accessible. Not only is it good for the field, it also signals strong transparency and credibility on your part as a researcher.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#testing",
    "href": "x-04-skinner-phil.html#testing",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Testing",
    "text": "Testing\nAs you wrangle your data, you should take advantage of your computer’s power not only to perform the munging tasks you require, but also to test that inputs and outputs are as expected.\n\nExpected data values\nLet’s say you are investigating how college students use their time. You have data from a time use study in which 3,000 students recorded how they spent every hour for a week. The data come to you at the student-week level and grouped by activity type. This means you see that Student A spent 15 hours working, 20 hours studying/doing homework, etc. You create a new column that adds the time for all categories together and take the summary. You find:\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\ntotal_hrs\n167.99\n1.93\n152.48\n183.46\n\n\n\nSomething is wrong.\n\nQuick question\nWhat’s wrong?\n\nRather than going through an ad-hoc set of “ocular” checks, you can build in more formal checks. We’ve done a little of this when using identical() or all.equal() to compare data frames. You could build these into your scripts. For example:\n\nsumming activity hours in a week, check if any exceed the total number of hours that are possible\nreading in data with student GPAs, check if they are negative\ncombining data from 100 students at 5 schools, make sure your final data frame has 500 rows\ncreating a new percentage from two values, check that the value between 0 and 100 (or between 0 and 1 if a proportion)\n\nYou can use simple tests to check, and include a stop() command inside an if() statement:\n## halt if max number of months is greater than 12 since data are only\n## for one year\nif (max(length(months)) &gt; 12) {\n    stop(\"Reported months greater than 12! Check!\")\n}\nThis only works if you know your data. Knowing your data requires both domain knowledge (“Why are summer enrollments so much higher than fall enrollments…that doesn’t make sense.”) and particular knowledge about your data (“All values of x between 50 and 100, except these two, which are -1 and 20000…something’s up”).\n\n\nExpected output from functions\nProgrammers are big on unit testing their functions. This simply means giving their functions or applications very simple input that should correspond to straightforward output and confirming that that is the case.\nFor example, if their function could have three types of output depending on the input, they run at least three tests, one for each type of output they expect to see. In practice, they might run many more tests for all kinds of conditions — even those that should “break” the code to make sure the proper errors or warnings are returned.\nAs data analysts, you are unlikely to need to conduct a full testing regime for each project. But if you write your own functions, you should definitely stress test them a bit. This testing shouldn’t be in your primary script but rather in another script, perhaps called tests.R. These can be informal tests such as\n## should return TRUE\n(function_to_return_1() == 1)\nor more sophisticated tests using R’s testthat library.\nAgain, the point isn’t that you spend all your time writing unit tests. But spending just a bit of time checking that your functions do what you think they do will go a long way.\n\nQuick question\n\n\nImagine you’ve written your own version of scale(), which lets you normalize a set of numbers to N(0,1):\nmy_scale &lt;- function(x) {\n    ## get mean\n    mean_x &lt;- mean(x)\n    ## set sd\n    sd_x &lt;- sd(x)\n    ## return scaled values\n    ##             x - mean_x\n    ## scaled_x = ------------\n    ##                sd_x\n    return((x - mean_x) / sd_x)\n}\nWhat kind of tests do you think you should run, i.e., what kind of inputs should you give it and what should you expect?\n\n\n\nFail loudly and awkwardly\nThere is a notion in software development that a program should fail gracefully, which means that when a problem occurs, the program should at the very least give the user a helpful error message rather than just stopping without note or looping forever until the machine halts and catches fire.\nAs data analysts, I want your scripts to do the opposite: I want your code to exit loudly and awkwardly if there’s a problem. You aren’t writing a program but rather an analysis. Therefore, the UX I care about is the reader of your final results and your future replicators. Rather than moving forward silently when something strange happens — which is lovely on the front end because everything keeps running but deadly on the back end — your code should stop and yell at you when something unexpected happens. It’s annoying, but that’s the point.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#four-stages-of-data-a-taxonomy",
    "href": "x-04-skinner-phil.html#four-stages-of-data-a-taxonomy",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Four stages of data: a taxonomy",
    "text": "Four stages of data: a taxonomy\nIn the lessons so far, we’ve generally read in raw data, done some processing that we printed to the console, and then finished. This is fine for our small lessons and assignments, but in your own work, you will often want to save your changes to the data along the way — that is, save updated data files at discrete points in your data analysis. This is particularly true for large projects in which wrangling your data is distinct from its analysis and you don’t want to run each part every time.\nBroadly, there are four stages of data:\n\nRaw data\nBuilt data (objective changes)\nClean data (subjective changes)\nAnalysis data\n\nFor small projects, you may not end up with data files for each stage. That said, your workflow should be organized in terms of moving from one stage to the next.\nWe’ll talk about each below.\nACKNOWLEDGMENT I want to give a quick hat tip to two people who’ve really helped me clarify my thinking about these stages of data, Will Doyle and Sally Hudson: Will for first showing the importance of keeping raw data untouched; and Sally for showing how useful it is to make a distinction between objective and subjective data processing, particularly in large projects. Thanks and credit to both.\n\nRaw data\nRaw data, that is, the data you scrape, download, receive from an advisor or research partner, etc, are sacrosanct. Do not change them! They are to be read into R (or whatever software you use) and that is all.\nYou will be tempted from time to time to just fix this one thing that is wonky about the data. Maybe you have a *.csv file that has a bunch of junk notes written above the actual data table (not uncommon). The temptation will be to delete those rows so you have a clean file. NO! There are ways to skip rows in a file like this, for example. R is particularly good at reading in a large number of data types with a large number of special situations.\nIf you are at all concerned that you or a collaborator may accidentally save over your raw files, your operating system will allow you to change the file permissions such that they are read-only files (Dropbox will let you protect files in this way, too). Particularly if working with a number of collaborators, protecting your raw data files in this way is a good idea.\n\n\nBuilt data (objective changes)\nAs a first step in processing your data, you should only make objective modifications. What do I mean by objective? They are those modifications that:\n\nAre required to get your data in good working order\nAre modifications that will always be needed across projects using the same data (in the context of multiple projects)\n\nTo the first point, let’s say you receive student-level administrative data that includes a column, state, that is the student’s home state. However, values in the column include:\n\n\"Kentucky\"\n\"KY\"\n\"Ken\"\n\"21\" (state FIPS code)\n\nAnother example would be a date column, month, in which you see:\n\n\"February\"\n\"Feb\"\n\"2\"\n\nIn both cases, it’s clear that you want a consistent format for the data. Whatever you pick, these are objective changes that you would make at this time.\nTo the second point, let’s say this same administrative data are going to be used across multiple projects. In each project, you need to compute the proportion of in-state vs. out-of-state students. The data column that indicates this is called residency and is coded (with no data labels) as\n\n1 := in-state\n2 := out-of-state\n\nComing back to the data later and taking a look at the first few rows in residency, will it be clear\n\nwhat residency means?\nwhat the values of 1 and 2 represent?\n\nNo.\nAn objective change that you can use across analyses will be to make a new column called instate in which\n\n0 := out-of-state\n1 := in-state\n\nThe benefits are three-fold:\n\n0 and 1 are logically coded as FALSE and TRUE\ninstate gives a direction to the 0s and 1s: 1 means in-state\ntaking the mean of instate will give you the proportion of in-state students in your data\n\nIf your data or project (or number of projects) is large, you many want to save data files after your objective wrangling. These can be reused across projects, which will make sure that objective data wrangling decisions are consistent.\n\n\nClean data (subjective changes)\nWhatever changes are left over after you’ve make objective changes are subjective changes. These changes are project and analysis specific.\nFor example, let’s say your student-level administrative data have information about the student’s expected family contribution or EFC. In your data, you have the actual values. However, you want to compare your data to other published research in which EFC is binned into discrete categories. This means that in order to make a comparison, you have to bin your data the same way. This is a subjective task for two reasons:\n\nthere’s not an objective reason to bin your data in this manner, i.e. to make the data consistent or clearer\nit’s unlikely you’ll need this new column of data for other projects\n\nSome other subjective changes you might make to a data set:\n\ncreate an indicator variable that equals 1 when a student’s family income is below $35,000/year (why $30k? why not $25k or $35k?)\nconvert institutional enrollments to the log scale (do you always need to do this?)\njoin in data on the unemployment rate in the student’s county (do you always need this information at the county level? at all?)\n\nBut wait! Maybe you do want to use the binned value of EFC in multiple projects — perhaps the binned categories are standard across the literature — or the other examples appear always useful and otherwise standard. Isn’t that objective?\nMaybe!\nThe thing about determining the difference between objective vs subjective cleaning is that on the margins, well, it’s subjective! The point is not to have hard and fast rules, but rather to do your best to clearly separate those data wrangling tasks that must be done for the sake of consistency and clarity versus those that may change depending on your research question(s).\n\nQuick question\nWhat might be some other subjective data wrangling tasks? Think about other research you’ve conducted or seen. Or think about what our lessons and assignments so far: what have been subjective tasks?\n\n\n\nAnalysis data\nFinally, analysis data are data that are properly structured for the analysis you want to run (or the table or figure you want to make). In many cases, your analysis data set will be the same as your clean data set:\n\nit’s in the shape you want (rectangular, with observations in rows and variables in columns)\nit has all the variables you need to compute summary statistics or run regression models\n\nMost R statistical routines (e.g., mean(), sd(), lm()) do a fair amount of data wrangling under the hood to make sure it can do what you ask. For example, you may have missing values (NA) in your clean data set, but R’s linear model function, lm(), will just drop observations with missing values by default. If you are okay with that in your analysis, R can handle it.\nHowever, for some special models or figures, you may need to reshape or pre-process your clean data in one last step before running the specific analysis you want. For example, your clean data frame may have a column of parental occupational field (parocc) as a categorical variable in which the first few categories are\n\n11 := Management Occupations\n13 := Business and Financial Operations Occupations\n15 := Computer and Mathematical Occupations\n17 := Architecture and Engineering Occupations\n\n…and so on. You want to run an analysis in which you can’t use this categorical variable but instead need a vector of dummy variables, that is, converting this single column into a bunch of columns that only take on 0/1 values if parocc equals that occupation:\n\nmanage_occ == 1 if parocc == 11 else manage_occ == 0\nbusfin_occ == 1 if parocc == 13 else busfin_occ == 0\ncommat_occ == 1 if parocc == 15 else commat_occ == 0\narceng_occ == 1 if parocc == 17 else arceng_occ == 0\n\n…and so on. Particularly if\n\nyour data are large\nthe analysis takes a while to run\nyou want to use the same data set up for multiple (but not all) models or analytic tasks\n\nyou may want to save this particular analysis data set.\nAs with the difference between objective and subjective data, the difference between subjective and analysis data does not align to a set of hard and fast rules. That’s okay! The main point is that as a data analyst, you make data wrangling decisions thinking about how you will go from raw to objective to subjective to analysis data in a logical and clear manner. Collaborators, replicators, and future you will appreciate it!",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#recommended-texts",
    "href": "x-04-skinner-phil.html#recommended-texts",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Recommended texts",
    "text": "Recommended texts\nThere are a number of great texts that talk about the process of data wrangling. Two that have particularly informed this lecture and are worth looking at more fully:\n\nThe Pragmatic Programmer: From Journeyman to Master by Andrew Hunt and David Thomas\nCode and Data for the Social Sciences: A Practitioner’s Guide by Matthew Gentzkow and Jesse Shapiro",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-02-dload-ipeds.html#option-one-downloadipeds.r-by-dr.-skinner",
    "href": "x-02-dload-ipeds.html#option-one-downloadipeds.r-by-dr.-skinner",
    "title": "II: Skinner’s Download IPEDS",
    "section": "Option One: downloadipeds.R by Dr. Skinner",
    "text": "Option One: downloadipeds.R by Dr. Skinner\n\nDr. Skinner, the original instructor of the class, wrote an R script that downloads IPEDS files directly to a project folder\n\nI actually used this for my final project when I took the class with him\nDoing this enables true “push button reproducibility” of your report\n\nAs in, all I need to do to replicate your report is hit “render”, no pre-downloading of the data required\n\n\nDownload IPEDS is an R-script available on Dr. Skinner’s personal website, here\nThat’s all the instruction you get for this lesson, Dr. Skinner’s site should be enough to work out the rest. Turn to the assignment tab to see how to earn the extra credit points\n\n\nOption Two: ipeDTAs.do by Matt\n\nBuilding off Dr. Skinner’s work, I have written a Stata-ified version, which goes one step further creating labeled .dta versions of the IPEDS files you download\n\nYou can find it here: https://github.com/ttalVlatt/ipeDTAs\n\nUnfortunately, this does require Stata to run, and currently does not work on UF Apps (I have a ticket in with UFIT to try and address this)\nAgain, the documentation should be sufficient to work out how to run it\n\nThe purpose of this limited instruction is to push you a little into playing with resources you find in the wild!",
    "crumbs": [
      "Extra Credit",
      "II: Skinner's Download IPEDS"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Contemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all required content\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students.\n\n\n\nR cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\n\n\n\n\n\ngit (version control software) git-scm.com & GitHub (git-specific cloud storage) github.com\n\nUsing git to version-control your project is optional for extra-credit on your final report (see extra credit assignment)\nStudents can sign up for a free GitHub account if they haven’t already : github.com/join\nAdditionally, students (using their university email address) can request a free GitHub pro account education.github.com/benefits which enables access to GitHub Co-Pilot add-in to RStudio\n\n\n\n\n\n\n\nClass Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue Next Tuesday 12:00pm\n\n\n\n\n1\nJan-15\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-22\n\nReading Data Into R\nIntro to IPEDS\nIntro to Copilot\n\n\nAssignment 1\n\n\n\n3\nJan-29\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nFeb-5\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-12\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-19\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-26\n\nReproducible Reports with Quarto\n\n\nAssignment 6\n\n\n\n8\nMar-5\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-12\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n10\nMar-19\nSpring Break, No Class Meeting\nNone, enjoy the break!\n\n\n11\nMar-26\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n12\nApr-2\n\nModeling Basics\n\n\nAssignment 9\nReproducible Report: Draft (Optional)\n\n\n\n13\nApr-9\n\nData Visualization III\n\n\nAssignment 10\n\n\n\n14\nApr-16\n\nReproducible Report: Presentations\n\n\nReproducible Report: Presentation\n\n\n\n15\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report: Final Report\nExtra-Credit Assignments (Optional)\n\n\n\n16\nApr-30\nFinals Week, No Class Meeting\n\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Tuesday 12pm following the lesson.\nYou will receive one of the following grades\n\n5/5 – everything is correct\n4.5/5 – mostly correct, concepts are all understood, but slight error(s)\n2.5/5 – at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get 4/5\n\n0/5 – not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment and get 2.5/5 with a chance to improve to 4/5 than miss the deadline\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nThere will be 5 quizzes throughout the semester, each worth 2 points. These quizzes will serve as attendance checks and assess your knowledge of the course content.\n\n\n\nYour final project grade comes from four elements, see the final project section of the class website for details.\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\nEach extra credit assignment is worth 2.5 points\nSimply make a good faith effort to complete the assignment and you will receive the points\nAll extra credit assignments are due during week 15 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nIf you use Github Copilot to help you with your code\n\nPlease keep your prompts and acknowledge with a ## h/t\n\nOffice hours or email\n\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Contemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Students will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto"
  },
  {
    "objectID": "syllabus.html#texts",
    "href": "syllabus.html#texts",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "No required textbook\nCourse website capaldi.info/7916 contains all required content\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free"
  },
  {
    "objectID": "syllabus.html#required-tools-software-and-registrations",
    "href": "syllabus.html#required-tools-software-and-registrations",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Students will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students.\n\n\n\nR cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\n\n\n\n\n\ngit (version control software) git-scm.com & GitHub (git-specific cloud storage) github.com\n\nUsing git to version-control your project is optional for extra-credit on your final report (see extra credit assignment)\nStudents can sign up for a free GitHub account if they haven’t already : github.com/join\nAdditionally, students (using their university email address) can request a free GitHub pro account education.github.com/benefits which enables access to GitHub Co-Pilot add-in to RStudio"
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Class Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue Next Tuesday 12:00pm\n\n\n\n\n1\nJan-15\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-22\n\nReading Data Into R\nIntro to IPEDS\nIntro to Copilot\n\n\nAssignment 1\n\n\n\n3\nJan-29\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nFeb-5\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-12\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-19\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-26\n\nReproducible Reports with Quarto\n\n\nAssignment 6\n\n\n\n8\nMar-5\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-12\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n10\nMar-19\nSpring Break, No Class Meeting\nNone, enjoy the break!\n\n\n11\nMar-26\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n12\nApr-2\n\nModeling Basics\n\n\nAssignment 9\nReproducible Report: Draft (Optional)\n\n\n\n13\nApr-9\n\nData Visualization III\n\n\nAssignment 10\n\n\n\n14\nApr-16\n\nReproducible Report: Presentations\n\n\nReproducible Report: Presentation\n\n\n\n15\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report: Final Report\nExtra-Credit Assignments (Optional)\n\n\n\n16\nApr-30\nFinals Week, No Class Meeting"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "There are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Tuesday 12pm following the lesson.\nYou will receive one of the following grades\n\n5/5 – everything is correct\n4.5/5 – mostly correct, concepts are all understood, but slight error(s)\n2.5/5 – at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get 4/5\n\n0/5 – not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment and get 2.5/5 with a chance to improve to 4/5 than miss the deadline\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nThere will be 5 quizzes throughout the semester, each worth 2 points. These quizzes will serve as attendance checks and assess your knowledge of the course content.\n\n\n\nYour final project grade comes from four elements, see the final project section of the class website for details.\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\nEach extra credit assignment is worth 2.5 points\nSimply make a good faith effort to complete the assignment and you will receive the points\nAll extra credit assignments are due during week 15 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60"
  },
  {
    "objectID": "syllabus.html#getting-coding-help",
    "href": "syllabus.html#getting-coding-help",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Take a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nIf you use Github Copilot to help you with your code\n\nPlease keep your prompts and acknowledge with a ## h/t\n\nOffice hours or email"
  },
  {
    "objectID": "syllabus.html#uf-graduate-school-policies",
    "href": "syllabus.html#uf-graduate-school-policies",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "See UF Graduate School policies on grading, attendance, academic integrity, and more."
  },
  {
    "objectID": "syllabus.html#honor-code",
    "href": "syllabus.html#honor-code",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "UF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Students with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester."
  },
  {
    "objectID": "syllabus.html#course-evaluations",
    "href": "syllabus.html#course-evaluations",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Students are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here."
  },
  {
    "objectID": "syllabus.html#in-class-recording",
    "href": "syllabus.html#in-class-recording",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Students are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code."
  },
  {
    "objectID": "12-pro-model.html#data-preparation",
    "href": "12-pro-model.html#data-preparation",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nBefore we can run any kind of models, we need to make sure our data is prepared\n\nThis involves using skills from our data wrangling lessons such as\nData Wrangling I\n\nHandling missing data\nMaking sure our data is the right format (numeric, factor, character, etc.)\nPerforming basic calculations (e.g., percentages, differences, etc.)\n\nData Wrangling II\n\nJoining multiple data sets together\nPivoting data wider and/or longer\n\nData Wrangling III\n\nCleaning up text data\nTransforming dates into\n\nData Wrangling IV\n\nPerforming any of the above tasks across() multiple columns\ncoalesce()-ing multiple columns into one variable\n\n\nFor the purpose of today’s lesson, we are going to focus on two of these tasks, dealing with missing data, and making sure our data is in the right format\n\n\nHandling Missing Data\n\nWhen modeling, by default, R will simply drop any rows that have an NA in any variable you are modeling on (this is a little different to the cautious R we ran into in Data Wrangling I)\nIn real world applications, you need to think carefully about how you handle these…\n\nShould I impute the missing data? If so, using what method?\nShould I use this variable at all if it’s missing for a bunch of observations?\n\nFor this lesson, however, we are just going to drop NA values so we can focus on the main content\nThe below code uses the combines the logic we use for making NAs in Data Wrangling I with the ability to work across multiple columns in Data Wrangling IV\nFirst, we read our data and select() the columns we want to use\n\n\ndata &lt;- read_csv(\"data/hsls-small.csv\") |&gt;\n  select(stu_id, x1sex, x1race, x1txmtscor, x1paredu, x1ses, x1poverty185, x1paredexpct)\n\n\nSecond we use a combination of !, filter(), and if_any() to say…\n“If a row…”\n\n“has a -8 or -9”\n\n.fns = ~ . %in% c(-8, -9)\n\n“in any columns”\n\n.cols = everything()\n\n“do NOT keep it”\n\nfilter(!)\n\n\n\n\ndata &lt;- data |&gt;\n  filter(! if_any(.cols = everything(),\n                  .fns = ~ . %in% c(-8, -9)))\n\n\n\nMaking Sure Our Data is the Right Format\n\nIn our Data Viz I and Data Viz II lessons, we saw that for R to accurately plot categorical variables, we had to convert them into factor()s\n\nThe same is true for using categorical variables in models\nThose more familiar with stats may know that you have to “dummy code” categorical variables as 0 and 1 with one category serving as the “reference level” and all other categories getting their own binary variable\nThe wonderful thing is that R handles that all for us if we tell it to treat the variable as a factor()\n\n\n\n\nThe below code combines the logic of turning variables into a factor() from Data Viz I with working across multiple columns for Data Wrangling IV to say\n“Modify”\n\nmutate()\n\n“Each of these columns”\n\nacross(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185)\n\n“Into a factor”\n\n.fns = ~ factor(.)\n\n\n\ndata &lt;- data |&gt;\n  mutate(across(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185),\n                .fns = ~ factor(.)))\n\n\nWith that, our data is ready for some basic analysis!\nNote: In most real-world projects your data preparation will be much more thorough, usually taking up the vast majority of the lines of code in your entire project, this is just the bare minimum to have to models run",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#t-tests-with-t.test",
    "href": "12-pro-model.html#t-tests-with-t.test",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "t-tests with t.test()",
    "text": "t-tests with t.test()\n\nOne of the first inferential statistical tests you will have learned (or will learn) is the t-test\n\nFor those unfamiliar, the basic concept of a t-test if variance between two groups (i.e., the difference between treatment and control) is greater than the variance within those groups (i.e., random variance between people within the same group)\n\nIf that between-group-variance is great enough compared to the within-group-variance, the t-test will be “statistically significant”\n\nThis means we are (most often) 95% confident that the there is a genuine difference between the groups\n\nThere are also a handful of statistical assumptions we have to satisfy, which are beyond our scope here, but hopefully the general concept will hope those of you yet to take your stats foundations follow along\n\n\n\n\nt.test(x1txmtscor ~ x1sex, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  x1txmtscor by x1sex\nt = 0.38555, df = 16321, p-value = 0.6998\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.2455338  0.3657777\nsample estimates:\nmean in group 1 mean in group 2 \n       52.14309        52.08297 \n\n\n\nLuckily, the code for t.test() is actually very simple (as is the case for regression too)\n\nThe first argument is a forumla, which for a t-test is just outcome ~ group where group must only have 2 levels - In this case, we are looking at math score as our outcome and sex as our group\nThe second argument is data = which we supply our prepared data frame\n\nNote: the pipe |&gt; doesn’t play as nicely with models as it does other commands it’s usually easier to just specify data = in a new line (don’t pipe anything in)\n\n\nThis code simply prints out our t.test() result\n\nAs our p-value is above 0.05, our result is not significant - This indicates there is not a significant difference between male and female math scores in our sample",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#regression-with-lm",
    "href": "12-pro-model.html#regression-with-lm",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Regression with lm()",
    "text": "Regression with lm()\n\nThe problem with t-tests for our research, is that they don’t provide any ability to control for external variables\n\nThey work great in experimental setting with random-treatment-assignment, but in the messy world of educational research, that’s rarely what we have\n\nWhat we far more commonly use is a regression (or more advanced methods that build off regression) which allows use to control for other variables\nThe basic premise of regression very much builds off the logic of t-tests, testing if the variance associated with our treatment variable is great enough compared to a) residual/random variance and b) variance associated with our control variables, to say with confidence that there is a significant difference associated with our treatment\n\nOverall, this looks relatively similar to our code above, with three main differences\n\n\n\nWe use lm() (which stands for linear model) instead of t.test()\nInstead of our formula just being x1txmtscor ~ x1sex we have added + x1poverty185 + x1paredu to “control” for these variables\nWe assigned &lt;- our lm() results to an object rather than just spitting them out\n\n\nThat’s because the summary() function is much more useful for lm() objects, plus, we are going to explore the lm() object more in the next steps\n\n\nregression &lt;- lm(x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\nsummary(regression)\n\n\nCall:\nlm(formula = x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.748  -5.715   0.160   6.136  30.413 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    49.0485     0.3379 145.139  &lt; 2e-16 ***\nx1sex2         -0.0412     0.1429  -0.288    0.773    \nx1poverty1851  -3.0224     0.1729 -17.483  &lt; 2e-16 ***\nx1paredu2       1.3419     0.3234   4.149 3.36e-05 ***\nx1paredu3       2.4834     0.3584   6.929 4.38e-12 ***\nx1paredu4       6.1227     0.3507  17.460  &lt; 2e-16 ***\nx1paredu5       8.1726     0.3814  21.426  &lt; 2e-16 ***\nx1paredu7      10.7669     0.4294  25.074  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.161 on 16421 degrees of freedom\nMultiple R-squared:  0.1605,    Adjusted R-squared:  0.1601 \nF-statistic: 448.4 on 7 and 16421 DF,  p-value: &lt; 2.2e-16\n\n\n\nOur results show that, sex still had no significant association with math scores, but, our control variables of poverty and parental education seem to have some very strong associations\n\n\nQuick Question\n\nYou may notice we actually have more variables in the regression table than we put in, why? What do they represent?\n\n\n\nThat’s correct, they represent the different levels of our factor()-ed categorical variables",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#creating-pretty-regression-output-tables",
    "href": "12-pro-model.html#creating-pretty-regression-output-tables",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Creating Pretty Regression Output Tables",
    "text": "Creating Pretty Regression Output Tables\n\nRunning regressions in R is all well and good, but the output you see here isn’t exactly “publication ready”\nThere are multiple ways of creating regression (and other model) output tables, each with their own pros and cons\nHere, we will go over three of the most common methods\n\n\nstargazer Package\n\nOne of the most common packages for getting “publication ready” regression tables is stargazer\n\nI personally find these tables a little inflexible, but it’s very common in the world of R, so it’s worth covering here\nThe code is very simple, at minimum, provide the regression model you fitted, and the type of table you want\nWe are using “html” here so it formats for the website, you could use “text” or “latex”, I don’t think there’s currently support for typst though\nIn general, the biggest drawback of stargazer is the lack of flexibility and limited compatibility with formats other than LaTeX\n\n\n\n\nExtra trick: To get captions and table numbers if we are using Quarto, we use the “chunk options” to cross reference them, not manually adding them to the table code\n\nYou see more about that on the Quarto guide for cross referencing\nTo get the table to appear like this I…\n\nLabeled the chunk {r tbl-stargazer}\nAdded #| tbl-cap: \"Regression Table Using stargazer\" as the first line\n\n\n\n\n\n\nTable 1: Regression Table Using stargazer\n\n\nstargazer(regression, type = \"html\")\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nx1txmtscor\n\n\n\n\n\n\n\n\nx1sex2\n\n\n-0.041\n\n\n\n\n\n\n(0.143)\n\n\n\n\n\n\n\n\n\n\nx1poverty1851\n\n\n-3.022***\n\n\n\n\n\n\n(0.173)\n\n\n\n\n\n\n\n\n\n\nx1paredu2\n\n\n1.342***\n\n\n\n\n\n\n(0.323)\n\n\n\n\n\n\n\n\n\n\nx1paredu3\n\n\n2.483***\n\n\n\n\n\n\n(0.358)\n\n\n\n\n\n\n\n\n\n\nx1paredu4\n\n\n6.123***\n\n\n\n\n\n\n(0.351)\n\n\n\n\n\n\n\n\n\n\nx1paredu5\n\n\n8.173***\n\n\n\n\n\n\n(0.381)\n\n\n\n\n\n\n\n\n\n\nx1paredu7\n\n\n10.767***\n\n\n\n\n\n\n(0.429)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n49.049***\n\n\n\n\n\n\n(0.338)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n16,429\n\n\n\n\nR2\n\n\n0.160\n\n\n\n\nAdjusted R2\n\n\n0.160\n\n\n\n\nResidual Std. Error\n\n\n9.161 (df = 16421)\n\n\n\n\nF Statistic\n\n\n448.368*** (df = 7; 16421)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\n\n\ngtsummary Package\n\nPersonally, I find the gtsummary package (which we saw in our Intro to Quarto lesson) much more like what I want\n\nWhen I’m using a package to create my tables, this is my preferred option\ntbl_regression() creates a pretty great table when you just provide the regression model\nOne thing I particularly like is how gtsummary handles factors, I think it makes the it super-duper clear what is going on\n\n\n\ntbl_regression(regression)\n\n\n\nTable 2: Regression Table Using gtsummary (basic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx1sex\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n-0.04\n-0.32, 0.24\n0.8\n\n\nx1poverty185\n\n\n\n\n\n\n\n\n    0\n—\n—\n\n\n\n\n    1\n-3.0\n-3.4, -2.7\n&lt;0.001\n\n\nx1paredu\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n1.3\n0.71, 2.0\n&lt;0.001\n\n\n    3\n2.5\n1.8, 3.2\n&lt;0.001\n\n\n    4\n6.1\n5.4, 6.8\n&lt;0.001\n\n\n    5\n8.2\n7.4, 8.9\n&lt;0.001\n\n\n    7\n11\n9.9, 12\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\nWith a couple of extra lines to\n\nhandle variable labels\n\nlabel = list(x1sex ~ \"Sex\" ... )\n\nadd significance stars (without hiding the p-value of confidence intervals)\n\nadd_significance_stars(hide_ci = FALSE, hide_p = FALSE)\n\nadd model statistics\n\nadd_glance_source_note(include = c(r.squared, nobs))\n\nforce the standard error column to show\n\nmodify_column_unhide(std.error)\n\n\nwe can get something that looks pretty great\n\n\n\nSimilarly to above, we add table numbers and captions using Quarto cross referencing\n\n{r tbl-gtsummary}\n#| tbl-cap: \"Regression Table Using gtsummary\"\n\n\n\ntbl_regression(regression,\n               label = list(x1sex ~ \"Sex\",\n                            x1poverty185 ~ \"Below Poverty Line\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\nTable 3: Regression Table Using gtsummary (custom)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.143\n-0.32, 0.24\n0.8\n    Below Poverty Line\n\n\n\n\n        0\n—\n—\n—\n\n        1\n-3.0***\n0.173\n-3.4, -2.7\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n1.3***\n0.323\n0.71, 2.0\n&lt;0.001\n        3\n2.5***\n0.358\n1.8, 3.2\n&lt;0.001\n        4\n6.1***\n0.351\n5.4, 6.8\n&lt;0.001\n        5\n8.2***\n0.381\n7.4, 8.9\n&lt;0.001\n        7\n11***\n0.429\n9.9, 12\n&lt;0.001\n  \n  \n    \n      R² = 0.160; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\nReminder: We saw in Intro to Quarto how you can create matching descriptive statistics tables using the tbl_summary() function\n\nWe also saw how you can force them to as_kable() if needed for you output format\nIf you like gtsummary tables and want to learn more, check out\n\ngtsummary documentation\n\n\n\n\n\n“Homemade” Regression Tables with kable()\n\nWhile a little more work, we can also create our own table using the default summary() and kable() like we saw in Intro to Quarto for descriptive tables\n\nYou might want to do this to specifically format a table in a way gtsummary doesn’t allow, or, to match some other tables you already created with kable\n\nFirst things first, let’s save the summary() output to a new object\n\n\nsummary_object &lt;- summary(regression)\n\n\nWe will do this more below, but if you click on the object summary_object in the Environment (top right) you can see all the different pieces of information it holds\n\nWe are most interested in coefficients, if you click on the right hand side of that row, you will see the code summary_object[[\"coefficients\"]] auto-populate\n\n\nTip: This works for most objects like this\n\n\nWe then turn that into as data frame with as.data.frame()\nAdd a new column that contains the correct significance stars using case_when()\nPipe all that into kable() with updated column names and rounded numbers\nSimilarly to above, we add table numbers and captions using Quarto cross referencing\n\n\n{r tbl-manual}\n#| tbl-cap: \"Regression Table Using Kable\"\n\n\n\nsummary_object[[\"coefficients\"]] |&gt;\n  as.data.frame() |&gt;\n  mutate(sig = case_when(`Pr(&gt;|t|)` &lt; 0.001 ~ \"***\",\n                         `Pr(&gt;|t|)` &lt; 0.01 ~ \"**\",\n                         `Pr(&gt;|t|)` &lt; 0.05 ~ \"*\",\n                         TRUE ~ \"\")) |&gt;\n  kable(col.names = c(\"estimate\", \"s.e.\", \"t\", \"p\", \"\"),\n        digits = 3)\n\n\n\nTable 4: Regression Table Using kable\n\n\n\n\n\n\n\nestimate\ns.e.\nt\np\n\n\n\n\n\n(Intercept)\n49.049\n0.338\n145.139\n0.000\n***\n\n\nx1sex2\n-0.041\n0.143\n-0.288\n0.773\n\n\n\nx1poverty1851\n-3.022\n0.173\n-17.483\n0.000\n***\n\n\nx1paredu2\n1.342\n0.323\n4.149\n0.000\n***\n\n\nx1paredu3\n2.483\n0.358\n6.929\n0.000\n***\n\n\nx1paredu4\n6.123\n0.351\n17.460\n0.000\n***\n\n\nx1paredu5\n8.173\n0.381\n21.426\n0.000\n***\n\n\nx1paredu7\n10.767\n0.429\n25.074\n0.000\n***\n\n\n\n\n\n\n\n\n\nThere are other ways as well, but between these three options, you should be able to get what you want!\n\n\n\nPredictions with lm()\n\nWhen you fit a regression model in R, there is a lot more saved than you see with summary()\nSince we have our lm() object saved as regression, let’s start by taking a look inside it by clicking on the object in our environment (top right) panel - Confusing, right?\n\nMost statistical models look something like this, it’s basically a collection of lists and tables containing different information about the model\n\nThere are functions such as summary() that are great at pulling out the most commonly needed information without having to go manually digging through the model object, but sometimes, it can be useful to know it’s there\nAnother great function is predict() which extracts estimated values of the outcome variable based on the predictor variables (some other models use fitted() for the same purpose)\nFor those more familiar with stats, you’ll know predicted values are often compared against the true values to see how strong the model is\n\n\n\nTo start, let’s save a full set of predictions to a new columns in our data frame\n\n\ndata &lt;- data |&gt;\n  mutate(prediction = predict(regression))\n\ndata |&gt; select(stu_id, x1txmtscor, prediction)\n\n# A tibble: 16,429 × 3\n   stu_id x1txmtscor prediction\n   &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 10001        59.4       57.2\n 2 10002        47.7       48.5\n 3 10003        64.2       59.8\n 4 10004        49.3       55.1\n 5 10005        62.6       55.2\n 6 10006        58.1       51.5\n 7 10007        49.5       50.3\n 8 10008        54.6       59.8\n 9 10009        53.2       50.4\n10 10010        63.8       51.5\n# ℹ 16,419 more rows\n\n\n\nNext, we can compare these to our actual results using a simple plot (no formatting) from Data Viz I\n\nThe only new things we add here is\n\ngeom_abline(slope = 1, intercept = 0)\n\nThis adds a reference line that represents a perfect 1 to 1 relationship (which would be if there was 0 prediction error)\n\ncoord_obs_pred() which is from the tidymodels package\n\nThis fixes the axes so that the predictions and observed values are plotted on the same scal\n\n\n\n\n\nQuick Excercise\n\nTry removing the final line coord_obs_pred() and see what happens. Which plot do you think is better?\n\n\n\nggplot(data,\n       aes(x = prediction,\n           y = x1txmtscor)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n(Easier) Quick Question\n\nWhat do we think about our model? Does it look like it’s doing a great job of predicting? Why/why not?\n\n\n\n(Harder) Quick Question\n\nYou’ll notice our plot looks kind of clumped together, why do you think that it? What about the model would lead to that?\n\n\n\nGiven what we just discussed, can we change one of the variables we are using in the model to make it less “clumpy” but caputre the same information?\n\n\nregression_2 &lt;- lm(x1txmtscor ~ x1sex + x1ses + x1paredu, data = data)\n\ndata &lt;- data |&gt;\n  mutate(prediction_2 = predict(regression_2))\n\nggplot(data,\n       aes(x = prediction_2,\n           y = x1txmtscor)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\nQuick Question\n\nDoes that look better? What else is odd about our predictions?\n\n\n\nWe can also use predict() to estimate potential outcome values for new students who don’t have the outcome for\nThis is a common way you evaluate machine learning models\nIf you think you’re model is a really good predictor (which ours is not) you may feel comfortable using something like this to help your office predict student outcomes/identify students in need of additional help\n\n\n\nTo demonstrate this, we are first going to split out 10% of our data using slice_sample() and drop the math score from it\n\n\ndata_outcome_unknown &lt;- data |&gt;\n  slice_sample(prop = 0.1) |&gt;\n  select(-x1txmtscor)\n\n\nThen, we can use anti_join() which is basically the opposite of the joins we used in Data Wrangling II\n\nIt looks for every row in x that isn’t in y and keeps those\n\n\n\ndata_outcome_known &lt;-  anti_join(x = data, y = data_outcome_unknown, by = \"stu_id\")\n\n\nNow, we can fit one more lm() using our data we “know” the outcome for\n\n\nregression_3 &lt;- lm(x1txmtscor ~ x1sex + x1ses + x1paredu, data = data_outcome_known)\n\n\nFinally, we can predict() outcomes for the data we “don’t know” the outcome for\n\nWe add the regression_3 we just fitted as the model, same as before\nBut we also add newdata = data_outcome_unknown to say predict the outcome for this new data, instead of extract the predictions the model originally made\n\n\n\ndata_outcome_unknown &lt;- data_outcome_unknown |&gt;\n  mutate(prediction_3 = predict(regression_3, newdata = data_outcome_unknown))\n\n\nLastly, let’s see how similar our predictions we made using our model without the outcome were to those made when the outcome was known for everyone using cor() to get the correlation\n\n\ncor(data_outcome_unknown$prediction_2, data_outcome_unknown$prediction_3)\n\n[1] 0.9999509\n\n\n\nPretty close!\n\n\n\nChecking Residuals\n\nMany of the assumptions relating to regression are tested by looking at the residuals\n\nWe aren’t going to go over those assumptions, again, this is not a stats class\nBut it might be useful to see how to get them out of a model object\n\nLet’s start by viewing the lm object again (environment, top right panel), then clicking on the little white box on the right hand side of the screen for the row “residuals” - That is a magic tip, if you ever want to get something specific out of a model object, often they’ll be something you can click on to generate the code needed to access it in the console\n\nFor residuals, it is regression_2[[\"residuals\"]]\n\n\n\ndata &lt;- data |&gt;\n  mutate(residual = regression_2[[\"residuals\"]])\n\n\nNow, again, not to get too deep into assumptions, but one of the key things to check is that your residuals have a normal distribution\n\nSo let’s revisit some Data Visualization I content and make a simple ggplot() histogram to of them\n\n\n\nggplot(data) +\n  geom_histogram(aes(x = residual),\n                 color = \"black\",\n                 fill = \"skyblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nWow, that is almost a perfect normal distribution!\nBonus points: can anyone remember/think of something about the variable x1txmtscor that made this result quite likely? Think about what kind of score it is",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#formula-objects",
    "href": "12-pro-model.html#formula-objects",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "formula() Objects",
    "text": "formula() Objects\n\nThe second from last thing is really simple, but, it can be a time & error saver if you want to get more advanced like our final step\nAbove, we simply put our formula into the t.test() or lm() command\n\nInstead, we can actually specify it as a formula object first, then call that object, which has two advantages\n\n\n\nIf we run multiple tests with the same formula, we only have to change it once in our code for updates\n\n\nHere, we will run both standard lm() and lm_robust() from the estimatr package\n\n\nIf we want to run multiple tests in a loop like below, it makes that possible too\n\n\n\nTo demonstrate this, we will fit the same model using standard lm() and lm_robust() which for those versed in stats, is one option we can use when we have a violation of heteroskedasticity\n\n\nregression_formula &lt;- formula(x1txmtscor ~ x1sex + x1ses + x1paredu)\n\nregression_4 &lt;- lm(regression_formula, data = data)\n\ntbl_regression(regression_4,\n               label = list(x1sex ~ \"Sex\",\n                            x1ses ~ \"Socio-Economic Status\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.142\n-0.32, 0.24\n0.8\n    Socio-Economic Status\n3.8***\n0.163\n3.5, 4.1\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.05\n0.333\n-0.70, 0.61\n0.9\n        3\n-0.16\n0.390\n-0.92, 0.61\n0.7\n        4\n2.0***\n0.424\n1.2, 2.9\n&lt;0.001\n        5\n2.6***\n0.495\n1.6, 3.5\n&lt;0.001\n        7\n2.7***\n0.609\n1.5, 3.9\n&lt;0.001\n  \n  \n    \n      R² = 0.172; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval\n    \n  \n\n\n\nregression_robust &lt;- lm_robust(regression_formula, data = data, se_type = \"stata\")\n\ntbl_regression(regression_robust,\n               label = list(x1sex ~ \"Sex\",\n                            x1ses ~ \"Socio-Economic Status\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.142\n-0.32, 0.24\n0.8\n    Socio-Economic Status\n3.8***\n0.165\n3.5, 4.1\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.05\n0.320\n-0.67, 0.58\n0.9\n        3\n-0.16\n0.377\n-0.89, 0.58\n0.7\n        4\n2.0***\n0.417\n1.2, 2.9\n&lt;0.001\n        5\n2.6***\n0.489\n1.6, 3.5\n&lt;0.001\n        7\n2.7***\n0.621\n1.5, 3.9\n&lt;0.001\n  \n  \n    \n      R² = 0.172; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#modeling-programatically-with-loops",
    "href": "12-pro-model.html#modeling-programatically-with-loops",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Modeling Programatically with Loops",
    "text": "Modeling Programatically with Loops\n\nFinally, we can also bring in content from Functions & Loops and fit regression models using loops\n\nThis is kind of thing you might want to do if you are testing the same model on a set of outcomes\n\n\n\nQuick Question\n\nThinking back to that lesson, why might we want to go through the hassle of fitting regressions using loops? What are the advantages of using loops vs coding it all out separately?\n\n\n\nFor example, we might be interested in modeling both a students math score and their parental education expectation\n\nwe make a list containing our outcome variables (x1txmtscor and x1paredexpct)\nUse a for() loop to loop through these outcomes, which paste()s i (which takes on the name of each outcome variable) into the formula and then runs the model\n\n\noutcomes &lt;- c(\"x1txmtscor\", \"x1paredexpct\")\n\nfor(i in outcomes) {\n  \n  print(i)\n  \n  loop_formula &lt;- formula(paste0(i, \"~ x1sex + x1ses + x1paredu\"))\n  \n  loop_regression &lt;- lm(loop_formula, data = data)\n  \n  tbl_regression(loop_regression,\n                 label = list(x1sex ~ \"Sex\",\n                              x1ses ~ \"Socio-Economic Status\",\n                              x1paredu ~ \"Parental Education\")) |&gt;\n    add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n    add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n    modify_column_unhide(std.error) |&gt;\n    print()\n  \n}\n[1] “x1txmtscor”\n\n\n\n\n\n\nCharacteristic\n\n\nBeta1\n\n\nSE2\n\n\n95% CI2\n\n\np-value\n\n\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.04\n\n\n0.142\n\n\n-0.32, 0.24\n\n\n0.8\n\n\n\n\nSocio-Economic Status\n\n\n3.8***\n\n\n0.163\n\n\n3.5, 4.1\n\n\n&lt;0.001\n\n\n\n\nParental Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.05\n\n\n0.333\n\n\n-0.70, 0.61\n\n\n0.9\n\n\n\n\n    3\n\n\n-0.16\n\n\n0.390\n\n\n-0.92, 0.61\n\n\n0.7\n\n\n\n\n    4\n\n\n2.0***\n\n\n0.424\n\n\n1.2, 2.9\n\n\n&lt;0.001\n\n\n\n\n    5\n\n\n2.6***\n\n\n0.495\n\n\n1.6, 3.5\n\n\n&lt;0.001\n\n\n\n\n    7\n\n\n2.7***\n\n\n0.609\n\n\n1.5, 3.9\n\n\n&lt;0.001\n\n\n\n\n\n\nR² = 0.172; No. Obs. = 16,429\n\n\n\n\n\n\n1 p&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n2 SE = Standard Error, CI = Confidence Interval\n\n\n\n\n\n[1] “x1paredexpct”\n\n\n\n\n\n\nCharacteristic\n\n\nBeta1\n\n\nSE2\n\n\n95% CI2\n\n\np-value\n\n\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n0.47***\n\n\n0.040\n\n\n0.39, 0.55\n\n\n&lt;0.001\n\n\n\n\nSocio-Economic Status\n\n\n0.28***\n\n\n0.046\n\n\n0.19, 0.38\n\n\n&lt;0.001\n\n\n\n\nParental Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.39***\n\n\n0.094\n\n\n-0.58, -0.21\n\n\n&lt;0.001\n\n\n\n\n    3\n\n\n-0.28*\n\n\n0.110\n\n\n-0.50, -0.07\n\n\n0.010\n\n\n\n\n    4\n\n\n-0.08\n\n\n0.119\n\n\n-0.31, 0.16\n\n\n0.5\n\n\n\n\n    5\n\n\n0.29*\n\n\n0.139\n\n\n0.02, 0.56\n\n\n0.037\n\n\n\n\n    7\n\n\n0.85***\n\n\n0.172\n\n\n0.52, 1.2\n\n\n&lt;0.001\n\n\n\n\n\n\nR² = 0.049; No. Obs. = 16,429\n\n\n\n\n\n\n1 p&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n2 SE = Standard Error, CI = Confidence Interval",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#question-one",
    "href": "12-pro-model.html#question-one",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Question One",
    "text": "Question One\na)",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#submission",
    "href": "12-pro-model.html#submission",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#solution",
    "href": "12-pro-model.html#solution",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Modeling Basics Solution]\n##' [INIT: January 12 2025]\n##' [AUTH: Jue Wue]\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "99-final.html",
    "href": "99-final.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project for this class is to create a truly “reproducible report” on a topic of your choosing related to higher education\n\nThe topic can really be almost anything of interest related to higher ed, so long as you can find public data to use\n\nYour report should be 3-5 pages including multiple graphs and visual elements (i.e., not too much text)\n\nYour goal is something like what you might hand a senior administrator at your university to summarize a trend/issue/topic\n\nYou will likely only have a handful of citations\nYou should devote around half your page space to data visualizations and tables\n\n\nThe primary focus of this report is reproducibility\n\nYour data must be publicly available with no IRB restrictions, as you will not submit it, I will go and collect it (unless you download it as part of the project code)\n\n\n\nProposal (5 points)Initial Analyses (10 points)Presentation (5 Points)Final Report (20 Points)\n\n\nThis assignment should be submitted as a text entry directly on Canvas consisting of;\n\nA paragraph describing your project:\n\nWhat will you be investigating/exploring/predicting?\nWhy is it interesting?\n\nA description of where you will find this data.\nA few lines describing your main outcome variable in detail\n\nHow is it coded/what scale is it on?\nHow can you interpret it?\n\n\nThis assignment is worth 5 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\nNOTE: For your initial analyses, the most important thing is that you submit code that sources/renders in full, this assignment will not be successfully completed until that happens, you may have to resubmit multiple times.\nSubmit the following in either a cleanly formatted R (.R) script or Quarto (.qmd) file:\n\nComments (if an R script) or text (if a Quarto script) that describe\n\nWhere your data is from (a link is preferable)\nHow to download it\nWhere to save it in order for your code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nCode that:\n\nReads in the data set that contains (at least) your dependent variable (must read in EXACTLY what downloads from following your instructions above)\nIf appropriate\n\nConverts missing values to NA\nReshapes the data wider or longer\nJoins in additional data files\n\n\nCode that creates at least three of the following:\n\nA plot that shows the overall distribution of your dependent/outcome variable\n\nHint: A histogram or density plot might be a good option here\n\nA plot that shows the distribution of your dependent/outcome variable grouped by a variable in your data\n\nHint: A histogram or density plot with fill might be a good option here\n\nA plot that shows the median, interquartile range, and potential outliers of your dependent/outcome variable grouped by a variable in your data\n\nHint: A box plot with x and/or fill might be a good option here\n\nA plot that shows how your dependent/outcome variable changes by another continuous variable in your data\n\nHint: A scatter plot might be a good option here\n\n\n\nThis assignment is worth 10 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\n\nYou will present the results of your report in class during the penultimate week of the semester (see date in Canvas)\nThe presentation format is up to you, previous students have\n\nPresented an image of one figure they created\nCreated a short PowerPoint presentation\nCreated presentations using Quarto\n\nThe primary rule for this presentation is that it is to be 3-5 mins long (read min 3 mins, max 5 mins, ideal 4 mins)\n\nAs this is meant to replicate the you presenting your report to senior administrators, this is a hard time-limit, you will be stopped if you go over 5 mins\n\n\nThis assignment is worth 5 points, your grade will be determined by:\n\n3 points: Did you present a plot/table/finding from your report that tells a story about your topic?\n1 point: Did you present the information in a professional and engaging manner?\n1 point: Did you finish within the allotted time limit of 3-5 mins?\n\n\n\n\nYour final report is a 3-5 page (single-spaced, not including citations) document that summarizes the analysis you have done using plenty of figures and summary tables along the way\n\nThis is NOT a traditional academic paper, it is meant to be concise report intended for a university administrator or policymaker\n5 pages is a hard limit, anything beyond the 5th page won’t be graded\nNOTE: You can submit a draft (for feedback only) by the due date on Canvas\n\nYou will submit the report as Quarto (.qmd) file\n\nThe output format: should be either docx, pdf (traditional way using LaTeX), or typst (brand new way to create a .pdf)\n\nI would strongly recommend docx for most students\n\nSee the Quarto documentation for word output\n\nIf you’re feeling more adventurous, have a go with typst\n\nSee the Quarto documentation for typst output\nYou’ll need Quarto 1.4, which came out after the start of the class, see me if you need help installing it\n\nIf you want to cause yourself unnecessary misery and frustration by doing it the old fashioned way, use pdf\n\nSee the Quarto documentation for pdf output\n\n\nOptional: If your analysis code becomes long, you might want to submit accompanying .R scripts that are source()-ed as discussed in the Quarto Lesson\n\n\n\nRequired Report Content\n\nBefore the introduction of your document, a section called “Instructions to Run” that states\n\nWhere your data is from (a link if preferable)\nHow to download it\nWhere to save it in order for the code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nWell commented code (either in the .qmd file or a source()-ed .R script) that:\n\nReads in all your raw data (must read in EXACTLY what downloads from following your instructions above)\nPerforms all data wrangling tasks to clean, join, and reshape your data as necessary for your project\nCreates:\n\nRequired: 3 or more plots with ggplot2\nRequired: 1 or more overall descriptive statistics table(s) made with summarize\nRequired: 1 or more other summary table(s) made with summarize\nOptional: Basic statistics like t.test() or lm()\n\n\nWritten text that should be clearly structured with subheadings and describe:\n\nWhy this is interesting and/or important\n\nThis should be a single concise but convincing paragraph, think an “elevator pitch” argument as to why this matters\nThere should NOT be any lengthy literature review in this assignment, this is it\n\nWhy your chose your data source and what the data represents\nWhat analysis you did and why, in layman’s terms (not an R or stats expert)\nWhat each individual plot and table shows\nWhat you found overall\nAny limitations or future research\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nReport Element\nCriteria\nPoints\n\n\n\n\nDoes it run?\n\nIf I hit render, does the code run and produce the output report without errors?\n\n4\n\n\nData Wrangling: Reading & Cleaning\n\nAre the “instructions to run” provided and correct?\nIs the data read in correctly?\nIs the data joined and/or reshaped correctly where necessary?\nIs the data cleaned where necessary with reasonable justification provided for subjective decisions?\n\n4\n\n\nData Wrangling: Analysis\n\nIs there at least 1 overall descriptive statistics summarize table in the report?\nIs there at least 1 additional summarize table in the report?\nDo the summary tables show interesting and relevant information to the report topic?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nData Visualization\n\nAre there at least 3 ggplot2 plots in the report?\nDo the plots show interesting and relevant information to the report topic?\nAre the plots aesthetically pleasing with good plot design, color choices, and labels?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nWritten Content\n\nDoes the written content address the points outlined above?\nDoes the report make a convincing argument?\nIs the text of the report generally well written and in layman’s terms?\nIs the text of the report well structured with subheadings?\n\n4",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "x-01-git.html#installing-git",
    "href": "x-01-git.html#installing-git",
    "title": "I: Git & GitHub",
    "section": "Installing git",
    "text": "Installing git\n\nThe first thing you are going to need to do is install git\n\nFrom the git website http://git-scm.com/\n\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\n\n\nIf you’re using a Mac, there’s a decent chance git is already installed, you may already have git on Windows if you’ve used something that installed it in the past\n\nTo check if you have git copy this command into the terminal (note: not the console, the terminal which is next to console in RStudio)\n\nOnce installed, you could keep using the terminal for git, but RStudio has a much more beginner friendly point-and-click system we will use instead\n\n\n\nwhich git\nIf it provides a file path to something called git, you have git, move on!\n\nIf you need to install git\n\nThere’s a great resource “Happy git with R” by Jenny Bryan which cover a whole range of git topics beyond what we need today\n\n“Happy git with R”’s installation page has pretty clear instructions for installing git\nNote: You should always choose “Option 1” unless you\n\n\n\n\nHopefully with those instructions you managed to get git installed if it wasn’t already, which is honestly the hardest part of this lesson!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-github-account",
    "href": "x-01-git.html#creating-a-github-account",
    "title": "I: Git & GitHub",
    "section": "Creating a GitHub Account",
    "text": "Creating a GitHub Account\n\ngit is a language which handles the version control, if you just wanted to use version control and store is all locally, git is all you need\nHowever, the real advantage of git is that you keep the version control both on your computer and in a “remote” repository (similar to OneDrive etc.)\nThere are plenty of git clients that offer this service, but by far the biggest is GitHub, which is what we will use\nSo you need to create an account, which is just like signing up for any other online account\n\n\nGo to https://github.com\nClick “sign up” and follow the prompts on screen…\nDone!\n\n\nFYI: If you plan to use GitHub regularly, students are eligible for free GitHub Pro, which I have, find our more here. This will allow you to create private GitHub repos, by default, all GitHub repos are public (which is how all open source stuff like R works)",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-new-github-repo",
    "href": "x-01-git.html#creating-a-new-github-repo",
    "title": "I: Git & GitHub",
    "section": "Creating a New GitHub Repo",
    "text": "Creating a New GitHub Repo\n\nThere are two ways you can create a new GitHub repo\n\nCreate a repo on your computer, then start tracking it with git, then link it to GitHub\nCreate a repo on GitHub and clone it to your computer\n\n\nIMHO, this is far easier, so it’ what we will do!\n\n\n\nStep One: Create Repo on GitHub\n\nWhen on https://github.com\n\nNavigate to “Your Repositories”\nClick the green “new” button\nChoose a name for the new repo\nUnder “Add .gitignore” select the R template\nEverything else is optional, so don’t worry about it for now\n\n\nNote: If you signed up for GitHub pro, you can make the repo private\n\nIf you do, make sure to add me @ttalVlatt so I can see it to give you credit\n\n\n\nYou should be taken to your shiny new repo, yay!\n\n\n\n\nStep Two: Setup GitHub SSH Access\n\nThis is how your computer will have access to edit your repo\nIt sounds scary, but luckily RStudio make it easy-peasy!\n\nIn RStudio go to “Tools” and then “Global Options”\nSelect “Git/SVN” from the left hand menu\nUnder “SSH Key” select “Create SSH Key”\nLeave the optional pass phrase boxes blank and click “create”\nClose the pop-up box that appears\nBack on the “Git/SVN” page select “View Public Key”\nCopy that to your clipboard\nGo to GitHub.com\nGo to “Settings” on the menu under your profile icon\nSelect “SSH and GPG Keys” from the left-hand menu\nSelect the green “New SSH Key”\nGive the key a name\n\n\nIf you’re using RStudio on your computer, this will be set for a while, so just call it “MacBook Pro” or something similar\nIf you’re using RStudio Cloud, you need a new key for each project, so name it accordingly\n\n\nPaste the SSH Key you copied from RStudio into the “key” box\nLeave it set as “authentication key”\nClick “Add New SSH Key” and you’re done!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "href": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "title": "I: Git & GitHub",
    "section": "Cloning a Repo Down to Your Computer",
    "text": "Cloning a Repo Down to Your Computer\n\nThis step is a little different depending on if you’re using RStudio on your computer or the cloud, so I will outline each separately\n\n\nRStudio on Your Computer\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” select “SSH”\nCopy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nClick on the blue cube in the top right (where we set up projects before)\nClick on “New Project” then “Version Control”\nPaste what you copied from GitHub as the URL\nChoose a file name and location that make sense (this is where the repo will be kept)\nDone!\n\n\n\nposit.cloud\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” and keep it on “HTTPS”\nCopy the address that should look like https://github.com/&lt;Username&gt;/&lt;Repo&gt;.git\nOn posit.cloud select “New Project” and then “From GitHub Repository”\nPaste the URL you copied in the URL box and select a name for the project\nGo back to your repo on GitHub and reselect the green “Code” button\nThis time select “SSH” and copy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nOnce the project is opened, go to the terminal (next to the console)\nType git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git replacing &lt;Username&gt; and &lt;Repo&gt; with the correct names (you can copy from the block below)\nDone!\n\n\ngit remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\n\n\nOkay, with git and GitHub set up, the hard part is over! Now we will just go over how to use what we set up\n\nKeep in mind, we are just going to cover one purpose git, this is just the beginning",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#using-git-for-version-control-and-backup",
    "href": "x-01-git.html#using-git-for-version-control-and-backup",
    "title": "I: Git & GitHub",
    "section": "Using git for Version Control and Backup",
    "text": "Using git for Version Control and Backup\n\nGetting a change from your computer to GitHub has three steps\n\n“Stage” the change, which tells git to pay attention to the change\n“Commit” the change, which saves it to your local (on computer) version of git\n“Push” the change, which save it to your remote (GitHub) version of git\n\n\n\nLet’s see what that looks like in RStudio\n\nIn the top right corner panel of RStudio (same area as the “Environment”) there’s a “Git” tab, select it\nYou’ll see a few things here\n\nAlong the top are some buttons for the core git commands of “Commit”, “Pull”, and “Push”\nRight now, there is probably nothing in the main area of the panel\n\nGo ahead and make a new .R script (doesn’t need to be anything in it) and save it in the project folder\n\nNow you’ll see it in the main area of the “Git” panel\n\nAny changes you make to the repo will appear here, new files, changed files, deleted files, etc.\n\n\n\nTo backup these changes to GitHub, follow these steps 1. Click the white square box left of the file in the “Git” main panel - This “stages” the change, i.e., tells git to pay attention to it 2. Click “Commit” - This will open a new box/window - In the top right hand box you can (and should) add an informative message about the change you made - E.g. “Created a test script” - Then hit the “Commit” button right underneath that 3. Finally, hit “Push” - You can do this in the same window, or at the top of the Git panel, it doesn’t matter - This “pushes” the changes you just “committed” up to GitHub - The very first time you do this, you may get a warning that the key isn’t know - Type “yes” as your response, you won’t see this again unless you make a new key\n\nYou can stage, commit, and push lots of change at once, or one by one\n\nThe big difference is that the less each individual commit and push does, the less you have to reverse\n\nFor that reason, always push up things you’re sure about first, then things your not, in separate commits\n\n\nThis process may seem like a lot, but, it will become second nature once you start using it\n\nThe ability to version control your code and easily track back to specific points is alone more than worth it\n\nThat’s not to mention this is only an intro, git can do so much more as you get familiar with it\n\nPlus, if you can use git you will stand out from the crowd in serious data management jobs\n\n\n\n\nThe Need to .gitignore\n\nWhen something appears in the RStudio “Git” panel that you don’t want to push you can right-click and hit the “ignore” option\n\nThis will add that file to a .gitignore file in your repo, and means git will never try and track that file again\nYou can also add file names and/or patterns directly to the .gitignore file\n\nThis is useful for anything you don’t want sharing (as GitHub repos are public by default), or anything too large for git (big data sets etc.)\n\n\n\nSidenote: The Need to pull\n\nAnother git command that is super common is pull, this will just check for any changes in the GitHub copy of your repo and pull them down\n\nIf you keep things simple and only push changes to this project from one computer, there should never be anything to pull down\n\nYou can always hit the button if you’re curious, it will just say “already up to date”\n\nIf you set this up on more than one computer, or start collaborating with someone else, you’ll need to commit and push when you’re finished working then pull from before you start work\n\nNow we’ve covered some of the basics, I just to suggest a few rules you stick by with git",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#git-ground-rules",
    "href": "x-01-git.html#git-ground-rules",
    "title": "I: Git & GitHub",
    "section": "git Ground Rules",
    "text": "git Ground Rules\n\nGenerally, git is best suited for plain text based files, like .R scripts and .qmd files\n\n\ngit can and will track other files, but it’s primarily meant for code, that is where version control is most powerful\nParticularly is a non-code file is large, it is best to ignore it with .gitignore which we will talk about below\n\n\npush regularly and often\n\n\nWhenever you finish something, it’s generally a good idea to push those change up to GitHub\nThis makes each version git stores more granular, so you can undo one thing without undoing a bunch of things. That will make more sense over time, but for, just push\n\n\npull at the start of each work session\n\n\nThis isn’t important if you’re using git in the simple way we are, but the second you start collaberating with git or even using on multiple computers, always pull first\nThis will add any changes that have been push-ed to your files before you edit them, avoiding conflicts and making everyone’s lives easier\n\n\nWrite useful commit messages\n\n\nEverytime you commit then push you have to write a message, if we have to go back in time, this is how you will find the point to go back to, so don’t say make them descriptive\n\n\nDon’t panic\n\n\nSometimes, git can get messed up, particularly when collaborating with others\nThe beuaty is that with version control, we can always go back and fix things\nIf you run into git issues, I am happy to help, and if I can’t I know plenty of people who can!\n\n\nNever, ever, ever, put private or restricted information in git or GitHub\n\n\nBy default GitHub repos are public, and even if they’re private, they are not approved places for private or restricted data\nEven if you’re just backing up code that uses restricted data, you should check-in with your data security/IT team to make sure you’re following institutional rules",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#summary",
    "href": "x-01-git.html#summary",
    "title": "I: Git & GitHub",
    "section": "Summary",
    "text": "Summary\n\nIf you’ve made it to here, congratulations, you’re now officially a git user\n\nI encourage you to keep at it, the more you use it, the easier it becomes\n\nUsing git as a version control and backup for a single computer is the simplest way to use git and more than enough for a lot of people\n\nIf you get comfortable with git, it can do so much more\n\nWorking across multiple computers\nCollaborating with other researchers\nCreating branches of work to try out new approaches\nfork-ing existing repos to make a new version of something someone else did\npull request-ing something you fork-ed and improved/fixed to get your change added to the main project\nHosting websites with gh-pages (like this one!)\nA whole lot more!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "href": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\nNOTE: Since we’re not using the vanilla R, we don’t need to load any packages\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NONE",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#check-working-directory",
    "href": "x-03-vanilla.html#check-working-directory",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Check working directory",
    "text": "Check working directory\n\nBefore we get started, make sure your working directory is set to your class folder\n\n\n## Check working directory is correct\nsetwd(this.path::here())",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#read-in-data",
    "href": "x-03-vanilla.html#read-in-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Read in data",
    "text": "Read in data\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read.csv(), which is base R function\ndf &lt;- read.csv(file.path(\"data\", \"hsls-small.csv\"))\n\n\nUnlike the read_csv() function we’ve used before, read.csv() doesn’t print anything\n\nnotice the difference: a . instead of an _\n\nSo that we can see our data, well print to the console. BUT before we do that…\n\n-read.csv() returns a base R data.frame() rather than the special data frame or tibble() that the tidyverse uses. - It’s mostly the same, but one difference is that whereas R will only print the first 10 rows of a tibble, it will print the entire data.frame - We don’t need to see the whole thing, so we’ll use the head() function to print only the first 10 rows.\n\n## show first 10 rows\nhead(df, n = 10)\n\n   stu_id x1sex x1race x1stdob x1txmtscor x1paredu x1hhnumber x1famincome\n1   10001     1      8  199502    59.3710        5          3          10\n2   10002     2      8  199511    47.6821        3          6           3\n3   10003     2      3  199506    64.2431        7          3           6\n4   10004     2      8  199505    49.2690        4          2           5\n5   10005     1      8  199505    62.5897        4          4           9\n6   10006     2      8  199504    58.1268        3          6           5\n7   10007     2      8  199409    49.4960        2          2           4\n8   10008     1      8  199410    54.6249        7          3           7\n9   10009     1      8  199501    53.1875        2          3           4\n10  10010     2      8  199503    63.7986        3          4           4\n   x1poverty185   x1ses x1stuedexpct x1paredexpct x1region x4hscompstat\n1             0  1.5644            8            6        2            1\n2             1 -0.3699           11            6        1            1\n3             0  1.2741           10           10        4            1\n4             0  0.5498           10           10        3            1\n5             0  0.1495            6           10        3            1\n6             0  1.0639           10            8        3           -8\n7             0 -0.4300            8           11        1            1\n8             0  1.5144            8            6        1            1\n9             0 -0.3103           11           11        3            1\n10            0  0.0451            8            6        1           -8\n   x4evratndclg x4hs2psmos\n1             1          3\n2             1          3\n3             1          4\n4             0         -7\n5             0         -7\n6            -8         -8\n7             1          2\n8             1          3\n9             1          8\n10           -8         -8",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#select-variables-columns",
    "href": "x-03-vanilla.html#select-variables-columns",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\n\nData frames are like special matrices\n\nThey have rows and columns\nYou can access these rows and columns using square bracket notation ([])\nBecause data frames have two dimensions, you use a comma inside the square brackets to indicate what you mean ([,]):\n\ndf[&lt;rows&gt;,&lt;cols&gt;]\n\n\nAt it’s most basic, you can use numbers to represent the index of the cell or cells you’re interested in\n\nFor example, if you want to access the value of the cell in row 1, column 4, you can use:\n\n\n\n## show value at row 1, col 4\ndf[1, 4]\n\n[1] 199502\n\n\n\nBecause data frames have column names (the variable names in our data set), we can also refer to them by name\n\nThe fourth column is the student date of birth variable, x1stdob\n\nWe can use that instead of 4 (notice the quotation marks \"\"):\n\n\n\n\n## show value at row 1, x1stdob column\ndf[1, \"x1stdob\"]\n\n[1] 199502\n\n\n\nIf we want to see more than one column, we can put the names in a concatenated vector using the c() function:\n\n\n## show values at row 1, stu_id & x1stdob column\ndf[1, c(\"stu_id\", \"x1stdob\")]\n\n  stu_id x1stdob\n1  10001  199502\n\n\n\nSo far, we’ve not assigned these results to anything, so they’ve just printed to the console.\n\nHowever, we can assign them to a new object\nIf we want to slice our data so that we only have selected columns, we can leave the rows section blank (meaning we want all rows) and include all the columns we want to keep in our new data frame object.\n\n\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- df[, c(\"stu_id\", \"x1stuedexpct\", \"x1paredexpct\", \"x1region\")]\n\n## show 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region\n1   10001            8            6        2\n2   10002           11            6        1\n3   10003           10           10        4\n4   10004           10           10        3\n5   10005            6           10        3\n6   10006           10            8        3\n7   10007            8           11        1\n8   10008            8            6        1\n9   10009           11           11        3\n10  10010            8            6        1",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#mutate-data-into-new-forms",
    "href": "x-03-vanilla.html#mutate-data-into-new-forms",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\n\nChanging existing variables (columns)\n\nTo conditionally change a variable, we’ll once again use the bracket notation to target our changes\nThis time, however, we do a couple of things differently:\n\ninclude square brackets on the LHS of the assignment\nuse conditions in the &lt;rows&gt; part of the bracket\n\nAs before, we need to account for the fact that our two expectation variables, x1stuedexpct and x1paredexpct, have values that need to be converted to NA: -8, -9, and 11\n\nSee the first data wrangling lesson for the rationale behind these changes.\n\nFirst, let’s look at the unique values using the table() function\n\nThis somewhat similar to count() in tidyverse\n\nSo that we see any missing values, we’ll include an extra argument useNA = \"ifany\"\n\nThis just means we will see counts for NAs if there are any\n\n\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n  -8    1    2    3    4    5    6    7    8    9   10   11 \n2059   93 2619  140 1195  115 3505  231 4278  176 4461 4631 \n\n## see unique values for parental expectation\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n  -9   -8    1    2    3    4    5    6    7    8    9   10   11 \n  32 6715   55 1293  149 1199  133 4952   76 3355   37 3782 1725 \n\n\n\nNotice that we use a dollar sign, $, to call the column name from the data frame\n\nUnlike with the tidyverse, we cannot just use the column name\n\nBase R will look for that column name not as a column in a data frame, but as its own object\nIt probably won’t find it (or worse, you’ll have another object in memory that it will find and you’ll get the wrong thing!).\n\n\nTo modify a variable when it’s a certain value, we can use the [] square brackets in a more advanced way\n\nStart by identifying the column you’d like\n\ne.g, df_tmp$x1stuedexpct -Then add the [] square brackets and inside them\n\nInside them we can add a condition to them, such as when a column is equal to -8\n\ne.g., df_tmp$x1stuedexpct == -8\n\nThink of this a bit like filter() from the tidyverse\n\n\nIf we just print this, you’ll see a load of -8, not that useful…\n\n\n\n## This will just print a bunch of -8s\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8]\n\n   [1] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [25] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [49] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [73] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [97] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [121] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [145] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [169] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [193] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [217] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [241] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [265] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [289] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [313] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [337] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [361] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [385] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [409] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [433] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [457] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [481] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [505] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [529] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [553] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [577] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [601] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [625] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [649] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [673] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [697] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [721] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [745] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [769] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [793] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [817] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [841] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [865] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [889] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [913] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [937] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [961] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [985] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1009] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1033] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1057] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1081] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1105] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1129] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1153] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1177] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1201] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1225] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1249] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1273] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1297] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1321] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1345] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1369] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1393] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1417] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1441] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1465] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1489] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1513] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1537] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1561] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1585] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1609] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1633] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1657] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1681] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1705] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1729] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1753] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1777] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1801] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1825] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1849] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1873] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1897] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1921] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1945] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1969] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1993] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2017] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2041] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n\n\n\nBut, instead of printing it, we can assign NA to it, which will replace all those -8s with NA\n\nWe can do the same for 11 to while we are at it\n\n\n\n## replace student expectation values\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8] &lt;- NA\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == 11] &lt;- NA\n\n\nIf you think back to our previous lesson, we can be a little more slick than this though\n\nIf we change the statement to %in% c(-8, -9, 11) it will do it all at once\n\n\n\n## replace parent expectation values\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct %in% c(-8, -9, 11)] &lt;- NA\n\nLet’s confirm using table() again. The values that were in -8, -9, and 11 should now be summed under NA.\n\n## see unique values for student expectation (confirm changes)\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  93 2619  140 1195  115 3505  231 4278  176 4461 6690 \n\n## see unique values for parental expectation (confirm changes)\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  55 1293  149 1199  133 4952   76 3355   37 3782 8472 \n\n\n\n\nAdding new variables (columns)\n\nAdding a new variable to our data frame is just like modifying an existing column\nThe only difference is that instead of putting an existing column name after the first $ sign, we’ll make up a new name\nThis tells R to add a new column to our data frame\nAs with the tidyverse version, we’ll use the ifelse() function to create a new variable that is the higher of student or parental expectations\n\n\n## add new column\ndf_tmp$high_expct &lt;- ifelse(df_tmp$x1stuedexpct &gt; df_tmp$x1paredexpct, # test\n                            df_tmp$x1stuedexpct,                       # if TRUE\n                            df_tmp$x1paredexpct)                       # if FALSE\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1         NA\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1         NA\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nJust like in the original lesson, it doesn’t handle NA values how we want it to\n\nLook at student 10002 in the second row:\n\nWhile the student doesn’t have an expectation (or said “I don’t know”), the parent does.\n\nHowever, our new variable records NA. Let’s fix it with this test:\n\n\n\n\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\n\n## NB: We have to include [is.na(df_tmp$high_expct)] each time so that\n## everything lines up\n\n## step 1 student\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]), \n    ## if TRUE do this...\n    df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n## step 2 parent\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1paredexpct[is.na(df_tmp$high_expct)]),\n    ## if TRUE do this...\n    df_tmp$x1paredexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n\nThat’s a lot of text!\nWhat’s happening is that we are trying to replace a vector of values with another vector of values, which need to line up and be the same length\n\nThat’s why we start with df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]\n\nWhen our high_expct column has missing values, we want to replace with non-missing x1stuedexpct values in the same row\n\nThat means we also need to subset that column to only include values in rows that have missing high_expct values\n\nBecause we must do this each time, our script gets pretty long and unwieldy.\n\n\n\n\n\nLet’s check to make sure it worked as intended.\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#filter-observations-rows",
    "href": "x-03-vanilla.html#filter-observations-rows",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\n\nLet’s check the counts of our new variable:\n\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ntable(df_tmp$high_expct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  71 2034  163 1282  132 4334  191 5087  168 6578 3463 \n\n\n\nSince we’re can’t use the missing values we’ll drop those observations from our data frame\nJust like when we selected columns above, we’ll use the [] square brackets notation\n\nAs with dplyr’s filter(), we want to filter in what we want (i.e., when it’s not NA)\n\nSince we want to filter rows, we set this condition before the comma in the square brackets\n\nBecause we want all the columns, we leave the space after the comma blank\n\n\n\n## filter in values that aren't missing\ndf_tmp &lt;- df_tmp[!is.na(df_tmp$high_expct),]\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n10  10010            8            6        1          8\n11  10011            8            6        3          8\n\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values)\nSince we haven’t removed rows until now, to double check, we can compare the number of rows in the original data frame, df, to what we have now\n\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column\n\nWhile not a formal test, it does support what we expected\n\nIn other words, if the number were different, we’d definitely want to go back and investigate",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#summarize-data",
    "href": "x-03-vanilla.html#summarize-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. For an overall average, we can just use the mean() function.\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nmean(df_tmp$high_expct)\n\n[1] 7.272705\n\n\n\nOverall, we can see that students and parents have high post-secondary expectations on average: to earn some graduate credential beyond a bachelor’s degree\nHowever, this isn’t what we want. We want the values across census regions.\n\n\n## check our census regions\ntable(df_tmp$x1region, useNA = \"ifany\")\n\n\n   1    2    3    4 \n3128 5312 8177 3423 \n\n\n\nWe’re not missing any census data, which is good!\nTo calculate our average expectations, we need to use the aggregate function\nThis function allows to compute a FUNction by a group\n\nWe’ll use it to get our summary.\n\n\n\n## get average (assigning this time)\ndf_tmp &lt;- aggregate(df_tmp[\"high_expct\"],                # var of interest\n                    by = list(region = df_tmp$x1region), # by group\n                    FUN = mean)                          # function to run\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n2      2   7.168110\n3      3   7.357833\n4      4   7.125329\n\n\n\nSuccess! Expectations are similar across the country, but not the same by region.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#write-out-updated-data",
    "href": "x-03-vanilla.html#write-out-updated-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Write out updated data",
    "text": "Write out updated data\n\nWe can use this new data frame as a table in its own right or to make a figure\nFor now, however, we’ll simply save it using the opposite of read.csv() — write.csv()\n\n\n## write with useful name\nwrite.csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\n\nAnd with that, we’ve met our task: we can show average educational expectations by region\nTo be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region\n\nThis caveat doesn’t necessarily make our analysis less useful, but rather sets its scope.\n\nFurthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#questions",
    "href": "x-03-vanilla.html#questions",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nWhat is the average standardized math test score?\nHow does this differ by gender?\n\n\n\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (include what that category represents)\n\n\n\n\nOf the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nHow does this differ by region?\n\n\n\n\n\n\n\nWhat percentage of students ever attended a post-secondary institution by February 2016?\nOptional: Give the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35k within each region\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the final lesson). Good faith efforts (as determined by the instructor) at extra credit assignments will earn full credit if submitted on time.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#solution",
    "href": "x-03-vanilla.html#solution",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: 7916]\n##' [FILE: Data Wrangling I Redux: Vanilla R Solution]\n##' [INIT: February 5th 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\n## None!\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf &lt;- read.csv(file.path(\"data\", \"hsls-small.csv\"))\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Part One\ndf$x1txmtscor[df$x1txmtscor == -8] &lt;- NA\ndf_math &lt;- na.omit(df) # No other NAs, so omit works fine\n\nmean(df_math$x1txmtscor)\n\n## Part Two\naggregate(df_math[\"x1txmtscor\"],          # var of interest\n          by = list(sex = df_math$x1sex), # by group\n          FUN = mean)                # function to run\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf_pov &lt;- df[df$x1poverty185 == 1,]\n\npaste(\"The median household income cat is\", median(df_pov$x1famincome),\n      \"which represents incomes between $15k and $35k\")\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n##'[Part I]\n\ndf_hs &lt;- df[df$x4hscompstat %in% c(1,2),]\n\nhs_counts &lt;- table(df_hs$x4hscompstat)\nhs_counts[\"total\"] &lt;- hs_counts[\"1\"] + hs_counts[\"2\"]\npercent &lt;- round(hs_counts[\"2\"]/hs_counts[\"total\"]*100, 2)\npaste(percent, \"% of those with HS credential have a GED\")\n\n\n##'[Part II]\nhs_counts_region &lt;- aggregate(df_hs[\"x4hscompstat\"],          # var of interest\n                              by = list(region = df_hs$x1region), # by group\n                              FUN = table)\n\nhs_counts_region[\"total\"] &lt;- hs_counts_region$x4hscompstat[, \"1\"] + hs_counts_region$x4hscompstat[, \"2\"]\nhs_counts_region[\"percent\"] &lt;- hs_counts_region$x4hscompstat[, \"2\"]/hs_counts_region[\"total\"]*100\n\nhs_counts_region[c(\"region\", \"percent\")]\n\n## ---------------------------\n##' [Q4]\n## ---------------------------\n\n## Part One\n\ndf_col &lt;- df[df$x4evratndclg != -8,]\n\ncol_counts &lt;- table(df_col$x4evratndclg)\ncollege &lt;- col_counts[\"1\"]\ntotal &lt;- col_counts[\"0\"] + col_counts[\"1\"]\npercent &lt;- round(college/total*100, 2)\n\npaste(percent, \"% of student ever attended college\")\n\n## Part Two\n\n## Drop any missing household income and make new variable\ndf_col &lt;- df_col[!df_col$x1famincome %in% c(-8,-9),]\ndf_col$below_35k &lt;- ifelse(df_col$x1famincome %in% c(1,2), 1, 0)\n\n## Now, we can use the aggregate function again, but use the table function\n## we used \n## but inside the \"by\" list\n## we can put both columns in there, a bit like we put two variables in\n## tidyverse's group by\ncol_counts &lt;- aggregate(df_col[\"x4evratndclg\"],          # var of interest\n                        by = list(b35k = df_col$below_35k, # by group(s)\n                                  region = df_col$x1region), \n                        FUN = table)                # function to run\n\n## The ouput of this is a little messy, so we can clean it up\ncol_counts$attend &lt;- col_counts$x4evratndclg[,\"1\"]\ncol_counts$total &lt;- col_counts$x4evratndclg[,\"1\"] + col_counts$x4evratndclg[,\"0\"]\ncol_counts$perc &lt;- col_counts$attend/col_counts$total * 100\n\nprint(col_counts)\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "href": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "title": "I: Enter the tidyverse",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\n\nThroughout this lesson (and class), we are going to lean heavily on the tidyverse collection of packages\n\nIf you don’t already have this installed, use install.packages(\"tidyverse\")\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#check-working-directory",
    "href": "03-wrangle-i.html#check-working-directory",
    "title": "I: Enter the tidyverse",
    "section": "Check working directory",
    "text": "Check working directory\n\nThis script — like the one from the organizing lesson — assumes that the class folder is the working directory and that the required data file is in the data sub-directory\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that we’re not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, we rely on relative paths once you know where you need to be and have gotten there\n\n\nReading Data in with read_csv()\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\n## data are CSV, so we use read_csv() from the readr library\ndf &lt;- read_csv(\"data/hsls-small.csv\")\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## alternatively, you can also use read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\n\nYou may notice the read_csv() prints out information about the data just read in. Nothing is wrong! The read_csv() function, like many other functions in the tidyverse, assumes you’d rather have more rather than less information and acts accordingly\nHere we assign our data to an object called df (short for data frame), which is common in the coding community\n\nYou can call it whatever you want, such as data if that makes more sense to you\n\nUp to now, this should all seem fairly consistent with last week’s lesson. This is just the set up, now it’s time to dive in!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#native-pipe-operator-in-r",
    "href": "03-wrangle-i.html#native-pipe-operator-in-r",
    "title": "I: Enter the tidyverse",
    "section": "Native Pipe Operator |> in R",
    "text": "Native Pipe Operator |&gt; in R\nThe pipe is one of the things that makes R code more intuitive than other programming languages, as it allows us write code in the order we think about it, passing it one from one function to another, rather than nesting it like traditional code would be written\n\nA Brief History of the R Pipe\n\n\n\nbadge\n\n\n\nThe pipe was originally a tidyverse invention, and used %&gt;% symbol, which is probably still a common pipe you see “in the wild”\nThe pipe we are using was brought into the Vanilla version of R a few years ago as |&gt; (the “native pipe”)\nThe reason for the change is some benefits that are beyond the scope of this class, but you just need to know that |&gt; and %&gt;% are essentially the same thing\n\nThe default shortcut for pipe (ctrl + shift + m) will generate %&gt;%, but you can change it to |&gt; by going to Tools &gt; Global Options &gt; Code &gt; Use native pipe operator, |&gt;\n\n\n\n\nHow the Pipe Works\nFor this example don’t worry about the actual processes (we will go over them more below), just look at how much more intuitive the code is with |&gt;s.\nFirst, let’s say I want to take the data we just read in and select the x1txmtscor (math test scores) column\n\n\nWithout |&gt;\n\n## Without |&gt;\nselect(df, x1txmtscor)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor)\n\nNeither is that confusing… But, what if we want to take that output and select only students with a math score above 50?\n\n\nWithout |&gt;\n\n## Without |&gt;\nfilter(select(df, x1txmtscor), x1txmtscor &gt; 50)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50)\n\nSee how the non-piped version is getting messy? Let’s add one more level to really make the point, creating a new variable that is the square root of the test score\n\n\nWithout |&gt;\n\n## Without |&gt;\nmutate(filter(select(df, x1txmtscor), x1txmtscor &gt; 50), square_root = sqrt(x1txmtscor))\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50) |&gt; mutate(square_root = sqrt(x1txmtscor))\n\nAs we are getting longer, let’s use a new line for each pipe, just to make it even clearer (it makes no difference to R)\n\n## Best to use  a new line for each pipe when code gets longer\ndf |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\nEven though we haven’t covered any of these commands yet, we can see that the |&gt; is pretty easy to know roughly what’s going on. Whereas, the traditional nested way gets really tricky beyond a couple of commands.\nOf course, if you wanted, you could do each step separately, constantly assigning and overwriting an object like below\n\n## Without the |&gt;, we could technically break it down step by step\ntemp &lt;- select(df, x1txmtscor)\ntemp &lt;- filter(temp, x1txmtscor &gt; 50)\ntemp &lt;- mutate(temp, square_root = sqrt(x1txmtscor))\ntemp\n\nBut it’s less intuitive than simply piping the results.\n\nIf we do want to see step-by-step output in a piped command, you can either\n\nRun the code as you write it line-by-line\nHighlight sections of the piped code to run up to a point\n\n\n\n\nAssigning Output from Pipes\n\nAssigning output from pipes is the same as we have covered a few times, we use a &lt;- to pass it backwards to an object (in this case we called that object df_backward_pass)\n\n\n## Always assign backwards\ndf_backward_pass &lt;- df |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\n\nIf you want to think of it this way, we are effectively continuing to pass it forward like this…\n\n\n## You can think of the assignment as a continuation of the pipe like this\n## but don't write it this way, it's then hard to find what you called something later\ndf |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor)) -&gt;\n  df_forward_pass\n\n\nThat’s how pipes and assignment work together, and you can see the outputs are all.equal()\n\n\n## Checking they are the same\nall.equal(df_backward_pass, df_forward_pass)\n\n[1] TRUE\n\n\n\nHowever, you really shouldn’t write code like this, as although it runs, it’s then hard to later find where you created df_forward_pass.\nStarting the string of code with where you store the result is MUCH clearer.\n\nBut hopefully it will help some of you understand how |&gt; and &lt;- work together",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#basic-tidyverse-commands",
    "href": "03-wrangle-i.html#basic-tidyverse-commands",
    "title": "I: Enter the tidyverse",
    "section": "Basic Tidyverse Commands",
    "text": "Basic Tidyverse Commands\n\nNow we have the pipe |&gt; covered, it’s time to dig into some basic data wrangling commands\n\n\nSelecting Variables/Columns with select()\n\nOften data sets contain hundreds, thousands, or even tens of thousands of variables, when we are only interested in a handful. Our first tidyverse command select() helps us deal with this, by, as you may have guessed, select()-ing the variables we want\nSince we are going to pipe our R commands, we start with our data then pipe it into the select() command\nIn the select() command, we list out the variables we want\n\nstu_id, x1stuedexpct, x1paredexpct, x1region\n\nNotice: in this (and most tidyverse) command(s) we don’t have to use “quotes” around the variable names. A common error message when you did need to put something in quotes is Error: object &lt;thing you should have \"quoted\"&gt; not found\n\n\n\n\ndf |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n# ℹ 23,493 more rows\n\n\n\nQuick question: if we want to use this reduced data-frame going forward, what should we do?\n\nThat’s right, assign it to an object! Let’s call that df_small\n\ndf_small &lt;- df |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n\nOur next step is to create a variable that’s the higher of student and parent expectations. Sounds simple enough, but first, we have a problem…\n\nCan anyone guess what it is?\n\n\n\n\nUnderstanding our data\nFirst things first, however, we need to check the code book to see what the numerical values for our two education expectation variables represent. To save time, we’ve copied them here:\n\nx1stuedexpct\nHow far in school 9th grader thinks he/she will get\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n\n\n\nx1paredexpct\nHow far in school parent thinks 9th grader will go\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n-9\nMissing\n\n\n\n\nThe good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression\nThe bad news is that we have three values — -8, -9, and 11 — that we need to deal with so that the averages we compute later represent what we mean\n\n\n\n\nExploring Catagorical Variables with count()\n\nFirst, let’s see how many observations are affected by these values using count()\n\n\nNotice that we don’t assign to a new object; this means we’ll see the result in the console, but nothing in our data or object will change\n\n\n## see unique values for student expectation\ndf_small |&gt; count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -8  2059\n 2            1    93\n 3            2  2619\n 4            3   140\n 5            4  1195\n 6            5   115\n 7            6  3505\n 8            7   231\n 9            8  4278\n10            9   176\n11           10  4461\n12           11  4631\n\n## see unique values for parental expectation\ndf_small |&gt; count(x1paredexpct)\n\n# A tibble: 13 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -9    32\n 2           -8  6715\n 3            1    55\n 4            2  1293\n 5            3   149\n 6            4  1199\n 7            5   133\n 8            6  4952\n 9            7    76\n10            8  3355\n11            9    37\n12           10  3782\n13           11  1725\n\n\n\nDealing with -8 and -9 is straight forward — we’ll convert it missing.\n\nIn R, missing values are technically stored as NA.\n\nNot all statistical software uses the same values to represent missing values (for example, STATA uses a dot .)\n\nNCES has decided to represent missing values as a limited number of negative values. In this case, -8 and -9 represent missing values\n\nNote: how to handle missing values is a very important topic, one we could spend all semester discussing\n\nFor now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results\nIn real research, a better approach is usually to impute missing values, but that is beyond our scope right now\n\nDeciding what to do with 11 is a little trickier. While it’s not a missing value per se, it also doesn’t make much sense in its current ordering, that is, to be “higher” than completing a professional degree\n\nFor now, we’ll make a decision to convert these to NA as well, effectively deciding that an answer of “I don’t know” is the same as missing an answer\n\nSo first step: convert -8, -9, and 11 in both variables to NA. For this, we’ll use the mutate() and ifelse() functions\n\n\n\nConditional Values with ifelse()\n\nifelse() is a really common command in R and has three parts\n\nstatement that can be TRUE or FALSE\nWhat to return if the statement is TRUE\nelse what to return when the statement is FALSE\n\n\n\n\nModifying an Existing Variable with mutate()\n\nWhen we want to add variables and change existing ones, we can use the mutate() function\n\nThe basic idea of mutate commands is mutate(&lt;where to go&gt; = &lt;what to go there&gt;)\n\nThis is probably the trickiest function we cover today to understand\n\nNew variables are created if you provide &lt;where to go&gt; a new variable name (or nothing)\nVariables are modified if you provide &lt;where to go&gt; an existing variable name\n&lt;what to go there&gt; can as simple as a single number all the way to a chain of piped functions, so long as there’s a clear answer for every row\n\nIn this case, we want to modify x1stuedexpct to be NA when x1stuedexpct is -8, -9, or 11\n\nNow, we have three values we want to covert to NA, so we could do them one-at-a-time, like below\n\ndf_small &lt;- df_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct == -8, NA, x1stuedexpct))\n\n\nLet’s walk through this code\n\n\nAssign the results back to df_small (which will overwrite our previous df_small)\nTake df_small and pipe |&gt; it into mutate()\nInside mutate() assign our results to x1stuedexpct (which will modify the existing variable)\nModify x1stuedexpct with an ifelse() statement, which remember has 3 parts\n\n\n\nstatement which is asking “is x1stuedexpct == -8”? - Notice == means “is equal to”, while = means “assign to”. Yes it’s confusing, but you’ll get it over time!\nif that statement is true, make it NA\nelse (if the statement is false) return the original variable x1stuedexpct\n\nOkay, make sense? Let’s see what we just did (look at row 26)\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           11           11        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           11           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           -8        2\n19  10020            8            8        4\n20  10021            8           11        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           -8        4\n24  10026            7           10        3\n25  10027           11            6        1\n26  10028           NA           -8        3\n# ℹ 23,477 more rows\n\n\n\nThis is fine, but we have to do it 3 times for both parent and student expectation\n\nInstead, can anyone think (not in R code, just in terms of logic) how we could change our statement piece of the ifelse() to be more efficient?\n\n\n\n\nBeing Efficient with %in% and c()\n\nWhat we can do, is group -8, -9, and 11 together into a list using c()\n\nc() is a very common function in R used to create a list\n\nThen, we can use the %in% operator to ask if that result is any of the numbers in that list\n\nThis keeps our code shorter and easier to read\n\n\n\ndf_small &lt;- df_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct %in% c(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% c(-8, -9, 11), NA, x1paredexpct))\n\n\nThe code now works just as above, but instead of asking if x1stuedexpct is equal to -8, it asks if it’s in the list of -8, -9, and 11, then does the same for parental expectations!\n\nLet’s view those first 26 rows again to see what we did\n\n\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           NA            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           NA        1\n 8  10008            8            6        1\n 9  10009           NA           NA        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           NA           NA        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           NA           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           NA        2\n19  10020            8            8        4\n20  10021            8           NA        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           NA        4\n24  10026            7           10        3\n25  10027           NA            6        1\n26  10028           NA           NA        3\n# ℹ 23,477 more rows\n\n\n\nJust to be doubly-sure, lets check count() again\n\n\ndf_small |&gt; count(x1stuedexpct) \n\n# A tibble: 11 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    93\n 2            2  2619\n 3            3   140\n 4            4  1195\n 5            5   115\n 6            6  3505\n 7            7   231\n 8            8  4278\n 9            9   176\n10           10  4461\n11           NA  6690\n\ndf_small |&gt; count(x1paredexpct)\n\n# A tibble: 11 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    55\n 2            2  1293\n 3            3   149\n 4            4  1199\n 5            5   133\n 6            6  4952\n 7            7    76\n 8            8  3355\n 9            9    37\n10           10  3782\n11           NA  8472\n\n\nSuccess!\n\n\nCreating a New Variable with mutate()\n\nSo, with that tangent out of the way, let’s get back to our original task, creating a new variable that is the highest of parental and student expectations\nTo make a new variable which is the highest of two variables, we can use our friends mutate() and ifelse() some more\n\n\ndf_small &lt;- df_small |&gt;\n  mutate(high_exp = ifelse(x1stuedexpct &gt; x1paredexpct, x1stuedexpct, x1paredexpct))\n\n\nThat code is almost what we want to do\n\nIf x1stuedexpct is higher then take that, if not, take x1paredexpct\n\nThere’s two things we haven’t fully accounted for though…\n\nOne doesn’t actually matter here, but might in other circumstances\nOne definitely matters here\n\nWithout scrolling past the duck, does anyone know what they might be?\n\n\n\n\n\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.\n\n\n\nSloppy Mistake 1 (doesn’t matter here)\n\nWe were a little sloppy with the statement piece, we just asked if x1stuedexpct was greater than x1paredexpct or not\n\nIf we were being more careful, we might have said “greater than or equal to”\n\nWhy doesn’t this matter in this context, and when might it matter?\n\n\n\n\n\nSloppy Mistake 2 (does matter here)\n\nNow let’s check our data frame to see the one that does matter\n\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10002           NA            6        1       NA\n 3  10003           10           10        4       10\n 4  10004           10           10        3       10\n 5  10005            6           10        3       10\n 6  10006           10            8        3       10\n 7  10007            8           NA        1       NA\n 8  10008            8            6        1        8\n 9  10009           NA           NA        3       NA\n10  10010            8            6        1        8\n11  10011            8            6        3        8\n12  10012           NA           NA        2       NA\n13  10013            8           10        3       10\n14  10014            2            6        3        6\n15  10015           NA           10        3       NA\n16  10016            4            6        2        6\n17  10018            6            7        2        7\n18  10019            8           NA        2       NA\n19  10020            8            8        4        8\n20  10021            8           NA        2       NA\n21  10022           10            8        4       10\n22  10024            6            8        2        8\n23  10025            8           NA        4       NA\n24  10026            7           10        3       10\n25  10027           NA            6        1       NA\n26  10028           NA           NA        3       NA\n# ℹ 23,477 more rows\n\n\n\nHmm, that seems odd, why would R consider NA to be greater than 6?\n\nAny thoughts?\n\nGenerally, R is overly-cautious when dealing with NAs to ensure you don’t accidentally drop them without realizing it\n\nFor example, if you were asked what the mean(c(5, 6, 4, NA)) would be, you’d probably say 5, right?\n\nR is never going to just ignore the NA values like that unless we tell it to\n\n\n\n\nmean(c(5, 6, 4, NA))\n\n[1] NA\n\n\n\nSee, what have to explicitly tell it to remove the NA values\n\n\nmean(c(5, 6, 4, NA), na.rm = T)\n\n[1] 5\n\n\n\nSo in our case of trying to get the highest expectation, R doesn’t want us to forget we have NA values, so it throws them at us.\nDealing with missing values is a huge topic in data analysis, and there are many ways to handle them, which is beyond the scope of this lesson\n\nFor now, let’s remove rows that have NA values in either x1stuedexpct or x1paredexpct or both\n\n\n\n\n\nDealing With Missing Values with is.na(), &, and !\n\nTo do this, we will add another couple of code helpers -is.na(x1stuedexpct) simply asks if the x1stuedexpct is NA or not\n\nR doesn’t let you just say x1stuedexpct == NA -! is really helpful tool, which can be used to negate or invert a command\n\n!is.na(x1stuedexpct) just returns the opposite of is.na(x1stuedexpct) so it tells us that the column is not NA -& can be useful inside conditional statements, as it means both must be TRUE (FYI: | means or)\n\n\n\n\ndf_small_cut &lt;- df_small |&gt;\n  filter(!is.na(x1stuedexpct) & !is.na(x1paredexpct))\n\n\nSo !is.na(x1stuedexpct) & !is.na(x1paredexpct) makes sure that both x1stuedexpct and x1paredexpct are not NA\nNow what does filter() do here?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "href": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "title": "I: Enter the tidyverse",
    "section": "Keeping Rows Based on a Condition with filter()",
    "text": "Keeping Rows Based on a Condition with filter()\n\nThe filter() command from tidyverse works by only keeping observations that meet the condition(s) we set\n\nAs in, to make it through the filter, a row must answer “yes” to “does it meet this condition?”\n\nSo in this case, we are keeping all rows where x1stuedexpct and x1paredexpct are not NA\n\nNotice, instead of overwriting df_small we assigned this to a new object df_small_cut\n\nGenerally, when making substantial changes to a data set like dropping observations, we might want to be able to double check what we did, which is easier if we make a new df\n\n\n\n\nQuick Question: A common confusion from this lesson is between filter() and select(). Can someone explain when you’d use select() over filter()?\n\n\nLet’s check the counts of our x1stuedexpct and x1paredexpct again to see if filter() worked\n\n\ndf_small_cut |&gt; count(x1stuedexpct) \n\n# A tibble: 10 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    39\n 2            2  1483\n 3            3    83\n 4            4   769\n 5            5    78\n 6            6  2539\n 7            7   170\n 8            8  3180\n 9            9   127\n10           10  3336\n\ndf_small_cut |&gt; count(x1paredexpct)\n\n# A tibble: 10 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    41\n 2            2   911\n 3            3   105\n 4            4   825\n 5            5    97\n 6            6  3761\n 7            7    66\n 8            8  2746\n 9            9    30\n10           10  3222\n\n\n\nOkay, so no NAs, perfect!\nJust to be extra sure we only removed NAs, we can check the difference in how many rows our original df_small has with df_small_cut\n\nWho can tell me how many rows should have been dropped?\n\n\n\n## does the original # of rows - current # or rows == NA in count?\nnrow(df_small) - nrow(df_small_cut)\n\n[1] 11699\n\n\n\nnrow() does what you’d expect, counts the number of rows\n\nYou could also just check by looking at the dfs in the environment tab, but this way leaves no room for mental math errors\n\nNow let’s check our high_exp variable in the new df_small_cut\n\n\nprint(df_small_cut, n = 26)\n\n# A tibble: 11,804 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10003           10           10        4       10\n 3  10004           10           10        3       10\n 4  10005            6           10        3       10\n 5  10006           10            8        3       10\n 6  10008            8            6        1        8\n 7  10010            8            6        1        8\n 8  10011            8            6        3        8\n 9  10013            8           10        3       10\n10  10014            2            6        3        6\n11  10016            4            6        2        6\n12  10018            6            7        2        7\n13  10020            8            8        4        8\n14  10022           10            8        4       10\n15  10024            6            8        2        8\n16  10026            7           10        3       10\n17  10029            2            2        2        2\n18  10030            6            8        3        8\n19  10036            8            6        2        8\n20  10037            8            8        3        8\n21  10038            6            4        4        6\n22  10039            8           10        3       10\n23  10041            3            1        3        3\n24  10044           10           10        3       10\n25  10045            4            4        2        4\n26  10049            8            8        4        8\n# ℹ 11,778 more rows\n\n\n\nTo be more straightforward, let’s compare the counts of high_exp in df_small and df_small_cut\n\n\ndf_small |&gt; count(high_exp)\n\n# A tibble: 11 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1     3\n 2        2   516\n 3        3    62\n 4        4   482\n 5        5    59\n 6        6  2177\n 7        7   120\n 8        8  3380\n 9        9   112\n10       10  4893\n11       NA 11699\n\ndf_small_cut |&gt; count(high_exp)\n\n# A tibble: 10 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1     3\n 2        2   516\n 3        3    62\n 4        4   482\n 5        5    59\n 6        6  2177\n 7        7   120\n 8        8  3380\n 9        9   112\n10       10  4893\n\n\n\nYay, all NAs are gone!\n\n\nSummarizing Data with summarize()\n\nOkay, so we have our data selected, we made our high_exp variable, and we’ve done some work to handle missing data\nIn our week one data talk we discussed how massive tables of data are not particularly helpful to for someone to read or interpret\n\nWe will cover how to make graphs of our data in a few weeks\nFor our final task today, we are going to make some summary tables using summarize() from the tidyverse\n\nsummarize() allows us to apply a summary statistic (mean, sd, median, etc.) to a column in our data\nsummarize() takes an entire data frame as an input, and spits out a small data frame with the just the summary variables\n\nNote: for this reason, you rarely ever want to assign &lt;- the output of summarize() back to the main data frame object, as you’ll overwrite it\n\nYou can either spit the summary tables out into the console without assignment (which we will do) or if you need to use them for something else, assign them to a new object\n\n\n\n\n## get average (without storing)\ndf_small_cut |&gt; summarize(mean(high_exp))\n\n# A tibble: 1 × 1\n  `mean(high_exp)`\n             &lt;dbl&gt;\n1             7.99\n\n\n\nSee, the output is a 1x1 table with the mean expectation mean(high_exp) of 7.99, almost about completing a master’s degree\n\nNote: if we want to name the summary variable, we can name it just like we did earlier in mutate() with a single =\n\n\n\ndf_small_cut |&gt; summarize(mean_exp = mean(high_exp))\n\n# A tibble: 1 × 1\n  mean_exp\n     &lt;dbl&gt;\n1     7.99\n\n\n\nBut, that wasn’t quite the question we were asked\n\nWe were asked if it varied by region…\n\nFor time’s sake, we’re going to tell you the region variable is x1region and splits the US in 4 Census regions\n\n\n\n\n\nGrouping Data with group_by()\n\nThe group_by() function, following the tidyverse principle of intuitive naming, groups the data and outputs by the variable(s) you say\n\nSo, since we want to calculate the average high expectation by region, we group_by(x1region)\n\nSince we just want it for our summarize(), we just add it to the pipe\n\nIf you wanted to save the data in it’s group_by()-ed state, you could assign it to something\n\n\n\n\n\n## get grouped average\ndf_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp))\n\n# A tibble: 4 × 2\n  x1region mean_exp\n     &lt;dbl&gt;    &lt;dbl&gt;\n1        1     8.13\n2        2     7.88\n3        3     8.06\n4        4     7.86\n\n\n\nSuccess! While expectations are similar across the country, there’s some variance by region\n\nWhile there are few things we could do to make this a little fancier (e.g., changing the region numbers to names, formatting the table, etc.) we have answered our question, and have clear documentation of how we got here, so let’s call that a win!\n\n\n\n\nSaving Data with write_csv()\n\nSometimes we want to be able to access objects from scripts without having to re-run the whole thing\n\nRemember: one of the main advantages of R is the data we read in is untouched\n\nTo do this, we want to write_ a new csv() file, containing our modified data\n\nunlike read_csv() which only needed a file name/path, write_csv() needs to know what you’re trying to save and the file name/path you want to save it to\n\nThe only way you can overwrite or change the original data is by saving to the same file name as the original data, so NEVER do that!\n\n\nSince we didn’t assign our summary table to anything, we can just add write_csv() to the end of the pipe and add a file.path()\n\nIf you want to save a data frame you already assigned to an object write_csv(&lt;object&gt;, file.path(&lt;path&gt;)) would work just fine!\n\n\n\n## write with useful name\n\ndf_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  write_csv(file.path(\"data\", \"region-expects.csv\"))\n\nPhew!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#appendix-all-at-once",
    "href": "03-wrangle-i.html#appendix-all-at-once",
    "title": "I: Enter the tidyverse",
    "section": "Appendix: All at Once",
    "text": "Appendix: All at Once\nWe went through that piece by piece to demonstrate each function, but, there’s no reason we can’t just |&gt; pipe it all together\n\n## Let's redo the analysis above, but with a fully chained set of\n## functions.\n\n## start with original df\ndf |&gt;\n  ## select columns we want\n  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |&gt;\n  ## If expectation is -8, -9. or 11, make it NA\n  mutate(x1stuedexpct = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |&gt;\n  ## Make a new variable called high_exp that is the higher or parent and student exp\n  mutate(high_exp = ifelse(x1stuedexpct &gt; x1paredexpct, x1stuedexpct, x1paredexpct)) |&gt;\n  ## Drop if either or both parent or student exp is NA\n  filter(!is.na(x1stuedexpct) & !is.na(x1paredexpct)) |&gt; \n  ## Group the results by region\n  group_by(x1region) |&gt;\n  ## Get the mean of high_exp (by region)\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  ## Write that to a .csv file\n  write_csv(file.path(\"data\", \"region-expects-chain.csv\"))\n\nTo double check, let’s just check these are the same…\n\nnon_chain &lt;- read_csv(file.path(\"data\", \"region-expects.csv\"))\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchain &lt;- read_csv(file.path(\"data\", \"region-expects-chain.csv\"))\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nall.equal(non_chain, chain)\n\n[1] TRUE\n\n\nWooHoo!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#final-notes",
    "href": "03-wrangle-i.html#final-notes",
    "title": "I: Enter the tidyverse",
    "section": "Final notes",
    "text": "Final notes\n\nThis rather lengthy lesson has thrown you in the (medium) deep end of the coding pool\n\nBy no means are you expected to get everything we just did\n\nWe will continue to revisit all these commands throughout the class, by the end of the semester, they will be second nature!\n\n\nWe also saw how to use code to answer a realistic question we might be asked in a data management job, a translation skill that will prove invaluable later on!\n\nWe had to plan out steps and then make some adjustments along the way (e.g., our NA issues), that’s all part of the process!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-one",
    "href": "03-wrangle-i.html#question-one",
    "title": "I: Enter the tidyverse",
    "section": "Question One",
    "text": "Question One\na) What is the average (mean) standardized math score?\nb) How does this differ by gender?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-two",
    "href": "03-wrangle-i.html#question-two",
    "title": "I: Enter the tidyverse",
    "section": "Question Two",
    "text": "Question Two\na) Among those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (Include a description what that category represents)",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-three",
    "href": "03-wrangle-i.html#question-three",
    "title": "I: Enter the tidyverse",
    "section": "Question Three",
    "text": "Question Three\na) Of the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nb) How does this differ by region?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-four",
    "href": "03-wrangle-i.html#question-four",
    "title": "I: Enter the tidyverse",
    "section": "Question Four",
    "text": "Question Four\na) What percentage of students ever attended a post-secondary institution? (as of the data collection in february 2016)\nb) Give the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35,000 within each region\n\n\nHint: group_by() can be given more than one group",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#submission",
    "href": "03-wrangle-i.html#submission",
    "title": "I: Enter the tidyverse",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the PDF rendered to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "02-set-data.html#organizing-a-project-folder",
    "href": "02-set-data.html#organizing-a-project-folder",
    "title": "II: Reading Data & IPEDS",
    "section": "Organizing a Project Folder",
    "text": "Organizing a Project Folder\nWe’ll begin with how to organize your course and project files.\n\nThe Kitchen Metaphor (from Dr. Skinner)\n\n“Every data analysis project should have its own set of organized folders. Just like you might organize a kitchen so that ingredients, cookbooks, and prepared food all have a specific cabinet or shelf, so too should you organize your project. We’ll organize our course directory in a similar fashion.”\n\n\nIn Dr. Skinner’s kitchen, think of pristine space where NOTHING sits out on the counter.\nHowever, we are going to have kitchen where some things are stored away in cupboards, but what we regularly work with (the scripts) sits out on the counter\n\nThe kitchen metaphor also works to explain why you might take these approaches\n\nIt’s definitely tidier to keep everything in your kitchen stored away, but, it adds an extra step whenever you want to cook a meal. The same is true here.\nWhy? We will get to that later…\n\n\n\n\n\nEDH 7916 Folder Setup\n\nWith this kitchen metaphor in mind, let’s set up our folder for the class\nWe’ve made an EDH-7916 example folder you can download here which is also available on the class homepage\n\nIn here, you will see\n\nAn .R script template\nA set of numbered .R scripts for our lessons\nA set of numbered .qmd files for our assignments\nA data folder\nA reproducible-report folder (with it’s own data sub-folder)\nA .pdf copy of the syllabus\n\n\nDownload and save this folder wherever you usually keep class folders\n\nHere we use Desktop, but you can use your Documents folder etc. if you wish\nYou can rename the folder if you’d like (but please don’t rename the internal folders)\n\nSee naming guidelines below on how best to name files\n\n\nThroughout the class (and especially in your final project) you may feel the need for other sub-folders for other items (such as one to keep graphs in), but this should be fine for now\n\n\n\nR Project Setup\n\nRStudio has some really helpful features, one of which is creating R Projects easily\n\nAt their very simplest, these can be ways of keeping your RStudio environment saved (especially helpful switching between projects), but also enable more feature like using git (see extra credit lesson)\nNote: If you’re using posit.cloud, you already have a RStudio project by default\n\nIt’s pretty easy to set up a project now we have our class folder set up\n\nIn the top right corner of RStudio you’ll see a blue cube with “none” next to it\nClick there, then “new project”\nThen click “from existing directory”\nFind the class folder we just created, select it, and we’re done!\n\nThis is really useful for keeping track of multiple projects, but if this is all you use it for, it will be helpful to keep working directory correct!\n\n\n\nNaming Guidelines\n\nYour class scripts and data files are already named, but there will be numerous files you need to create throughout the class (assignment scripts, everything for your final report, etc.). So it’s best we get on the same page\nAlways name your files something logical\n\nThe file name should always tell something about the purpose of that file or folder\n\nScript numbering\n\nFollowing Hadley Wickham (the founder of RStudio)’s script numbering\n\nBasically start all your script names with the number that they should be run in\n\n01-data-reading\n02-data-cleaning\n03-data-analysis\n\nThis can be especially helpful if you’re keeping scripts in the top level of the project directory to keep things organized\n\n\nGenerally, a good programming tip is to avoid spaces at all costs, use dashes or underscores instead\nIt’s also good to be consistent with capitalization, most traditional programmers will avoid it completely, but if you do it, do it consistently throughout that project\n\nWe used no capitalization through this class\n\nWhatever you do, never (ever, ever) have different versions of files with the same name but different capitalization\n\n\nLastly, try to keep files names as short as possible\n\nLater on we will be be in situations where we have to type out file names, so if you go too long, it can become frustrating\n\nHow do we understand the names of our class scripts?\n\nHadley Wickham’s script numbering corresponding to the order of the lessons\nset, wrangle, viz, quarto, or pro indicate which group of lessons it belongs to\nAnything else is just descriptive, roman numerals for the lesson series, or a descriptive word\n\nWith our class folder now set up, it’s time to go over some other key organization principles\n\n\n\nWorking Directory\n\nThe working directory is almost certainly the most common cause of issues in this class, and continues to be something I get tripped up by from time to time, so this may take a minute to get your head around\nAs a general rule, no matter how you have your folders organized in the future, you usually want your working directory set to where your script is\n\nThat way, you’re always giving directions from the common point of “where we are we are right now”\n\nThis will then be the same if we move the project folder on our computer, or run it on someone else’s computer\n\n\nBy default, when we open a project in RStudio, RStudio helpfully sets our working directory to the project folder\n\nThis is why we are keeping our scripts out on the counter top so to speak, the default working directory should be the correct one\n\nThat said, there will be times when you need to change your working directory, so, let’s go over the basics of that quickly\n\nFor instance, if you forget to open a project, RStudio will often the leave working directory as your root folder\n\nYou can see the currently working directory path next to the little R icon and version at the top of your console panel\nIf it’s wrong, there are a few ways to change it\n\nFind “session” on the top drop-down menu\n\n\nThen “set working directory”\nThen “To source file location”\n\nThis should be the same as “To project directory” as our scripts are stored at the top level of the project folder\n\n\n\nInstall the this.path package (recall how to do that from last week)\n\n\nWith that installed, call setwd(this.path::here()) at the top of the script\n\nNote: this.path::here() is the same as doing library(this.path) followed by here() but is more efficient if you only want one thing from a package\nAssuming you want the working directory to be the script location, this never hurts to always leave at the top\n\n\n\nNavigate to the desired folder in the files pane (bottom right)\n\n\nSelect the cog symbol\n\nSelect “Set as working directory”\n\nNote: “Go to working directory” can be useful to see what’s in the folder if you navigate away\n\n\n\n\nThe old school vanilla R way setwd(\"&lt;path to your script&gt;\")\n\n\nBut, this really isn’t usually the most efficient\n\n\n\nIf we organize our folder as outlined in this lesson, and use an R project, we shouldn’t need to change this much, but it’s inevitable you will need to change it every now and then\n\n\n\nFile Paths\n\nWhen we are working with R, we (most of the time) need to bring in other items, such as data\n\nIn order to do that, our computer has to find these items, and there are two ways it can do that\n\n\n\nAbsolute Paths\n\nAbsolute paths are directions to what you’re looking for starting from the root of your computer, and list out exactly where a file is. For example, the absolute path of this Quarto script we are now looking is\n\n\"/Users/juewu/Desktop/7916/02-set-data.qmd\"\n\nThis is perfectly fine, assuming two things\n\nWe don’t move the project directory\nIt only needs to run on this computer\n\nOftentimes, we cannot rely on both these assumptions being true\n\nPlus, if we start with these absolute paths and then need to change, it will then become a real pain to update everything\n\nSo, we should ALWAYS use relative paths instead (this one of the only strict rules for assignments)\n\n\n\n\n\nRelative Paths\n\nImagine you are going to a College of Education cookout, but, you are given directions from my house. That’s only any use if you know where my house is…\n\nInstead, you really want directions from somewhere we all know, like Norman Hall\n\nThat is (basically) how relative paths work, we give directions to to our data from a common point\n\n\nRelative paths are directions to what you’re looking for from where you are right now (a.k.a your “working directory”)\nIf we assume have our working directory set to our shiny new class folder, then, that becomes the starting point for all our directions\n\nTherefore, to access hd2007.csv in out data sub-folder, we just need to say file.path(\"data\", \"hd2007.csv\")\n\n\n\n\nThe file.path() Function\n\nOne last thing, you see how file paths are typically written with / or \\? (which depends on your computer)\n## R has a nice function that means we don't have to\n\n```         \na)  worry about which way around the slash should be\n```\n\n-   \n\n    b)  avoid issues with different computers expecting different slashes\n\nfile.path()\n\nInside, we just type the name of each folder/file in “quotes”, turning\n\n\"Users/YourName/Desktop/7916\" into\nfile.path(\"Users\", \"YourName\", \"Desktop\", \"7916\")\nThis may look longer, but, it’s more compatible and easier to remember”\n\n\n\n\n\n\nWhat If I Need to Go Back a Level?\n\nSometimes we are in a folder, but want to go back a level, i.e. not the folder our current folder is in\n\nThis is very common if with we were using the “pristine kitchen” approach\n\nTo do so is easy, we just add a \"..\" to our file.path()\n\nSo, if we are in my class folder on my desktop, and we want to go to another folder on the desktop\n\nfile.path(\"..\", \"&lt;folder we want&gt;\")",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#script-template",
    "href": "02-set-data.html#script-template",
    "title": "II: Reading Data & IPEDS",
    "section": "Script Template",
    "text": "Script Template\n\nIn our shiny new class folder, you’ll see an r-script-template.R file (thanks to Matt and Ben)\n\nThis a resource for you to use for assignments and other work, feel free to change it to suit your needs\n\nGenerally you can just “Save As” the template everytime you make a new script\n\n\nThe script header block is a useful way to keep more info than a file name can\nThe main reason to use a template is to keep your work organized into sections\n\nThis template has\n\nLibraries to load needed packages\nInput to load data\nPrep to clean the data\nAnalysis to run our analyses\nOutput to save our modified data\n\nHowever, these will not always be the sections you need\n\nIn bigger projects, you might have a whole script for data cleaning\nIn other projects, you might want a section or script just for making plots\nIn your assignments, you’ll likely want a section for each question\n\nThe main point is to ensure you have some kind of sections in your scripts\n\nScripts can be really hard to navigate if you don’t!\n\n\n\n\nQuarto Files\n\nYou should also see a set of .qmd files in your folder\n\nYou will use Quarto for your assignments and final project, and we will go through Quarto in more detail in a few weeks. For now, you only need to know that you can write text along with codes in Quarto, and that you will render it to a PDF file for your assignment submission.",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#reading-in-data",
    "href": "02-set-data.html#reading-in-data",
    "title": "II: Reading Data & IPEDS",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nNext, let’s apply some of this thrilling knowledge about file paths and working directories to read in some data from IPEDS\nTo do this, open 02-set-data.R from your class folder\nFirst up, check your working directory by either\n\nLooking at the top of your console or\nTyping getwd() into the console\n\nThis should be your class folder, but if not, we need to set it there\n\nOn the top drop-down menu, select “Session”, “Set Working Directory,”To Source File Location”\nQuick Question: Without scrolling up, who can remember the other ways of doing this?\n\nOkay, with this set, it’s time to read in our first dataset!\n\nQuick Question 1: Where is our data?\nQuick Question 2: Who remembers how we assign something in R?\n\nWith those questions answered, we have everything we need\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_ipeds &lt;- read_csv(file.path(\"data\", \"hd2007.csv\"))\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSuccess!\nWe will cover other ways of reading in data over the course of the class (we can download data directly to R somtimes), but this is most common way, so we are ready for some analysis next week!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#setting-up-github-copilot",
    "href": "02-set-data.html#setting-up-github-copilot",
    "title": "II: Reading Data & IPEDS",
    "section": "Setting up GitHub Copilot",
    "text": "Setting up GitHub Copilot\nOne major change of the course this year is the introduction of GitHub Copilot.\n\nWhat is GitHub Copilot?\nGitHub Copilot is an AI-powered coding assistant that helps write code faster by offering suggestions, autocompleting code, or generating code snippets based on comments or existing code. This is a tool that can be very helpful, but also can be a crutch if not used correctly. We will go over how to set it up and some best practices for using it.\n\n\nHow to set up GitHub Copilot?\n\nFirst, you will need a GitHub account. If you don’t have one, you can sign up one. Feel free to sign up for GitHub Student Developer Pack as a student.\nNext, you will need to enable GitHub Copilot from the Copilot page.\nThen, enable GitHub Copilot Plugin for RStudio.\n\nOn your top menu bar, go to Tools &gt; Global Options &gt; Copilot, and enable it\nYou will be prompted to sign in to your GitHub account and authorize GitHub Copilot to use your account\n\n\n\n\nUsing GitHub Copilot for your projects\nIf you do use GitHub Copilot to help with your codes, please keep your prompts in your comment line and acknowledge with a ## h/t\nThat’s it for R today, phew!\nNow let’s go and explore IPEDS Data Center and see where a lot of contemporary higher education research data comes from!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "href": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "title": "II: Reading Data & IPEDS",
    "section": "IPEDS Exploration Key Points (for review)",
    "text": "IPEDS Exploration Key Points (for review)\n\nIPEDS is an annual federally mandated data collection process (and compliance is a significant portion of many Institutional Researcher jobs)\nThere are a few ways of downloading IPEDS data, if you click through the website you may well find a point-and-click way of selecting specific variables\n\nThis is NOT reproducible and therefore NOT the best practice for research\n\nWe want to use the IPEDS data center to access the complete data files then select and join variables to get our desired data set\n\nFear not, we will go over how to do those things in the first two data wrangling lessons!\n\n\n\n“Data File” vs. “STATA Data File”\n\nFor some NCES data sets, such as HSLS downloaded from NCES DataLab, selecting the Stata file option will download a .dta STATA file format version of the data, which is often nicely labelled\n\nWe can actually read these into R using the haven library from the tidyverse\n\nHowever, for IPEDS, the STATA file option is actually just another .csv file, it’s formatted slightly differently to read into STATA, so there’s no reason to bother with it when using R\n\n\n\nUsing IPEDS Codebooks\n\nTo be able to use most of these big data sets, you need to be able to understand the code. Let’s look together at the codebook for EFFY (headcount enrollment) for 2021\n\nFor IPEDS, the code book is called the dictionary, and is always an Excel file (.xlsx). Other data sources will look different but the general principle will be the same",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "href": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "title": "II: Reading Data & IPEDS",
    "section": "Other common data sources for higher education research",
    "text": "Other common data sources for higher education research\n\nNational Center for Educational Statistics (also the owner of IPEDS)\n\nLongitudinal Surveys such as HSLS-High School Longitudinal Study and ECLS-Early Childhood Longitudinal Studies\nAdministrative Data (including IPEDS)\nNCES has a good amount of publicly available data, but they also have a LOT of restricted data\n\nTypically publicly available data will be either institution level (school, college, university wide) or fully anonymized. Meanwhile restricted data will often be student level and have some more detailed information\n\nGetting restricted data is tough, but not impossible\n\nYou will need a clear purpose of your study and to know exactly what data you want access to (see available data here)\nYou’ll then need to take this idea to your advisor\n\nFor your final project in this class, your data MUST be publicly available\n\nThis means we must be able to go and download it ourselves, you won’t submit data with your final project submission\n\n\n\n\nCollege Scorecard\n\nDesigned more as a tool for potential college students, college scorecard has data points of interest to this audience, but some things useful for our research too, in particular graduate earning levels\n\nSimilarly to IPEDS, if using College Scorecard, we want to avoid the point-and-click interface and download the entire data files available here\n\n\nNational Bureau of Labor Statistics\n\nLongitudinal surveys, some have educational variables similar to NCES but are often much broader in scope\n\nNational Longitudinal Survey of Youth (NLSY) is one of the most used\n\nThere are publicly available portions of these surveys, but other sections are restricted, see BLS’s accessing data page for more info\n\nCensus & American Community Survey\n\nUseful for population statistics, not student specific\n\nCommon variables for higher ed research include education and income levels for a population\n\nFor example of what is available, see the variables available in the 2019 ACS here\n\nWe will actually do some cool stuff to download ACS data directly to R later in Data Viz III using the tidycensus package\n\n\nMSI Data Project\n\nDetails about MSI classification and funding, includes IPEDS ID numbers to easily link to additional data\n\nMany, many more, have fun exploring!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#question-one",
    "href": "02-set-data.html#question-one",
    "title": "II: Reading Data & IPEDS",
    "section": "Question One",
    "text": "Question One\na) Create an Excel spreadsheet called r-class-family.xlsx with three columns; name, degree_program, and years_at_uf\nb) Add your information to it\nc) Optional: Add some of your classmates information (recall from class introductions, or, re-introduce yourself after class)\nd) Read this file into R and assign it to an object called data (just like we did in class)\n\nHint: You’ll need to load a new package to read Excel files, between Google and asking your friends, you should be able to figure it out",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#question-two",
    "href": "02-set-data.html#question-two",
    "title": "II: Reading Data & IPEDS",
    "section": "Question Two",
    "text": "Question Two\na) Pick any single data file from IPEDS that peaks your interest from IPEDS complete data files\nb) Download it and save it to your data subfolder in your class folder\nc) Read it into R and assign it to an object called data_ipeds",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#submission",
    "href": "02-set-data.html#submission",
    "title": "II: Reading Data & IPEDS",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  }
]