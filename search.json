[
  {
    "objectID": "x-04-skinner-phil.html#organization",
    "href": "x-04-skinner-phil.html#organization",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Organization",
    "text": "Organization\nFrom the organizing lesson, we’ve been very concerned about how our project is organized. This includes both the directory of folders and files on our machine and the scripts that run our analyses.\n\nProject directory\n\nNote from Matt: Recall we discussed the difference between Dr. Skinner’s “pristine kitchen” approach (tidier) and our approach (easier) with scripts left “out on the counter” early in the class. Take some time to read Dr. Skinner’s opinion on that here and decide which you agree with.\n\nWhile it is certainly easier on the front end to store all project files in a single folder\n./project\n|\n|--+ analysis.R\n|--+ clean.R\n|--+ clean_data.rds\n|--+ data1.csv\n|--+ data2.csv\n|--+ histogram_x.png\n|--+ density_z.png\nit’s better to separate files by type/purpose in your project directory (“a place for everything and everything in its place”):\n./project\n|\n|__/data\n|   |--+ clean_data.rds\n|   |--+ data1.csv\n|   |--+ data2.csv\n|__/figures\n|   |--+ histogram_x.png\n|   |--+ density_z.png\n|__/scripts \n    |--+ analysis.R\n    |--+ clean.R\nJust like there’s something to be said for visiting the library in order to scan titles in a particular section — it’s easier to get an idea of what’s available by looking at the shelf than by scanning computer search output — it’s useful to be able to quickly scan only the relevant files in a project. With well-named files (discussed below) and an organized directory structure, a replicator may be able to guess the purpose, flow, and output of your project without even running your code.\n\nQuick question\nWhen using a project directory instead of just a single folder, we need to use file paths. In particular, we use relative paths rather than fixed paths. Why?\n\n\n\nScript\nIn the lesson on organizing, I also shared a template R script. While you do not need to use my particular template — in fact, you should modify it to meet your needs — it does follow a few organizational rules that you should follow as well.\n\nClear header with information about the project, this file, and you.\nLoad libraries, set paths and macros, and write functions towards the top of the file.\nClear sections for reading in data, process, and output (ingredients, recipe, prepared dish).\n\n\nQuick question\nWhy put things like file paths or macros at the top of the file?",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#clarity",
    "href": "x-04-skinner-phil.html#clarity",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Clarity",
    "text": "Clarity\nRemember: even though our scripts are instructions for the computer, they are also meant to be read by us humans. How well you comment your code and how well you name your objects, scripts, data, output, etc, determine the clarity of your intent.\n\nCommenting\nDon’t assume that your intent is clear. Particularly because so much working code comes as the result of many (…many…many…) revisions to non-working code, it’s very important that you comment liberally.\n\nWhat are you doing?\n## Creating a dummy variable for each state from current categorical variable\nWhy are you doing it this way?\n## Converting 1/2 indicator to 0/1 so 1 == variable name\nLinks to supporting documents/websites\n## see &lt;url&gt; for data codebook\nHat-tip (h/t) for borrowed code\n## h/t &lt;url&gt; for general code that I slightly modified \nFormula / logic behind method\n## log(xy) = log(x) + log(y)\n## using logarithms for more numerically stable calculations \n\nAll of these items are good uses for comments.\n\nQuick question\nIn which situation are comments not useful?\n\n\n\nNaming\nFor R, all the following objects are functionally equivalent:\n## v.1\nx &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.2\nstuff &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.3\ncities &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\n\n## v.4\nstates &lt;- c(\"Alabama\",\"Alaska\",\"Arkansas\")\nHowever, only the last object, states, makes logical sense to us based on its contents. So while R will work just as happily with x, stuff, cities, or states, a collaborator will appreciate states since it gives an idea of what it contains (or should contain) — even without running the code.\n\nQuick question\nWithout seeing the initial object assignment, what might you expect to see as output from the following code coming from a fellow higher education researcher? Why?\nfor (i in flagship_names) {\n    print(i)\n}\n\nWhen objects are well named, your code may become largely self-documenting, which is particularly nice since you don’t have to worry about drift between your comments and code over time.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#automation",
    "href": "x-04-skinner-phil.html#automation",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Automation",
    "text": "Automation\nIn the functional programming lesson, we discussed the difference between Don’t Repeat Yourself programming and Write Every Time programming. As much as possible and within the dictates of organizing and clarity, it’s a good idea to automate as much as you can.\n\nPush-button replication\nIn general, the goal for an analysis script — in particular, a replication script — is that it will run from top to bottom with no errors by just pushing a button. What I mean by this is that I want to:\n\nDownload the project directory\nOpen the script in R Studio\n(If necessary, make sure I’m in the correct working directory)\nPush the “Run” button and have all the analyses run with all estimates/figures/tables being produced and sent to their proper project folders.\n\nThe dream!\nWhen projects are complicated or data isn’t easily shared (or can’t be shared), the push button replication may not be feasible. But your goal should always be to get as close to that as possible. Make your life a little harder so your collaborators, replicators — and future you! — have it a little easier.\nONE NOTE Like writing a paper, the process going from a blank script to a final product typically isn’t linear. You’ll have starts and stops, dead-ends and rewrites. You shouldn’t expect a polished paper from the first draft, and you shouldn’t expect to code up a fully replicable script from the get-go.\nThere’s something to be said for just getting your ideas down on paper, no matter how messy. Similarly, it may be better to just get your code to work first and then clean it up later. My only warning here is that while you have to clean up a paper eventually (someone’s going to read it), code still remains largely hidden from public view. There will be a temptation to get it to work and then move on since “no one will see it anyway, right…right?” Fight that urge!\nJust like real debt, the technical debt that you take out each time you settle for a hacky solution that’s good enough can grow exponentially. At some point, the bill will come due in the form of obviously (or even scarier, not-so-obviously) incorrect results. Tracking down the source of errors for large projects or those that reuse code from past projects can be maddening. Take a little extra time to write good code now and it will pay dividends in saved hours down the road, not to mention fewer 2 a.m. cold-sweat awakenings, Is that weird finding maybe because I constructed my main outcome variable wrong…wait, how did I code that…?\n\nQuick question\nIn a world in which not everyone knows how to use R, what’s another benefit of push-button automation?\n\n\n\nMultiple scripts\nWe’ve only worked with one script so far, but you can use the source() function in R to call other R scripts. For a very large project, you may decide to have multiple scripts. For example, you may have one script that downloads data, one that cleans the data, one that performs the analyses, and one that makes the figures.\nRather than telling a would-be replicator to “run the following files in the following order”\n\nget_data.R\nclean_data.R\ndo_analysis.R\nmake_figures.R\n\nyou could instead include a run_all.R script that only sources each script in order and tell the replicator to run that:\n## To run all analyses, set the working directory to \"scripts\" and\n## enter\n##\n## source(\"./run_all.R\")\n##\n## into the console. \n\n## download data\nsource(\"./get_data.R\")\n\n## clean data\nsource(\"./clean_data.R\")\n\n## run analyses\nsource(\"./do_analysis.R\")\n\n## make figures\nsource(\"./make_figures.R\")\nNot only is that easier for others to run, you now have a single script that makes your analytic workflow much clearer. Note that there are other ways to do something like this, such as using makefiles, though using only R works pretty well.\nHere are some replication examples for projects that I’ve done:\n\nSkinner, B. T., & Doyle, W. R. (2021). Do civic returns to higher education differ across subpopulations? An analysis using propensity forests. Journal of Education Finance, 46(4), 519–562. (Replication files)\nSkinner, B. T. (2019). Choosing College in the 2000s: An Updated Analysis Using the Conditional Logistic Choice Model. Research in Higher Education, 60(2), 153–183. (Replication files)\nSkinner, B. T. (2019). Making the connection: Broadband access and online course enrollment at public open admissions institutions. Research in Higher Education, 60(7), 960–999. (Replication files)\n\nYou can find the rest on my website as well as on my GitHub profile. As you complete projects and create replication scripts, be sure to make them accessible. Not only is it good for the field, it also signals strong transparency and credibility on your part as a researcher.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#testing",
    "href": "x-04-skinner-phil.html#testing",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Testing",
    "text": "Testing\nAs you wrangle your data, you should take advantage of your computer’s power not only to perform the munging tasks you require, but also to test that inputs and outputs are as expected.\n\nExpected data values\nLet’s say you are investigating how college students use their time. You have data from a time use study in which 3,000 students recorded how they spent every hour for a week. The data come to you at the student-week level and grouped by activity type. This means you see that Student A spent 15 hours working, 20 hours studying/doing homework, etc. You create a new column that adds the time for all categories together and take the summary. You find:\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\ntotal_hrs\n167.99\n1.93\n152.48\n183.46\n\n\n\nSomething is wrong.\n\nQuick question\nWhat’s wrong?\n\nRather than going through an ad-hoc set of “ocular” checks, you can build in more formal checks. We’ve done a little of this when using identical() or all.equal() to compare data frames. You could build these into your scripts. For example:\n\nsumming activity hours in a week, check if any exceed the total number of hours that are possible\nreading in data with student GPAs, check if they are negative\ncombining data from 100 students at 5 schools, make sure your final data frame has 500 rows\ncreating a new percentage from two values, check that the value between 0 and 100 (or between 0 and 1 if a proportion)\n\nYou can use simple tests to check, and include a stop() command inside an if() statement:\n## halt if max number of months is greater than 12 since data are only\n## for one year\nif (max(length(months)) &gt; 12) {\n    stop(\"Reported months greater than 12! Check!\")\n}\nThis only works if you know your data. Knowing your data requires both domain knowledge (“Why are summer enrollments so much higher than fall enrollments…that doesn’t make sense.”) and particular knowledge about your data (“All values of x between 50 and 100, except these two, which are -1 and 20000…something’s up”).\n\n\nExpected output from functions\nProgrammers are big on unit testing their functions. This simply means giving their functions or applications very simple input that should correspond to straightforward output and confirming that that is the case.\nFor example, if their function could have three types of output depending on the input, they run at least three tests, one for each type of output they expect to see. In practice, they might run many more tests for all kinds of conditions — even those that should “break” the code to make sure the proper errors or warnings are returned.\nAs data analysts, you are unlikely to need to conduct a full testing regime for each project. But if you write your own functions, you should definitely stress test them a bit. This testing shouldn’t be in your primary script but rather in another script, perhaps called tests.R. These can be informal tests such as\n## should return TRUE\n(function_to_return_1() == 1)\nor more sophisticated tests using R’s testthat library.\nAgain, the point isn’t that you spend all your time writing unit tests. But spending just a bit of time checking that your functions do what you think they do will go a long way.\n\nQuick question\n\n\nImagine you’ve written your own version of scale(), which lets you normalize a set of numbers to N(0,1):\nmy_scale &lt;- function(x) {\n    ## get mean\n    mean_x &lt;- mean(x)\n    ## set sd\n    sd_x &lt;- sd(x)\n    ## return scaled values\n    ##             x - mean_x\n    ## scaled_x = ------------\n    ##                sd_x\n    return((x - mean_x) / sd_x)\n}\nWhat kind of tests do you think you should run, i.e., what kind of inputs should you give it and what should you expect?\n\n\n\nFail loudly and awkwardly\nThere is a notion in software development that a program should fail gracefully, which means that when a problem occurs, the program should at the very least give the user a helpful error message rather than just stopping without note or looping forever until the machine halts and catches fire.\nAs data analysts, I want your scripts to do the opposite: I want your code to exit loudly and awkwardly if there’s a problem. You aren’t writing a program but rather an analysis. Therefore, the UX I care about is the reader of your final results and your future replicators. Rather than moving forward silently when something strange happens — which is lovely on the front end because everything keeps running but deadly on the back end — your code should stop and yell at you when something unexpected happens. It’s annoying, but that’s the point.",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#four-stages-of-data-a-taxonomy",
    "href": "x-04-skinner-phil.html#four-stages-of-data-a-taxonomy",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Four stages of data: a taxonomy",
    "text": "Four stages of data: a taxonomy\nIn the lessons so far, we’ve generally read in raw data, done some processing that we printed to the console, and then finished. This is fine for our small lessons and assignments, but in your own work, you will often want to save your changes to the data along the way — that is, save updated data files at discrete points in your data analysis. This is particularly true for large projects in which wrangling your data is distinct from its analysis and you don’t want to run each part every time.\nBroadly, there are four stages of data:\n\nRaw data\nBuilt data (objective changes)\nClean data (subjective changes)\nAnalysis data\n\nFor small projects, you may not end up with data files for each stage. That said, your workflow should be organized in terms of moving from one stage to the next.\nWe’ll talk about each below.\nACKNOWLEDGMENT I want to give a quick hat tip to two people who’ve really helped me clarify my thinking about these stages of data, Will Doyle and Sally Hudson: Will for first showing the importance of keeping raw data untouched; and Sally for showing how useful it is to make a distinction between objective and subjective data processing, particularly in large projects. Thanks and credit to both.\n\nRaw data\nRaw data, that is, the data you scrape, download, receive from an advisor or research partner, etc, are sacrosanct. Do not change them! They are to be read into R (or whatever software you use) and that is all.\nYou will be tempted from time to time to just fix this one thing that is wonky about the data. Maybe you have a *.csv file that has a bunch of junk notes written above the actual data table (not uncommon). The temptation will be to delete those rows so you have a clean file. NO! There are ways to skip rows in a file like this, for example. R is particularly good at reading in a large number of data types with a large number of special situations.\nIf you are at all concerned that you or a collaborator may accidentally save over your raw files, your operating system will allow you to change the file permissions such that they are read-only files (Dropbox will let you protect files in this way, too). Particularly if working with a number of collaborators, protecting your raw data files in this way is a good idea.\n\n\nBuilt data (objective changes)\nAs a first step in processing your data, you should only make objective modifications. What do I mean by objective? They are those modifications that:\n\nAre required to get your data in good working order\nAre modifications that will always be needed across projects using the same data (in the context of multiple projects)\n\nTo the first point, let’s say you receive student-level administrative data that includes a column, state, that is the student’s home state. However, values in the column include:\n\n\"Kentucky\"\n\"KY\"\n\"Ken\"\n\"21\" (state FIPS code)\n\nAnother example would be a date column, month, in which you see:\n\n\"February\"\n\"Feb\"\n\"2\"\n\nIn both cases, it’s clear that you want a consistent format for the data. Whatever you pick, these are objective changes that you would make at this time.\nTo the second point, let’s say this same administrative data are going to be used across multiple projects. In each project, you need to compute the proportion of in-state vs. out-of-state students. The data column that indicates this is called residency and is coded (with no data labels) as\n\n1 := in-state\n2 := out-of-state\n\nComing back to the data later and taking a look at the first few rows in residency, will it be clear\n\nwhat residency means?\nwhat the values of 1 and 2 represent?\n\nNo.\nAn objective change that you can use across analyses will be to make a new column called instate in which\n\n0 := out-of-state\n1 := in-state\n\nThe benefits are three-fold:\n\n0 and 1 are logically coded as FALSE and TRUE\ninstate gives a direction to the 0s and 1s: 1 means in-state\ntaking the mean of instate will give you the proportion of in-state students in your data\n\nIf your data or project (or number of projects) is large, you many want to save data files after your objective wrangling. These can be reused across projects, which will make sure that objective data wrangling decisions are consistent.\n\n\nClean data (subjective changes)\nWhatever changes are left over after you’ve make objective changes are subjective changes. These changes are project and analysis specific.\nFor example, let’s say your student-level administrative data have information about the student’s expected family contribution or EFC. In your data, you have the actual values. However, you want to compare your data to other published research in which EFC is binned into discrete categories. This means that in order to make a comparison, you have to bin your data the same way. This is a subjective task for two reasons:\n\nthere’s not an objective reason to bin your data in this manner, i.e. to make the data consistent or clearer\nit’s unlikely you’ll need this new column of data for other projects\n\nSome other subjective changes you might make to a data set:\n\ncreate an indicator variable that equals 1 when a student’s family income is below $35,000/year (why $30k? why not $25k or $35k?)\nconvert institutional enrollments to the log scale (do you always need to do this?)\njoin in data on the unemployment rate in the student’s county (do you always need this information at the county level? at all?)\n\nBut wait! Maybe you do want to use the binned value of EFC in multiple projects — perhaps the binned categories are standard across the literature — or the other examples appear always useful and otherwise standard. Isn’t that objective?\nMaybe!\nThe thing about determining the difference between objective vs subjective cleaning is that on the margins, well, it’s subjective! The point is not to have hard and fast rules, but rather to do your best to clearly separate those data wrangling tasks that must be done for the sake of consistency and clarity versus those that may change depending on your research question(s).\n\nQuick question\nWhat might be some other subjective data wrangling tasks? Think about other research you’ve conducted or seen. Or think about what our lessons and assignments so far: what have been subjective tasks?\n\n\n\nAnalysis data\nFinally, analysis data are data that are properly structured for the analysis you want to run (or the table or figure you want to make). In many cases, your analysis data set will be the same as your clean data set:\n\nit’s in the shape you want (rectangular, with observations in rows and variables in columns)\nit has all the variables you need to compute summary statistics or run regression models\n\nMost R statistical routines (e.g., mean(), sd(), lm()) do a fair amount of data wrangling under the hood to make sure it can do what you ask. For example, you may have missing values (NA) in your clean data set, but R’s linear model function, lm(), will just drop observations with missing values by default. If you are okay with that in your analysis, R can handle it.\nHowever, for some special models or figures, you may need to reshape or pre-process your clean data in one last step before running the specific analysis you want. For example, your clean data frame may have a column of parental occupational field (parocc) as a categorical variable in which the first few categories are\n\n11 := Management Occupations\n13 := Business and Financial Operations Occupations\n15 := Computer and Mathematical Occupations\n17 := Architecture and Engineering Occupations\n\n…and so on. You want to run an analysis in which you can’t use this categorical variable but instead need a vector of dummy variables, that is, converting this single column into a bunch of columns that only take on 0/1 values if parocc equals that occupation:\n\nmanage_occ == 1 if parocc == 11 else manage_occ == 0\nbusfin_occ == 1 if parocc == 13 else busfin_occ == 0\ncommat_occ == 1 if parocc == 15 else commat_occ == 0\narceng_occ == 1 if parocc == 17 else arceng_occ == 0\n\n…and so on. Particularly if\n\nyour data are large\nthe analysis takes a while to run\nyou want to use the same data set up for multiple (but not all) models or analytic tasks\n\nyou may want to save this particular analysis data set.\nAs with the difference between objective and subjective data, the difference between subjective and analysis data does not align to a set of hard and fast rules. That’s okay! The main point is that as a data analyst, you make data wrangling decisions thinking about how you will go from raw to objective to subjective to analysis data in a logical and clear manner. Collaborators, replicators, and future you will appreciate it!",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-04-skinner-phil.html#recommended-texts",
    "href": "x-04-skinner-phil.html#recommended-texts",
    "title": "IV: Skinner’s Philosophy of Data Wrangling",
    "section": "Recommended texts",
    "text": "Recommended texts\nThere are a number of great texts that talk about the process of data wrangling. Two that have particularly informed this lecture and are worth looking at more fully:\n\nThe Pragmatic Programmer: From Journeyman to Master by Andrew Hunt and David Thomas\nCode and Data for the Social Sciences: A Practitioner’s Guide by Matthew Gentzkow and Jesse Shapiro",
    "crumbs": [
      "Extra Credit",
      "IV: Skinner's Philosophy of Data Wrangling"
    ]
  },
  {
    "objectID": "x-02-dload-ipeds.html#option-one-downloadipeds.r-by-dr.-skinner",
    "href": "x-02-dload-ipeds.html#option-one-downloadipeds.r-by-dr.-skinner",
    "title": "II: Skinner’s Download IPEDS",
    "section": "Option One: downloadipeds.R by Dr. Skinner",
    "text": "Option One: downloadipeds.R by Dr. Skinner\n\nDr. Skinner, the original instructor of the class, wrote an R script that downloads IPEDS files directly to a project folder\n\nI actually used this for my final project when I took the class with him\nDoing this enables true “push button reproducibility” of your report\n\nAs in, all I need to do to replicate your report is hit “render”, no pre-downloading of the data required\n\n\nDownload IPEDS is an R-script available on Dr. Skinner’s personal website, here\nThat’s all the instruction you get for this lesson, Dr. Skinner’s site should be enough to work out the rest. Turn to the assignment tab to see how to earn the extra credit points\n\n\nOption Two: ipeDTAs.do by Matt\n\nBuilding off Dr. Skinner’s work, I have written a Stata-ified version, which goes one step further creating labeled .dta versions of the IPEDS files you download\n\nYou can find it here: https://github.com/ttalVlatt/ipeDTAs\n\nUnfortunately, this does require Stata to run, and currently does not work on UF Apps (I have a ticket in with UFIT to try and address this)\nAgain, the documentation should be sufficient to work out how to run it\n\nThe purpose of this limited instruction is to push you a little into playing with resources you find in the wild!",
    "crumbs": [
      "Extra Credit",
      "II: Skinner's Download IPEDS"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\n\nEmail: melvinjtanner@ufl.edu\nOffice Hours: Wednesday 12pm - 1pm and by appointment\nOffice Location: Tigert Hall 355\n\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-P\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2033\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit students also can sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account\n\n\n\n\n\n\nClass Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan-09\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-16\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan-23\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan-30\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-6\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-13\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-20\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb-27\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-5\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n10\nMar-12\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar-19\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n12\nMar-26\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr-2\n\nMethods & Applications\n\n\nAssignment 10\n\n\n\n14\nApr-9\n\nModeling Basics\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr-16\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4/5\n\n0/5 not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment, get 2.5/5 with a chance to improve to 4/5 than miss the deadline\n\n\n\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nTwo or more unaccetpable absences could negatively affect your participation grade. Acceptable reasons for absences include illness, serious family emergency, professional conferences, severe weather conditions, religious holidays, and other university-approved reasons. Please let us know if you unable to make class for any reason. Due to the nature of the class and the need for trouble-shooting on your computer, Zoom attendance is not typically an option.\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nMatt’s office hours or email\n\nTrying the above steps first really helps me help you\n\nI’d probably start by going through them anyway\n\nI rarely will give direct answers, I just help you think through the issue\n\n\nNote: As one of the main purposes of this class is to teach you the basics of R programming, the use of AI-based coding tools (such as ChatGPT, GitHub Co-Pilot, Google Bard, etc.) is not permitted.\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#spring-2024",
    "href": "syllabus.html#spring-2024",
    "title": "EDH 7916: Contemporary Research in Higher Education",
    "section": "",
    "text": "Instructor: Melvin J. Tanner, Ph.D.\n\nEmail: melvinjtanner@ufl.edu\nOffice Hours: Wednesday 12pm - 1pm and by appointment\nOffice Location: Tigert Hall 355\n\nTA & R-Instructor: Matt Capaldi\n\nEmail: m.capaldi@ufl.edu\nOffice Hours: Thursday 2pm - 4pm\nOffice Location: Norman Hall 2705-P\n\nClass Meeting Time: Tuesday 5:10pm - 8:10pm\nClass Location: NRN 2033\n\n\n\n\nContemporary higher education researchers have a wide variety of quantitative tools at their disposal. Yet as the number and sophistication of these tools grows, so too do expectations about the quality of final analyses. Furthermore, increasing scrutiny of non-replicable results demands that researchers follow a proper workflow to mitigate errors. In this course, students will learn the fundamentals of a quantitative research workflow and apply these lessons using common open-source tools. We will cover key skills for crafting reports & publications including project organization, data wrangling/cleaning, and data visualization. Throughout, students will use coding best-practices so that their workflow may be shared and easily reproduced.\n\n\n\nStudents will learn\n\nBasic data management principles & skills needed for contemporary research in higher education\nHow to create reproducible research as increasingly required in contemporary higher education research\nA foundational knowledge of the R, R Studio, & Quarto\n\n\n\n\n\n\n\nNo required textbook\nCourse website capaldi.info/7916 contains all R required reading\nDiscussion section readings can be found in Canvas\n\n\n\n\n\nThere are numerous online resources to help learn R, one of the most comprehensive being R for data science, which is available online for free\n\n\n\n\n\nStudents will be expected to bring a laptop to class. It does not matter whether the machine runs MacOS, Windows, or Linux; however, the student’s machine needs to be relatively up to date and in good running order. It needs to be able to connect to the internet during class. All software is freely available to UF students. Students need to download and install the following software on their machines:\n\nR : cran.r-project.org\n\nNOTE: if you have installed R on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\nYou will also be required to install a number of R packages throughout the course\n\nRStudio : posit.co/download/rstudio-desktop/\n\nNOTE: if you have installed RStudio on your machine in the past, make sure that you have the most up-to-date version (new versions are released about once a quarter)\n\ngit : git-scm.com\n\nOptional for extra-credit students also can sign up for a free GitHub account if they haven’t already: github.com/join\n\nStudents should sign up using their University of Florida email address and request a Education discount at education.github.com/benefits\n\n\nMicrosoft Office: https://cloud.it.ufl.edu/collaboration-tools/office-365/\n\nOffice 365 is free to all UF students, simply log in with your UF account\n\n\n\n\n\n\nClass Schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson(s)\nDue (Sunday 11:59pm)\n\n\n\n\n1\nJan-09\n\nWelcome\nSyllabus\nInstalling R & R Studio\n\nNone\n\n\n2\nJan-16\n\nExcel Basics & Limitations\nReading Data Into R\nIntro to IPEDS\n\n\nAssignment 1\n\n\n\n3\nJan-23\n\nData Wrangling I\n\n\nAssignment 2\n\n\n\n4\nJan-30\n\nData Wrangling II\n\n\nAssignment 3\n\n\n\n5\nFeb-6\n\nData Visualization I\n\n\nAssignment 4\nReproducible Report: Proposal\n\n\n\n6\nFeb-13\n\nData Visualization II\n\n\nAssignment 5 (Graph replication challenge)\n\n\n\n7\nFeb-20\n\nIntroduction to Quarto\n\n\nAssignment 6\n\n\n\n8\nFeb-27\n\nData Wrangling III\n\n\nAssignment 7\n\n\n\n9\nMar-5\n\nFunctional Programming\n\n\nAssignment 8\n\n\n\n10\nMar-12\nSpring Break\nNone, enjoy the break!\n\n\n11\nMar-19\n\nData Wrangling IV\n\n\nReproducible Report: Initial Analysis\n\n\n\n12\nMar-26\n\nData Visualization III\n\n\nAssignment 9\n\n\n\n13\nApr-2\n\nMethods & Applications\n\n\nAssignment 10\n\n\n\n14\nApr-9\n\nModeling Basics\n\n\nReproducible Report: Draft (Optional)\n\n\n\n15\nApr-16\n\nReproducible Report: Presentations\nReproducible Report: Lab Time\n\n\nReproducible Report: Presentation\n\n\n\n16\nApr-23\n\nCourse Summary\nReproducible Report: Lab Time\n\n\nReproducible Report\nExtra-Credit Assignments (Optional)\n\n\n\n\n\n\n\nThere are a total of 100 points for the class, plus extra credit opportunities.\n\n\nThere are 10 lesson assignments (see assignment tab on each lesson for details and Canvas for due dates), each worth 5 points. Assignments are always due by Sunday 11:59pm following the lesson.\nYou will receive one of the following grades\n\n5/5 everything is correct\n4.5/5 mostly correct, concepts are all understood, but slight error(s)\n2.5/5 at least one significant error, please re-submit\n\nYou will have the chance to revise and resubmit the following week with corrections to get a 4/5\n\n0/5 not turned in on time (unless excused)\n\nYou will have the chance to submit the following week for 2.5/5\n\nIf you are struggling and haven’t been able to complete the assignment, it is far better to turn in an incomplete assignment, get 2.5/5 with a chance to improve to 4/5 than miss the deadline\n\n\n\nThis grading system is designed to encourage you to revisit concepts that didn’t click the first time, not to be punitive. There are opportunities for extra credit to make up for lost points.\n\n\n\nYour final project grade comes from four elements\n\nProposal: 5 points\nInitial analysis: 10 points\nPresentation: 5 points\nReport: 20 points\n\nOptional: You can submit a draft of the report for a preliminary grade and feedback on how to improve (see schedule and/or Canvas for due date)\n\n\n\n\n\nWe will use class time to work through lesson modules together. Students are expected to follow along with the presentation and run code on their own machine. Students are also expected to answer questions, participate in discussions, and work through example problems throughout the class session.\nTwo or more unaccetpable absences could negatively affect your participation grade. Acceptable reasons for absences include illness, serious family emergency, professional conferences, severe weather conditions, religious holidays, and other university-approved reasons. Please let us know if you unable to make class for any reason. Due to the nature of the class and the need for trouble-shooting on your computer, Zoom attendance is not typically an option.\n\n\n\nOn the class website you will see lessons titled “Extra: …” which are opportunities for extra credit.\n\nLesson follow the same structure as class lessons, review the lesson content then complete tasks in the assignment tab\n\nTime permitting, some extra credit lessons may be completed in class, assignments will still count for extra credit\n\nEach extra credit assignment is worth 2.5 points\n\nSimply make a good faith effort to complete the assignment and you will receive the points\n\nAll extra credit assignments are due during week 16 (see schedule and/or Canvas for due date)\n\n\n\n\n\nGrading Scale\n\n\nGrade\nScore\n\n\n\n\nA\n93 or Above\n\n\nA-\n90 to 92.5\n\n\nB+\n87.5 to 89.5\n\n\nB\n83 to 87\n\n\nB-\n80 to 82.5\n\n\nC+\n77.5 to 79.5\n\n\nC\n73 to 77\n\n\nC-\n70 to 72.5\n\n\nD+\n67.5 to 69.5\n\n\nD\n63 to 67\n\n\nD-\n60 to 62.5\n\n\nE\nBelow 60\n\n\n\n\n\n\n\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nMatt’s office hours or email\n\nTrying the above steps first really helps me help you\n\nI’d probably start by going through them anyway\n\nI rarely will give direct answers, I just help you think through the issue\n\n\nNote: As one of the main purposes of this class is to teach you the basics of R programming, the use of AI-based coding tools (such as ChatGPT, GitHub Co-Pilot, Google Bard, etc.) is not permitted.\n\n\n\nSee UF Graduate School policies on grading, attendance, academic integrity, and more.\n\n\n\nUF students are bound by The Honor Pledge which states,\n\nWe, the members of the University of Florida community, pledge to hold ourselves and our peers to the highest standards of honor and integrity by abiding by the Honor Code. On all work submitted for credit by students at the University of Florida, the following pledge is either required or implied: “On my honor, I have neither given nor received unauthorized aid in doing this assignment.”\n\nThe Honor Code specifies a number of behaviors that are in violation of this code and the possible sanctions. Furthermore, you are obligated to report any condition that facilitates academic misconduct to appropriate personnel. If you have any questions or concerns, please consult with the instructor or TAs in this class.\n\n\n\nStudents with disabilities who experience learning barriers and would like to request academic accommodations should connect with the disability Resource Center. It is important for students to share their accommodation letter with their instructor and discuss their access needs, as early as possible in the semester.\n\n\n\nStudents are expected to provide professional and respectful feedback on the quality of instruction in this course by completing course evaluations online via GatorEvals. Guidance on how to give feedback in a professional and respectful manner is available here. Students will be notified when the evaluation period opens, and can complete evaluations through the email they receive from GatorEvals, in their Canvas course menu under GatorEvals, or here. Summaries of course evaluation results are available to students here.\n\n\n\nStudents are allowed to record video or audio of class lectures. However, the purposes for which these recordings may be used are strictly controlled. The only allowable purposes are (1) for personal educational use, (2) in connection with a complaint to the university, or (3) as evidence in, or in preparation for, a criminal or civil proceeding. All other purposes are prohibited. Specifically, students may not publish recorded lectures without the written consent of the instructor.\nA “class lecture” is an educational presentation intended to inform or teach enrolled students about a particular subject, including any instructor-led discussions that form part of the presentation, and delivered by any instructor hired or appointed by the University, or by a guest instructor, as part of a University of Florida course. A class lecture does not include lab sessions, student presentations, clinical presentations such as patient history, academic exercises involving solely student participation, assessments (quizzes, tests, exams), field trips, private conversations between students in the class or between a student and the faculty or lecturer during a class session.\nPublication without permission of the instructor is prohibited. To “publish” means to share, transmit, circulate, distribute, or provide access to a recording, regardless of format or medium, to an- other person (or persons), including but not limited to another student within the same class section. Additionally, a recording, or transcript of a recording, is considered published if it is posted on or uploaded to, in whole or in part, any media platform, including but not limited to social media, book, magazine, newspaper, leaflet, or third party note/tutoring services. A student who publishes a recording without written consent may be subject to a civil cause of action instituted by a person injured by the publication and/or discipline under UF Regulation 4.040 Student Honor Code and Student Conduct Code.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "99-final.html",
    "href": "99-final.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project for this class is to create a truly “reproducible report” on a topic of your choosing related to higher education\n\nThe topic can really be almost anything of interest related to higher ed, so long as you can find public data to use\n\nYour report should be 3-5 pages including multiple graphs and visual elements (i.e., not too much text)\n\nYour goal is something like what you might hand a senior administrator at your university to summarize a trend/issue/topic\n\nYou will likely only have a handful of citations\nYou should devote around half your page space to data visualizations and tables\n\n\nThe primary focus of this report is reproducibility\n\nYour data must be publicly available with no IRB restrictions, as you will not submit it, I will go and collect it (unless you download it as part of the project code)\n\n\n\nProposal (5 points)Initial Analyses (10 points)Presentation (5 Points)Final Report (20 Points)\n\n\nThis assignment should be submitted as a text entry directly on Canvas consisting of;\n\nA paragraph describing your project:\n\nWhat will you be investigating/exploring/predicting?\nWhy is it interesting?\n\nA description of where you will find this data.\nA few lines describing your main outcome variable in detail\n\nHow is it coded/what scale is it on?\nHow can you interpret it?\n\n\nThis assignment is worth 5 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\nNOTE: For your initial analyses, the most important thing is that you submit code that sources/renders in full, this assignment will not be successfully completed until that happens, you may have to resubmit multiple times.\nSubmit the following in either a cleanly formatted R (.R) script or Quarto (.qmd) file:\n\nComments (if an R script) or text (if a Quarto script) that describe\n\nWhere your data is from (a link is preferable)\nHow to download it\nWhere to save it in order for your code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nCode that:\n\nReads in the data set that contains (at least) your dependent variable (must read in EXACTLY what downloads from following your instructions above)\nIf appropriate\n\nConverts missing values to NA\nReshapes the data wider or longer\nJoins in additional data files\n\n\nCode that creates at least three of the following:\n\nA plot that shows the overall distribution of your dependent/outcome variable\n\nHint: A histogram or density plot might be a good option here\n\nA plot that shows the distribution of your dependent/outcome variable grouped by a variable in your data\n\nHint: A histogram or density plot with fill might be a good option here\n\nA plot that shows the median, interquartile range, and potential outliers of your dependent/outcome variable grouped by a variable in your data\n\nHint: A box plot with x and/or fill might be a good option here\n\nA plot that shows how your dependent/outcome variable changes by another continuous variable in your data\n\nHint: A scatter plot might be a good option here\n\n\n\nThis assignment is worth 10 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\n\nYou will present the results of your report in class during the penultimate week of the semester (see date in Canvas)\nThe presentation format is up to you, previous students have\n\nPresented an image of one figure they created\nCreated a short PowerPoint presentation\nCreated presentations using Quarto\n\nThe primary rule for this presentation is that it is to be 3-5 mins long (read min 3 mins, max 5 mins, ideal 4 mins)\n\nAs this is meant to replicate the you presenting your report to senior administrators, this is a hard time-limit, you will be stopped if you go over 5 mins\n\n\nThis assignment is worth 5 points, your grade will be determined by:\n\n3 points: Did you present a plot/table/finding from your report that tells a story about your topic?\n1 point: Did you present the information in a professional and engaging manner?\n1 point: Did you finish within the allotted time limit of 3-5 mins?\n\n\n\n\nYour final report is a 3-5 page (single-spaced, not including citations) document that summarizes the analysis you have done using plenty of figures and summary tables along the way\n\nThis is NOT a traditional academic paper, it is meant to be concise report intended for a university administrator or policymaker\n5 pages is a hard limit, anything beyond the 5th page won’t be graded\nNOTE: You can submit a draft (for feedback only) by the due date on Canvas\n\nYou will submit the report as Quarto (.qmd) file\n\nThe output format: should be either docx, pdf (traditional way using LaTeX), or typst (brand new way to create a .pdf)\n\nI would strongly recommend docx for most students\n\nSee the Quarto documentation for word output\n\nIf you’re feeling more adventurous, have a go with typst\n\nSee the Quarto documentation for typst output\nYou’ll need Quarto 1.4, which came out after the start of the class, see me if you need help installing it\n\nIf you want to cause yourself unnecessary misery and frustration by doing it the old fashioned way, use pdf\n\nSee the Quarto documentation for pdf output\n\n\nOptional: If your analysis code becomes long, you might want to submit accompanying .R scripts that are source()-ed as discussed in the Quarto Lesson\n\n\n\nRequired Report Content\n\nBefore the introduction of your document, a section called “Instructions to Run” that states\n\nWhere your data is from (a link if preferable)\nHow to download it\nWhere to save it in order for the code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nWell commented code (either in the .qmd file or a source()-ed .R script) that:\n\nReads in all your raw data (must read in EXACTLY what downloads from following your instructions above)\nPerforms all data wrangling tasks to clean, join, and reshape your data as necessary for your project\nCreates:\n\nRequired: 3 or more plots with ggplot2\nRequired: 1 or more overall descriptive statistics table(s) made with summarize\nRequired: 1 or more other summary table(s) made with summarize\nOptional: Basic statistics like t.test() or lm()\n\n\nWritten text that should be clearly structured with subheadings and describe:\n\nWhy this is interesting and/or important\n\nThis should be a single concise but convincing paragraph, think an “elevator pitch” argument as to why this matters\nThere should NOT be any lengthy literature review in this assignment, this is it\n\nWhy your chose your data source and what the data represents\nWhat analysis you did and why, in layman’s terms (not an R or stats expert)\nWhat each individual plot and table shows\nWhat you found overall\nAny limitations or future research\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nReport Element\nCriteria\nPoints\n\n\n\n\nDoes it run?\n\nIf I hit render, does the code run and produce the output report without errors?\n\n4\n\n\nData Wrangling: Reading & Cleaning\n\nAre the “instructions to run” provided and correct?\nIs the data read in correctly?\nIs the data joined and/or reshaped correctly where necessary?\nIs the data cleaned where necessary with reasonable justification provided for subjective decisions?\n\n4\n\n\nData Wrangling: Analysis\n\nIs there at least 1 overall descriptive statistics summarize table in the report?\nIs there at least 1 additional summarize table in the report?\nDo the summary tables show interesting and relevant information to the report topic?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nData Visualization\n\nAre there at least 3 ggplot2 plots in the report?\nDo the plots show interesting and relevant information to the report topic?\nAre the plots aesthetically pleasing with good plot design, color choices, and labels?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nWritten Content\n\nDoes the written content address the points outlined above?\nDoes the report make a convincing argument?\nIs the text of the report generally well written and in layman’s terms?\nIs the text of the report well structured with subheadings?\n\n4",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "11-pro-functions.html#review-of-assignment--",
    "href": "11-pro-functions.html#review-of-assignment--",
    "title": "I: Functions & Loops",
    "section": "Review of Assignment <-",
    "text": "Review of Assignment &lt;-\n\nIn essence, both of these skills are built off something we have been doing this whole class, assignment\nWe have assigned data\n\n\ndf &lt;- haven::read_dta(\"data/hsls-small.dta\")\n\n\nWe have assigned plots\n\n\nplot &lt;- ggplot(df) +\n  geom_histogram(aes(x = x1txmtscor))\n\n\nWe have assigned summary tables\n\n\ndf_sum &lt;- df |&gt;\n  summarize(mean = mean(x1txmtscor, na.rm = T))\n\n\nWe even assigned results\n\n\nuf_age &lt;- 2024 - 1853\n\n\nEverything we are going to cover today comes back to this basic principle, things being assigned names\n\nWe will be working with more than one assigned or named thing at once, which gets confusing at first, but it always comes back to this idea",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#why-write-loops-functions-dry-vs-wet-code",
    "href": "11-pro-functions.html#why-write-loops-functions-dry-vs-wet-code",
    "title": "I: Functions & Loops",
    "section": "Why Write Loops & Functions: DRY vs WET Code",
    "text": "Why Write Loops & Functions: DRY vs WET Code\nThe watchwords for this lesson are DRY vs WET:\n\nDRY: Don’t repeat yourself\nWET: Write every time\n\nLet’s say you have a three-step analysis process for 20 files (read, lower names, add a column). Under a WET programming paradigm in which each command gets its own line of code, that’s 60 lines of code. If the number of your files grows to 50, that’s now 150 lines of code — for just three tasks! When you write every time, you not only make your code longer and harder to parse, you also increase the likelihood that your code will contain bugs while simultaneously decreasing its scalability.\nIf you need to repeat an analytic task (which may be a set of commands), then it’s better to have one statement of that process that you repeat, perhaps in a loop or in a function. Don’t repeat yourself — say it once and have R repeat it for you!\nThe goal of DRY programming is not abstraction or slickness for its own sake. That runs counter to the clarity and replicability we’ve been working toward. Instead, we aspire to DRY code since it is more scalable and less buggy than WET code. To be clear, a function or loop can still have bugs, but the bugs it introduces are often the same across repetitions and fixed at a single point of error. That is, it’s typically easier to debug when the bug has a single root cause than when it could be anywhere in 150 similar but slightly different lines of code.\nAs we work through the lesson examples, keep in the back of your mind:\n\nWhat would this code look like if I wrote everything twice (WET)?\nHow does this DRY process not only reduce the number of lines of code, but also make my intent clearer?",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#setup",
    "href": "11-pro-functions.html#setup",
    "title": "I: Functions & Loops",
    "section": "Setup",
    "text": "Setup\nWe’ll use a combination of nonce data and the school test score data we’ve used in a past lesson. We won’t read in the school test score data until the last section, but we’ll continue following our good organizational practice by setting the directory paths at the top of our script.",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#for-loops",
    "href": "11-pro-functions.html#for-loops",
    "title": "I: Functions & Loops",
    "section": "for() Loops",
    "text": "for() Loops\n\nThe idea of loops if relatively simple\n\nTake a list of things\n\nfor(i in matts_list) {\n\nDo a set of things\n\nprint(i) }\n\n\nWait, but what’s i\n\ni is the most common word to use here, but we could call it anything\n\nIt is just the name we are assigning to the item (think i for item) in the list\n\n\nOkay, but what’s { and }\n\nSince we are doing one or more things for each i in the list\n\n\n\nmatts_list &lt;- c(\"Let's\", \"go\", \"Gators\", \"!\")\n\nfor(i in matts_list) { print(i) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nBut, we can use anything we want instead of i\n\n\nfor(word in matts_list) { print(word) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nLiterally anything\n\n\nfor(gator_egg in matts_list) { print(gator_egg) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nAll we are doing is assigning a name to the item in the list\nWe can do the name thing with numbers\n\n\ngators_points_23 &lt;- c(11, 49, 29, 22, 14, 38, 41, 20, 36, 35, 31, 15)\n\nfor(i in gators_points_23) { print(i) }\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nAgain, we can literally use anything as the name\n\n\nfor(billy_napier in gators_points_23) { print(billy_napier) }\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nQuick exercise\nCreate a list of the names of every school you’ve attended, then use a for loop to print them out",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#adding-if-and-else-to-loops",
    "href": "11-pro-functions.html#adding-if-and-else-to-loops",
    "title": "I: Functions & Loops",
    "section": "Adding if() and else() to Loops",
    "text": "Adding if() and else() to Loops\n\nLoops that print things are all well and good, but really we want to be able to do a little more than that\nWe are going to use if() and else() to that\n\nRemember ifelse() from Data Wrangling?\nThis is just splitting that up, if() something if true, do this, else() do that\n\nLet’s just start with an if()\n\n\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    print(i)\n  }\n}\n\n[1] 49\n[1] 38\n[1] 41\n[1] 36\n[1] 35\n[1] 31\n\n\n\nNotice we only got scores if they were above 30\nNext, we can add an else() to say what to do if the score was not above 30\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    print(i)\n  } else {\n    print(i)\n  }\n}\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nQuick Question: Is that the same list we had before? Why or why not?\n\n\nLet’s see how we can make it different\n\nWe are going to use a new command paste() which combines strings, then print() that\n\nTop tip: You’re going to want to use paste() in your homework as well\n\n\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    paste(\"Yay the Gators scored\", i, \"points, which is more than 30!\") |&gt; print()\n  } else {\n    print(i)\n  }\n}\n\n[1] 11\n[1] \"Yay the Gators scored 49 points, which is more than 30!\"\n[1] 29\n[1] 22\n[1] 14\n[1] \"Yay the Gators scored 38 points, which is more than 30!\"\n[1] \"Yay the Gators scored 41 points, which is more than 30!\"\n[1] 20\n[1] \"Yay the Gators scored 36 points, which is more than 30!\"\n[1] \"Yay the Gators scored 35 points, which is more than 30!\"\n[1] \"Yay the Gators scored 31 points, which is more than 30!\"\n[1] 15\n\n\n\nThen we can extend that same logic to the else() statement\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    paste(\"Yay, the Gators scored\", i, \"points, which is more than 30!\") |&gt; print()\n  } else {\n    paste(\"Sad times, the Gators only scored\", i, \"points...\") |&gt; print()\n  }\n}\n\n[1] \"Sad times, the Gators only scored 11 points...\"\n[1] \"Yay, the Gators scored 49 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 29 points...\"\n[1] \"Sad times, the Gators only scored 22 points...\"\n[1] \"Sad times, the Gators only scored 14 points...\"\n[1] \"Yay, the Gators scored 38 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 41 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 20 points...\"\n[1] \"Yay, the Gators scored 36 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 35 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 31 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 15 points...\"\n\n\n\nThis may seem a little silly right now, but this fun example was just meant to show the basics of we are doing\nWe will cover these in a more serious way at the end of the lesson",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#writing-your-own-functions",
    "href": "11-pro-functions.html#writing-your-own-functions",
    "title": "I: Functions & Loops",
    "section": "Writing Your Own Functions",
    "text": "Writing Your Own Functions\n\nFunctions work much the same way as loops, whatever we say inside { } is done\nThe difference is that instead of doing it for each item in a list, we do it for a single input\nWe also have to use the function for it to work\n\nWe have been using functions all semester filter(), summarize(), mutate() are all functions just like the one we are going to make\n\nTo demonstrate, let’s make a function that prints a welcome message for students arriving at UF\n\n\nwelcome &lt;- function() { print(\"Welcome to UF!\") }\n\nwelcome()\n\n[1] \"Welcome to UF!\"\n\n\n\nTo do this, we need some data, so I am just going to make some up\n\ntribble() is just a way of making a tidyverse data frame, don’t worry about it for now, it’s not the main idea for the lesson\n\n\n\nfake_data &lt;- tribble(~ufid, ~name, ~dorm, ~first_class, ~meal_plan, ~roommate,\n                     1853, \"Jack\", \"Cyprus\", \"BIO-1001\", 1, \"Mike\",\n                     1854, \"Hailey\", \"Simpson\", \"BIO-1001\", 0, \"Jessica\",\n                     1855, \"Tamika\", \"Simpson\", \"CHEM-1002\", 1, \"Hannah\",\n                     1856, \"Jessica\", \"Simpson\", \"ARCH-1003\", 1, \"Hailey\",\n                     1857, \"Mike\", \"Cyrpus\", \"STA-1002\", 0, \"Jack\",\n                     1858, \"Hannah\", \"Simpson\", \"EDF-1005\", 1, \"Tamika\")\n\n\nFor our function to be able to work, it needs to be able to take an input, in this case UFID\n\nYou can imagine a much more sophisticated version of this could be used for automated dorm check in\n\nLet’s run our same function again, but adding ufid to the brackets, saying that it takes ufid as the only input\n\n\nwelcome &lt;- function(id) { print(\"Welcome to UF!\") }\n\nwelcome()\n\n[1] \"Welcome to UF!\"\n\n\n\nQuick question: It ran, but why did this not change anything?\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  print(student)\n  \n}\n\nwelcome(1853)\n\n# A tibble: 1 × 6\n   ufid name  dorm   first_class meal_plan roommate\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;   \n1  1853 Jack  Cyprus BIO-1001            1 Mike    \n\n\n\nOkay so that ran, but it spat a data frame, how can we make it more of a welcome message?\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  name &lt;- student |&gt; pull(name)\n  \n  paste(\"Welcome to UF\", name)\n  \n}\n\nwelcome(1853)\n\n[1] \"Welcome to UF Jack\"\n\n\n\nOkay, now we’re getting somewhere!\nLet’s add a bit more info to say where they live and what their first class will be\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  name &lt;- student |&gt; pull(name)\n  dorm &lt;- student |&gt; pull(dorm)\n  first_class &lt;- student |&gt; pull(first_class)\n  \n  paste(\"Welcome to UF\", name, \"you will be living in\", dorm, \"and your first class is\", first_class)\n  \n}\n\nwelcome(1853)\n\n[1] \"Welcome to UF Jack you will be living in Cyprus and your first class is BIO-1001\"\n\n\n\nQuick exercise: Add to the above block of code, to also say who their roommate is",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#practical-example-batch-reading-files",
    "href": "11-pro-functions.html#practical-example-batch-reading-files",
    "title": "I: Functions & Loops",
    "section": "Practical Example: Batch Reading Files",
    "text": "Practical Example: Batch Reading Files\n\nRemember, to use a loop, we need a list to loop through\nTo get that, let’s use the list.files() function\n\nBy default, this will list files in the current working directory\nIf we want to list files in a different folder, we need to say where using a relative path from the working directory\n\nFor this, we want to list the by school files we used in Data Wrangling II, so we give it a path to that folder\n\nWe also include the argument full.names = TRUE\n\n\n\nQuick Exercise\nTry removing the full.names = TRUE see what the differences are, and think why we need to include it\n\n\nfiles &lt;- list.files(\"data/sch-test/by-school\",\n                    full.names = T)\n\n\nAs a starting point, let’s print out what this list looks like using a loop just like before\n\n\nfor(i in files) {\n  print(i)\n}\n\n[1] \"data/sch-test/by-school/bend-gate-1980.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1981.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1982.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1983.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1984.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1985.csv\"\n[1] \"data/sch-test/by-school/east-heights-1980.csv\"\n[1] \"data/sch-test/by-school/east-heights-1981.csv\"\n[1] \"data/sch-test/by-school/east-heights-1982.csv\"\n[1] \"data/sch-test/by-school/east-heights-1983.csv\"\n[1] \"data/sch-test/by-school/east-heights-1984.csv\"\n[1] \"data/sch-test/by-school/east-heights-1985.csv\"\n[1] \"data/sch-test/by-school/niagara-1980.csv\"\n[1] \"data/sch-test/by-school/niagara-1981.csv\"\n[1] \"data/sch-test/by-school/niagara-1982.csv\"\n[1] \"data/sch-test/by-school/niagara-1983.csv\"\n[1] \"data/sch-test/by-school/niagara-1984.csv\"\n[1] \"data/sch-test/by-school/niagara-1985.csv\"\n[1] \"data/sch-test/by-school/spottsville-1980.csv\"\n[1] \"data/sch-test/by-school/spottsville-1981.csv\"\n[1] \"data/sch-test/by-school/spottsville-1982.csv\"\n[1] \"data/sch-test/by-school/spottsville-1983.csv\"\n[1] \"data/sch-test/by-school/spottsville-1984.csv\"\n[1] \"data/sch-test/by-school/spottsville-1985.csv\"\n\n\n\nOkay, that looks like what we want, a list of files to read in\nNow let’s try reading these\n\n\nfor(i in files) {\n  read_csv(i)\n}\n\n\nOkay, that read the files, but we didn’t save them anywhere, we to assign them\nNow, this gets a little tricky, as anything we assign with &lt;- in a loop only exists in a loop, so we two extra steps\n\nFirst, we read it in mostly like normal, into something called file , which is temporary and only exists within the loop\nSecond, we make another temporary object, which is the name we want to assign the data frame to\n\nFor now, let’s just use df_&lt;name of the file&gt;\n\nThird, we use assign(), which basically does what &lt;- would do in normal circumstances, but keep the object after the loop\n\nname &lt;- read_csv() becomes file &lt;- read_csv() then assign(name, file)\n\n\n\n\nfor(i in files) {\n  file &lt;- read_csv(i)\n  name &lt;- paste0(\"df_\", i)\n  assign(name, file)\n}\n\n\nCool! That seems to have worked, but, our df_ names are really really long, which probably isn’t that useful for future analysis\nSo, instead of simply pasting together df_ and i, let’s use our friend regular expressions to get something more usable\n\nGet the school name (anything that matches the options we give)\nGet the year (any digits)\nPaste those together with df_ to get our nicer object names\n\n\n\nfor(i in files) {\n  school &lt;- str_extract(i, \"niagara|bend|east|spot\")\n  year &lt;- str_extract(i, \"\\\\d+\")\n  name &lt;- paste0(\"df_\", school, year)\n  file &lt;- read_csv(i)\n  assign(name, file)\n}\n\n\nMuch better!\nOne last thing, instead of reading each of those into a new data frame, we could bind them all together\n\nNote: This is only appropriate here due to the nature of the school data, with each data frame having the school name and year in it, other times we may want to join data instead. Review Data Wrangling II for a refresher on how to bind and join data appropriately\n\nTo do this, we first need to set up an empty data frame (a.k.a., “tibble”), as otherwise we will get an error the first time through the loop, as we would be attempting to bind something that doesn’t exist\nThen, we simply run the loop like before, but use bind_rows() to stack each file onto the existing list\n\n\ndf_bind &lt;- tibble()\n\nfor(i in files) {\n  file &lt;- read_csv(i)\n  df_bind &lt;- bind_rows(df_bind, file)\n}\n\n\nGreat, that worked!\nFinally, what if we wanted to this only for certain schools?\n\n\nHint: this might be useful in the homework\n\n\nWith a loop, we do something for each item in the list\nSo do something only for certain schools, we want to change the list, not the loop\n\nIn this case, we can add a pattern to our list.files() function saying to only list files that match that pattern\n\nThis could be some fancy regex, but in our case, we just need any files that have the word “niagara” it their name\n\n\n\n\nfiles_niagara &lt;- list.files(\"data/sch-test/by-school\",\n                            full.names = T,\n                            pattern = \"niagara\")\n\ndf_niagara &lt;- tibble()\n\nfor(i in files_niagara) {\n  file &lt;- read_csv(i)\n  df_niagara &lt;- bind_rows(df_niagara, file)\n}\n\n\nLet’s see what our final output looks like\n\n\nprint(df_niagara)\n\n# A tibble: 6 × 5\n  school   year  math  read science\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Niagara  1980   514   292     787\n2 Niagara  1981   499   268     762\n3 Niagara  1982   507   310     771\n4 Niagara  1983   497   301     814\n5 Niagara  1984   483   311     818\n6 Niagara  1985   489   275     805\n\n\n\nPerfect!",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#summary",
    "href": "11-pro-functions.html#summary",
    "title": "I: Functions & Loops",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson we have mostly print()-ed our output in this lesson, because it’s one of the easiest ways to see what’s going on\nBut, you can use functions and loops for other things too, like modifying variables, reading data, etc.\n\nThis said, whenever I am writing a loop, I usually start by print()-ing what I am looping through, then make is more sophisticated from there\n\nLoops and Functions were something that I didn’t fully grasp until I had been using R for about a year, so I don’t expect you to get everything\n\nWhile confusing, they are super useful in the real world\n\nYou might see some variations of the loops we learned today such as\n\nwhile() loops\nfor() loops that use the index (line) number of the list instead\nWhat’s important is that the underlying logic remains the same\n\nTime permitting, let’s take a look a few examples of my code that uses loops and functions!",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#solution",
    "href": "11-pro-functions.html#solution",
    "title": "I: Functions & Loops",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Functions & Loops Solution]\n##' [INIT: March 18 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\nfiles_to_read &lt;- list.files(\"data/sch-test/by-school\", # look in this folder\n                            full.names = TRUE, # we want to keep the full path, not just the file names\n                            pattern = \"bend|niagara\") # only list files that contain either \"bend\" or \"niagara\" \n\ndata &lt;- tibble() # create a blank tibble to store out data in\n\nfor(i in files_to_read) { # each loop through i becomes an item from the list of file path we created above)\n  \n  temp_data &lt;- read_csv(i) |&gt; # read in the file i \n    mutate(relative_path = i) # make a new variable relative_path that stores i (remember i is the path to the file, not the file itself)\n  \n  data &lt;- bind_rows(data, temp_data) # bind data and the temp_data we just read in\n\n}\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf &lt;- haven::read_dta(\"data/hsls-small.dta\")\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n##'[Without the optional part]\n\nid &lt;- 10007 # Tip: if you're debugging a function or loop, manually assign something to input values (for a function) or i (for a loop) and then you can test it\n\ndid_they_go &lt;- function(id) {\n  \n  student &lt;- df |&gt; filter(stu_id == id) # pull out the student id (so we can directly use the value)\n  college &lt;- student |&gt; pull(x4evratndclg) # pull out if the student attend college\n  \n  parent &lt;- student |&gt; pull(x1paredexpct) # pull out the parent student expectation\n  expect &lt;- if(is.na(parent)|parent == 11) { # if the parent expectation is either NA or 11\n    \"and we do not know if their parents wanted them to\" # assign this text to expect\n  } else if(parent &gt;= 5) { # otherwise if the parent expectation is 5 or above\n    \"and their parents expected them to\" # assign this text to expect\n  } else if(parent &lt; 5) { # otherwise if the parent expectation is less than 5\n    \"and their parents did not expect them to\" # assign this text to expect\n  }\n  \n  if(is.na(college)) { # if whether they went to college is missing\n    paste(\"We do not know if this student went to college\") # print this message\n  } else if(college == 1) { # if they went to college\n    paste(\"This student went to college\") # paste this message\n  } else if(college == 0) { # if they did not go to college\n    paste(\"This student never went to college\", expect) # paste this message\n    \n  }\n} \n\n\n## Test it using a for loop\ntest_ids &lt;- df |&gt; slice_head(n = 50) |&gt; pull(stu_id)\nfor(i in test_ids) { print(did_they_go(i)) }\n\n\n##'[With the optional part]\n\ndid_they_go &lt;- function(id) {\n  \n  student &lt;- df |&gt; filter(stu_id == id) # pull out the student id (so we can directly use the value)\n  college &lt;- student |&gt; pull(x4evratndclg) # pull out if the student attend college\n  \n  parent &lt;- student |&gt; pull(x1paredexpct) # pull out the parent student expectation\n  expect &lt;- if(is.na(parent)|parent == 11) { # if the parent expectation is either NA or 11\n    \"and we do not know if their parents wanted them to\" # assign this text to expect\n  } else if(parent &gt;= 5) { # otherwise if the parent expectation is 5 or above\n    \"and their parents expected them to\" # assign this text to expect\n  } else if(parent &lt; 5) { # otherwise if the parent expectation is less than 5\n    \"and their parents did not expect them to\" # assign this text to expect\n  }\n  \n  ## NEW PART\n  median_delay &lt;- df |&gt; summarize(median = median(x4hs2psmos, na.rm = T)) |&gt; pull(median) # summarize the median completion and pull the value out (notice we start with df not student)\n  delay &lt;- student |&gt; pull(x4hs2psmos) # pull out the students months delay\n  difference &lt;- delay - median_delay # calculate the difference between the median delay and the students delay\n  delay_statement &lt;- if(is.na(delay)) { # if the students delay was NA (note, it doesn't matter if they didn't go to college, as we only use this if they went)\n    \"and we do not know how long they delayed college\" # paste this message\n  } else if(delay == 0) { # if the students didn't delay going to college\n    \"and they did not delay attending college at all\" # paste this message\n  } else if(difference == 0) { # if the students delay was the median\n    paste(\"and they delayed college by\", delay, \"months which is the average amount of time\") # paste this, using the value delay from above\n  } else if(difference &lt; 0) { # if the students delay was below the median\n    paste(\"and they delayed college by\", delay, \"months which is\", abs(difference), \"months less than the average\") # paste this, using delay and abs(difference) which is the absolute value of the difference (i.e., the value regardless of positive vs negative)\n  } else if(difference &gt; 0) { # if the students delay was above the median\n    paste(\"and they delayed college by\", delay, \"months which is\", abs(difference), \"months above than the average\") # paste this, using delay and abs(difference) which is the absolute value of the difference (i.e., the value regardless of positive vs negative)\n  }\n  \n  if(is.na(college)) { # if whether they went to college is missing\n    paste(\"We do not know if this student went to college\") # print this message\n  } else if(college == 1) { # if they went to college\n    paste(\"This student went to college\", delay_statement) # paste this message NEW added delay_statement\n  } else if(college == 0) { # if they did not go to college\n    paste(\"This student never went to college\", expect) # paste this message\n    \n  }\n} \n\n## Test it using a for loop\ntest_ids &lt;- df |&gt; slice_head(n = 50) |&gt; pull(stu_id)\nfor(i in test_ids) { print(did_they_go(i)) }\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Programming",
      "I: Functions & Loops"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html#tidyverse-tricks",
    "href": "09-wrangle-iv.html#tidyverse-tricks",
    "title": "IV: Tidyverse Tricks & SQL",
    "section": "Tidyverse Tricks",
    "text": "Tidyverse Tricks\n\nNow you have a decent grasp on the core functions of tidyverse we can start exploring some helper functions that can make our lives much easier!\nThese commands have been chosen as I have personally found them wildly helpful in working with IPEDS and other data\n\nTo demonstrate, we are going to use IPEDS finance data files for 2018/19 school year\nNotice: IPEDS uses separate data files for public _f1a non-profit _f2 and for-profit _f3 colleges\n\n\n\ndf_18_pub &lt;- read_csv(file.path(\"data\", \"ipeds-finance\", \"f1819_f1a_rv.csv\"))\ndf_18_np &lt;- read_csv(file.path(\"data\", \"ipeds-finance\", \"f1819_f2_rv.csv\"))\ndf_18_fp &lt;- read_csv(file.path(\"data\", \"ipeds-finance\", \"f1819_f3_rv.csv\"))\n\n\nFirst, since there should be no college in more than one of these data files, and each college only has one row, we can bind_rows to stack each one on top of the other\n\nThen run a quick test to check no UNITID appears more than once (no duplicates)\n\n\n\ndf_18 &lt;- bind_rows(df_18_pub, df_18_np, df_18_fp)\n\ndf_18 |&gt;\n  count(UNITID) |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: UNITID &lt;dbl&gt;, n &lt;int&gt;\n\n\n\nNotice: we now have 6069 obs. of 663 variables\n\nDoes this pass the “eyeball test”?\n\nThe number of obs. looks right as it is just sum of the obs. from the 3 dfs\nThe number of vars. may look concerning at first\n\nWhy, if all we did was “stack” rows on top of each other, would the number of vars increase?\n\n\n\nWhat we have created is a somewhat “sparse” data frame, as each institution category has differently named variables (in part due to differing reporting requirements)\n\nOur data looks something like this\n\nDepiction of Sparse DataFrame\n\n\n\nPublic Vars\nNon-Profit Vars\nFor-Profit Vars\n\n\n\n\nPublic IDs\nvalues\nNA\nNA\n\n\nNon-Profit IDs\nNA\nvalues\nNA\n\n\nFor-Profit IDs\nNA\nNA\nvalues\n\n\n\n\nHmm, sounds like this could get tricky… Luckily tidyverse is here to help!\n\n\ncoelesce() Data Split Across Columns\n\nLet’s say we want to combine this information into one variable to show how much all institutions spend on instruction, research, and student services, respectively\n\nSide note: If combining variables like this in your own work, check the code book to ensure you know what you’re combining and why\nIn this case, the variables we will be combining appear to be close equivalents\n\nFirst things first, let’s select() the relevant variables from the larger data set using the IPEDS dictionary files\n\nQuick question: How did I find the variable names to use below?\n\n\n\ndf_18 &lt;- df_18 |&gt;\n  select(UNITID,\n         F1C011, F1C021, F1C061,\n         F2E011, F2E021, F2E051,\n         F3E011, F3E02A1, F3E03B1)\n\nprint(df_18[100:105,])\n\n# A tibble: 6 × 10\n  UNITID    F1C011  F1C021   F1C061 F2E011 F2E021 F2E051 F3E011 F3E02A1 F3E03B1\n   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 109819  81760530       0 13575072     NA     NA     NA     NA      NA      NA\n2 109907   8880360  241606  5093396     NA     NA     NA     NA      NA      NA\n3 110246  40754283       0 13656135     NA     NA     NA     NA      NA      NA\n4 110334  54226032       0 18432007     NA     NA     NA     NA      NA      NA\n5 110398  28937539       0  5591440     NA     NA     NA     NA      NA      NA\n6 110422 219559005 2847727 78536402     NA     NA     NA     NA      NA      NA\n\nprint(df_18[3000:3005,])\n\n# A tibble: 6 × 10\n  UNITID F1C011 F1C021 F1C061   F2E011 F2E021   F2E051 F3E011 F3E02A1 F3E03B1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 206862     NA     NA     NA 17364863      0  8130862     NA      NA      NA\n2 207157     NA     NA     NA  1055254      0   975876     NA      NA      NA\n3 207324     NA     NA     NA 16367709      0  9449815     NA      NA      NA\n4 207403     NA     NA     NA 14538443      0 10456812     NA      NA      NA\n5 207458     NA     NA     NA 39875546      0 12083472     NA      NA      NA\n6 207582     NA     NA     NA 26366669 101944 12445590     NA      NA      NA\n\n\n\n\nLuckily, tidyverse has a command to help us, coalesce()\n\nReturns the first non-missing value across any number of columns\n\nThis works perfectly in situations like this, when you only have one data point for each row, it could just be in any column\n\n\n\n\nlet’s do all the columns and get a clean data frame\n\ndf_18_clean &lt;- df_18 |&gt;\n  mutate(inst_spend = coalesce(F1C011, F2E011, F3E011),\n         rsch_spend = coalesce(F1C021, F2E021, F3E02A1),\n         serv_spend = coalesce(F1C061, F2E051, F3E03B1)) |&gt;\n  select(UNITID, inst_spend, rsch_spend, serv_spend)\n\nprint(df_18_clean[100:105,])\n\n# A tibble: 6 × 4\n  UNITID inst_spend rsch_spend serv_spend\n   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 109819   81760530          0   13575072\n2 109907    8880360     241606    5093396\n3 110246   40754283          0   13656135\n4 110334   54226032          0   18432007\n5 110398   28937539          0    5591440\n6 110422  219559005    2847727   78536402\n\nprint(df_18_clean[3000:3005,])\n\n# A tibble: 6 × 4\n  UNITID inst_spend rsch_spend serv_spend\n   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 206862   17364863          0    8130862\n2 207157    1055254          0     975876\n3 207324   16367709          0    9449815\n4 207403   14538443          0   10456812\n5 207458   39875546          0   12083472\n6 207582   26366669     101944   12445590\n\n\n\nThis is a real time saver if working with IPEDS finance data\nAnother use case for this might be if you had 30 columns one for each test date and rows for student scores, they all took the test on one of the dates, you want a single column for student scores, but it could have been recorded in any of the date columns\n\n\n\nFinding if_any() Issues\n\nAlthough compliance is a federal requirement, completeness of reporting remains a struggle when using IPEDS data\nFrom the snapshots of the data we have seen so far, it seems that there’s a good number of $0 reports for these values\nLet’s try and get a data frame containing only institutions that reported $0 for one of these spending categories\n\n\n\ndf_0 &lt;- df_18_clean |&gt;\n  filter(if_any(everything(), ~ . == 0)) ## h/t https://stackoverflow.com/questions/69585261/dplyr-if-any-and-numeric-filtering\n\nprint(df_0)\n\n# A tibble: 4,803 × 4\n   UNITID inst_spend rsch_spend serv_spend\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 100733          0          0          0\n 2 100760    8740330          0    3605430\n 3 100812   17101084          0    2918417\n 4 101028    6298586          0    2062946\n 5 101143    7532616          0    3094312\n 6 101161   23308825          0    7205781\n 7 101240   20593662          0    7278193\n 8 101286   17872473          0    4563448\n 9 101295   20945006          0    6225596\n10 101301    5373481          0    3216811\n# ℹ 4,793 more rows\n\n\n\nLet’s walk through this code\n\nAssign our results to a new object called df_0\nTake df_18_clean and pipe it into filter\nInside filter() we have our if_any helper function, which has two arguments\n\n\n\n.cols which columns to look across - Here we have just gone for all columns with everything(), but you could input a list of specific column names, or another selection function like where(is.numeric))\n.fns function to test the columns against - Here we have use ~ . == 0\n\nWe haven’t used purrr from tidyverse in this class, but the ~ comes from there, in short, it starts a very simple function for us\n\nThe function takes any value . and asks if it equals 0 == 0\nIf the function returns TRUE (as in it equals 0) for any column, that row will be filter()-ed in\n\n\n\n\n\n\nWorking across() Multiple Columns\n\nNow let’s explore that data a little more with if_any()’s sister function across()\n\nInternally if_any() and across() are set up the same\n\nThey both take\n\n.cols which columns to look across\n.fns function to test the columns against\n\n\nThe difference between them comes down to which function they work in\n\nif_any() is used in a handful of functions like filter()\nacross is used in most functions like summarize() and mutate()\n\nIf you try to use across() where you aren’t meant to tidyverse will throw you a warning\n\n\n\nHere, we will use the across function inside count() to get a breakdown of which spending categories are unreported most often\n\n\ndf_0 |&gt;\n  select(-UNITID) |&gt;\n  count(across(everything(), ~ . == 0))\n\n# A tibble: 7 × 4\n  inst_spend rsch_spend serv_spend     n\n  &lt;lgl&gt;      &lt;lgl&gt;      &lt;lgl&gt;      &lt;int&gt;\n1 FALSE      FALSE      TRUE          12\n2 FALSE      TRUE       FALSE       4411\n3 FALSE      TRUE       TRUE         336\n4 TRUE       FALSE      FALSE          5\n5 TRUE       FALSE      TRUE           2\n6 TRUE       TRUE       FALSE         12\n7 TRUE       TRUE       TRUE          25\n\n\n\nThe internal logic of the across() here is identical to the if_any() above\n\neverything() works across all columns, ~ . == 0 is a simple function to test if any value equals 0\n\nacross() just works in the count() function\n\nOutputs a count table counting combinations of the variables equaling 0\n\nBy far the most common variable to report zero is research spending, with 4411 reporting only that variable as 0\n25 schools reported 0 for all three variables\n\n\nAlthough we’ve only been working across 3 variables in these examples, the power of these commands is that they can work across an unlimited number of columns, so the bigger your data, the more should be thinking across() and if_any()\n\n\n\nMoving Beyond ifelse() with case_when()\n\nKeeping digging into differences in spending categories, next, let’s say we want to create a new variable that says which order the three spending categories were for each school\nLet’s walk through the case_when() code\n\nMuch like we used ifelse() inside mutate to make a new variable in Data Wranling I, we can use case_when() when we have more than a binary test\n\n\ncase_when() goes down the list of conditions in order until it finds one that it answers TRUE at which point it returns the value on the right hand side of the ~\nHere we have listed out all possible orders of spending categories with a label for each scenario\n\n\nUnlike ifelse() there is a unique danger that would don’t cover every eventuality with your conditions\n\n\nThis is why I like to end with TRUE, as that will be TRUE no matter what\n\nI then assign some kind of catch-all phrase that makes me know I made an error (it will just NA if you don’t do this)\n\n\n\nTo check if a case_when() worked, I like to pipe it into a count() for the new variable we made\n\n\n\ndf_18_clean |&gt;\n  mutate(highest_cat = case_when(inst_spend &gt; rsch_spend & rsch_spend &gt; serv_spend ~ \"inst_rsch_serv\",\n                                 inst_spend &gt; serv_spend & serv_spend &gt; rsch_spend ~ \"inst_serv_rsch\",\n                                 rsch_spend &gt; inst_spend & inst_spend &gt; serv_spend ~ \"rsch_inst_serv\",\n                                 rsch_spend &gt; serv_spend & serv_spend &gt; inst_spend ~ \"rsch_serv_inst\",\n                                 serv_spend &gt; inst_spend & inst_spend &gt; rsch_spend ~ \"serv_inst_rsch\",\n                                 serv_spend &gt; rsch_spend & rsch_spend &gt; inst_spend ~ \"serv_rsch_inst\",\n                                 TRUE ~ \"You missed a condition Matt\")) |&gt;\n  count(highest_cat)\n\n# A tibble: 7 × 2\n  highest_cat                     n\n  &lt;chr&gt;                       &lt;int&gt;\n1 You missed a condition Matt   377\n2 inst_rsch_serv                262\n3 inst_serv_rsch               5004\n4 rsch_inst_serv                 35\n5 rsch_serv_inst                  4\n6 serv_inst_rsch                384\n7 serv_rsch_inst                  3\n\n\n\nLooks like I missed something in my case_when(), can anyone guess what it is?\nWhat would happen to this school\n\ninst_spend 35,000,000\nrsch_spend 20,000,000\nserv_spend 20,000,000\n\nAs I have only specified for all order of categories being “greater than” the other, situations where two categories are equal slip through the cracks\n\nThis was a genuine mistake when writing the lesson, precisely why I always include that catch all\n\n\n\nLet’s see how our results change if I use “greater than or equal to” signs\n\n\ndf_18_clean |&gt;\n  mutate(highest_cat = case_when(inst_spend &gt;= rsch_spend & rsch_spend &gt;= serv_spend ~ \"inst_rsch_serv\",\n                                 inst_spend &gt;= serv_spend & serv_spend &gt;= rsch_spend ~ \"inst_serv_rsch\",\n                                 rsch_spend &gt;= inst_spend & inst_spend &gt;= serv_spend ~ \"rsch_inst_serv\",\n                                 rsch_spend &gt;= serv_spend & serv_spend &gt;= inst_spend ~ \"rsch_serv_inst\",\n                                 serv_spend &gt;= inst_spend & inst_spend &gt;= rsch_spend ~ \"serv_inst_rsch\",\n                                 serv_spend &gt;= rsch_spend & rsch_spend &gt;= inst_spend ~ \"serv_rsch_inst\",\n                                 TRUE ~ \"You missed a condition Matt\")) |&gt;\n  count(highest_cat)\n\n# A tibble: 6 × 2\n  highest_cat        n\n  &lt;chr&gt;          &lt;int&gt;\n1 inst_rsch_serv   624\n2 inst_serv_rsch  5005\n3 rsch_inst_serv    37\n4 rsch_serv_inst     4\n5 serv_inst_rsch   396\n6 serv_rsch_inst     3\n\n\n\nNo missing conditions this time, hooray!\nAlso, I’m definitely surprised how few institutions spend most on research, and just how many spend instruction &gt; services &gt; research\n\nDoes this surprise you as well?\n\n\n\n\nTidyverse Tricks Summary\n\nI hope some of these more advanced tidyverse commands will prove helpful, particularly as you move into the world of bigger data sets with more variables!\nNext, we are going to revisit some code from Data Wrangling I and Data Wrangling II and see how it translates to SQL",
    "crumbs": [
      "Data Wrangling",
      "IV: Tidyverse Tricks & SQL"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html#from-tidyverse-to-sql",
    "href": "09-wrangle-iv.html#from-tidyverse-to-sql",
    "title": "IV: Tidyverse Tricks & SQL",
    "section": "From tidyverse to SQL",
    "text": "From tidyverse to SQL\n\nFor this section, we are going to see how tasks from Data Wrangling II could be performed using SQL, a common tool used to work with databases\n\n\nWhat is SQL?\n\nSQL, short for Structured Query Language, is a way of retrieving, modifying, and storing data from databases. For a solid background page on SQL see this overview from AWS\nOne of thing that confuses people about SQL is that there’s the base language of SQL that all SQL-based products share and then multiple commercial product implementations of SQL that take the base language and add additional and unique functions\n\nThe dbplyr package we are going to use below attempts to mimic the implementation we tell it to\n\nA key difference between R and SQL is that we don’t have the option to assign our results to an object like we do with R\n\nInstead, an SQL query is simply the code to retrieve the data, what happens to that data (e.g., does it show up on your screen, is it saved somewhere, etc.) will be determined by the SQL-based software you’re using\n\n\n\n\nHow does SQL Relate to this Class?\n\nFirst, SQL is a really important tool for data management jobs in higher education. Larger institutions like UF almost certainly store their institutional data in a database that uses SQL\n\nHowever, it is rarely taught in Higher Ed degree programs, so, this little intro already sets you up for success\n\nSecond, the tidyverse language we have been learning this semester is in many ways similar to SQL\n\nIn fact, a lot of the functions and function names in tidyverse come directly from SQL\nThere’s an entire package dbplyr which can translate our standard dplyr commands to SQL queries\n\nThat’s what we are going to play around with today!\n\n\n\n\n\nData Wrangling II in SQL\n\nTo explore SQL in a familiar environment, we are going to re-visit Data Wrangling II\n\nWhile more complicated data wrangling is certainly possible in SQL the most common use in education is to pull or “query” data out of institutional databases with some simple summaries or joins\n\n\nFirst, we have to simulate an SQL database for dbplyr\n\nThis tells dbplyr exactly how to translate our code\nFor today’s class, we will simulate a Microsoft Access database with the simulate_access() command\n\nSecond, instead of a normal df, we want R to pretend that df is a table in a database, which we do with the memdb_frame() command\n\nIf you’re curious what this command does, try print(df) and print(db) to see the difference\n\n\n\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\nmicrosoft_access &lt;- simulate_access()\n\ndb &lt;- memdb_frame(df)\n\n\nCreate Summary Table\n\nOur first command is pretty simple, we want to group our data by year then calculate the mean test score, which will give us average test scores for each year\n\n\n# https://stackoverflow.com/questions/76724279/syntax-highlight-quarto-output\ndf_sum &lt;- db |&gt;\n    ## grouping by year so average within each year\n    group_by(year) |&gt;\n    ## get mean(&lt;score&gt;) for each test\n    summarize(math_m = mean(math),\n              read_m = mean(read),\n              science_m = mean(science)) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `year`,\n  AVG(`math`) AS `math_m`,\n  AVG(`read`) AS `read_m`,\n  AVG(`science`) AS `science_m`\nFROM `dbplyr_ciBkeQWCoi`\nGROUP BY `year`\n\n\n\nWe start with SELECT\n\nWe list out what we want, which is the year variable and AVG()s of math, reading, and science test scores\n\nWe are taking them from our simulated databases dbplyr_002 which we want GROUP BY-ed year\n\nAs SQL code is nested, we can’t read it top to bottom, the GROUP BY takes place as we pull the data out of dbplyr_002 before we calculate the AVG()\n\n\n\n\nLeft-Join\n\nNext, we want to join these averages back into the the main data frame\n\n\ndf_joined &lt;- db |&gt;\n    ## pipe into left_join to join with df_sum using \"year\" as key\n    left_join(df_sum, by = \"year\") |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `dbplyr_ciBkeQWCoi`.*, `math_m`, `read_m`, `science_m`\nFROM `dbplyr_ciBkeQWCoi`\nLEFT JOIN (\n  SELECT\n    `year`,\n    AVG(`math`) AS `math_m`,\n    AVG(`read`) AS `read_m`,\n    AVG(`science`) AS `science_m`\n  FROM `dbplyr_ciBkeQWCoi`\n  GROUP BY `year`\n) AS `RHS`\n  ON (`dbplyr_ciBkeQWCoi`.`year` = `RHS`.`year`)\n\n\n\nWe see our preceding query nested inside our new query from the second SELECT statement to the GROUP BY statement\n\nThat all sits become RHS inside our LEFT JOIN statement\n\nWe see this is joined ON the year variable with the original data dbplyr_002\n\n\n\nPivot-Longer\n\nOur next query pivots the data longer, which remember from our original lesson take the math, reading, and science score columns, and turns them into one column for score type and column for score\n\n\ndf_long &lt;- db |&gt;\n    ## cols: current test columns\n    ## names_to: where \"math\", \"read\", and \"science\" will go\n    ## values_to: where the values in cols will go\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"),\n                 names_to = \"test\",\n                 values_to = \"score\") |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `school`, `year`, 'math' AS `test`, `math` AS `score`\nFROM `dbplyr_ciBkeQWCoi`\n\nUNION ALL\n\nSELECT `school`, `year`, 'read' AS `test`, `read` AS `score`\nFROM `dbplyr_ciBkeQWCoi`\n\nUNION ALL\n\nSELECT `school`, `year`, 'science' AS `test`, `science` AS `score`\nFROM `dbplyr_ciBkeQWCoi`\n\n\n\nWhat we see here is a bit different, as it’s manual\nFirst we SELECT school and year as they are, math as test and math as score\nThis then UNION ALL-ed (think bind_rows style stacking) with the same query for reading and then again for science\n\n\n\nPivot-Wider\n\nNext, let’s pivot that data back wide\n\n\ndf_wide &lt;- df_long |&gt;\n    ## names_from: values in this column will become new column names\n    ## values_from: values in this column will become values in new cols\n    pivot_wider(names_from = \"test\",\n                values_from = \"score\") |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `school`,\n  `year`,\n  MAX(CASE WHEN (`test` = 'math') THEN `score` END) AS `math`,\n  MAX(CASE WHEN (`test` = 'read') THEN `score` END) AS `read`,\n  MAX(CASE WHEN (`test` = 'science') THEN `score` END) AS `science`\nFROM (\n  SELECT `school`, `year`, 'math' AS `test`, `math` AS `score`\n  FROM `dbplyr_ciBkeQWCoi`\n\n  UNION ALL\n\n  SELECT `school`, `year`, 'read' AS `test`, `read` AS `score`\n  FROM `dbplyr_ciBkeQWCoi`\n\n  UNION ALL\n\n  SELECT `school`, `year`, 'science' AS `test`, `science` AS `score`\n  FROM `dbplyr_ciBkeQWCoi`\n) AS `q01`\nGROUP BY `school`, `year`\n\n\n\nOur first SELECT statement asks for school and year, then…\nWe use CASE WHEN for each type of test, creating a new variable for each subject WHEN the test type equaled that subject\nBeneath that, we see our previous query that creates the long data that we pivoted back wider",
    "crumbs": [
      "Data Wrangling",
      "IV: Tidyverse Tricks & SQL"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#quarto-documentation",
    "href": "07-quarto-intro.html#quarto-documentation",
    "title": "Reproducable Reports with Quarto",
    "section": "Quarto Documentation",
    "text": "Quarto Documentation\n\nQuarto also comes with one of the best sets of documentation I’ve come across on their website, check it out here\n\nFun fact, the whole website is a massive Quarto project, which is kind of meta!\n\nRather than trying to re-explain something they put so clearly, we will look to this site for guidance throughout this lesson",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#why-quarto-over-word",
    "href": "07-quarto-intro.html#why-quarto-over-word",
    "title": "Reproducable Reports with Quarto",
    "section": "Why Quarto over Word?",
    "text": "Why Quarto over Word?\n\nUp until now, you’ve likely written most of your assignments using a Microsoft Word or something similar\nThere are a few major reasons Quarto has most replaced Word for 90% of my work\n\nIntegration with code/analysis\n\n\nThis is the main one\nAs you’ll see in this lesson, Quarto seamlessly integrates text with code output\n\nSo any tables and plots you create will be integrated into the report\n\nNo more spending hours copying your output from one screen to another and likely typing some of it wrong!\n\nYou can even have the text you type update\n\nAs a result, if when reviewing your final paper you spot an error right at the start of your analysis, as you often will (I did with my first publication), you just correct it and everything else automatically updates\n\n\nReproducibility/Transparancy\n\n\nAs the code and text are integrated, it makes it easier to share and reproduce your results, there’s no way someone could fake a figure/table without it being obvious\n\n\nZotero integration\n\n\nThe way Quarto’s visual editor integrates with Zotero on your laptop is superb, Word does integrate with Zotero as well, but it’s not as smooth\nZotero is an open source reference manager\n\nIf you aren’t using a reference manager, you 110% should be\n\nZotero is a fantastic choice, as it’s open-source and free, check it out here\n\n\nIf you have zotero on your computer, to cite anything in your Zotero library, you just type @ and you can search and add any article you have saved\n\n\nVersion control with git\n\n\nAs Quarto .qmd files are plain text/code files, they’re very small and work well with git for version control\n\nSee the git extra credit lesson for more information on git\n\n\n\nOne file, variety of formats\n\n\nSometimes, you need to share your work in a variety of formats\n\nWord and PDF is pretty easy, you can just save as…\nBut what if you’re asked to publish your research to a website?\nWhat if you want to work on HiPerGator so need your work in as .ipynb Jupyter Notebook?\n\nQuarto enables you to take the same work and switch formats easily and even publish to multiple formats at the same time\nYou can also make presentations from as PowerPoint, PDF, or my new favorite, RevealJS\n\nAgain, if you fix a mistake, all your tables and plots will be corrected automatically, a lifesaver 2 hours before you go on stage at ASHE!",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#text-portions-of-a-quarto-report",
    "href": "07-quarto-intro.html#text-portions-of-a-quarto-report",
    "title": "Reproducable Reports with Quarto",
    "section": "Text Portions of a Quarto Report",
    "text": "Text Portions of a Quarto Report\n\nNo matter how good your data viz skills are, the majority of most reports are still going to be text\nText in Quarto reports is written as markdown text (that’s what the md part of .qmd stands for)\nMarkdown is a relatively simple plain text way of formatting text\nQuarto provide a great overview of markdown basics on their site here, let’s take a look\n\n\n\nQuarto Visual Editor\n\nHowever, Quarto also has a visual editor, which looks like a simplified Microsoft Word, with point and click options for everything\n\nYou can toggle between source and visual modes at the top left of any .qmd file in RStudio\nAll the visual editor does is write actual markdown based on what you clicked\n\nIf you want to learn markdown, flipping between the two is a great way to do that\n\n\nIn my day to day life - If you start using Quarto on a regular basis, you’ll inevitably become familiar with markdown basics, as it is so much faster!\n\nIf I’m writing extended passages of text, I usually use the visual editor (primarily for the Zotero referencing feature I metioned above)\n\nIf you know markdown syntax (e.g., headings, bold, etc.), you can type it into the visual editor rather than clicking on options\n\nIf I’m writing, I generally prefer the source editor\n\nFor this class, you should be able to use the visual editor for most of your needs, no need to learn markdown!\n\n\nQuick Excercise\n\nInside the Quarto template you downloaded at the top of the lesson write a few sentances descrbing your proposal for your reproducible report\nAdd an appropriate header (e.g., “Introduction”)\nAdd bold face around your dependent variable\nMake your data source name italic",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#code-portions-of-a-quarto-document",
    "href": "07-quarto-intro.html#code-portions-of-a-quarto-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Code Portions of a Quarto Document",
    "text": "Code Portions of a Quarto Document\n\nThe text is all well and good, but, the real power of Quarto comes from the ability to integrate code into work\nThere are two main ways of doing this\n\nInline code chunks\nSourcing .R scripts\n\n\n\nInline Code Chunks\n\nFor shorter chunks of code such as printing tables and plots, or short calculations, you can create code chunks in the .qmd file\n\nThese are like you cut an .R script up into little pieces and put them in between text\nThey still run like one continuous .R script, from top to bottom\n\nAnything you &lt;- assign in an earlier chunk will be available in later chunks\n\nThe results/output of code in a chunk will print where the chunk is in the text in the rendered document\nYou can add a code chunk by clicking on the insert code button along the top, or, by keyboard shortcut\n\ncommand + option + I on Mac\nctrl + alt + I on PC\n\nOnce inserted, you can name chunks for cross-referencing and set executation options to change if/how they output results, both of which are beyond the scope of this class\n\n\n\nLet’s start out by reading in data, then making a simple ggplot of math test scores\n\nSince we want to see the results, we will just call it rather than assigning it to an object\nNote: We need to load packages in our quarto script every time\n\nUnless they are loaded by a script we source (see below)\n\n\n\n\nlibrary(tidyverse)\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\nggplot(data = df) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nQuick Excercise\n\nAdd a new code chunk to your Quarto document\nRead in the all-schools.csv from the sch-test folder\nMake a line ggplot for math scores with colored lines for each school\n\n\n\n\n\n\n\n\n\n\n\n\nOkay, those are printing, but last week we put so much work into creating a pretty plot, do I really have to copy all that code?\n\nThe answer is no!",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#sourcing-scripts",
    "href": "07-quarto-intro.html#sourcing-scripts",
    "title": "Reproducable Reports with Quarto",
    "section": "Sourcing scripts",
    "text": "Sourcing scripts\n\nTechnically, you could do everything in code chunks, from data joining and cleaning right through to analysis\n\nBut, this will get really long, and make it harder to skip around and navigate the text of your document while you’re editing\n\nA better solution is to source() plain R scripts that have already been written\nWhen you do this, any objects created in those scripts will then be in the environment and able to be added seamlessly to your Quarto document\nTo demonstrate, let’s source() our R script from last week’s lesson\n\nThere are two things that need to be right for this to work\n\nThe script has to be able to be run top to bottom with no errors\n\nAnything that creates an error needs to be ## commented out\n\nYou need to know where the file is\n\nIn our case, because we used a slightly messy kitchen approach (see our setup lesson on reading data for a reminder) all our scripts should be in the top level of your project folder, so you should be able to source them by name\n\nIf they’re somewhere else, you can use file.path() like we use for data etc.\n\n\n\n\n\n\nsource(\"06-viz-ii.R\")\n\n\nNow, with that script sourced, we can call objects we created in that script, and they will appear in our Quarto document\n\nFor example, if we source() a script that joins and cleans data and leaves it assigned to an object df_clean, we can then call df_clean in a code chunk and it will be waiting\nRemember right at the end of last week we saved our final plot to patch and I said why was a surprise, this is why!\n\nWe simply call patch and our fancy patchwork will print out beneath the chunk\n\nNote, since we loaded the patchwork library in the script we sourced, we don’t need to load it here\n\n\n\n\n\npatch\n\n\n\n\n\n\n\n\n\nThis logic is really useful to keep long streams of code out of your Quarto document\nYou also probably have lot’s of code already written in an .R script, and this is much easier than copy and pasting it all\n\n\nQuick Exercise\n\nsource() 04-wrangle-ii.R\nprint out the data object we made called df_long\n\n\n\nprint(df_long)\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# ℹ 62 more rows\n\n\n\nThat covers the basics of quarto document content! No matter what output you choose, this all stays the same!",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#yaml-header-options",
    "href": "07-quarto-intro.html#yaml-header-options",
    "title": "Reproducable Reports with Quarto",
    "section": "YAML Header Options",
    "text": "YAML Header Options\n\nThe final thing to be aware of is the YAML header, that stuff at the top of the .qmd file\nIn your template, I have set some basic things up that you can change\n\nThere are so, so, so, many options, many of which change with the format you are using\nSome basic ones to be aware of for now are\n\ntitle: kind of self-explanatory\nauthor: again, kind of self-explanatory\ndate: this can take a specific date, or, it “today” will use the current date when you render the document\neditor: visual defaults to the visual editor, easier for now\neditor_options:\n\nchunk_output_type: console Just means that when editing the document and running code chunks, the output will appear in the console (like it always has with R scripts) rather than appearing as a preview in the script\n\nThis is a personal preference, feel free to play around!\n\n\nformat: this is power of Quarto, there are so many options which will be discussed next\nexecute: these are the default way we want code chunks to output\n\necho: FALSE means don’t print out the code (e.g., this site use echo: TRUE)\nmessage: FALSE means don’t print out information like when tidyverse reads in data\n\nThere’s a bunch more of these options, setting them in the YAML Header makes that the default behavior for the document, you can set them at the chunk level to only apply to that chunk\nYou don’t need to alter these for this class, but if you want to learn more, see executation options documentation\n\n\n\n\n\n\nQuick Exercise\n\nChange the title to something appropriate for your final report\nChange the author to your name",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#quarto-output-formats",
    "href": "07-quarto-intro.html#quarto-output-formats",
    "title": "Reproducable Reports with Quarto",
    "section": "Quarto Output Formats",
    "text": "Quarto Output Formats\n\nAs I said at the start of this lesson, Quarto has so many different output formats you can choose from\n\nThe majority fall into three main categories\n\nhtml\npdf\nMicrosoft Office (docx and pptx)\n\nWe will mostly focus of office output for this class, but I will give a quick overview of the others first\n\n\n\nhtml\n\nhtml is the file format that web browsers read for almost every website you visit\nThis is probably my favorite output style, as it’s far less limited than the other types\n\nFun fact: this entire website is built with Quarto as html output\n\nClick on the little GitHub icon in the lower right corner to see to .qmd files\n\nYou can include fancy content like interactive graphics\n\nUnfortunately, the majority of work in academia (for now at least) still relies on traditional paper-based document formats, so we can’t justify spending much time on this today\nIf you want to learn more about html output, start with the Quarto documentation on Quarto Websites and Reveal.js Presentations\n\n\n\npdf\n\nThe first paper-based document format is straight to .pdf\nTraditionally, this has been dominated by LaTeX\n\nStudents under Dr. Skinner’s version of the class used this\nPersonally, I find LaTeX slow, unintuitive, and inflexible\n\nIn Quarto 1.4 (the version that came out a couple of weeks ago), support for a new much more user-friendly pdf generator typst has been added\n\nI haven’t had chance to play around with this yet, but from first impressions I’m hopefull!\n\nYou can also create pdfs straight from Quarto via word using an R script like this which I use for the syllabus\n\n\n\ndocx\n\ndocx output is, IMHO, the best starting place\nWhen you render your Quarto document, the result is a .docx file that you can then open in word or send to a supervisor/advisor\n\nFor example, if I’m running the data analysis for a project, I can create all the plots and tables, write up the methods section, then pass to my advisor to fill in the literature sections\n\nSure, straight to .pdf is nice is everyone is working in Quarto, but often (sadly) they won’t be\n\n.docx is a just more practical option a lot of the time\n\nYou still get a document rendered with all the tables and plots which will update if the data changes upstream\n\n\nPlus, even if you craft the entire document in Quarto, it can be nice to have the option to run Grammarly on it in word, or tweak one little layout feature you can’t figure out how to get right in Quarto\nThis is the output format I’d like you to use for your reproducible reports (unless you can convince me you need a different output)\n\nSo let’s take a look through the Quarto documentation for .docx output\n\n\n\nAs I say, the Quarto documentation is fantastic, I’m not asking you to do anything beyond the default docx output for the class - But, if you want to, docx output is formatted by a template docx file, read more about those in the Word templates documentation",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#rendering-quarto-reports",
    "href": "07-quarto-intro.html#rendering-quarto-reports",
    "title": "Reproducable Reports with Quarto",
    "section": "Rendering Quarto Reports",
    "text": "Rendering Quarto Reports\n\nTo see the magic happen, we simply have to hit the “Render” button at the top of the Quarto file\n\nThis will go from top to bottom of our file turning the text/markdown sections into formatted text, and the code output to\n\n\n\nA Common Trap when Rendering Quarto Reports\n\nOne thing that people always get tripped up on is when something runs while editing the Quarto document, and doesn’t when rendering/running it\n\nThe main thing to be aware of is that when it starts to render, it will work with an entirely clean environment\n\nIf you have anything (a plot, a dataframe) saved in your environment you can access that when running the chunks one by one\n\nWhen it renders, anything you call needs to created, either by source()-ing a script that makes it, or by making it within an earlier code chunk\nIf you move chunks around, make sure you don’t try and call something before it’s been assigned!",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#basic-tables-and-kables",
    "href": "07-quarto-intro.html#basic-tables-and-kables",
    "title": "Reproducable Reports with Quarto",
    "section": "Basic Tables and Kable()s",
    "text": "Basic Tables and Kable()s\n\nOne of the most common things you need when writing reports and/or papers are tables, whether than be descriptive statistics, results of a regression model, or even qualitative information about participants\nThere are many ways of creating tables in R\n\nIf you want to get fancy with customization, gt and gtsummary offer more advanced customization options\nIf you’re looking for easy and consistent output from regressions and other models stargazer might be what you’re looking for\nBut for now, we will focus on basic markdown tables and kable\n\nkableextra is also an option if you want to customize kables outside what the basic package allows\n\n\n\n\nBasic Markdown Tables\n\nBasic markdown tables are somewhat like tables in word, you manually fill them in\n\nYou could use inline code in one, but you will see why that would rarely be necessary when we look at kable\n\nBefore Quarto’s visual editor, making basic markdown tables was a real pain\n\nThey basically involve using a series of ————, ::::, and |||| to craft a table\n\nIf you’re interested you can see more on the Quarto table documentation\n\nWith the visual editor, however, you can just click on “Table” from the drop down menu, and write out the contents of your table\n\nThis is great for certain things, such as a table where we describe something qualitatively, such as syllabus for this class (which uses a markdown table).\n\n\n\nTables from data frames with kable\n\nSo far in this class, there have been plenty of times we have used tables to answer questions, when we have had them print out the console, e.g.\n\n\n## Read in using .dta so we have nice labels\ndf &lt;- haven::read_dta(\"data/hsls-small.dta\") |&gt;\n  drop_na(x1txmtscor)\n\ndf |&gt;\n  summarize(mean(x1txmtscor))\n\n# A tibble: 1 × 1\n  `mean(x1txmtscor)`\n               &lt;dbl&gt;\n1               51.1\n\n\n\nThis is kind of a table, but, when we render our Quarto document it doesn’t look great\nLuckily, the fix is really easy\n\nFirst, we load the knitr package\nSecond, we just pipe our output into kable()\n\n\n\nlibrary(knitr)\n\ndf |&gt;\n  summarize(mean(x1txmtscor)) |&gt;\n  kable()\n\n\n\n\nmean(x1txmtscor)\n\n\n\n\n51.10957\n\n\n\n\n\n\nkable() is relatively basic in terms of customization, but, it can do most things you’re going to need\nFor now, we will address the two most obvious issues with this table, the column names and the rounding (or lack thereof)\n\nWe just need to add two simple arguments to our kable()\n\n\n\ndf |&gt;\n  summarize(mean(x1txmtscor)) |&gt;\n  kable(col.names = c(\"Mean of Math Score\"),\n        digits = 2)\n\n\n\n\nMean of Math Score\n\n\n\n\n51.11\n\n\n\n\n\n\nkable() will turn any data you pass to it into a table, let’s make a slightly more interesting summary table\n\nNotice, we will use as_factor() to get the labels to show up\n\nfactor() allows us to make a factor and apply our own labels\nas_factor() works with haven labeled data and gets the labels out\n\n\n\n\ndf |&gt; \n  group_by(as_factor(x1region), as_factor(x1sex)) |&gt;\n  summarize(mean = mean(x1txmtscor),\n            median = median(x1txmtscor),\n            min = min(x1txmtscor),\n            max = max(x1txmtscor)) |&gt;\n  kable(col.names = c(\"Region\", \"Sex\", \"Mean\", \"Median\", \"Min\", \"Max\"),\n        digits = 2,\n        caption = \"Math Score by Region and Sex\")\n\n\nMath Score by Region and Sex\n\n\nRegion\nSex\nMean\nMedian\nMin\nMax\n\n\n\n\nNortheast\nMale\n52.13\n51.97\n24.9468\n82.1876\n\n\nNortheast\nFemale\n52.22\n52.00\n25.5304\n78.9298\n\n\nMidwest\nMale\n51.23\n51.18\n24.0999\n82.1876\n\n\nMidwest\nFemale\n51.22\n50.94\n24.7966\n82.1876\n\n\nSouth\nMale\n51.01\n51.02\n24.0180\n82.1876\n\n\nSouth\nFemale\n51.07\n50.86\n24.5797\n81.0926\n\n\nWest\nMale\n50.04\n49.80\n24.0744\n82.1876\n\n\nWest\nFemale\n50.26\n50.19\n25.0437\n78.9298\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#tips-for-technical-writing-citations",
    "href": "07-quarto-intro.html#tips-for-technical-writing-citations",
    "title": "Reproducable Reports with Quarto",
    "section": "Tips for Technical Writing & Citations",
    "text": "Tips for Technical Writing & Citations\n\nOne of the most frustrating things in Microsoft Word is technical aspects of writing such as equations needed for quantitative research articles\n\nBy no means do you need to write equations in your project for this class, but this is something you’ll need in future stats classes\n\nAs always, the Quarto documentation does a great job of explaining the basics of technical writing, so let’s take a look!\n\nI mentioned this above, but this page also discusses the Zotero integration in a bit more depth as well",
    "crumbs": [
      "Quarto",
      "Reproducable Reports with Quarto"
    ]
  },
  {
    "objectID": "05-viz-i.html#progress-check-in",
    "href": "05-viz-i.html#progress-check-in",
    "title": "I: Basics",
    "section": "Progress Check-In",
    "text": "Progress Check-In\n\nIn keeping with the theme of today’s lesson, I made a little plot that represents roughly the breakdown of “core R content” covered in each lesson (as in, the basic knowledge you need to go off and do your own data work)\n\nCongratulations, after the first two data-wrangling lessons, you’re already half-way done!\nOver the next three weeks, we will cover how to communicate the results of these basic skills using visualization with ggplot2 and report generation with quarto\nThen, we will get into the final series of lessons, each of which gives you a little taste of some more sophisticated and/or niche applications of R\nThroughout all this, we will continue to become familiar with the basics of data wrangling, as that really is the key to it all\n\nAny questions or concerns?",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#setup",
    "href": "05-viz-i.html#setup",
    "title": "I: Basics",
    "section": "Setup",
    "text": "Setup\n\nOne key part of understanding your data and presenting your analyses lies in making plots, which we will cover through this section of the class\nThere are multiple graphing systems in R, but we are going to focus on two (primarily one);\n\nvanilla R plots\nggplot2\n\n\n\nVanilla R Plots\n\nVanilla R (i.e., R without any packages loaded) can create some basic plots\n\nThey aren’t the prettiest, so I wouldn’t recommend them in reports and papers\n\nBut, they’re super-easy and quick to create, so they’re perfect for quickly checking your data during the exploration phase\n\n\n\n\n\nggplot2 Plots\n\nggplot or ggplot2 (ggplot was originally a different library, but doesn’t exist anymore)\nThe gg stands for grammar of graphics\n\nThis is a whole world of detail to dive into if you want\nThe basic idea is that the graphs are made up of layers\n\nThis allows us to create some really cool and detailed plots\n\nI’d strongly recommend ggplot2 for making plots you want to share\n\nEven stata users admit the plots can’t match ggplot2\n\n\n\n\n\n\n\nLibraries\n\nWe’re using two libraries today (plus the vanilla R plot functions)\nggplot2\nThe ggplot2 library is part of the tidyverse\n\nWe don’t need to load it separately (we can just use library(tidyverse) as always)\n\n\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n\nWe’re also going to use haven,\nHaven allows us to read in data files from other software such as SPSS, SAS, and Stata\n\nWe’ll use it to read in a Stata (*.dta) version of the small HSLS data we’ve used before\n\nThe Stata version, unlike the plain .csv version, has labels for the variables and values, which will be useful when plotting\n\n\nHaven is also part of the tidyverse but not loaded by default\nWe could load the package with library(haven), but, if we only need one function from a package, it’s often easier to call it directly (plus this is useful trick to know)\n\nlibrary(&lt;package&gt;) pre-loads all the functions so that we can easily call them with just the function name\nIf you only need one function from a package, or, you want to use a function from a package with a conflict (i.e., two packages with functions with the same name) we can specify where the function should come from like this\n\n&lt;package&gt;::&lt;function&gt;\nI.e., haven::read_dta()\n\n\n\n\n## ---------------------------\n## input data\\\n## ---------------------------\n\n## read_dta() ==&gt; read in Stata (*.dta) files\ndf_hs &lt;- haven::read_dta(file.path(\"data\", \"hsls-small.dta\"))\n## read_csv() ==&gt; read in comma separated value (*.csv) files\ndf_ts &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\n\nNote that since we have two data files this lesson, I gave them unique names instead of the normal df:\ndf_hs := hsls_small.dta\ndf_ts := all_schools.csv",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-base-r",
    "href": "05-viz-i.html#plots-using-base-r",
    "title": "I: Basics",
    "section": "Plots using base R",
    "text": "Plots using base R\n\nEven though new graphics libraries have been developed, the base R graphics system remains powerful\nThe base system is also very easy to use in a pinch\n\nWhen I want a quick visual of a data distribution that’s just for me, I often use base R\n\nNote that for the next few plots, I’m not much concerned with how they look\n\nSpecifically, the axis labels won’t look very nice\nWe could spend time learning to make really nice base R plots for publication\n\nBut I’d rather we spend that time with ggplot2 graphics.\n\n\nAlso note that we’ll have to switch to using the vanilla R data frame $ notation to pull out the columns we want\n\nIn short, to reference a column in a data frame in vanilla R, you say dataframe$column\nIf you need some more information on using $ notation, check out the supplemental lesson on data wrangling with vanilla R.\n\n\n\nHistogram\n\nFor continuous variables, a histogram is a useful plot\nThough the hist() function has many options to adjust how it looks\n\nThe default settings work really well if you just want a quick look at the distribution.\n\n\n\n## histogram of math scores (which should be normal by design)\nhist(df_hs$x1txmtscor)\n\n\n\n\n\n\n\n\n\nQuick exercise\nCheck the distribution of the students’ socioeconomic score (SES).\n\n\n\nDensity\n\nDensity plots are also really helpful for checking the distribution of a variable\nVanilla R doesn’t have formal density plot function, but you can get a density plot with a trick\n\nplot() the density() of a continuous variable\n\n\n\nQuick question: What does the na.rm = TRUE and why might we need it?\n\n\n## density plot of math scores\ndensity(df_hs$x1txmtscor, na.rm = TRUE) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nQuick exercise\nFirst, plot the density of SES Then, add the col argument in plot() to change the color of the line to \"red\"\n\n\n\nBox plot\n\nA box plot will let you see the distribution of a continuous variable at specific values of a categorical variable\n\nFor example, test scores ranges at each student expectation level\n\nCall a box plot using the boxplot() function\n\nThis one is a little trickier because it uses the R formula construction to set the continuous variable against the discrete variable\nThe formula uses a tilde, ~, and should be constructed like this:\n\n&lt;continuous var&gt; ~ &lt;discrete var&gt;\nWe will talk in more detail about formulas in the Programming: Modeling Basics lesson\n\nAs we are using a formula, notice how we can use the data = df_hs argument instead of adding df_hs$ in front of the variable names, which saves some typing\n\n\n\n## box plot of math scores against student expectations\nboxplot(x1txmtscor ~ x1stuedexpct, data = df_hs)\n\n\n\n\n\n\n\n\nFrom the boxplot, we can see that math test scores tend to increase as students’ educational expectations increase (remember that 11 means “I don’t know [how far I’ll go in school]”), though there’s quite a bit of overlap in the marginal distributions.\n\n\nScatter\n\nPlot two continuous variables against one another using the base plot() function\nThe main way to make a scatter plot is plot(x, y)\n\nThe x is the variable that will go on the x-axis and y the one that will go on the y-axis\n\n\n\n## scatter plot of math against SES\nplot(df_hs$x1ses, df_hs$x1txmtscor)\n\n\n\n\n\n\n\n\nFrom the scatter plot we see the data seem to show a positive correlation between socioeconomic status and math test score, there’s also quite a bit of variation in that association (notice that the cloud-like nature of the circles).\n\nQuick exercise\nRerun the above plot, but this time store it in an object, plot_1, then get the plot to print out",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-ggplot2",
    "href": "05-viz-i.html#plots-using-ggplot2",
    "title": "I: Basics",
    "section": "Plots using ggplot2",
    "text": "Plots using ggplot2\n\nggplot2 is my — and many R users’ — primary system for making plots\nIt is based on the idea of a grammar of graphics\n\nJust as we can use finite rules of a language grammar to construct an endless number of unique sentences, so too can we use a few graphical grammatical rules to make an endless number of unique figures.\n\nThe ggplot2 system is too involved to cover in all of its details\n\nBut that’s kind of the point of the grammar of graphics\nOnce you see how it’s put together, you can anticipate the commands you need to build your plot.\n\nWe’ll start by covering the same plots as above.\n\n\nHistogram\nAs the main help site says, all ggplot2 plots need three things:\n\n[data]: The source of the variables you want to plot\n[aesthetics]: How variables in the data map onto the plot (e.g., what’s on the x-axis? what’s on the y-axis?)\n[geom]: The geometry of the figure or the kind of figure you want to make (e.g., what do you want to do with those data and mappings? A line graph? A box plot?…)\nWe’ll start by making a histogram again\nTo help make these pieces clearer, I’ll use the argument names when possible\n\nAs you become familiar, you probably will stop naming some of these core arguments\n\nThe first function, which initializes the plot is ggplot()\n\nIts first argument is the data, which want to use df_hs\n\n\n\n## init ggplot \nggplot(data = df_hs)\n\n\n\n\n\n\n\n\n\n…nothing! Well, not nothing, but no histogram.\n\nThat’s because the plot knows the data but doesn’t know what do with it. What do we want?\n\nSince we want a histogram, we add the geom_histogram() function to the existing plot object with a plus sign(+). Once we do that, we’ll try to print the plot again…\nThe aesthetic mappings, that is, which variables go where or how they function on the plot, go inside the aes() function.\n\nSince we only have one variable, x1txmtscor, it is assigned to x.\n\n\n\n## add histogram instruction (notice we can add pieces using +)\nggplot(data = df_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\nSuccess!\n\nQuick excercise: try assigning the historgram to an object, then getting it to print out\n\n\nAs you can see, the code to make a ggplot2 figure looks a lot like what we’ve seen with other tidyverse libraries, e.g. dplyr.\nThe key difference between ggplot2 and our previous code, however, is that\n\nUp to now we have used the pipe (|&gt;) to pass output to the next function\nggplot2 uses a plus sign (+) add new layers to the plot\n\nLayers is exactly how you want to think about ggplots\n\nThe ggplot() is the base layer of the graph\n\nAnything you place in here will become the default for every other layer\n\nFor example, if we say data = df in ggplot(), that will be the default data for every layer\n\nGenerally I specify the data here, and mapping = aes() in the specific plots\n\n\n\n\n\n\nQuick question(s): Why might that make sense? Which is more likely to change? Are there times you might want to set an aesthetic for the whole plot?\n\n\n\nDensity\n\nUnlike the base R graphics system, ggplot2 does have a density plotting command geom_density()\n\nThe rest of the code remains the same as for geom_histogram()\n\n\n\n## density\nggplot(data = df_hs) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nQuick exercise\nIf we wanted to see the histogram and density plot on top of each other, what might we do? Give it a go, and, tell me why it didn’t work…\n\n\nggplot(data = df_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor)) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nThe issue is that the histogram y scale is much much larger than the density\n\nTo fix that, let’s modify the geom_histogram() aesthetic to use the density function rather than the raw counts\n\nWe use the after_stat() function, which basically means after ggplot calculates the statistics, it converts them to density\n\n\n\n\n## histogram with density plot overlapping\nggplot(data = df_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor, y = after_stat(density))) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nIt worked, but it’s not the greatest visual since the colors are the same and the density plot is thin with no fill.\nAdding to what came before, the geom_histogram() and geom_density() both take on new arguments that change the defaults\nNow the resulting plot should look nicer and be easier to read\n\n\n## histogram with density plot overlapping (add color to see better)\nggplot(data = df_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor, y = after_stat(density)),\n                 color = \"black\",\n                 fill = \"white\") +\n  geom_density(mapping = aes(x = x1txmtscor),\n               fill = \"red\",\n               alpha = 0.2)\n\n\n\n\n\n\n\n\n\nQuick exercise\nTry changing some of the arguments in the last plot. What happens when you change alpha (keep the value between 0 and 1)? What does the color argument change? And fill? What happens if you switch the geom_*() functions, call geom_histogram() after you call geom_density()?\n\n\nA critical thing to note, in the previous plot color, fill, and alpha were all outside the aes()\n\nThis means they take a single value and apply it uniformly, it should portray no information and just change the appearance\nTo use these elements to portray information, we need to place the arguments inside aes() like we will do in the next plot\n\n\n\n\nTwo-way\n\nPlotting the difference in a continuous distribution across groups is a common task\nLet’s see the difference between student math scores between students with parents who have any postsecondary degree and those without.\nSince we’re using data that was labeled in Stata, we’ll see the labels when we use count()\n\n\n## see the counts for each group\ndf_hs |&gt; count(x1paredu)\n\n# A tibble: 7 × 2\n  x1paredu                                         n\n  &lt;dbl+lbl&gt;                                    &lt;int&gt;\n1  1 [Less than high school]                    1010\n2  2 [High school diploma or GED]               5909\n3  3 [Associate's degree]                       2549\n4  4 [Bachelor's degree]                        4102\n5  5 [Master's degree]                          2116\n6  7 [Ph.D/M.D/Law/other high lvl prof degree]  1096\n7 NA                                            6721\n\n\n\nWe can see that all values of x1paredu greater than 2 represent parents with some college credential\n\nSince we want only two distinct groups, we can use mutate, ifelse and the operator &gt;= to make a new 0/1 binary variable. - If a value of x1paredu is above 3, then the new indicator pared_coll will be 1; if not, 0.\n\n\nNOTE that in the Stata version of hsls_small, all the missing values, which are normally negative numbers, have already been properly converted to NA values. That’s why we see a count column for NA and not labels for missingness that we might have expected based on prior lessons.\n\nThe ggplot() function doesn’t need to use our full data\nIn fact, our data needs to be set up a bit differently to make this plot\n\nThis is a common thing people forget when plotting, data wrangling lessons one and two are your friend here\n\nWe’ll make a new temporary data object that only has the data we need.\nNotice, after we create pared_coll, we use the factor() command to make it a factor type of variable\n\nThis is R’s built in way of handling categorical variables (i.e., so that is doesn’t think it’s continuous)\nCreating factors is really useful for plotting, and later on for statistical models\n\n\n\n## need to set up data\nplot_df &lt;- df_hs |&gt;\n  ## select the columns we need\n  select(x1paredu, x1txmtscor) |&gt;\n  ## can't plot NA so will drop\n  drop_na() |&gt;\n  ## create new variable that == 1 if parents have any college, then make it a factor\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0),\n         pared_coll = factor(pared_coll)) |&gt;\n  ## drop (using negative sign) the original variable we don't need now\n  select(-x1paredu) \n\n## show\nhead(plot_df)\n\n# A tibble: 6 × 2\n  x1txmtscor pared_coll\n  &lt;dbl+lbl&gt;  &lt;fct&gt;     \n1 59.4       1         \n2 47.7       1         \n3 64.2       1         \n4 49.3       1         \n5 62.6       1         \n6 58.1       1         \n\n\n\nTo plot against the two groups we’ve made, we need to add it to the aesthetic feature, aes()\nThe math score, x1txmtscor, is still mapped to x\nSince we want two side-by-side histograms, we set the fill aesthetic to our new indicator variable\n\n\n## two way histogram\nggplot(plot_df) +\n  geom_histogram(aes(x = x1txmtscor,\n                     fill = pared_coll),\n                 alpha = 0.5,\n                 color = \"black\")\n\n\n\n\n\n\n\n\n\nBy assigning pared_coll to the fill aesthetic, we can see a difference in the distribution of math test scores between students whose parents have at least some college and those whose parents do not\n\nNote: there are more students with no parental college education, so that whole histogram is bigger\n\nIf we want to compare the shape of distribution more easily, we should use geom_density()\n\n\n\n\n## two way histogram\nggplot(plot_df) +\n  geom_density(aes(x = x1txmtscor,\n                   fill = pared_coll),\n               alpha = 0.5,\n               color = \"black\")\n\n\n\n\n\n\n\n\n\nQuick question\nWhy does the color = \"black\" not mean we have two black density/histogram plots? What happens if you remove it? Can you make it &lt;something else&gt; = \"black\" to get rid of the colors?\n\n\n\nBox plot\n\nBy this point, you’re hopefully seeing the pattern in how ggplot2 figures are put together\nTo make a box plot, we need to add a y mapping to the aes() in addition to the x mapping\nWe’ve also added the same variable to fill as we did to x\n\nWe do this so that in addition to having different box and whisker plots along the x-axis, each plot is given its own color\n\nNotice: this time, we just threw factor() around the variable in the plot, rather than using mutate to change the data\n\n\nQuick question: What do you think the pros and cons of using factor() in the plot over mutating the data might be?\n\n\n## box plot using both factor() and as_factor()\nggplot(data = df_hs,\n       mapping = aes(x = factor(x1paredu),\n                     y = x1txmtscor,\n                     fill = factor(x1paredu))) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nIn a way, this plot is similar to the dual histogram above\n\nBut since we want to see the distribution of math scores across finer-grained levels of parental education, the box and whisker plot is clearer than trying to overlap seven histograms.\n\n\n\nQuick exercise\nWe will get more into making things look pretty in Data Vizualization II, but, what is a real problem with this graph? Does it even need to be there?\n\n\n\nScatter\n\nTo make a scatter plot, make sure that the aes() has mappings for the x axis and y axis and then use geom_point() to plot.\nTo make things easier to see (remembering the over-crowded cloud from the base R plot above), we’ll reduce the data to 10% of the full sample using sample_frac() from dplyr\nWe’ll also limit our 10% to those who aren’t missing information about student education expectations\n\n\n## sample 10% to make figure clearer\ndf_hs_10 &lt;- df_hs |&gt;\n  ## drop observations with missing values for x1stuedexpct\n  drop_na(x1stuedexpct) |&gt;\n  ## sample\n  sample_frac(0.1)\n\n## scatter\nggplot(data = df_hs_10) +\n  geom_point(mapping = aes(x = x1ses, y = x1txmtscor))\n\n\n\n\n\n\n\n\n\nNow that we have our scatter plot, let’s say that we want to add a third dimension\n\nSpecifically, we want to change the color of each point based on whether a student plans to earn a Bachelor’s degree or higher\n\nThat means we need a new dummy variable that is 1 for those with BA/BS plans and 0 for others.\n\n\n\nWe can look at the student base year expectations with count():\n\n## see student base year plans\ndf_hs |&gt;\n  count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct                                     n\n   &lt;dbl+lbl&gt;                                    &lt;int&gt;\n 1  1 [Less than high school]                      93\n 2  2 [High school diploma or GED]               2619\n 3  3 [Start an Associate's degree]               140\n 4  4 [Complete an Associate's degree]           1195\n 5  5 [Start a Bachelor's degree]                 115\n 6  6 [Complete a Bachelor's degree]             3505\n 7  7 [Start a Master's degree]                   231\n 8  8 [Complete a Master's degree]               4278\n 9  9 [Start Ph.D/M.D/Law/other prof degree]      176\n10 10 [Complete Ph.D/M.D/Law/other prof degree]  4461\n11 11 [Don't know]                               4631\n12 NA                                            2059\n\n\n\nWe see that x1stuedexpct &gt;= 6 means a student plans to earn a Bachelor’s degree or higher.\nBut since we need to account for the fact that 11 means “I don’t know”, we need to make sure our test includes x1stuedexpct &lt; 11\nRemember from a prior lesson that we can connect these two statements together with the operator &\nLet’s create our new variable\n\nNotice this time when I create the factor, I specify levels and labels\n\nThis applies labels much like the haven version of our data has, which will print out in our plot\n\n\n\n\n## create variable for students who plan to graduate from college\ndf_hs_10 &lt;- df_hs_10 |&gt;\n  mutate(plan_col_grad = ifelse(x1stuedexpct &gt;= 6 & x1stuedexpct &lt; 11,\n                                1,        # if T: 1\n                                0),       # if F: 0\n         plan_col_grad = factor(plan_col_grad,\n                                levels = c(0, 1),\n                                labels = c(\"No\", \"Yes\")))      \n\n\nNow that we have our new variable plan_col_grad, we can add it the color aesthetic, aes() in geom_point().\n\n\n## scatter\nggplot(data = df_hs_10,\n       mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = plan_col_grad), alpha = 0.5)\n\n\n\n\n\n\n\n\n\nQuick exercise\nRemake the plot so the variables on each axis are flipped\n\n\n\nFitted lines\n\nIt’s often helpful to plot fitted lines against a scatter plot to help see the underlying trend\n\nThere are a number of ways to do this with the geom_smooth() function\n\n\n\nLinear fit\n\nSetting method = lm in geom_smooth() will fit a simple straight line of best fit with 95% confidence interval shaded around it.\nSince we want the points and the line to share the same x and y aesthetics, let’s put them in the ggplot() base layer\n\n\n## add fitted line with linear fit\nggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n  geom_smooth(method = lm, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nLoess\n\nFinally, we can skip trying to adjust a linear line and just fit a LOESS curve, which is a smooth line produced by fitting a large number of local polynomial regressions on subsets of the data.\n\n\n## add fitted line with loess\nggplot(data = df_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n  geom_smooth(method = loess, color = \"black\")\n\n\n\n\n\n\n\n\n\nTo be clear, these semi-automated lines of best fit should not be used to draw final conclusions about the relationships in your data\nYou will want to do much more analytic work to make sure any correlations you observe aren’t simply spurious and that fitted lines are telling you something useful\nThat said, fitted lines via ggplot2 can be useful when first trying to understand your data or to more clearly show observed trends.\n\n\n\n\nLine graph\n\nWhen you want to show changes in one variable as a function of another variable\n\ne.g., changes in test scores over time\n\nA line graph is often a good choice.\nSince our hsls_small data is cross-sectional, we’ll shift to using our school test score data df_ts from Data Wrangling II\n\n\nQuick question: What does cross-sectional mean? What is the opposite of it?\n\n\nAs a reminder, here’s what the schools data looks like\n\n\n## show test score data\ndf_ts\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\nSimple Line-Graph\n\nTo keep it simple for our first line plot, we’ll filter our plot data to keep only scores for one school\n\nNotice how we can do that directly with pipes inside the ggplot() function\n\nWe want to see changes in test scores over time, so we’ll map\n\nyear to the x axis\nmath to the y axis\n\nTo see a line graph, we add geom_line().\n\n\n## line graph\nggplot(data = df_ts |&gt; filter(school == \"Spottsville\"),\n       mapping = aes(x = year, y = math)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nQUICK EXERCISE\nChange the school in filter() to “East Heights” and then “Bend Gate”.\n\n\n\nMultiple-Line Graphs\n\nEasy enough, but let’s say that we want to add a third dimension — to show math scores for each school in the same plot area. -To do this, we can map a third aesthetic to school. Looking at the help file for geom_line(), we see that lines (a version of a path) can take colour, which means we can change line color based on a variable.\n\nThe code below is almost exactly the same as before less two things:\n\nWe don’t filter df_ts this time, because we want all schools\nWe add colour = school inside aes()\n\n\n## line graph for math scores at every school over time\nggplot(data = df_ts,\n       mapping = aes(x = year, y = math, colour = school)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nThis is nice (though maybe a little messy at the moment) because it allows us to compare math scores across time across schools.\nBut we have two more test types — reading and science — that we would like to include as well\nOne approach that will let us add yet another dimension is to use facets\n\n\n\n\nFacets\n\nWith facets, we can put multiple plots together, each showing some subset of the data\nFor example, instead of plotting changes in math scores across schools over time on the same plot area (only changing the color), we can use facet_wrap() to give each school its own little plot.\nCompared to the code just above, notice how we’ve removed colour = school from aes() and included facet_wrap(~school)\n\nThe tilde (~) works like the tilde in plot(y ~ x) above: it means “plot against or by X”. In this case, we are plotting math test scores over time by each school.\n\n\n\n## facet line graph\nggplot(data = df_ts,\n       mapping = aes(x = year, y = math)) +\n  facet_wrap(~ school) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nIs this faceted plot better than the color line plot before it?\n\nTo my eyes, it’s a little clearer, but not so much so that I couldn’t be convinced to use the first one\n\nWhether you use the first or the second would largely depend on your specific data and presentation needs.\nFaceting has a clearer advantage, however, when you want to include the fourth level of comparison:\n\nscores across\ntime across\nschools\ndifferent tests.\n\nTo make this comparison, we first need to reshape our data, which is currently long in year, to be long in test, too.\n\nAs we saw in Data Wrangling II, we’ll use pivot_longer() to place each test type in its own column (test) with the score next to it.\n\n\n\n## reshape data long\ndf_ts_long &lt;- df_ts |&gt;\n  pivot_longer(cols = c(\"math\",\"read\",\"science\"), # cols to pivot long\n               names_to = \"test\",                 # where col names go\n               values_to = \"score\")               # where col values go\n\n## show\ndf_ts_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQUICK EXERCISE\nIf we have 4 schools, 6 years, and 3 tests, how many observations should df_ts_long have in total? Does it?\n\n\nWith our reshaped data frame, we now reintroduce colour into the aes(), this time set to test\nWe make one other change: y = score now, since that’s the column for test scores in our reshaped data\nAll else is the same.\n\n\n## facet line graph, with colour = test and ~school\nggplot(data = df_ts_long) +\n  geom_line(mapping = aes(x = year, y = score, colour = test)) +\n  facet_wrap(~school)\n\n\n\n\n\n\n\n\n\nHmm, currently, each test score is on its own scale, which means this plot isn’t super useful\n\nThe difference between the types of score is so much greater than the variance within the test scores, we have 3 pretty flat lines\n\nBut maybe what we really want to know is how they’ve changed relative to where they started\n\nYou can imagine a superintendent who took over in 1980 would be keen to know how scores have changed during their tenure.\n\nTo see this, we need to standardize the test scores\n\nOften in statistics, you will mean-standardize a variable\n\nDifference between current score and the mean of the score, divided by the standard deviation of the score\n\nYou’ve probably heard of this as a “Z-score” in stats classes\n\n\n\\[\n\\frac{x_i - \\bar{x}}{\\sigma(x)}\n\\]\n\nIn simple terms, this puts all scores on the same scale\nTo get the graph we want, we are going to do something very similar, but instead of using the mean of the score, we are going to take the score in 1980\n\nTherefore, every subsequent year will show the change since 1980, and since it’s standardized, all test scores will be on the same scale\n\n\n\ndf_ts_long_std &lt;- df_ts_long |&gt;\n  group_by(test, school) |&gt;\n  arrange(year) |&gt; \n  mutate(score_year_one = first(score),\n         ## note that we're using score_year_one instead of mean(score)\n         score_std_sch = (score - score_year_one) / sd(score)) |&gt;\n  ungroup()\n\nprint(df_ts_long_std, n = 13)\n\n# A tibble: 72 × 6\n   school        year test    score score_year_one score_std_sch\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Bend Gate     1980 math      515            515          0   \n 2 Bend Gate     1980 read      281            281          0   \n 3 Bend Gate     1980 science   808            808          0   \n 4 East Heights  1980 math      501            501          0   \n 5 East Heights  1980 read      318            318          0   \n 6 East Heights  1980 science   782            782          0   \n 7 Niagara       1980 math      514            514          0   \n 8 Niagara       1980 read      292            292          0   \n 9 Niagara       1980 science   787            787          0   \n10 Spottsville   1980 math      498            498          0   \n11 Spottsville   1980 read      288            288          0   \n12 Spottsville   1980 science   813            813          0   \n13 Bend Gate     1981 math      503            515         -1.07\n# ℹ 59 more rows\n\n\nLet’s walk through that code\n\nStart with our df_ts_long and assign the output to df_ts_long_std\nGroup the data by both test and school\n\n\nThis is for us to be able to get a starting point for each test at each school\n\n\nArrange the data in order of date (it already way, but since we are reliant on that, it’s best to check)\nCreate a new variable which is the first() score (since we are grouped by test and school, it will do it for each test/school combo)\nCreate another new variable score_std_sch using the score_year_one as zero\nUngroup the data since we are done with the calculation\n\n\n## facet line graph, with colour = test and ~school\nggplot(data = df_ts_long_std) +\n  geom_line(mapping = aes(x = year, y = score_std_sch, colour = test)) +\n  facet_wrap(~school)\n\n\n\n\n\n\n\n\nAnd there we have a really informative plot, showing how test scores of all types have changed since 1980 across all four schools. Hopefully this shows that good plots and good data wrangling (and sometimes a little bit of stats) go hand in hand!",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#questions",
    "href": "05-viz-i.html#questions",
    "title": "I: Basics",
    "section": "Questions",
    "text": "Questions\n\nHow does student socioeconomic status differ between students who ever attended college and those who did not?\nHow do educational expectations (of both students and parents) differ by high school completion status?\n\n\nHint: you will want to borrow some code from last week’s assignment here\n\n\nWhat is the relationship between student socioeconomic status and math test score?\n\nDo not worry about axis labels or other visual elements, that is covered next week.\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the lesson). Assignments will be graded on the following Monday (time permitting) in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#solution",
    "href": "05-viz-i.html#solution",
    "title": "I: Basics",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Viz I Solution(s)]\n##' [INIT: February 10 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\")) |&gt;\n  mutate(x1ses = ifelse(x1ses %in% c(-8, -9), NA, x1ses),\n         x4evratndclg = ifelse(x4evratndclg %in% c(-8, -9), NA, x4evratndclg),\n         x1txmtscor = ifelse(x1txmtscor %in% c(-8, -9), NA, x1txmtscor),\n         x1poverty185 = ifelse(x1poverty185 %in% c(-8, -9), NA, x1poverty185),\n         x1stuedexpct = ifelse(x1stuedexpct %in% c(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% c(-8, -9, 11), NA, x1paredexpct)) |&gt;\n  drop_na()\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Option One: Filled Histogram/Density Plot\n\nggplot(df) +\n  geom_histogram(aes(x = x1ses,\n                     fill = factor(x4evratndclg)))\n\n## or...\n\nggplot(df) +\n  geom_density(aes(x = x1ses,\n                     fill = factor(x4evratndclg)))\n\n\n## Option Two: Box Plot\n\nggplot(df) +\n  geom_boxplot(aes(x = factor(x4evratndclg),\n                   y = x1ses,\n                   fill = factor(x4evratndclg)))\n\n\n## Option Three: Jitter Scatter Plot\n\nggplot(df) +\n  geom_point(aes(x = x1ses,\n                  y = factor(x4evratndclg),\n                  color = factor(x4evratndclg)),\n              #position = \"jitter\",\n              size = 0.5,\n              alpha = 0.5)\n\nggplot(df) +\n  geom_jitter(aes(x = x1ses,\n                  y = factor(x4evratndclg),\n                  color = factor(x4evratndclg)),\n              size = 0.5,\n              alpha = 0.5)\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\n## Note: Just plotting each type separately was okay for the assignment, but it's better if we\n## can get everything on one plot, let's look at a few ways of doing that\n\n## Pivot the data long in expectation type\n\ndf_long &lt;- df |&gt;\n  pivot_longer(cols = c(x1stuedexpct, x1paredexpct),\n               names_to = \"expect_type\",\n               values_to = \"expect_value\")\n\n## Option One: Bar Chart (Treat as Categorical)\n\n## This is skewed so heavily how many people graduated high school\nggplot(df_long) +\n  geom_bar(aes(x = factor(x4hscompstat),\n               fill = factor(expect_value)),\n           color = \"white\") + \n  facet_wrap(~expect_type) ## We didn't get to this, so I didn't expect anyone to use it\n\n## But we can add position = \"fill\" to change the bar chart to proportions, which\n## makes it a bit better, still not great\nggplot(df_long) +\n  geom_bar(aes(x = factor(x4hscompstat),\n               fill = factor(expect_value)),\n           color = \"white\",\n           position = \"fill\") + ## https://stackoverflow.com/questions/46984296/proportion-with-ggplot-geom-bar\n  facet_wrap(~expect_type)\n\n\n## Option 2: Density Plot (Treat as Continuous)\n\nggplot(df_long) +\n  geom_boxplot(aes(x = factor(x4hscompstat),\n                   y = expect_value,\n                   fill = factor(expect_type)))\n\n## Or...\n\nggplot(df_long) +\n  geom_density(aes(x = expect_value,\n                      fill = factor(expect_type)),\n               alpha = 0.5) +\n  facet_wrap(~x4hscompstat)\n\n\n## Option 3: Wrangle the Data First\n\ndf_sum &lt;- df_long |&gt;\n  ## Make a new variable if expectation is bachelors or above\n  mutate(bach_plus = ifelse(expect_value &gt;= 6, 1, 0)) |&gt;\n  group_by(x4hscompstat, expect_type) |&gt;\n  ## Get the count of each value grouped by completion status and expect_type\n  count(bach_plus)\n\nggplot(df_sum) +\n  geom_col(aes(x = expect_type,\n               y = n,\n               fill = factor(bach_plus)),\n           position = \"fill\", ## https://stackoverflow.com/questions/46984296/proportion-with-ggplot-geom-bar\n           alpha = 0.8) +\n  facet_wrap(~x4hscompstat)\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n## Option One: Scatter Plot with Line\n\nggplot(df,\n       aes(x = x1ses,\n           y = x1txmtscor)) +\n  geom_point(size = 0.1) + ##'[Using size is a better way to avoid the blob effect]\n  geom_smooth(method = \"lm\")\n\n\n## Option Two: Heatmap with Line\n\nggplot(df,\n       aes(x = x1ses,\n           y = x1txmtscor)) +\n  geom_bin_2d() +\n  geom_smooth(method = \"lm\") +\n  scale_fill_gradient(high = \"navy\", low = \"skyblue\")\n## This also gets close to falling in the trap of \"looking cool for the sake of it\"\n\n\n## Option Three: Use a Categorical SES Indicator\n\nggplot(df) +\n  geom_boxplot(aes(x = factor(x1poverty185),\n                   fill = factor(x1poverty185),\n                   y = x1txmtscor))\n\n## Or...\n\nggplot(df) +\n  geom_jitter(aes(x = x1txmtscor,\n                  y = factor(x1poverty185),\n                  color = factor(x1poverty185)),\n              size = 0.5,\n              alpha = 0.5)\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "href": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "title": "I: Enter the tidyverse",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\n\nThroughout this lesson (and class), we are going to lean heavily on the tidyverse collection of packages\n\nIf you don’t already have this installed, use install.packages(\"tidyverse\")\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#check-working-directory",
    "href": "03-wrangle-i.html#check-working-directory",
    "title": "I: Enter the tidyverse",
    "section": "Check working directory",
    "text": "Check working directory\n\nThis script — like the one from the organizing lesson — assumes that the class folder is the working directory and that the required data file is in the data sub-directory\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that I’m not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, I tell you where you need to be (a common landmark), let you get there, and then rely on relative paths afterwards\n\n\nReading Data in with read_csv()\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\n## data are CSV, so we use read_csv() from the readr library\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nYou may notice the read_csv() prints out information about the data just read in. Nothing is wrong! The read_csv() function, like many other functions in the tidyverse, assumes you’d rather have more rather than less information and acts accordingly\nUp to now, this should all seem fairly consistent with last week’s lesson. This is just the set up, now it’s time to dive in!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#pipe-operator-in-r",
    "href": "03-wrangle-i.html#pipe-operator-in-r",
    "title": "I: Enter the tidyverse",
    "section": "Pipe Operator |> in R",
    "text": "Pipe Operator |&gt; in R\nThe pipe is one of the things that makes R code more intuitive than other programming languages, as it allows us write code in the order we think about it, passing it one from one function to another, rather than nesting it like traditional code would be written\nFor this example don’t worry about the actual processes (we will go over them more below), just look at how much more intuitive the code is with |&gt;s.\nFirst, let’s say I want to take the data we just read in and select the x1txmtscor (math test scores) column\n\nWithout |&gt;\n\n## Without |&gt;\nselect(df, x1txmtscor)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor)\n\nNeither is that confusing… But, what if we want to take that output and select only students with a math score above 50?\n\n\nWithout |&gt;\n\n## Without |&gt;\nfilter(select(df, x1txmtscor), x1txmtscor &gt; 50)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50)\n\nSee how the non-piped version is getting messy? Let’s add one more level to really make the point, creating a new variable that is the square root of the test score\n\n\nWithout |&gt;\n\n## Without |&gt;\nmutate(filter(select(df, x1txmtscor), x1txmtscor &gt; 50), square_root = sqrt(x1txmtscor))\n\n\n\nWith |&gt;\n\n## With |&gt;\ndf |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50) |&gt; mutate(square_root = sqrt(x1txmtscor))\n\nAs we are getting longer, I’m going to use a new line for each pipe, just to make it even clearer (it makes no difference to R)\n\n## Best to use  a new line for each pipe when code gets longer\ndf |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\nEven though we haven’t covered any of these commands yet, I think (tell me if I’m wrong) the |&gt; is still pretty easy to know roughly what’s going on. Whereas, the traditional nested way gets really tricky beyond a couple of commands.\nOf course, if you wanted, you could do each step separately, constantly assigning and overwriting an object like below\n\n## Without the |&gt;, we could technically break it down step by step\ntemp &lt;- select(df, x1txmtscor)\ntemp &lt;- filter(temp, x1txmtscor &gt; 50)\ntemp &lt;- mutate(temp, square_root = sqrt(x1txmtscor))\ntemp\n\nBut again, I think it’s less intuitive than simply piping the results.\n\nIf we do want to see step-by-step output in a piped command, you can either\n\nRun the code as you write it line-by-line (usually what I do)\nHighlight sections of the piped code to run up to a point\n\n\n\n\nAssigning Output from Pipes\n\nAssigning output from pipes is the same as we have covered a few times, we use a &lt;- to pass it backwards to an object (in this case we called that object df_backward_pass)\n\n\n## Always assign backwards\ndf_backward_pass &lt;- df |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\n\nIf you want to think of it this way, we are effectively continuing to pass it forward like this…\n\n\n## You can think of the assignment as a continuation of the pipe like this\n## but don't write it this way, it's then hard to find what you called something later\ndf |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor)) -&gt;\n  df_forward_pass\n\n\nThat’s how I first thought about how pipes and assignment work together, and you can see the outputs are all.equal()\n\n\n## Checking they are the same\nall.equal(df_backward_pass, df_forward_pass)\n\n[1] TRUE\n\n\n\nHowever, you really shouldn’t write code like this, as although it runs, it’s then hard to later find where you created df_forward_pass.\nStarting the string of code with where you store the result is MUCH clearer.\n\nBut, it really helped me to think about it this way at first, so hopefully it will help some of you understand how |&gt; and &lt;- work together\n\n\n\n\nA Brief History of the R Pipe\n\n\nThe pipe was originally a tidyverse invention, and used %&gt;% symbol, which is probably still a common pipe you see “in the wild”\nThe pipe we are using was brought into the Vanilla version of R a few years ago as |&gt;\nThe reason for the change is some benefits that are beyond the scope of this class, but I wanted you to be aware that |&gt; and %&gt;% are essentially the same thing\n\nYou can intermingle them, but, but for clarity I would stick with |&gt;",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#basic-tidyverse-commands",
    "href": "03-wrangle-i.html#basic-tidyverse-commands",
    "title": "I: Enter the tidyverse",
    "section": "Basic Tidyverse Commands",
    "text": "Basic Tidyverse Commands\n\nNow we have the pipe |&gt; covered, it’s time to dig into some basic data wrangling commands\n\n\nSelecting Variables/Columns with select()\n\nOften data sets contain hundreds, thousands, or even tens of thousands of variables, when we are only interested in a handful. Our first tidyverse command select() helps us deal with this, by, as you may have guessed, select()-ing the variables we want\nSince we are going to pipe our R commands, we start with our data then pipe it into the select() command\nIn the select() command, we list out the variables we want\n\nstu_id, x1stuedexpct, x1paredexpct, x1region\n\nNotice: in this (and most tidyverse) command(s) we don’t have to use “quotes” around the variable names. A common error message when you did need to put something in quotes is Error: object &lt;thing you should have \"quoted\"&gt; not found\n\n\n\n\ndf |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n# ℹ 23,493 more rows\n\n\n\nQuick question: if we want to use this reduced data-frame going forward, what should we do?\n\nThat’s right, assign it to an object! Let’s call that df_small\n\ndf_small &lt;- df |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n\nOur next step is to create a variable that’s the higher of student and parent expectations. Sounds simple enough, but first, we have a problem…\n\nCan anyone guess what it is?\n\n\n\n\nUnderstanding our data\nFirst things first, however, we need to check the code book to see what the numerical values for our two education expectation variables represent. To save time, I’ve copied them here:\n\nx1stuedexpct\nHow far in school 9th grader thinks he/she will get\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n\n\n\nx1paredexpct\nHow far in school parent thinks 9th grader will go\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n-9\nMissing\n\n\n\n\nThe good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression\nThe bad news is that we have three values — -8, -9, and 11 — that we need to deal with so that the averages we compute later represent what we mean\n\n\n\n\nExploring Catagorical Variables with count()\n\nFirst, let’s see how many observations are affected by these values using count()\n\n\nNotice that we don’t assign to a new object; this means we’ll see the result in the console, but nothing in our data or object will change\n\n\n## see unique values for student expectation\ndf_small |&gt; count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -8  2059\n 2            1    93\n 3            2  2619\n 4            3   140\n 5            4  1195\n 6            5   115\n 7            6  3505\n 8            7   231\n 9            8  4278\n10            9   176\n11           10  4461\n12           11  4631\n\n## see unique values for parental expectation\ndf_small |&gt; count(x1paredexpct)\n\n# A tibble: 13 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -9    32\n 2           -8  6715\n 3            1    55\n 4            2  1293\n 5            3   149\n 6            4  1199\n 7            5   133\n 8            6  4952\n 9            7    76\n10            8  3355\n11            9    37\n12           10  3782\n13           11  1725\n\n\n\nDealing with -8 and -9 is straight forward — we’ll convert it missing.\n\nIn R, missing values are technically stored as NA.\n\nNot all statistical software uses the same values to represent missing values (for example, STATA uses a dot .)\n\nNCES has decided to represent missing values as a limited number of negative values. In this case, -8 and -9 represent missing values\n\nNote: how to handle missing values is a very important topic, one we could spend all semester discussing\n\nFor now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results\nIn real research, a better approach is usually to impute missing values, but that is beyond our scope right now\n\nDeciding what to do with 11 is a little trickier. While it’s not a missing value per se, it also doesn’t make much sense in its current ordering, that is, to be “higher” than completing a professional degree\n\nFor now, we’ll make a decision to convert these to NA as well, effectively deciding that an answer of “I don’t know” is the same as missing an answer\n\nSo first step: convert -8, -9, and 11 in both variables to NA. For this, we’ll use the mutate() and ifelse() functions\n\n\n\nConditional Values with ifelse()\n\nifelse() is a really common command in R and has three parts\n\nstatement that can be TRUE or FALSE\nWhat to return if the statement is TRUE\nelse what to return when the statement is FALSE\n\n\n\n\nModifying an Existing Variable with mutate()\n\nWhen we want to add variables and change existing ones, we can use the mutate() function\n\nThe basic idea of mutate commands is mutate(&lt;where to go&gt; = &lt;what to go there&gt;)\n\nThis is probably the trickiest function we cover today to understand\n\nNew variables are created if you provide &lt;where to go&gt; a new variable name (or nothing)\nVariables are modified if you provide &lt;where to go&gt; an existing variable name\n&lt;what to go there&gt; can as simple as a single number all the way to a chain of piped functions, so long as there’s a clear answer for every row\n\nIn this case, we want to modify x1stuedexpct to be NA when x1stuedexpct is -8, -9, or 11\n\nNow, we have three values we want to covert to NA, so we could do them one-at-a-time, like below\n\ndf_small &lt;- df_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct == -8, NA, x1stuedexpct))\n\n\nLet’s walk through this code\n\n\nAssign the results back to df_small (which will overwrite our previous df_small)\nTake df_small and pipe |&gt; it into mutate()\nInside mutate() assign our results to x1stuedexpct (which will modify the existing variable)\nModify x1stuedexpct with an ifelse() statement, which remember has 3 parts\n\n\n\nstatement which is asking “is x1stuedexpct == -8”? - Notice == means “is equal to”, while = means “assign to”. Yes it’s confusing, but you’ll get it over time!\nif that statement is true, make it NA\nelse (if the statement is false) return the original variable x1stuedexpct\n\nOkay, make sense? Let’s see what we just did (look at row 26)\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           11           11        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           11           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           -8        2\n19  10020            8            8        4\n20  10021            8           11        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           -8        4\n24  10026            7           10        3\n25  10027           11            6        1\n26  10028           NA           -8        3\n# ℹ 23,477 more rows\n\n\n\nThis is fine, but we have to do it 3 times for both parent and student expectation\n\nInstead, can anyone think (not in R code, just in terms of logic) how we could change our statement piece of the ifelse() to be more efficient?\n\n\n\n\nBeing Efficient with %in% and c()\n\nWhat we can do, is group -8, -9, and 11 together into a list using c()\n\nc() is a very common function in R used to create a list\n\nThen, we can use the %in% operator to ask if that result is any of the numbers in that list\n\nThis keeps our code shorter and easier to read\n\n\n\ndf_small &lt;- df_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct %in% c(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% c(-8, -9, 11), NA, x1paredexpct))\n\n\nThe code now works just as above, but instead of asking if x1stuedexpct is equal to -8, it asks if it’s in the list of -8, -9, and 11, then does the same for parental expectations!\n\nLet’s view those first 26 rows again to see what we did\n\n\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           NA            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           NA        1\n 8  10008            8            6        1\n 9  10009           NA           NA        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           NA           NA        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           NA           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           NA        2\n19  10020            8            8        4\n20  10021            8           NA        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           NA        4\n24  10026            7           10        3\n25  10027           NA            6        1\n26  10028           NA           NA        3\n# ℹ 23,477 more rows\n\n\n\nJust to be doubly-sure, lets check count() again\n\n\ndf_small |&gt; count(x1stuedexpct) \n\n# A tibble: 11 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    93\n 2            2  2619\n 3            3   140\n 4            4  1195\n 5            5   115\n 6            6  3505\n 7            7   231\n 8            8  4278\n 9            9   176\n10           10  4461\n11           NA  6690\n\ndf_small |&gt; count(x1paredexpct)\n\n# A tibble: 11 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    55\n 2            2  1293\n 3            3   149\n 4            4  1199\n 5            5   133\n 6            6  4952\n 7            7    76\n 8            8  3355\n 9            9    37\n10           10  3782\n11           NA  8472\n\n\nSuccess!\n\n\nCreating a New Variable with mutate()\n\nSo, with that tangent out of the way, let’s get back to our original task, creating a new variable that is the highest of parental and student expectations\nTo make a new variable which is the highest of two variables, we can use our friends mutate() and ifelse() some more\n\n\ndf_small &lt;- df_small |&gt;\n  mutate(high_exp = ifelse(x1stuedexpct &gt; x1paredexpct, x1stuedexpct, x1paredexpct))\n\n\nThat code is almost what we want to do\n\nIf x1stuedexpct is higher then take that, if not, take x1paredexpct\n\nThere’s two things I haven’t fully accounted for though…\n\nOne doesn’t actually matter here, but might in other circumstances\nOne definitely matters here\n\nWithout scrolling past the duck, can you tell me what they might be?\n\n\n\n\n\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.\n\n\n\nSloppy Mistake 1 (doesn’t matter here)\n\nI was a little sloppy with the statement piece, I just asked if x1stuedexpct was greater than x1paredexpct or not\n\nIf I was being more careful, I might have said “greater than or equal to”\n\nWhy doesn’t this matter in this context, and when might it matter?\n\n\n\n\n\nSloppy Mistake 2 (does matter here)\n\nNow let’s check our data frame to see the one that does matter\n\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10002           NA            6        1       NA\n 3  10003           10           10        4       10\n 4  10004           10           10        3       10\n 5  10005            6           10        3       10\n 6  10006           10            8        3       10\n 7  10007            8           NA        1       NA\n 8  10008            8            6        1        8\n 9  10009           NA           NA        3       NA\n10  10010            8            6        1        8\n11  10011            8            6        3        8\n12  10012           NA           NA        2       NA\n13  10013            8           10        3       10\n14  10014            2            6        3        6\n15  10015           NA           10        3       NA\n16  10016            4            6        2        6\n17  10018            6            7        2        7\n18  10019            8           NA        2       NA\n19  10020            8            8        4        8\n20  10021            8           NA        2       NA\n21  10022           10            8        4       10\n22  10024            6            8        2        8\n23  10025            8           NA        4       NA\n24  10026            7           10        3       10\n25  10027           NA            6        1       NA\n26  10028           NA           NA        3       NA\n# ℹ 23,477 more rows\n\n\n\nHmm, that seems odd, why would R consider NA to be greater than 6?\n\nAny thoughts?\n\nGenerally, R is overly-cautious when dealing with NAs to ensure you don’t accidentally drop them without realizing it\n\nFor example, if I asked you what the mean(c(5, 6, 4, NA)) would be, you’d probably say 5, right?\n\nR is never going to just ignore the NA values like that unless we tell it to\n\n\n\n\nmean(c(5, 6, 4, NA))\n\n[1] NA\n\n\n\nSee, what have to explicitly tell it to remove the NA values\n\n\nmean(c(5, 6, 4, NA), na.rm = T)\n\n[1] 5\n\n\n\nSo in our case of trying to get the highest expectation, R doesn’t want us to forget we have NA values, so it throws them at us.\nFor now, let’s keep the results we got, but, if it gave us an NA and there is a non-NA value in other other column, sub that in\n\n\n\n\nDealing With Missing Values with is.na(), &, and !\n\nTo do this, we will add another couple of code helpers -is.na(high_exp) simply asks if the high_exp is NA or not\n\nR doesn’t let you just say high_exp == NA -! is really helpful tool, which can be used to negate or invert a command\n\n!is.na(x1stuedexpct) just returns the opposite of is.na(x1stuedexpct) so it tells us that the column is not NA -& can be useful inside conditional statements, as it means both must be TRUE (FYI: | means or)\n\n\nUsed together, we can now ensure we have as few NAs as possible in the data (there will still be some when both student and parent were NA)\n\n\ndf_small &lt;- df_small |&gt;\n  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(x1stuedexpct), x1stuedexpct, high_exp),\n         high_exp = ifelse(is.na(high_exp) & !is.na(x1paredexpct), x1paredexpct, high_exp))\n\n\nLet’s print this one last time to check our work\n\n\nprint(df_small, n = 26)\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10002           NA            6        1        6\n 3  10003           10           10        4       10\n 4  10004           10           10        3       10\n 5  10005            6           10        3       10\n 6  10006           10            8        3       10\n 7  10007            8           NA        1        8\n 8  10008            8            6        1        8\n 9  10009           NA           NA        3       NA\n10  10010            8            6        1        8\n11  10011            8            6        3        8\n12  10012           NA           NA        2       NA\n13  10013            8           10        3       10\n14  10014            2            6        3        6\n15  10015           NA           10        3       10\n16  10016            4            6        2        6\n17  10018            6            7        2        7\n18  10019            8           NA        2        8\n19  10020            8            8        4        8\n20  10021            8           NA        2        8\n21  10022           10            8        4       10\n22  10024            6            8        2        8\n23  10025            8           NA        4        8\n24  10026            7           10        3       10\n25  10027           NA            6        1        6\n26  10028           NA           NA        3       NA\n# ℹ 23,477 more rows\n\n\n\nOkay, looks good!\nThere are other ways we could have gone about this analysis, some more sophisticated and slicker\n\nBut, what matters more is that we checked our work as we went, caught the issues, and finished with the correct data\n\nWe will learn more tools along the way that speed things up, especially in the brand new Data Wrangling IV Lesson\nHowever, the more steps you take at once, the more you have to check at once!\nThere’s often never a single correct way to get to the right answer, so long as you get there, it’s clear what you did, and you catch issues as they come up\n\n\n\n\n“The point to keep in mind that the process is often iterative (two steps forward, one step back…) and that there’s seldom an single correct way.” B.T. Skinner\n\n\nNow it’s made correctly, let’s check the counts of our new variable\n\n\n## get summary of our new variable\ndf_small |&gt; count(high_exp)\n\n# A tibble: 11 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1    71\n 2        2  2034\n 3        3   163\n 4        4  1282\n 5        5   132\n 6        6  4334\n 7        7   191\n 8        8  5087\n 9        9   168\n10       10  6578\n11       NA  3463\n\n\n\nHmm… We still have a large number of NAs, meaning neither the parent or student provided an expectation\n\nIn more sophisticated analyses, this is where we might need to think about imputation or something else to handle the missing-ness\n\nFor the sake of this lesson, however, we just want to drop the observations with NA for high_exp",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "href": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "title": "I: Enter the tidyverse",
    "section": "Keeping Rows Based on a Condition with filter()",
    "text": "Keeping Rows Based on a Condition with filter()\n\nTo do this, we are going to use the filter() command from tidyverse\nfilter() works by only keeping observations that meet the condition(s) we set\n\nAs in, to make it through the filter, a row must answer “yes” to “does it meet this condition?”\n\n\n\n## filter out missing values\ndf_small_cut &lt;- df_small |&gt; filter(!is.na(high_exp))\n\n\nI re-used our !is.na() helper to keep all rows that answer “yes” to “are you not NA?”\n\nDouble negatives are something you’ll have to get used to in coding, sorry\nNotice, instead of overwriting df_small we assigned this to a new object df_small_cut\n\nGenerally, when making substantial changes to a data set like dropping observations, I like to be able to double check what I did, which is easier if we make a new df\n\n\n\n\nQuick Question: A commmon confusion from this lesson is between filter() and select(). Can someone explain when you’d use select() over filter()?\n\n\nTo see if that worked, let’s count() our new variable one more time\n\n\ndf_small_cut |&gt; count(high_exp)\n\n# A tibble: 10 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1    71\n 2        2  2034\n 3        3   163\n 4        4  1282\n 5        5   132\n 6        6  4334\n 7        7   191\n 8        8  5087\n 9        9   168\n10       10  6578\n\n\n\nOkay, so no NAs, perfect!\nJust to be extra sure we only removed NAs, we can check the difference in how many rows our original df_small has with df_small_cut\n\nWho can tell me how many rows should have been dropped?\n\n\n\n## does the original # of rows - current # or rows == NA in count?\nnrow(df_small) - nrow(df_small_cut)\n\n[1] 3463\n\n\n\nnrow() does what you’d expect, counts the number of rows\n\nYou could also just check by looking at the dfs in the environment tab, but this way leaves no room for mental math errors\n\n\n\nSummarizing Data with summarize()\n\nOkay, so we have our data selected, we made our high_exp variable, and we’ve done some work to handle missing data\nIn our week one data talk we discussed how massive tables of data are not particularly helpful to for someone to read or interpret\n\nWe will cover how to make graphs of our data in a few weeks\nFor our final task today, we are going to make some summary tables using summarize() from the tidyverse\n\nsummarize() allows us to apply a summary statistic (mean, sd, median, etc.) to a column in our data\nsummarize() takes an entire data frame as an input, and spits out a small data frame with the just the summary variables\n\nNote: for this reason, you rarely ever want to assign &lt;- the output of summarize() back to the main data frame object, as you’ll overwrite it\n\nYou can either spit the summary tables out into the console without assignment (which we will do) or if you need to use them for something else, assign them to a new object\n\n\n\n\n## get average (without storing)\ndf_small_cut |&gt; summarize(mean(high_exp))\n\n# A tibble: 1 × 1\n  `mean(high_exp)`\n             &lt;dbl&gt;\n1             7.27\n\n\n\nSee, the output is a 1x1 table with the mean expectation mean(high_exp) of 7.27, just above a bachelors degree\n\nNote: if we want to name the summary variable, we can name it just like we did earlier in mutate() with a single =\n\n\n\ndf_small_cut |&gt; summarize(mean_exp = mean(high_exp))\n\n# A tibble: 1 × 1\n  mean_exp\n     &lt;dbl&gt;\n1     7.27\n\n\n\nBut, that wasn’t quite the question we were asked\n\nWe were asked if it varied by region…\n\nFor time’s sake, I can tell you the region variable is x1region and splits the US in 4 Census regions\n\n\n\n\n\nGrouping Data with group_by()\n\nThe group_by() function, following the tidyverse principle of intuitive naming, groups the data and outputs by the variable(s) you say\n\nSo, since we want to calculate the average high expectation by region, we group_by(x1region)\n\nSince we just want it for our summarize(), we just add it to the pipe\n\nIf you wanted to save the data in it’s group_by()-ed state, you could assign it to something\n\n\n\n\n\n## get grouped average\ndf_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp))\n\n# A tibble: 4 × 2\n  x1region mean_exp\n     &lt;dbl&gt;    &lt;dbl&gt;\n1        1     7.39\n2        2     7.17\n3        3     7.36\n4        4     7.13\n\n\n\nSuccess! While expectations are similar across the country, there’s some variance by region\n\nWhile there are few things we could do to make this a little fancier (e.g., changing the region numbers to names, formatting the table, etc.) we have answered our question, and have clear documentation of how we got here, so I will call that a win!\n\n\n\n\nSaving Data with write_csv()\n\nSometimes we want to be able to access objects from scripts without having to re-run the whole thing\n\nRemember: one of the main advantages of R is the data we read in is untouched\n\nTo do this, we want to write_ a new csv() file, containing our modified data\n\nunlike read_csv() which only needed a file name/path, write_csv() needs to know what you’re trying to save and the file name/path you want to save it to\n\nThe only way you can overwrite or change the original data is by saving to the same file name as the original data, so NEVER do that!\n\n\nSince we didn’t assign our summary table to anything, we can just add write_csv() to the end of the pipe and add a file.path()\n\nIf you want to save a data frame you already assigned to an object write_csv(&lt;object&gt;, file.path(&lt;path&gt;)) would work just fine!\n\n\n\n## write with useful name\n\ndf_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  write_csv(file.path(\"data\", \"region-expects.csv\"))\n\nPhew!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#appendix-all-at-once",
    "href": "03-wrangle-i.html#appendix-all-at-once",
    "title": "I: Enter the tidyverse",
    "section": "Appendix: All at Once",
    "text": "Appendix: All at Once\nWe went through that piece by piece to demonstrate each function, but, there’s no reason we can’t just |&gt; pipe it all together\n\n## Let's redo the analysis above, but with a fully chained set of\n## functions.\n\n## start with original df\ndf |&gt;\n  ## select columns we want\n  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |&gt;\n  ## If expectation is -8, -9. or 11, make it NA\n  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),\n         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |&gt;\n  ## Make a new variable called high_exp that is the higher or parent and student exp\n  mutate(high_exp = ifelse(student_exp &gt; parent_exp, student_exp, parent_exp)) |&gt;\n  ## If one exp is NA but the other isn't, keep the value not the NA\n  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),\n         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |&gt;\n  ## Drop is high_exp is still NA (neither parent or student answereed)\n  filter(!is.na(high_exp)) |&gt;\n  ## Group the results by region\n  group_by(x1region) |&gt;\n  ## Get the mean of high_exp (by region)\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  ## Write that to a .csv file\n  write_csv(file.path(\"data\", \"region-expects-chain.csv\"))\n\nTo double check, let’s just check these are the same…\n\nnon_chain &lt;- read_csv(file.path(\"data\", \"region-expects.csv\"))\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchain &lt;- read_csv(file.path(\"data\", \"region-expects-chain.csv\"))\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nall.equal(non_chain, chain)\n\n[1] TRUE\n\n\nWooHoo!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#final-notes",
    "href": "03-wrangle-i.html#final-notes",
    "title": "I: Enter the tidyverse",
    "section": "Final notes",
    "text": "Final notes\n\nThis rather lengthy lesson has thrown you in the (medium) deep end of the coding pool\n\nBy no means are you expected to get everything we just did\n\nWe will continue to revisit all these commands throughout the class, by the end of the semester, they will be second nature!\n\n\nWe also saw how to use code to answer a realistic question we might be asked in a data management job, a translation skill that will prove invaluable later on!\n\nWe had to plan out steps and then make some adjustments along the way (e.g., our NA issues), that’s all part of the process!\n\n\n\n“Becoming a better quantitative researcher mostly means becoming a better translator: question –&gt; data/coding –&gt; answer.” B.T. Skinner",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#questions",
    "href": "03-wrangle-i.html#questions",
    "title": "I: Enter the tidyverse",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nWhat is the average standardized math test score?\nHow does this differ by gender?\n\n\n\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (include what that category represents)\n\n\n\n\nOf the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nHow does this differ by region?\n\n\n\n\n\n\n\nWhat percentage of students ever attended a post-secondary institution by February 2016?\nGive the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35k within each region\nHint: group_by() can be given more than one group\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the lesson). Assignments will be graded on the following Monday (time permitting) in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#solution",
    "href": "03-wrangle-i.html#solution",
    "title": "I: Enter the tidyverse",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Wrangling I Solution]\n##' [INIT: Jan 28 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\n## note to matt\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Part One\ndf |&gt;\n  filter(!x1txmtscor %in% c(-8, -9)) |&gt;\n  summarize(mean = mean(x1txmtscor))\n\n## Part Two\nmath &lt;- df |&gt;\n  filter(!x1txmtscor %in% c(-8, -9),\n         x1sex != -9) |&gt;\n  group_by(x1sex) |&gt;\n  summarize(mean = mean(x1txmtscor))\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf |&gt;\n  filter(x1poverty185 == 1,\n         !x1famincome %in% c(-8,-9)) |&gt;\n  summarize(med_inc_cat = median(x1famincome))\n\nprint(\"Median income category in 2, which represents Family income &gt; $15,000 and &lt;= $35,000\")\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n## Simplified\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  count(x4hscompstat) |&gt;\n  mutate(perc = n / sum(n) * 100)\n\n## Part One\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  summarize(ged = sum(x4hscompstat == 2),\n            total = sum(x4hscompstat %in% c(1,2)),\n            perc = ged/total*100)\n\n## Part Two\n## Simplified\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  group_by(x1region) |&gt;\n  count(x4hscompstat) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  filter(x4hscompstat == 1)\n\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  group_by(x1region) |&gt;\n  summarize(ged = sum(x4hscompstat == 2),\n            total = sum(x4hscompstat %in% c(1,2)),\n            perc = ged/total*100)\n\n## ---------------------------\n##' [Q4]\n## ---------------------------\n\n## Part One\ndf |&gt;\n  filter(x4evratndclg != -8) |&gt;\n  count(x4evratndclg) |&gt;\n  mutate(perc = n / sum(n) * 100)\n\n\ndf |&gt;\n  filter(x4evratndclg != -8) |&gt;\n  summarize(college = sum(x4evratndclg == 1),\n            total = sum(x4evratndclg %in% c(0, 1)),\n            perc = college/total*100)\n\n## Part Two\ndf |&gt;\n  filter(x4evratndclg != -8,\n         !x1famincome %in% c(-8, -9)) |&gt;\n  mutate(below_35k = ifelse(x1famincome %in% c(1,2), 1, 0)) |&gt;\n  group_by(x1region, below_35k) |&gt;\n  count(x4evratndclg) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  filter(x4evratndclg == 1)\n\n\n\ndf |&gt;\n  filter(x4evratndclg != -8,\n         !x1famincome %in% c(-8, -9)) |&gt;\n  mutate(below_35k = ifelse(x1famincome %in% c(1,2), 1, 0)) |&gt;\n  group_by(x1region, below_35k) |&gt;\n  summarize(college = sum(x4evratndclg == 1),\n            total = sum(x4evratndclg %in% c(0, 1)),\n            perc = college/total*100)\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "01-set-install.html#getting-started",
    "href": "01-set-install.html#getting-started",
    "title": "I: Installing R & RStudio",
    "section": "Getting started",
    "text": "Getting started\nThe primary pieces of software you are going to need for this class are\n\nR\nRStudio\nMicrosoft Office\n\nI am going to assume you already have this, but we can meet to install it if not, it is free for UF students\n\n\nThere are also a few optional pieces of software you’ll need for extra credit lessons, but we will cover when needed.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-r",
    "href": "01-set-install.html#installing-r",
    "title": "I: Installing R & RStudio",
    "section": "Installing R",
    "text": "Installing R\n\nR is fantastic (hopefully you will see that throughout the course), but sometimes it can make things seem more complicated than they need to\n\nThe first time it does this is when trying to install it, there’s a bunch of options called “mirrors”\n\nThese are basically to reduce strain on the servers that you download from by using the closest location\n\nThe good news, however, is that a URL that automates this whole process for you came out recently\nThe even better news is that I’ve set up a little portal to that URL here, so you can download it without leaving this page\n\n\n\n\nClick the option for the OS you have (Windows/Mac/Linux)\nThen under the “latest release”\n\n\n\nFor windows users, select “base” then “Download R…” (the top options)\nFor mac users, select either apple silicon or intel options depending on how new your mac is\n\nIf you need to check which kind your mac is, hit the apple logo in the top left of your screen, then “About This Mac” then under “Processor” it will either intel or silicon. I can also help you with this in class if needed.\n\n\n\n\nR will then download, double click on the download when it’s finished and then follow the on-screen prompts\nR is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-rstudio",
    "href": "01-set-install.html#installing-rstudio",
    "title": "I: Installing R & RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nTechnically, R is all you need to do all of our analyses. However, to make it accessible and usable, we also need a “development environment”\n\nThe reason of this, unlike a computer program like Stata or SAS, R is a programming language (same as Python, C++, etc.), that’s what we just installed\nThe easiest way to use programming languages is through a “development environment”\n\nThere are multiple “development environments” you can use for R. VSCode is a great option by Microsoft for using a variety of languages, but, the best option for R is RStudio as it is purpose built for the language (it also works with Python too)\n\n\nI did try to create another little portal below, but, sadly Posit (the company who created RStudio) have disabled the way I do that. Instead…\n\n\nGo to this site and click the “Download RStudio Desktop for…” button underneath “2: Install RStudio” (we already did step 1)\n\n\nThis is simpler than installing R, Posit have a more sophisticated website which will automatically download the right version for your computer\n\n\nRStudio will then download, double click on the download when it’s finished and then follow the on-screen prompts\n\n\nFor mac users, this will just be drag n’ drop RStudio into your Applications folder\n\n\nRStudio is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#lets-see-what-we-just-installed",
    "href": "01-set-install.html#lets-see-what-we-just-installed",
    "title": "I: Installing R & RStudio",
    "section": "Let’s See What We Just Installed",
    "text": "Let’s See What We Just Installed\n\nHopefully, you should now be able to open RStudio on your computer (it should be the same place all your software is kept)\n\nGo ahead and open it up!\n\n\nBy default, RStudio has 3-4 main frames:\n\nTop left: Script window (will be closed at first if you don’t have any scripts open)\nBottom left: Console\nTop right: Environment / History / Connections\nBottom right: Files / Plots / Packages / Help / Viewer\n\nFor today, we are mostly going to explore some basic features of R using the console, copying and pasting commands from the website rather than saving them in a script. Fear not, however, as there is already an .R script containing all these commands saved in your class folder that we will set up next week. - All an .R script does is save your code and pass it line-by-line to the console - After today, anything we want to save will be done through a script, anything we just need to run one time will be done in console",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#basic-r-commands",
    "href": "01-set-install.html#basic-r-commands",
    "title": "I: Installing R & RStudio",
    "section": "Basic R Commands",
    "text": "Basic R Commands\nFirst, let’s try the traditional first command!\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nWe can also use R like a basic calculator\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#assignment",
    "href": "01-set-install.html#assignment",
    "title": "I: Installing R & RStudio",
    "section": "Assignment",
    "text": "Assignment\n\nThe first two commands we ran simply spat the output out in the console\n\nThis can be useful if you want to check something quickly or if we have our final output\n\nMore often, though, we want to save the output to our R Environment (top right panel)\nTo do this, we need to assign the output to an object”\n\nR is a type of object-oriented programming environment. This means that R thinks of things in its world as objects, which are like virtual boxes in which we can put things: data, functions, and even other objects.\n\nIn R (for quirky reasons), the primary means of assignment is the arrow, &lt;-, which is a less than symbol, &lt;, followed by a hyphen, -.\n\nYou can use = (which is more common across other programming languages), and you may see this “in the wild”\nBut R traditionalists prefer &lt;- for clarity and readability\nWhich you use is up to you! (I generally use &lt;-)\n\n\n\n## assign value to object x using &lt;-\nx &lt;- 1\n\n\nBut’s where’s the output?\n\nCheck out the “Environment” tab on the top left panel\n\nWe see something called x has a value of 1\n\nNow let’s call that object\n\n\n\n\n## what's in x?\nx\n\n[1] 1\n\n\nNote: the [1] is just the index (order) number, if we had more than 1 thing in our object, that would be more useful\n\nQuick exercise\nUsing the arrow, assign the output of 1 + 1 to x. Next subtract 1 from x and reassign the result to x. Show the value in x.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#comments",
    "href": "01-set-install.html#comments",
    "title": "I: Installing R & RStudio",
    "section": "Comments",
    "text": "Comments\nFor this section, let’s just open a blank .R script in RStudio (again, all these commands will be in a script in your class folder we set up next week)\n\nComments in R are set off using the hash or pound character at the beginning of the line: #\nThe comment character tells R to ignore the line\n\n\nQuick exercise\nType the phrase “This is a comment” directly into the R console both with and without a leading “#”. What happens each time?\n\n\nYou may notice I (Ben, and therefore, now me) use two hashes\n\nYou can use only a single # for your comments if you like, R treats them all the same\nIf you’re typing longer comments ##' (two hashes and an apostrophe) is really useful in RStudio, as it automatically comments the next line (although this can be annoying at times too)\nIf you want to take your comments to the next level ##' in RStudio also enables some fun color coding options with @, [], **, and :\n\nThese can be useful in longer scripts to draw attention to specific points\nNote: these only show up in RStudio\n\n\n\n\n##' @Matt needs to work on this  \n##' Matt needs to work on [this]\n##' Matt *needs* to work on this\n##' [Matt: needs to work on this]\n\n\nLastly, RStudio can comment/uncomment multiple lines of code you’ve already written\n\nOn the top menu bar select “Code” then “Commment/Uncomment Lines”\n\nAlso see the keyboard shortcut next to that option!\n\n\nThis is a big time saver!\n\n\n## Try commenting/uncommenting the below line\n\n# Matt &lt;- \"Hi\"",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#data-types-and-structures",
    "href": "01-set-install.html#data-types-and-structures",
    "title": "I: Installing R & RStudio",
    "section": "Data types and structures",
    "text": "Data types and structures\nR uses variety of data types and structures to represent and work with data. There are many, but the major ones that you’ll use most often are:\n\nlogical\nnumeric (integer & double)\ncharacter\nvector\nmatrix\nlist\ndataframe\n\nLet’s see what type of object x we created earlier is\n\ntypeof(x)\n\n[1] \"double\"\n\n\nWhat if we make it “1”?\n\nx &lt;- \"1\"\ntypeof(x)\n\n[1] \"character\"\n\n\nUnderstanding the nuanced differences between data types is not important right now. Just know that they exist and that you’ll gain an intuitive understanding of them as you become better aquainted with R.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#packages",
    "href": "01-set-install.html#packages",
    "title": "I: Installing R & RStudio",
    "section": "Packages",
    "text": "Packages\n\nUser-submitted packages are a huge part of what makes R great\nYou may hear me use the phrases “base R” or “vanilla R” during class\n\nWhat I mean by this is the R that comes as you download it with no packages loaded\nWhile it’s powerful in and of itself — you can do everything you need with base R — most of your scripts will make use of one of more contributed packages = These will make your data analytic life much nicer. We’ll lean heavily on the tidyverse suite of packages this semester.\n\n\n\nInstalling packages from CRAN\n\nMany contributed packages are hosted on the CRAN package repository. - What’s really nice about CRAN is that packages have to go through quite a few checks in order for CRAN to approve and host them. Checks include;\n\nMaking sure the package has documentation\nWorks on a variety of systems\nDoesn’t try to do odd things to your computer\n\nThe upshot is that you should feel okay downloading these packages from CRAN\n\nTo download a package from CRAN, use:\n\ninstall.packages(\"&lt;package name&gt;\")\n\nNOTE Throughout this course, if you see something in triangle brackets (&lt;...&gt;), that means it’s a placeholder for you to change accordingly.\nMany packages rely on other packages to function properly. When you use install.packages(), the default option is to install all dependencies. By default, R will check how you installed R and download the right operating system file type.\n\nQuick exercise\nInstall the tidyverse package, which is really a suite of packages that we’ll use throughout the semester. Don’t forget to use double quotation marks around the package name:\n\n\ninstall.packages(\"tidyverse\")\n\n\n\nLoading package libraries\nPackage libraries can loaded in a number of ways, but the easiest it to write:\n\nlibrary(\"&lt;library name&gt;\")\n\nwhere \"&lt;library name&gt;\" is the name of the package/library. You will need to load these before you can use their functions in your scripts. Typically, they are placed at the top of the script file.\nFor example, let’s load the tidyverse library we just installed:\n\n## load library (note quirk that you don't need quotes here)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that when you load the tidyverse (which, again, is actually loading a number of other libraries), you see a lot of output. Not all packages are this noisy, but the information is useful here because it shows all the libraries that are now loaded and ready for you to use.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#help",
    "href": "01-set-install.html#help",
    "title": "I: Installing R & RStudio",
    "section": "Help",
    "text": "Help\nI don’t have every R function and nuance memorized, so I certainly don’t expect that you will. With all the user-written packages, it would be difficult to keep up if I tried! When stuck, there are a few ways to get help.\n\nHelp files\nIn the console, typing a function name immediately after a question mark will bring up that function’s help file (in RStudio, you should see in the bottom right panel):\n\n## get help file for function\n?median\n\nTwo question marks will search for the command name in CRAN packages (again, in the bottom right facet):\n\n## search for function in CRAN\n??median\n\nAt first, using help files may feel like trying to use a dictionary to see how to spell a word — if you knew how to spell it, you wouldn’t need the dictionary! Similarly, if you knew what you needed, you wouldn’t need the help file. But over time, they will become more useful, particularly when you want to figure out an obscure option that will give you exactly what you need.\n\n\nPackage Website\n\nWhile all R packages have to have help files, not all R packages have nice webpages. However, a lot of the main ones do\n\nI find them much nicer than the CRAN helpfiles\n\n\nFor example, here’s another magic portal to the tidyverse’s dplyr website (you may spent a good amount of time here this semester)\n\nUsually I Google something like “&lt;package name&gt; R” and the website comes up\nYou can find links to all the tidyverse packages here\n\n\nGoogle it!\nGoogle is a coder’s best friend. If you are having a problem, odds are a 1,000+ other people have too and at least one of them has been brave enough (people can be mean on the internet) to ask about it in a forum like StackOverflow, CrossValidated, or R-help mailing list.\nIf you are lucky, you’ll find the exact answer to your question. More likely, you’ll find a partial answer that you’ll need to modify for your needs. Sometimes, you’ll find multiple partial answers that, in combination, help you figure out a solution. It can feel overwhelming at first, particularly if it’s a way of problem-solving that’s different from what you’re used to. But it does become easier with practice.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#useful-packages",
    "href": "01-set-install.html#useful-packages",
    "title": "I: Installing R & RStudio",
    "section": "Useful packages",
    "text": "Useful packages\nWe’re going to use a number of packages this semester. While we may need more than this list — and you almost certainly will in your own future work — let’s install these to get us started.\n\nQuick exercise\nInstall the following packages using the install.packages() function:\n\n\n\ndevtools\nknitr\n`rmarkdown``\nquarto",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "02-set-data.html#organizing-a-project-folder",
    "href": "02-set-data.html#organizing-a-project-folder",
    "title": "II: Reading Data & IPEDS",
    "section": "Organizing a Project Folder",
    "text": "Organizing a Project Folder\nWe’ll begin with how to organize your course and project files.\n\nNote: we are now arriving at one of the first small differences from previous versions of the course; how the sub-folders are organized. There’s a trade-off between simplicity and tidy-ness here, we have opted for simplicity\n\n\nSkinner’s Kitchen Metaphor\n\n“Every data analysis project should have its own set of organized folders. Just like you might organize a kitchen so that ingredients, cookbooks, and prepared food all have a specific cabinet or shelf, so too should you organize your project. We’ll organize our course directory in a similar fashion.”\n\n\nIn Dr. Skinner’s kitchen, think of pristine space where NOTHING sits out on the counter.\nHowever, we are going to have kitchen where some things are stored away in cupboards, but what we regularly work with (the scripts) sits out on the counter\n\nThe kitchen metaphor also works to explain why you might take these approaches\n\nIt’s definitely tidier to keep everything in your kitchen stored away, but, it adds an extra step whenever you want to cook a meal. The same is true here.\nWhy? We will get to that later…\n\n\n\n\n\nEDH 7916 Folder Setup\n\nWith this kitchen metaphor in mind, let’s set up our folder for the class\nI’ve made an EDH-7916 example folder you can download here which is also available on the class homepage\n\nIn here, you will see\n\nAn .R script template\nA set of numbered .R scripts (these are our lesson scripts)\nA data folder\nA reproducible-report folder (with it’s own data sub-folder)\nA .pdf copy of the syllabus\n\n\nDownload and save this folder wherever you usually keep class folders\n\nI personally use my Desktop, but you can use your Documents folder etc. if you wish\nYou can rename the folder if you’d like (but please don’t rename the internal folders)\n\nSee naming guidelines below on how best to name files\n\n\nThroughout the class (and especially in your final project) you may feel the need for other sub-folders for other items (such as one to keep graphs in), but this should be fine for now\n\n\n\nR Project Setup\n\nRStudio has some really helpful features, one of which is creating R Projects easily\n\nAt their very simplest, these can be ways of keeping your RStudio environment saved (especially helpful switching between projects), but also enable more feature like using git (see extra credit lesson)\nNote: If you’re using posit.cloud, you already have a RStudio project by default\n\nIt’s pretty easy to set up a project now we have our class folder set up\n\nIn the top right corner of RStudio you’ll see a blue cube with “none” next to it\nClick there, then “new project”\nThen click “from existing directory”\nFind the class folder we just created, select it, and we’re done!\n\nThis is really useful for keeping track of multiple projects, but if this is all you use it for, it will be helpful to keep working directory correct!\n\n\n\nNaming Guidelines\n\nYour class scripts and data files are already named, but there will be numerous files you need to create throughout the class (assignment scripts, everything for your final report, etc.). So it’s best we get on the same page\nAlways name your files something logical\n\nThe file name should always tell something about the purpose of that file or folder\n\nScript numbering\n\nI am personally a big fan of Hadley Wickham (the founder of RStudio)’s script numbering\n\nBasically start all your script names with the number that they should be run in\n\n01-data-reading\n02-data-cleaning\n03-data-analysis\n\nI think this is especially helpful if you’re keeping scripts in the top level of the project directory to keep things organized\n\n\nGenerally, a good programming tip is to avoid spaces at all costs, use dashes or underscores instead\n\nI prefer dashes as they format better on macs, so you will see all the files I give you will have dashes\n\nIt’s also good to be consistent with capitalization, most traditional programmers will avoid it completely, but if you do it, do it consistently throughout that project\n\nI used no capitalization through this class, but I often will use title case for aesthetic reasons in other situations\n\nWhatever you do, never (ever, ever) have different versions of files with the same name but different capitalization\n\nMost modern computers won’t let you have both at the same time, but even changing it can cause problems with case sensitive systems like git (I found this out the hard way…)\n\n\n\nLastly, try to keep files names as short as possible\n\nLater on we will be be in situations where we have to type out file names, so if you go too long, it can become frustrating\n\nYou may think the names of your class scripts are a little odd, so let me explain those as an example\n\nHadley Wickham’s script numbering corresponding to the order of the lessons\nset, wrangle, viz, quarto, or pro indicate which group of lessons it belongs to (it helps me automatically create the groups on the website sidebar)\nAnything else is just descriptive, roman numerals for the lesson series, or a descriptive word\n\nWith our class folder now set up, it’s time to go over some other key organization principles\n\n\n\nWorking Directory\n\nThe working directory is almost certainly the most common cause of issues in this class, and continues to be something I get tripped up by from time to time, so this may take a minute to get your head around\nAs a general rule, no matter how you have your folders organized in the future, you usually want your working directory set to where your script is\n\nThat way, you’re always giving directions from the common point of “where we are we are right now”\n\nThis will then be the same if I move the project folder on my computer, or run it on someone else’s computer\n\n\nBy default, when we open a project in RStudio, RStudio helpfully sets our working directory to the project folder\n\nThis is why we are keeping our scripts out on the counter top so to speak, the default working directory should be the correct one\n\nThat said, there will be times when you need to change your working directory, so, let’s go over the basics of that quickly\n\nFor instance, if you forget to open a project, RStudio will often the leave working directory as your root folder\n\nYou can see the currently working directory path next to the little R icon and version at the top of your console panel\nIf it’s wrong, there are a few ways to change it\n\nFind “session” on the top drop-down menu\n\n\nThen “set working directory”\nThen “To source file location”\n\nThis should be the same as “To project directory” as our scripts are stored at the top level of the project folder\n\n\n\nInstall the this.path package (recall how to do that from last week)\n\n\nWith that installed, call setwd(this.path::here()) at the top of the script\n\nNote: this.path::here() is the same as doing library(this.path) followed by here() but is more efficient if you only want one thing from a package\nAssuming you want the working directory to be the script location, this never hurts to always leave at the top\n\n\n\nNavigate to the desired folder in the files pane (bottom right)\n\n\nSelect the cog symbol\n\nSelect “Set as working directory”\n\nNote: “Go to working directory” can be useful to see what’s in the folder if you navigate away\n\n\n\n\nThe old school vanilla R way setwd(\"&lt;path to your script&gt;\")\n\n\nBut, this really isn’t usually the most efficient\n\n\n\nAs I say, if we organize our folder as outlined in this lesson, and use an R project, we shouldn’t need to change this much, but it’s inevitable you will every now and then\n\n\n\nFile Paths\n\nWhen we are working with R, we (most of the time) need to bring in other items, such as data\n\nIn order to do that, our computer has to find these items, and there are two ways it can do that\n\n\n\nAbsolute Paths\n\nAbsolute paths are directions to what you’re looking for starting from the root of your computer, and list out exactly where a file is. For example, as I’m writing this lesson, the absolute path of this Quarto script is\n\n\"/Users/Matt/Desktop/7916/02-set-data.qmd\"\n\nThis is perfectly fine, assuming two things\n\nI don’t move the project directory\nIt only needs to run on my computer\n\nOftentimes, we cannot rely on both these assumptions being true\n\nPlus, if we start with these absolute paths and then need to change, it will then become a real pain to update everything\n\nSo, we should ALWAYS use relative paths instead (this one of the only strict rules for assignments)\n\n\n\n\n\nRelative Paths\n\nImagine I am giving you directions to a College of Education cookout, but, I give you directions from my house. That’s only any use if you know where my house is…\n\nInstead, you really want directions from somewhere we all know, like Norman Hall\n\nThat is (basically) how relative paths work, we give directions to to our data from a common point\n\n\nRelative paths are directions to what you’re looking for from where you are right now (a.k.a your “working directory”)\nIf we assume have our working directory set to our shiny new class folder, then, that becomes the starting point for all our directions\n\nTherefore, to access hd2007.csv in out data sub-folder, we just need to say file.path(\"data\", \"hd2007.csv\")\n\n\n\n\nThe file.path() Function\n\nOne last thing, you see how file paths are typically written with / or \\? (which depends on your computer)\n\nR has a nice function that means we don’t have to\n\n\nworry about which way around the slash should be\n\n\navoid issues with different computers expecting different slashes\n\n\nfile.path()\n\nInside, we just type the name of each folder/file in “quotes”, turning\n\n\"Users/Matt/Desktop/7916\" into\nfile.path(\"Users\", \"Matt\", \"Desktop\", \"7916\")\nThis may look longer, but, it’s more compatible and easier to remember”\n\n\n\n\n\n\nWhat If I Need to Go Back a Level?\n\nSometimes we are in a folder, but want to go back a level, i.e. not the folder our current folder is in\n\nThis is very common if with we were using the “pristine kitchen” approach\n\nTo do so is easy, we just add a \"..\" to our file.path()\n\nSo, if we are in my class folder on my desktop, and we want to go to another folder on the desktop\n\nfile.path(\"..\", \"&lt;folder we want&gt;\")",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#script-template",
    "href": "02-set-data.html#script-template",
    "title": "II: Reading Data & IPEDS",
    "section": "Script Template",
    "text": "Script Template\n\nIn our shiny new class folder, you’ll see an r-script-template.R file. This is the template I use in my work, closely based on Dr. Skinner’s R script template (with a little bit of added color).\n\nThis a resource for you to use for assignments and other work, feel free to change it to suit your needs\n\nGenerally I just “Save As” the template everytime I make a new script\n\n\nThe script header block is a useful way to keep more info than a file name can\nThe main reason to use a template is to keep your work organized into sections\n\nThis template has\n\nLibraries to load needed packages\nInput to load data\nPrep to clean the data\nAnalysis to run our analyses\nOutput to save our modified data\n\nHowever, these will not always be the sections you need\n\nIn bigger projects, you might have a whole script for data cleaning\nIn other projects, you might want a section or script just for making plots\nIn your assignments, you’ll likely want a section for each question\n\nThe main point is to ensure you have some kind of sections in your scripts\n\nScripts can be really hard to navigate if you don’t!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#reading-in-data",
    "href": "02-set-data.html#reading-in-data",
    "title": "II: Reading Data & IPEDS",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nNext, let’s apply some of this thrilling knowledge about file paths and working directories to read in some data from IPEDS\nTo do this, open 02-set-data.R from your class folder\nFirst up, check your working directory by either\n\nLooking at the top of your console or\nTyping getwd() into the console\n\nThis should be your class folder, but if not, we need to set it there\n\nOn the top drop-down menu, select “Session”, “Set Working Directory,”To Source File Location”\nQuick Question: Without scrolling up, who can remember the other ways of doing this?\n\nOkay, with this set, it’s time to read in our first dataset!\n\nQuick Question 1: Where is our data?\nQuick Question 2: Who remembers how we assign something in R?\n\nWith those questions answered, we have everything we need\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_ipeds &lt;- read_csv(file.path(\"data\", \"hd2007.csv\"))\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSuccess!\nWe will cover other ways of reading in data over the course of the class (we can download data directly to R somtimes), but this is most common way, so we are ready for some analysis next week!\nThat’s it for R today, phew!\nNow let’s go and explore IPEDS Data Center and see where a lot of contemporary higher education research data comes from!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "href": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "title": "II: Reading Data & IPEDS",
    "section": "IPEDS Exploration Key Points (for review)",
    "text": "IPEDS Exploration Key Points (for review)\n\nIPEDS is an annual federally mandated data collection process (and compliance is a significant portion of many Institutional Researcher jobs)\nThere are a few ways of downloading IPEDS data, if you click through the website you may well find a point-and-click way of selecting specifc variables\n\nThis is NOT reproducible and therefore NOT the best practice for research\n\nWe want to use the IPEDS data center to access the complete data files then select and join variables to get our desired data set\n\nFear not, we will go over how to do those things in the first two data wrangling lessons!\n\n\n\n“Data File” vs. “STATA Data File”\n\nFor some NCES data sets, such as HSLS downloaded from NCES DataLab, selecting the Stata file option will download a .dta STATA file format version of the data, which is often nicely labelled\n\nWe can actually read these into R using the haven library from the tidyverse\n\nHowever, for IPEDS, the STATA file option is actually just another .csv file, it’s formatted slightly differently to read into STATA, so there’s no reason to bother with it when using R\n\n\n\nUsing IPEDS Codebooks\n\nTo be able to use most of these big data sets, you need to be able to understand the code. Let’s look together at the codebook for EFFY (headcount enrollment) for 2021\n\nFor IPEDS, the code book is called the dictionary, and is always an Excel file (.xlsx). Other data sources will look different but the general principle will be the same",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "href": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "title": "II: Reading Data & IPEDS",
    "section": "Other common data sources for higher education research",
    "text": "Other common data sources for higher education research\n\nNational Center for Educational Statistics (also the owner of IPEDS)\n\nLongitudinal Surveys such as HSLS-High School Longitudinal Study and ECLS-Early Childhood Longitudinal Studies\nAdministrative Data (including IPEDS)\nNCES has a good amount of publicly available data, but they also have a LOT of restricted data\n\nTypically publicly available data will be either institution level (school, college, university wide) or fully anonymized. Meanwhile restricted data will often be student level and have some more detailed information\n\nGetting restricted data is tough, but not impossible\n\nYou will need a clear purpose of your study and to know exactly what data you want access to (see available data here)\nYou’ll then need to take this idea to your advisor\n\nFor your final project in this class, your data MUST be publicly available&lt;&gt;\n\nThis means we must be able to go and download it ourselves, you won’t submit data with your final project submission\n\n\n\n\nCollege Scorecard\n\nDesigned more as a tool for potential college students, college scorecard has data points of interest to this audience, but some things useful for our research too, in particular graduate earning levels\n\nSimilarly to IPEDS, if using College Scorecard, we want to avoid the point-and-click interface and download the entire data files available here\n\n\nNational Bureau of Labor Statistics\n\nLongitudinal surveys, some have educational variables similar to NCES but are often much broader in scope\n\nNational Longitudinal Survey of Youth (NLSY) is one of the most used\n\nThere are publicly available portions of these surveys, but other sections are restricted, see BLS’s accessing data page for more info\n\nCensus & American Community Survey\n\nUseful for population statistics, not student specific\n\nCommon variables for higher ed research include education and income levels for a population\n\nFor example of what is available, see the variables available in the 2019 ACS here\n\nWe will actually do some cool stuff to download ACS data directly to R later in Data Viz III using the tidycensus package\n\n\nMSI Data Project\n\nDetails about MSI classification and funding, includes IPEDS ID numbers to easily link to additional data\n\nMany, many more, have fun exploring!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#data",
    "href": "04-wrangle-ii.html#data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Data",
    "text": "Data\n\nThe data for today’s lesson is all in your data/sch-test folder\n\nIt should look something like this:\n\n\n|__ data/\n    |-- ...\n    |__ sch_test/\n        |-- all_schools.csv\n        |-- all_schools_wide.csv\n        |__ by_school/\n            |-- bend_gate_1980.csv\n            |-- bend_gate_1981.csv\n            |...\n            |-- spottsville_1985.csv\n\nThese fake data represent test scores across three subjects — math, reading, and science — across four schools over six years.\nEach school has a file for each year in the by_school subdirectory.\nThe two files in sch_test directory, all_schools.csv and all_schools_wide.csv, combine the individual files but in different formats.\n\nWe’ll use these data sets to practice appending, joining, and reshaping.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#setup",
    "href": "04-wrangle-ii.html#setup",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Setup",
    "text": "Setup\nAs always, we begin by reading in the tidyverse library.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\nlibrary(tidyverse)\n\n\nAs we did in the past lesson, we will run this script assuming that our working directory is set to the project folder",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#appending-data",
    "href": "04-wrangle-ii.html#appending-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Appending data",
    "text": "Appending data\n\nOur first task is the most straightforward. When appending data, we simply add similarly structured rows to an exiting data frame.\nWhat do I mean by similarly structured? Imagine you have a data frame that looks like this:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\n\n\nNow, assume you are given data that look like this:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\n\nThese data are similarly structured: same column names in the same order. If we know that the data came from the same process (e.g., ids represent students in the same classroom with each file representing a different test day), then we can safely append the second to the first:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\n\nData that are the result of the exact same data collecting process across locations or time may be appended. In education research, administrative data are often recorded each term or year, meaning you can build a panel data set by appending. The NCES IPEDS data files generally work like this\n\nNote on IPEDS: Some of the variables have changed over time, one of the most notable being enrollment data between 2001 and 2002, so always check your dictionary!\n\nHowever, it’s incumbent upon you as the researcher to understand your data. Just because you are able to append (R will try to make it work for you) doesn’t mean you always should.\n\nWhat if the score column in our data weren’t on the same scale?\nWhat if the test date mattered but isn’t included in the file?\nWhat if the files actually represent scores from different grades or schools?\n\nIt’s possible that we can account for each of these issues as we clean our data, but it won’t happen automatically — append with care!\n\n\n\n\n\n\nExample\nLet’s practice with an example. First, we’ll read in three data files from the by_school directory.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data, storing in df_*, where * is a unique number\ndf_1 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1980.csv\"))\ndf_2 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1981.csv\"))\ndf_3 &lt;- read_csv(file.path(\"data\", \"sch-test\", \"by-school\", \"bend-gate-1982.csv\"))\n\n\nLooking at each, we can see that they are similarly structured, with the following columns in the same order: school, year, math, read, science:\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## show each\ndf_1\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\ndf_2\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\ndf_3\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nFrom the dplyr library, we use the bind_rows() function to append the second and third data frames to the first.\n\n## append files\ndf &lt;- bind_rows(df_1, df_2, df_3)\n\n## show\ndf\n\n# A tibble: 3 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n\n\n\nThat’s it!\n\n\nQuick exercise\nRead in the rest of the files for Bend Gate and append them to the current data frame.\n\n\nQuick exercise: Take Two\nIf bind_rows() stacks tables on top of each other, what do you think would stack them side-by-side? Copy the below code and try to figure how to get them back together\n\n\ndf_split_left &lt;- df[,1:2]\ndf_split_right &lt;- df[,3:5]\n\nprint(df_split_left)\n\n# A tibble: 3 × 2\n  school     year\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Bend Gate  1980\n2 Bend Gate  1981\n3 Bend Gate  1982\n\nprint(df_split_right)\n\n# A tibble: 3 × 3\n   math  read science\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1   515   281     808\n2   503   312     814\n3   514   316     816\n\n## Append them back together side-by-side",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#joining-data",
    "href": "04-wrangle-ii.html#joining-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Joining data",
    "text": "Joining data\n\nMore often than appending your data files, however, you will need to merge or join them.\nWith a join, you add to your data frame new columns (new variables) that come from a second data frame.\nThe key difference between joining and appending is that a join requires a key, that is, a variable or index common to each data frame that uniquely identifies observations.\n\nIt’s this key that’s used to line everything up.\n\n\nFor example, say you have these two data sets,\n\n\n\nid\nsch\nyear\nscore\n\n\n\n\nA\n1\n2020\n98\n\n\nB\n1\n2020\n95\n\n\nC\n2\n2020\n85\n\n\nD\n3\n2020\n94\n\n\n\n\n\n\nsch\ntype\n\n\n\n\n1\nelementary\n\n\n2\nmiddle\n\n\n3\nhigh\n\n\n\n\nYou want to add the school type to the first data set.\nYou can do this because you have a common key between each set: sch.\n\n\nAdd a column to the first data frame called type\nFill in each row of the new column with the type value that corresponds to the matching sch value in both data frames:\n\nsch == 1 --&gt; elementary\nsch == 2 --&gt; middle\nsch == 3 --&gt; high\n\n\nThe end result would then look like this:\n\n\n\nid\nsch\nyear\nscore\ntype\n\n\n\n\nA\n1\n2020\n98\nelementary\n\n\nB\n1\n2020\n95\nelementary\n\n\nC\n2\n2020\n85\nmiddle\n\n\nD\n3\n2020\n94\nhigh\n\n\n\n\nExample\n\nA common join task in education research involves adding group-level aggregate statistics to individual observations, for example;\n\nadding school-level average test scores to each student’s row.\nWith a panel data set (observations across time), we might want within-year averages added to each unit-by-time period row.\n\nLet’s do the second, adding within-year across school average test scores to each school-by-year observation.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in all_schools data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\n\nLooking at the data, we see that it’s similar to what we’ve seen above, with additional schools.\n\n\n## show\ndf\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\nOur task is two-fold:\n\nGet the average of each test score (math, reading, science) across all schools within each year and save the summary data frame in an object.\nJoin the new summary data frame to the original data frame.\n\n\n1. Get summary\n\n## ---------------------------\n## process\n## ---------------------------\n\n## get test score summary \ndf_sum &lt;- df |&gt;\n    ## grouping by year so average within each year\n    group_by(year) |&gt;\n    ## get mean(&lt;score&gt;) for each test\n    summarize(math_m = mean(math),\n              read_m = mean(read),\n              science_m = mean(science))\n\n## show\ndf_sum\n\n# A tibble: 6 × 4\n   year math_m read_m science_m\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  1980   507    295.      798.\n2  1981   496.   293.      788.\n3  1982   506    302.      802.\n4  1983   500    293.      794.\n5  1984   490    300.      792.\n6  1985   500.   290.      794.\n\n\n\nQuick exercise\nThinking ahead, why do you think we created new names for the summarized columns? Why the _m ending?\n\n\n\n2. Join\n\nWhile one can merge using base R, dplyr uses the SQL language of joins,\n\nThis can be conceptually clearer (particularly for those who already have experience with relational database structures).\nWe will learn more about SQL in Data Wrangling IV\n\nHere are the most common joins you will use:\nleft_join(x, y): keep all x, drop unmatched y\nright_join(x, y): keep all y, drop unmatched x\ninner_join(x, y): keep only matching\nfull_join(x, y): keep everything\nanti_join(x, y): keep only obs in x and that are not in y (more useful than you’d think)\nFor example, the result of a left join between data frame X and data frame Y will include all observations in X and those in Y that are also in X.\n\nX\n\n\n\nid\ncol_A\ncol_B\n\n\n\n\n001\na\n1\n\n\n002\nb\n2\n\n\n003\na\n3\n\n\n\nY\n\n\n\nid\ncol_C\ncol_D\n\n\n\n\n001\nT\n9\n\n\n002\nT\n9\n\n\n004\nF\n9\n\n\n\nXY (result of left join)\n\n\n\nid\ncol_A\ncol_B\ncol_C\ncol_D\n\n\n\n\n001\na\n1\nT\n9\n\n\n002\nb\n2\nT\n9\n\n\n003\na\n3\nNA\nNA\n\n\n\n\nObservations in both X and Y (001 and 002, above), will have data for the columns that were separately in X and Y before.\nThose in X only (003), will have missing values in the new columns that came from Y because they didn’t exist there.\nObservations in Y but not X (004) are dropped entirely.\n\nBack to our example…\n\nSince we want to join a smaller aggregated data frame, df_sum, to the original data frame, df, we’ll use a left_join().\n\nThe join functions will try to guess the joining variable (and tell you what it picked) if you don’t supply one, but we’ll specify one to be clear.\n\n\n\n## start with data frame...\ndf_joined &lt;- df |&gt;\n    ## pipe into left_join to join with df_sum using \"year\" as key\n    left_join(df_sum, by = \"year\")\n\n## show\ndf_joined\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nQuick exercise\nLook at the first 10 rows of df_joined. What do you notice about the new summary columns we added?\n\n\n\n\nMatt’s Re-Explanation of Joining (Updated Feb 1 2024)\n\nFollowing a productive conversation in my office hours, I wanted to re-explain joining in a way I think is a little clearer than explanation above. Hopefully this helps some of you!\nAll _join() functions takes three main “arguments” and they have always the same meaning\n\nx data one, a.k.a. the “left” data\ny data two, a.k.a. the “right” data\nby the variables to use as a “key”\n\n\n\n“left” and “right” just mean x and y\n\nThe left in left_join() simply means “start with x data and join y data into it”\n\nIt doesn’t have anything to do with being physically on the left\n\nIf it helps, think of left_join() as the x join\n\n\n## We can be overly specific to make the point\nleft_join(x = df,\n          y = df_sum,\n          by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nUsing the |&gt; pipe\n\nNow, if you remember from Data Wrangling I, the |&gt; makes R code more intuitive by “piping” one thing into the next\n\nThis makes things simpler 99% of the time (no one wants to be writing nested code)\nBut in this case it takes a second to get your head around\n\nBy default, the pipe |&gt; will always go into the first “argument” of a function, which in this case, is x\n\nWe can always specify where it should go with an _ underscore\n\n\n\n## Therefore \nleft_join(x = df,\n          y = df_sum,\n          by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n## Is exactly the same as\ndf |&gt;\n  left_join(x = _, ## If it helps to visualize, the _ is where the |&gt; will go\n            y = df_sum,\n            by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n## Is exactly the same as\ndf |&gt;\n  left_join(df_sum,\n            by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nYou can write joins whichever way you want (piped or not-piped)\n\nEventually, you will find piped is easier and more-efficient, but focus on whichever way makes more sense to you for now\n\n\n\n## Note: if we want to keep the joined data, we should assign it to df_join\ndf_join &lt;- df |&gt;\n  left_join(df_sum,\n            by = \"year\")\n\n\n\n\nby\n\nWhichever type of _join you are doing, the by argument is just how x and y are being matched up\n\nby has to be at least one variable that is in both x and y and identifies the same observation\n\ne.g., by = \"school_id\"\nMost often this will be some kind of identifying variable (student ID, school ID, city, state, region, etc.)\n\nIf have more than one piece of information to match on, such as school and year, we can specify that with c()\n\ne.g., by = c(\"school_id\", \"year\")\nThis will then find information for each school in each year, and join it in correctly\n\n\nIf you don’t provide any by arguments, R will try to be smart and look for columns names that are the same in both data sets\n\nThis can lead to incorrect joins though, so always best to specify\n\nLastly, if you need to join by columns that have different names in each data set use c(\"name in df_1\" = \"name in df_2\")\n\ne.g., by = c(\"student_id\" = \"id_number\")\n\n\n\n\n_join summary\n\nJoining data is one of the most important tasks in educational research\n\nIPEDS Complete Data Files are the perfect example, within each year all the data is stored in separate files that need joining\n\nRemember the same IPEDS files for different years need bind_rows() once you add a year identifier\n\n\nIt takes a while to get your head around, but if you can do it properly, you are well on the way to mastering data wrangling!",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#reshaping-data",
    "href": "04-wrangle-ii.html#reshaping-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nReshaping data is a common and important data wrangling task.\nWhether going from wide to long format or long to wide, it can be a painful process.\n\nBut with a little practice, the ability to reshape data will become a powerful tool in your toolbox.\n\n\n\nDefinitions\n\nWhile there are various definitions of tabular data structure, the two you will most often come across are wide and long.\n\nWide data are data structures in which all variable/values are columns.\n\nAt the extreme end, every id will only have a single row:\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmath_score_2019\nread_score_2019\nmath_score_2020\nread_score_2020\n\n\n\n\nA\n93\n88\n92\n98\n\n\nB\n99\n92\n97\n95\n\n\nC\n89\n88\n84\n85\n\n\n\n\nNotice how each particular score (by year) has its own column?\n\nCompare this to long data in which each observational unit (id test score within a given year) will have a row:\n\n\n\n\n\nid\nyear\ntest\nscore\n\n\n\n\nA\n2019\nmath\n93\n\n\nA\n2019\nread\n88\n\n\nA\n2020\nmath\n92\n\n\nA\n2020\nread\n98\n\n\nB\n2019\nmath\n99\n\n\nB\n2019\nread\n92\n\n\nB\n2020\nmath\n97\n\n\nB\n2020\nread\n95\n\n\nC\n2019\nmath\n89\n\n\nC\n2019\nread\n88\n\n\nC\n2020\nmath\n84\n\n\nC\n2020\nread\n85\n\n\n\n\nThe first wide and second long table present the same information in a different format.\n\nSo why bother reshaping?\n\nThe short answer is that you sometimes need one format and sometimes the other due to the demands of the analysis you want to run, the figure you want to plot, or the table you want to make.\n\n\n\n\nNote: Data in the wild are often some combination of these two types: wide-ish or long-ish. For an example, see our all-schools.csv data below, which is wide in some variables (test), but long in others (year). The point of defining long vs wide is not to have a testable definition, but rather to have a framework for thinking about how your data are structured and if that structure will work for your data analysis needs.\n\n\n\nExample: wide –&gt; long\nTo start, we’ll go back to the all_schools.csv file.\n\n## ---------------------------\n## input\n## ---------------------------\n\n## reading again just to be sure we have the original data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools.csv\"))\n\n\nNotice how the data are wide in test:\n\nEach school has one row per year, but each test gets its own column.\nWhile this setup can be efficient for storage, it’s not always the best for analysis or even just browsing.\n\nWhat we want is for the data to be long.\n\n\nInstead of each test having its own column, we would like to make the data look like our long data example above, with each row representing a single school, year, test, score:\n\n\n\n\n\n\n\n\n\n\nschool\nyear\ntest\nscore\n\n\n\n\nBend Gate\n1980\nmath\n515\n\n\nBend Gate\n1980\nread\n281\n\n\nBend Gate\n1980\nscience\n808\n\n\n…\n…\n…\n…\n\n\n\n\nAs with joins, you can reshape data frames using base R commands.\nBut again, we’ll use tidyverse functions in the tidyr library.\n\nSpecifically, we’ll rely on the tidyr pivot_longer() and pivot_wider() commands.\n\n\n\npivot_longer()\nThe pivot_longer() function can take a number of arguments, but the core things it needs to know are:\n\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\ncols: the names of the columns that you want to pivot into values of a single new column (thereby making the data frame “longer”)\nnames_to: the name of the new column that will contain the names of the cols you just listed\nvalues_to: the name of the column where the values in the cols you listed will go\nIn our current situation, our cols to pivot are \"math\", \"read\", and \"science\".\n\nSince they are test types, we’ll call our names_to column \"test\" and our values_to column \"score\".\n\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df |&gt;\n    ## cols: current test columns\n    ## names_to: where \"math\", \"read\", and \"science\" will go\n    ## values_to: where the values in cols will go\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"),\n                 names_to = \"test\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQuick (ocular test) exercise\nHow many rows did our initial data frame df have? How many unique tests did we have in each year? When reshaping from wide to long, how many rows should we expect our new data frame to have? Does our new data frame have that many rows?\n\n\n\n\nExample: long –&gt; wide\n\npivot_wider()\n\nNow that we have our long data, let’s reshape it back to wide format using pivot_wider(). In this case, we’re doing just the opposite from before — here are the main arguments you need to attend to:\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\nnames_from: the name of the column that contains the values which will become new column names\nvalues_from: the name of the column that contains the values associated with the values in names_from column; these will go into the new columns.\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## long to wide\ndf_wide &lt;- df_long |&gt;\n    ## names_from: values in this column will become new column names\n    ## values_from: values in this column will become values in new cols\n    pivot_wider(names_from = \"test\",\n                values_from = \"score\")\n\n## show\ndf_wide\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\n\npivot_()-ing with separation\n\nUnfortunately, it’s not always so clear cut to reshape data.\nIn this second example, we’ll again reshape from wide to long, but with an extra argument that helps when there’s more than one piece of information in the variable name\n\nFirst, we’ll read in a second file all_schools_wide.csv - This file contains the same information as before, but in a very wide format\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in very wide test score data\ndf &lt;- read_csv(file.path(\"data\", \"sch-test\", \"all-schools-wide.csv\"))\n\n## show\ndf\n\n# A tibble: 4 × 19\n  school       math_1980 read_1980 science_1980 math_1981 read_1981 science_1981\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Bend Gate          515       281          808       503       312          814\n2 East Heights       501       318          782       487       323          813\n3 Niagara            514       292          787       499       268          762\n4 Spottsville        498       288          813       494       270          765\n# ℹ 12 more variables: math_1982 &lt;dbl&gt;, read_1982 &lt;dbl&gt;, science_1982 &lt;dbl&gt;,\n#   math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;, science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;,\n#   read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;, math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;,\n#   science_1985 &lt;dbl&gt;\n\n\nYou see, each school has only one row and each test by year value gets its own column in the form &lt;test&gt;_&lt;year&gt;.\n\nWe will use pivot_longer() just as we did before\n\nBut instead of one column for names_to we use our friend c() to list two columns we want the information from column name to go to\nThen, we add names_sep = \"_\", which means separate the information from the names at every underscore\n\nI.e., this will put everything before the underscore in the first column test and everything after into the second column year\n\n\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_long &lt;- df |&gt;\n    ## NB: contains() looks for \"19\" in name: if there, it adds it to cols\n    pivot_longer(cols = contains(\"19\"),\n                 names_to = c(\"test\", \"year\"),\n                 names_sep = \"_\",\n                 values_to = \"score\")\n\n## show\ndf_long\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# ℹ 62 more rows\n\n\n\nQuick exercise(s)\n\nWhat do you think we’d need to change if the column name had 3 pieces of information all separated by an underscore?\nWhat about if the information was separated by a . period?\n\n\n\nNow, if we want to get our data back in to extra long form, we can use a very similar argument in pivot_wider()\n\nWe just use c() to say get the name information from two columns\nnames_sep = \"_\" is identical to before, but this time it’s saying to place and underscore as the separator\n\n\n\n## ---------------------------\n## process\n## ---------------------------\n\n## wide to long\ndf_wide &lt;- df_long |&gt;\n    pivot_wider(values_from = score,\n                names_from = c(test, year),\n                names_sep = \"_\")\n\n## show\ndf_wide\n\n# A tibble: 4 × 19\n  school       math_1980 read_1980 science_1980 math_1981 read_1981 science_1981\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Bend Gate          515       281          808       503       312          814\n2 East Heights       501       318          782       487       323          813\n3 Niagara            514       292          787       499       268          762\n4 Spottsville        498       288          813       494       270          765\n# ℹ 12 more variables: math_1982 &lt;dbl&gt;, read_1982 &lt;dbl&gt;, science_1982 &lt;dbl&gt;,\n#   math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;, science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;,\n#   read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;, math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;,\n#   science_1985 &lt;dbl&gt;",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#final-note",
    "href": "04-wrangle-ii.html#final-note",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Final note",
    "text": "Final note\n\nJust as all data sets are unique, so too are the particular steps you may need to take to append, join, or reshape your data.\n\nEven experienced coders rarely get all the steps correct the first try.\nBe prepared to spend time getting to know your data and figuring out, through trial and error, how to wrangle it so that it meets your analytic needs.\nCode books, institutional/domain knowledge, and patience are your friends here!",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#questions",
    "href": "04-wrangle-ii.html#questions",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Questions",
    "text": "Questions\n\nIf you want a challenge, do this all in one chained pipe, otherwise save step i output then pipe steps ii-iv;\n\n\n\nCompute the average test score by region\nJoin back into the full data frame.\nCompute the difference between each student’s test score and that of the region.\nFinally, show the mean of these differences by region\n\n\nHint: if you think about it, this should probably be a very very small number…\n\n\nCompute the average test score by region and family income level and join it back to the full data frame.\n\n\nHint: You can join on more than one key using c().\n\n\n\n\n\n\nSelect the following variables from the full data set\n\n- `stu_id`\n- `x1stuedexpct`\n- `x1paredexpct`\n- `x4evratndclg`  \n\nFrom this reduced data frame, reshape the data frame so that it is long in educational expectations - As in, each observation should have two rows, one for each educational expectation type\n\ne.g. (your column names and values may be different)\n\n\n\nstu_id\nexpect_type\nexpectation\nx4evratndclg\n\n\n\n\n0001\nx1stuedexpct\n6\n1\n\n\n0001\nx1paredexpct\n7\n1\n\n\n0002\nx1stuedexpct\n5\n1\n\n\n0002\nx1paredexpct\n5\n1\n\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the lesson). Assignments will be graded on the following Monday (time permitting) in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#solution",
    "href": "04-wrangle-ii.html#solution",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Solution",
    "text": "Solution\n RSolutionCode\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Wrangling II Solution]\n##' [INIT: Jan 31 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\ndf_q1 &lt;- df |&gt;\n  filter(x1txmtscor != -8)\n\n## Easier way\n\ndf_sum &lt;- df_q1 |&gt;\n  group_by(x1region) |&gt;\n  summarize(reg_mean_test = mean(x1txmtscor))\n\ndf_q1 |&gt;\n  left_join(df_sum, by = \"x1region\") |&gt;\n  mutate(diff = x1txmtscor - reg_mean_test) |&gt;\n  group_by(x1region) |&gt;\n  summarize(reg_diffs = mean(diff))\n\n## All in one\n\ndf_q1 |&gt;\n  group_by(x1region) |&gt;\n  summarize(reg_mean_test = mean(x1txmtscor)) |&gt;\n  ## output is summary, which we want to be the \"right\" or \"y\" of the join\n  left_join(x = df_q1, by = \"x1region\") |&gt;\n  ## so we specify x = df (original) leaving y open for the piped summary\n  mutate(diff = x1txmtscor - reg_mean_test) |&gt;\n  group_by(x1region) |&gt;\n  summarize(reg_diffs = mean(diff))\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf_q2 &lt;- df |&gt;\n  filter(x1txmtscor != -8,\n         !x1famincome %in% c(-8, -9))\n\n## Easier way\n\ndf_sum &lt;- df_q2 |&gt;\n  group_by(x1region, x1famincome) |&gt;\n  summarize(reg_inc_mean_test = mean(x1txmtscor))\n\ndf_q2_easy &lt;- df_q2 |&gt;\n  left_join(df_sum, by = c(\"x1region\", \"x1famincome\"))\n\n## All in one\n\ndf_q2_piped &lt;- df_q2 |&gt;\n  group_by(x1region, x1famincome) |&gt;\n  summarize(reg_inc_mean_test = mean(x1txmtscor)) |&gt;\n  left_join(x = df_q2, by = c(\"x1region\", \"x1famincome\"))\n\nall.equal(df_q2_easy, df_q2_piped)\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\ndf_long &lt;- df |&gt;\n  select(stu_id, x1paredexpct, x1stuedexpct, x4evratndclg) |&gt;\n  pivot_longer(cols = c(x1paredexpct, x1stuedexpct),\n               names_to = \"exp_type\",\n               values_to = \"exp_value\")\n\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "06-viz-ii.html#libraries",
    "href": "06-viz-ii.html#libraries",
    "title": "II: Customization",
    "section": "Libraries",
    "text": "Libraries\n\nIn addition to tidyverse, we’ll add a new library, patchwork, that we’ll use toward the end of the lesson.\nIf you haven’t already downloaded it, be sure to install it first using install.packages(\"patchwork\").\n\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n\nFinally, we’ll load the data file we’ll be using, hsls_small.csv\n\n\n## ---------------------------\n##' [Input data]\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#initial-plot-with-no-formatting",
    "href": "06-viz-ii.html#initial-plot-with-no-formatting",
    "title": "II: Customization",
    "section": "Initial plot with no formatting",
    "text": "Initial plot with no formatting\n\nRather than make a variety of plots, we’ll focus on making and incrementally improving a single figure (with some slight variations along the way)\nIn general, we’ll be looking at math test scores via the x1txtmscor data column\nFirst, we need to clean our variable of interest\n\nAs you may recall from an earlier lesson, x1txmtscor is a normed variable with a mean of 50 and standard deviation of 10.\n\nThat means any negative values are missing data, which for our purposes, will be dropped\n\n\n\n\nQuick Question: We didn’t have to do this last week as default behavior when plotting is simply to drop missing values, which worked fine using the .dta version of this file. Why wouldn’t that work with the .csv version of the same data?\n\n\n## -----------------------------------------------------------------------------\n##' [Initial plain plot]\n## -----------------------------------------------------------------------------\n\n## Drop missing values for math test score\ndf &lt;- df |&gt;\n  filter(x1txmtscor != -8)\n\n\nFirst, let’s make a plain histogram with no settings\nLast week we never assigned the plot to an object, we always just had it print out\nThis week, as we are going to be focusing on one plot, editing and adding to it, we are always going to assign the plot to p and then print it out\n\nRemember: ggplots are layered\n\nIf we want to change the base layers, we will have to start again and overwrite p\nIf we want to add more detail to p we can just add those layers like p + &lt;layer&gt;\n\nIf we want to overwrite something that is already in p, we just specify it again\n\nAnything we don’t say will be left as is\n\n\n\n\n\n\n## create histogram using ggplot\np &lt;- ggplot(data = df) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n## show\np\n\n\n\n\n\n\n\n\nSo there it is. Let’s make it better.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#titles-and-captions",
    "href": "06-viz-ii.html#titles-and-captions",
    "title": "II: Customization",
    "section": "Titles and captions",
    "text": "Titles and captions\n\nThe easiest things to improve on a figure are the title, subtitle, axis labels, and caption\nAs with a lot of ggplot2 commands, there are a few different ways to set these labels, but the most straightforward way is to use the labs() function\n\nThis can be added as another layer to our existing plot p\n\n\n\n## -----------------------------------------------------------------------------\n##' [Titles and captions]\n## -----------------------------------------------------------------------------\n\n## Add placeholder titles/labels/captions\np &lt;- p +\n  labs(title = \"Title\",\n       subtitle = \"Subtitle\",\n       caption = \"Caption\",\n       x = \"X axis label\",\n       y = \"Y axis label\")\n\n## show \np\n\n\n\n\n\n\n\n\n\nRather than accurately labeling the figure, I’ve repeated the arguments in strings so that it’s clearer where every piece goes\n\nThe title is of course on top\nWith the subtitle in a smaller font just below\nThe x and y axis labels go with their respective axes\nThe caption is right-aligned below the figure\n\nYou don’t have to use all of these options for every figure\nIf you don’t want to use one, you have a couple of options:\n\nIf the argument would otherwise be blank (title, subtitle, and caption), you can just leave the argument out of labs()\nIf the argument will be filled, as is the case on the axes (ggplot will use the variable name by default), you can use NULL\n\nTo make our figure nicer, we’ll add a title, axis labels, and caption describing the data source\n\nWe don’t really need a subtitle and since there’s no default value, we’ll just leave it out\nNotice: p right now still has the placeholder labs() arguments, so we can just call it and overwrite labs with a new call\n\n\n\n## ---------------------------\n## titles and captions: ver 2\n## ---------------------------\n\n## create histogram using ggplot\np &lt;- p +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\n\nQuick question: What about these labels is a little silly? Try to fix it so it looks like the below\n\n\n\n\n\n\n\n\n\n\nWith that fixed, now we’ll move to improving the axis scales!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#axis-formatting",
    "href": "06-viz-ii.html#axis-formatting",
    "title": "II: Customization",
    "section": "Axis formatting",
    "text": "Axis formatting\n\nIn general, the default tick mark spacing and accompanying labels are pretty good\nBut sometimes we want to change them, sometimes to have fewer ticks and sometimes to have more\nFor this figure, we could use more ticks on the x axis to make differences in math test score clearer\nWhile we’re at, we’ll increase the number of tick marks on the y axis too.\nTo change these values, we need to use scale_&lt; aesthetic &gt;_&lt; distribution &gt; function\n\nThese may seem strange at first, but they follow a logic. Specifically:\n\n&lt; aesthetic &gt;: x, y, fill, colour, etc (what is being changed?)\n&lt; distribution &gt;: is the underlying variable continuous, discrete, or do you want to make a manual change?\n\n\n\nTo change our x and y tick marks we will use:\n\nscale_x_continuous()\nscale_y_continuous()\n\nWe use x and y because those are the aesthetics being adjusted\nwe use continuous in both cases because math_test on the x axis and the histogram counts on the y axis are both continuous variables\n\nThere are a LOT of options within these scale_ functions\nThey depend on what kind of aesthetic you’re adjusting\n\nFor now we will focus on using two\n\nbreaks: where the major lines are going (they get numbers on the axis)\nminor_breaks: where the minor lines are going (they don’t get numbers on the axis)\n\n\nBoth breaks and minor_breaks take a vector of numbers\nWe can put each number in manually using c() (e.g., c(0, 10, 20, 30, 40)), but a better way is to use R’s seq() function:\n\nseq(start, end, by)\n\nNotice that within each scale_*() function, we use the same start and stop values for both minor and major breaks, just change the by option\n\nThis will give us axis numbers at spaced intervals with thinner, unnumbered lines between.\n\n\n\n## -----------------------------------------------------------------------------\n##' [Axis formatting]\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- p +\n    scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 100),\n                     minor_breaks = seq(from = 0, to = 2500, by = 50))\n\n## show \np\n\n\n\n\n\n\n\n\n\nWe certainly have more lines now. Maybe too many on the y axis, which is a sort of low-information axis\n\ndo we need really that much detail for histogram counts?\n\nLet’s keep what we have for the x axis and increase the by values of the y axis\n\n\n## ---------------------------\n## axis formatting: ver 2\n## ---------------------------\n\np &lt;- p +\n    scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100))\n\n\nThat seems like a better balance. We’ll stick with that and move on to legend labels",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#legend-labels",
    "href": "06-viz-ii.html#legend-labels",
    "title": "II: Customization",
    "section": "Legend labels",
    "text": "Legend labels\n\nLet’s make our histogram a little more complex by separating math scores by parental education\n\nSpecifically, we’ll use a binary variable that represents, did either parent attend college?\n\nFirst, we need to create a new variable, pared_coll, from the ordinal variable, x1paredu\n\nAlso, we are going make it a factor() so R knows the 0 and 1 don’t mean numbers, they mean cateogories\n\nWe did this last week, but this time we are going add labels too\n\nSpecifically, we want to levels and labels arguments\n\nThese pair up to make a labelled factor\n\nlevels should be a list of the values you have in the column\n\nIn this case just 0 and 1, so levels = c(0,1)\n\nlabels should be a list of the labels you want to use\n\nThese should be strings (in ““) in the same order as the levels you want to tie them to&lt;&gt;\nIn this case, we can say labels = c(\"No college\",\"College\") to match the levels = c(0,1)\n\n\n\n\n\n\n## -----------------------------------------------------------------------------\n##' [Legend labels]\n## -----------------------------------------------------------------------------\n\n## add indicator that == 1 if either parent has any college\ndf &lt;- df |&gt;\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0),\n         pared_coll = factor(pared_coll,\n                             levels = c(0,1),\n                             labels = c(\"No college\", \"College\")))\n\n\nNow we’ll make our same histogram, but add the fill aesthetic\n\nNote: since we are changing our underlying plot data, it’s best to start again and store the plot in p2\n\n\n\nQuick question: I also added one other new line, who can remember what alpha = 0.66 does?\n\n\np2 &lt;- ggplot(data = df) +\n  geom_histogram(mapping = aes(x = x1txmtscor,\n                               fill = factor(pared_coll)),\n                 alpha = 0.66) +\n  ## Below here is just what we had added to p in previous steps\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n      scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n    scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100))\n\n## show \np2\n\n\n\n\n\n\n\n\n\nClose! But, factor(pared_coll) isn’t the best name for our legend…\n\n\nQuick question: try adding something that will fix this issue and make it look like below",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#color-scales",
    "href": "06-viz-ii.html#color-scales",
    "title": "II: Customization",
    "section": "Color Scales",
    "text": "Color Scales\n\nPersonally, this graph still looks a little ugly\n\nI (and a lot of people) are red-green colorblind\n\nR chooses shades that aren’t too bad for this, but it’s still not great\n\nWhen we changed the color of a plot last week, it was purely decorative, so it went inside the geom_ function\nThis time, we want to change the colors associated with as aes() element, so, we are changing a scale_\n\nSpecifically scale_fill and we are going to choose manual colors, so scale_fill_manual()\n\nThis takes values = c(&lt;list of colors the same length as factor&gt;)\n\nIn this case our fill variable has two levels, so we need two colors\n\nFor school spirit, let’s pick a shade of orange and blue\n\nTip: You can add 1,2,3, or 4 next to a color in R to make it darker\n\n\n\n\n\n\n\n\n## ---------------------------\n##' [Color Scales]\n## ---------------------------\n\n## create histogram using ggplot\np2 &lt;- p2 +\n  scale_fill_manual(values = c(\"blue4\", \"orange2\"))\n\n## show \np2\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#facet_wrap",
    "href": "06-viz-ii.html#facet_wrap",
    "title": "II: Customization",
    "section": "facet_wrap()",
    "text": "facet_wrap()\n\nNow, what if we want to see how this trend varies by region?\nRemember from last week, we can use facet_wrap() to split data and make separate plots for each level of a variable\n\nfacet_wrap() uses a ~ tilde to mean “facet by this variable”\n\n\n\n## Add a facet wrap for region\np2 &lt;- p2 +\n  facet_wrap(~x1sex)\n\n## show \np2\n\n\n\n\n\n\n\n\n\nOkay, not a bunch of difference between sexes here, but this is interesting\n\n\nQuick question: without going back and changing the data (i.e., within the facet_wrap command, turn the x1sex variable into a factor with labels of the correct names)\n\n\nFrom the Codebook: 1 is Male, 2 is Female\n\n\n\n\n\n\n\n\n\n\n\nOkay, really getting close, but now that scale we set earlier is cramped…\n\n\nQuick task: add a new scale_x_continuous() to overwrite the “breaks” we set earlier with a sequence from 0 to 100 by 10, like below\n\n\n\n\n\n\n\n\n\n\n\nOkay, I think this is starting to come together!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#themes",
    "href": "06-viz-ii.html#themes",
    "title": "II: Customization",
    "section": "Themes",
    "text": "Themes\n\nNow that we have our figure mostly set up, we can adjust some of the overall appearence using theme() arguments\n\n\nPreset Themes\n\nThere are a number of preset themes in ggplot2\n\nI encourgae you to play around with these for the plots in your report\n\nFirst, let’s take a look at theme_bw()\n\n\n## -----------------------------------------------------------------------------\n##' [Preset themes]\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np2 &lt;- p2 +\n  theme_bw()\n\n## show \np2\n\n\n\n\n\n\n\n\n\nHow about theme_linedraw()?\n\n\np2 &lt;- p2 +\n  theme_linedraw()\n\n## show \np2\n\n\n\n\n\n\n\n\n\nYes, I think that looks pretty smart now!\n\n\n\nEditing Specific Theme Elements\n\nThis is where you can really dive into the depths of customization\n\nUnlike some proprietary analysis software cough, if you want to change something, you can (with some patience for tinkering around in the theme options)\n\nWe don’t have time to do much of this here, but for demonstration’s sake, let’s change one wildly specific thing\n\nLet’s tinker with the x axis ticks\n\nMake them a little thicker to 0.8 units\n\nggplot units are how you resize most things in ggplot, easiest way to pick a number is to test a value and adjust from there\n\nMake them round-ended pill shapes\n\n\n\n\np2 &lt;- p2 +\n  theme(axis.ticks.x = element_line(linewidth = 0.8,\n                                    lineend = \"round\"))\n\n## show \np2\n\n\n\n\n\n\n\n\n\nEditing theme elements is always the same, you need to play around to learn this on your own, but the general gist is\n\n\nAdd a theme() argument\n\n\nAlways below any preset themes, as those will overwrite anything you set\n\n\nInside theme() there are many many options which you can see listed here\nPick one of more and assign to it = an element_\nelement_ come in different types including element_line() like we used above element_text(), element_rect(), and others\nEach of these has specific options you can customize as you wish\n\n\nAs I say, there is a world of possibilities in customization, depending on how much time you want to dedicate!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#multiple-plots-with-patchwork",
    "href": "06-viz-ii.html#multiple-plots-with-patchwork",
    "title": "II: Customization",
    "section": "Multiple plots with patchwork",
    "text": "Multiple plots with patchwork\n\nIn this final section, we’ll practice putting multiple figures together\nAll the plots we’ve made so far have been one single ggplot object, but, we can put more than one together if we want\nWe use the patchwork library!\nWe technically have our original p plot saved, but we’ve made such progress since then, let’s whip up a nicer second plot and assign it to p3\nI’ve included a couple of new things here, but it should be mostly familiar\n\nscale_fill_viridis_d() sets color-blind friendly palettes, especially useful when you have a categorical variable\n\nGradient color sets (greyscale, shades of red, etc.) are pretty color-blind friendly, as without any color, they all will look like greyscale\n\nThis is also just useful for printing in black and white!\nTypically though, gradient colors imply increasing/decreasing values of a continuous variable (e.g., darker red means more GOP on election maps)\n\nThe viridis palettes in R have been designed to look like discrete color scales, i.e., they don’t look just like one color getting darker to most readers\n\nBut they also use shading/darkness/lightness of the colors, which means if printed in black and white, or read by someone who can’t see colors, they still work\n\nFor this reason, I try to use these for categorical variables in reports, unless I get carried away with school spirit!\n\nbegin and end just allow you to trim the more extreme ends of the color palette off - This is useful in situations like this when you only have two categories and don’t want them to look totally black and white\n\n\n\n## -----------------------------------------------------------------------------\n##' [Multiple plots with patchwork]\n## -----------------------------------------------------------------------------\n\n## Make a nice looking second plot of math scores by by parental education\np3 &lt;- ggplot(df) +\n  geom_boxplot(mapping = aes(x = pared_coll,\n                             y = x1txmtscor,\n                             fill = pared_coll),\n               alpha = 0.66) +\n  scale_fill_viridis_d(option = \"magma\", begin = 0.2, end = 0.8) +\n  labs(x = NULL,\n       y = \"Math Score\",\n       fill = \"Parental Education\") +\n  theme_linedraw()\n\n## show\np3\n\n\n\n\n\n\n\n\n\nNow that we have our new figure, let’s paste it side by side (left-right) with our first figure\nOnce we’ve loaded the patchwork library (like we already did at the top of the script), we can use a + sign between out two ggplot objects\n\np2 + p3\n\n\n\n## ---------------------------\n## patchwork: side by side\n## ---------------------------\n\n## use plus sign for side by side\np2 + p3\n\n\n\n\n\n\n\n\n\nThis is super squished, and even if we zoom in it’s not great…\n\nLet’s try on top of each other instead\n\nIn patchwork, this is / rather than +\n\n\n\n\np2 / p3\n\n\n\n\n\n\n\n\n\nCool! But there’s still a few things I don’t like…\n\nThe orange and blue clashes against the viridis colors\n\nSo let’s edit p2 to also use the magma viridis scale too\n\n\n\n\np2 &lt;- p2 +\n  scale_fill_viridis_d(option = \"magma\", begin = 0.2, end = 0.8)\n\np2 / p3\n\n\n\n\n\n\n\n\n\nThat’s definitely better! If I’m getting picky, the boxplot is a little stretched though, as it’s trying to fill the length of two histograms\n\nIf you want to specify a layout, you can do it using plot_layout() which takes a string of letters and #s\n\nA, B, C, etc. refer to the plots in the order you add them to the patchwork\n# Can be used to add space\nSo let’s create a design the histograms on top take up 5 spaces, and the boxplot takes up 3, with a spacer on either side\n\n\n\n\np2 / p3 + plot_layout(design = \"AAAAA\n                                #BBB#\")\n\n\n\n\n\n\n\n\n\nPretty good, but when can do even better…\n\nWe can add a guide_area() for all legend keys to go\nWe then need to add that as C in our patchwork design\nThen inside plot_layout() specify guides = \"collect\" which will try and put all the legends in one place\n\nIf our legends matched exactly, it would only keep one, but as the boxplot lines are part of the legend, they technically cannot be collapsed\n\nIf you really wanted, removing the lines from the boxplot legend would definitely by possible, but for the sake of the class, we will accept what we have!\n\n\n\n\n\np2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\")\n\n\n\n\n\n\n\n\n\nGetting somewhere now… But the caption we added to p2 is getting in the way, the title isn’t that great, and maybe some subtitles for each plot would be useful\n\n\nQuick excercise: Remove the title and caption labels from p2, add subtitles to p2 and p3, then replot the patchwork\n\n\n\n\n\n\n\n\n\n\n\nOkay, now finally, we can add some overall labess to our patchwork with plot_annotation()\n\nThese work very similarly to the ggplot labs()\n\n\n\np2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\") +\n  plot_annotation(title = \"Math Test Scores Differences by Parental Education\",\n                  caption = \"Data: High School Longitudinal Study, 2009\")\n\n\n\n\n\n\n\n\n\nThere we go, now the title better reflects our patchwork, and the caption sits below both plots as it should\nAs a note, we can also assign a whole patchwork to an object if we want\n\nLet’s assign this plot to an object called patch\n\nWhat we are going to do with it is a surprise for next week\n\n\n\n\npatch &lt;- p2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\") +\n  plot_annotation(title = \"Math Test Scores Differences by Parental Education\",\n                  caption = \"Data: High School Longitudinal Study, 2009\")\n\n\nGenerally, our plot shows there’s a bit of difference in math test scores by parental education, but that doesn’t differ much by sex",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#summary",
    "href": "06-viz-ii.html#summary",
    "title": "II: Customization",
    "section": "Summary",
    "text": "Summary\n\nThere is infinitely more customization you can do with both ggplot2 and patchwork, but this lesson has touched on a good amount of the basics!\nWe can always do more, of course, but remember that a figure doesn’t need to be complicated to be good\n\nIn fact, simpler is often better\n\nThe main thing is that it is clean and clear and tells the story you want the reader to hear\nWhat exactly that looks like is up to you and your project!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#ggplot2-challenge",
    "href": "06-viz-ii.html#ggplot2-challenge",
    "title": "II: Customization",
    "section": "ggplot2 Challenge!",
    "text": "ggplot2 Challenge!\n Data\n\nThis week, your assignment is to use the ggplot2-challenge-data.csv (download above) to recreate the below plot\nTo successfully complete the assignment, write ggplot2 code that creates a plot that closely resembles the below plot\n\nFor the full 5 points, the information portrayed needs to be identical with reasonably similar aesthetics\nIf you want to go the extra mile, there is a coveted ggplot2 magnet prize for the student who most closely copies the plot\nYou’re still welcome to work together, but, I only have 2 magnets… So any ties beyond that will have a slight delay in receiving their prize…\n\n\n\nHint: the data is already cleaned, but, there are still a couple of data wrangling tasks you’ll need to do before you can make the plot\n\n\n\n\nggplot2 Challenge Plot\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the lesson). Assignments will be graded on the following Monday (time permitting) in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#solution",
    "href": "06-viz-ii.html#solution",
    "title": "II: Customization",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Viz II Solution: ggplot2 Challenge]\n##' [INIT: 28 January 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\nlibrary(tidyverse)\n\ndf_plot &lt;- read_csv(file.path(\"data\", \"ggplot2-challenge-data.csv\"))\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Step one: pivot spending types to long\ndf_plot &lt;- df_plot |&gt;\n  pivot_longer(cols = ends_with(\"_prop\"),\n               names_to = \"spend_type\",\n               values_to = \"spend_prop\") |&gt;\n  ## Step two: make institution and spending type labeled factors\n  mutate(CONTROL = CONTROL |&gt; factor(levels = c(1,2,3),\n                                     labels = c(\"Public\", \"Non-Profit\", \"For-Profit\")),\n         spend_type = spend_type |&gt; factor(levels = c(\"inst_prop\", \"serv_prop\", \"rsch_prop\"),\n                                           labels = c(\"Instruction\", \"Student Services\", \"Research\")))\n\n## Step two: Plot\nggplot(df_plot) +\n  geom_density(aes(x = spend_prop,\n                   fill = spend_type),\n               alpha = 0.5) +\n  facet_wrap(~CONTROL,\n             strip.position = \"bottom\",\n             ncol = 2) +\n  scale_fill_manual(values = c(\"#FA4616\", \"#0021A5\", \"#22884C\")) +\n  labs(title = \"Ratio of Spending on Instruction, Student Services, and Research\",\n       subtitle = \"Differences Across Public, Non-Profit, and For-Profit Colleges\",\n       x = \"Proportion of Spending\",\n       fill = \"Type of Spending\") +\n  theme_minimal() +\n  theme(legend.position = c(0.8, 0.15),   ## h/t https://stackoverflow.com/questions/34022675/display-the-legend-inside-the-graph-when-wrapping-with-ggplot2\n        legend.justification = c(0.8, 0.15),\n        plot.subtitle = element_text(face = \"italic\"))\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#setup",
    "href": "08-wrangle-iii.html#setup",
    "title": "III: Working with strings & dates",
    "section": "Setup",
    "text": "Setup\nAs before, we’ll continue working within the tidyverse. We’ll focus, however, on using two specific libraries:\n\nstringr for strings\nlubridate for dates\n\nYou may have noticed already that when we load the tidyverse library with library(tidyverse), the stringr library is already loaded. The lubridate library, though part of the tidyverse, is not. We need to load it separately.\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NB: The stringr library is loaded with tidyverse, but\n## lubridate is not, so we need to load it separately\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\n\nNB: As we have done in the past few lessons, we’ll run this script assuming that our working directory is set to the scripts directory.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-1-working-with-strings",
    "href": "08-wrangle-iii.html#part-1-working-with-strings",
    "title": "III: Working with strings & dates",
    "section": "Part 1: Working with strings",
    "text": "Part 1: Working with strings\nTo practice working with strings, we’ll use data from Integrated Postsecondary Education Data System (IPEDS):\n\nThe National Center for Education Statistics (NCES) administers the Integrated Postsecondary Education Data System (IPEDS), which is a large-scale survey that collects institution-level data from postsecondary institutions in the United States (50 states and the District of Columbia) and other U.S. jurisdictions. IPEDS defines a postsecondary institution as an organization that is open to the public and has the provision of postsecondary education or training beyond the high school level as one of its primary missions. This definition includes institutions that offer academic, vocational and continuing professional education programs and excludes institutions that offer only avocational (leisure) and adult basic education programs. Definitions for other terms used in this report may be found in the IPEDS online glossary.\nNCES annually releases national-level statistics on postsecondary institutions based on the IPEDS data. National statistics include tuition and fees, number and types of degrees and certificates conferred, number of students applying and enrolled, number of employees, financial statistics, graduation rates, student outcomes, student financial aid, and academic libraries.\n\nYou can find more information about IPEDS here. As higher education scholars, IPEDS data are a valuable resource that you may often turn to (I do).\nWe’ll use one file (which can be found here), that covers institutional characteristics for one year:\n\nDirectory information, 2007 (hd2007.csv)\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## read in data and lower all names using rename_all(tolower)\ndf &lt;- read_csv(file.path(\"data\", \"hd2007.csv\")) |&gt;\n    rename_all(tolower)\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#finding-str_detect",
    "href": "08-wrangle-iii.html#finding-str_detect",
    "title": "III: Working with strings & dates",
    "section": "Finding: str_detect()",
    "text": "Finding: str_detect()\nSo far, we’ve filtered data using dplyr’s filter() verb. When matching a string, we have used == (or != for negative match). For example, if we wanted to limit our data to only those institutions in Florida, we could filter using the stabbr column:\n\n## filter using state abbreviation (not saving, just viewing)\ndf |&gt;\n    filter(stabbr == \"FL\")\n\n# A tibble: 316 × 59\n   unitid instnm    addr  city  stabbr zip    fips obereg chfnm chftitle gentele\n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 132268 Wyotech-… 470 … Ormo… FL     32174    12      5 Stev… Preside… 3.86e12\n 2 132338 The Art … 1799… Fort… FL     3331…    12      5 Char… Preside… 9.54e13\n 3 132374 Atlantic… 4700… Coco… FL     3306…    12      5 Robe… Director 7.54e 9\n 4 132408 The Bapt… 5400… Grac… FL     32440    12      5 Thom… Preside… 8.50e 9\n 5 132471 Barry Un… 1130… Miami FL     3316…    12      5 Sist… Preside… 8.01e 9\n 6 132523 Gooding … 615 … Pana… FL     32401    12      5 Dr. … CRNA Ph… 8.51e 9\n 7 132602 Bethune-… 640 … Dayt… FL     3211…    12      5 Dr T… Preside… 3.86e 9\n 8 132657 Lynn Uni… 3601… Boca… FL     3343…    12      5 Kevi… Preside… 5.61e 9\n 9 132666 Bradento… 5505… Brad… FL     34209    12      5 A. P… CEO      9.42e 9\n10 132675 Bradford… 609 … Star… FL     32091    12      5 Rand… Director 9.05e 9\n# ℹ 306 more rows\n# ℹ 48 more variables: ein &lt;chr&gt;, opeid &lt;chr&gt;, opeflag &lt;dbl&gt;, webaddr &lt;chr&gt;,\n#   adminurl &lt;chr&gt;, faidurl &lt;chr&gt;, applurl &lt;chr&gt;, sector &lt;dbl&gt;, iclevel &lt;dbl&gt;,\n#   control &lt;dbl&gt;, hloffer &lt;dbl&gt;, ugoffer &lt;dbl&gt;, groffer &lt;dbl&gt;, fpoffer &lt;dbl&gt;,\n#   hdegoffr &lt;dbl&gt;, deggrant &lt;dbl&gt;, hbcu &lt;dbl&gt;, hospital &lt;dbl&gt;, medical &lt;dbl&gt;,\n#   tribal &lt;dbl&gt;, locale &lt;dbl&gt;, openpubl &lt;dbl&gt;, act &lt;chr&gt;, newid &lt;dbl&gt;,\n#   deathyr &lt;dbl&gt;, closedat &lt;chr&gt;, cyactive &lt;dbl&gt;, postsec &lt;dbl&gt;, …\n\n\nThis works well because the stabbr column, even though it uses strings, is regular. But what happens when the strings aren’t so regular? For example, let’s look the different titles chief college administrators take.\n\n## see first few rows of distinct chief titles\ndf |&gt;\n    distinct(chftitle)\n\n# A tibble: 556 × 1\n   chftitle          \n   &lt;chr&gt;             \n 1 Commandant        \n 2 President         \n 3 Chancellor        \n 4 Interim President \n 5 CEO               \n 6 Acting President  \n 7 Director          \n 8 President/CEO     \n 9 Interim Chancellor\n10 President/COO     \n# ℹ 546 more rows\n\n\nWe find over 500 unique titles. Just looking at the first 10 rows, we see that some titles are pretty similar — President vs. CEO vs. President/CEO — but not exactly the same. Let’s look again, but this time get counts of each distinct title and arrange from most common to least.\n\n## return the most common titles\ndf |&gt;\n    ## get counts of each type\n    count(chftitle) |&gt;\n    ## arrange in descending order so we see most popular at top\n    arrange(desc(n))\n\n# A tibble: 556 × 2\n   chftitle               n\n   &lt;chr&gt;              &lt;int&gt;\n 1 President           3840\n 2 Director             560\n 3 Chancellor           265\n 4 Executive Director   209\n 5 Owner                164\n 6 Campus President     116\n 7 Superintendent       105\n 8 CEO                   90\n 9 &lt;NA&gt;                  85\n10 Interim President     75\n# ℹ 546 more rows\n\n\n\nQuick exercise\nWhat do you notice about the data frames returned by distinct() and count()? What’s the same? What does count() do that distinct() does not?\n\nGetting our counts and arranging, we can see that President is by far the most common title. That said, we also see Campus President and Interim President (and before we saw Acting President as well).\nIf your research question asked, how many chief administrators use the title of “President”? regardless the various iterations, you can’t really use a simple == filter any more. In theory, you could inspect your data, find the unique versions, get counts of each of those using ==, and then sum them up — but that’s a lot of work and likely to be error prone!\nInstead, we can use the stringr function str_detect(), which looks for a pattern in a vector of strings:\nstr_detect(&lt; vector of strings &gt;, &lt; pattern &gt;)\nGoing item by item in the vector, it compares what it sees to the pattern. If it matches, then it returns TRUE; it not, then FALSE. Here’s a toy example:\n\n## string vector example\nfruits &lt;- c(\"green apple\", \"banana\", \"red apple\")\n\n## search for \"apple\", which should be true for the first and third item\nstr_detect(fruits, \"apple\")\n\n[1]  TRUE FALSE  TRUE\n\n\nWe can use str_detect() inside filter() to select only certain rows in our data frame. In our case, we want only those observations in which the title \"President\" occurs in the chftitle column. Because we’re only detecting, as long as \"President\" occurs anywhere in the title, we’ll get that row back.\n\n## how many use some form of the title president?\ndf |&gt;\n    ## still starting with our count\n    count(chftitle) |&gt;\n    ## ...but keeping only those titles that contain \"President\"\n    filter(str_detect(chftitle, \"President\")) |&gt;\n    ## arranging as before\n    arrange(desc(n))\n\n# A tibble: 173 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 163 more rows\n\n\nNow we’re seeing many more versions. We can even more clearly see a few titles that are almost certainly the same title, but were just inputted differently — President/CEO vs. President and CEO vs. President & CEO.\n\nQuick exercise\nIgnoring the sub-counts of the various versions, how many chief administrators have the word “President” in their title?\n\nSeeing the different versions of basically the same title should have us stopping to think: since it seems that this data column contains free form input (e.g. Input chief administrator title:), maybe we should allow for typos? The easiest: Is there any reason to assume that “President” will be capitalized?\n\nQuick exercise\nWhat happens if we search for “president” with a lowercase “p”?\n\nAh! We find a few stragglers. How can we restructure our filter so that we get these, too? There are at least two solutions.\n\n1. Use regular expressions\nRegular expressions (aka regex) are strings that use a special syntax to create patterns that can be used to match other strings. They are very useful when you need to match strings that have some general form, but may differ in specifics.\nWe already used this technique in the a prior lesson when we matched columns in the all_schools_wide.csv with contains(\"19\") so that we could pivot_longer(). Instead of naming all the columns specifically, we recognized that each column took the form of &lt;test&gt;_19&lt;YY&gt;. This is a type of regular expression.\nIn the tidyverse some of the stringr and tidyselect helper functions abstract-away some of the nitty-gritty behind regular expressions. Knowing a little about regular expression syntax, particularly how it is used in R, can go a long way.\nIn our first case, we can match strings that have a capital P President or lowercase p president using square brackets ([]). If we want either “P” or “p”, then we can use the regex, [Pp], in place of the first character: \"[Pp]resident\". This will match either \"President\" or \"president\".\n\n## solution 1: look for either P or p\ndf |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(chftitle, \"[Pp]resident\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 175 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 165 more rows\n\n\nThough we don’t see the new observations in the abbreviated output, we note that the number of rows has increased by two. This means that there are at least two title formats in which \"president\" is lowercase and that we weren’t picking up when we only used the uppercase version of \"President\" before.\n\n\n2. Put everything in the same case and match with that case\nAnother solution, which is probably much easier in this particular case, is to set all potential values in chftitle to the same case and then match using that case. In many situations, this is preferable since you don’t need to guess cases up front.\nWe won’t change the values in chftitle permanently — only while filtering. To compare apples to apples (rather than \"Apples\" to \"apples\"), we’ll wrap our column name with the function str_to_lower(), which will make character lowercase, and match using lowercase \"president\".\n\n## solution 2: make everything lowercase so that case doesn't matter\ndf |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(str_to_lower(chftitle), \"president\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 177 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 167 more rows\n\n\nWe recover another two titles when using this second solution. Clearly, our first solution didn’t account for other cases (perhaps “PRESIDENT\"?).\nIn general, I find it’s a good idea to try a solution like the second one before a more complicated one like the first. But because every problem is different, so too are the solutions. You may find yourself using a combination of the two.\n\nNot-so-quick exercise\nAnother chief title that was high on the list was “Owner.” How many institutions have an “Owner” as their chief administrator? Of these, how many are private, for-profit institutions (control == 3)? How many have the word “Beauty” in their name?\n\n\n\nReplace using string position: str_sub()\nIn addition to filtering data, we sometimes need to create new variables from pieces of exiting variables. For example, let’s look at the zip code values that are included in the file.\n\n## show first few zip code values\ndf |&gt;\n    select(unitid, zip)\n\n# A tibble: 7,052 × 2\n   unitid zip       \n    &lt;dbl&gt; &lt;chr&gt;     \n 1 100636 36112-6613\n 2 100654 35762     \n 3 100663 35294-0110\n 4 100690 36117-3553\n 5 100706 35899     \n 6 100724 36101-0271\n 7 100733 35401     \n 8 100751 35487-0166\n 9 100760 35010     \n10 100812 35611     \n# ℹ 7,042 more rows\n\n\nWe can see that we have both regular 5 digit zip codes as well as those that include the extra 4 digits (ZIP+4). Let’s say we don’t need those last four digits for our analysis (particularly because not every school uses them anyway). Our task is to create a new column that pulls out only the main part of the zip code. It is has to work both for zip values that include the additional hyphen and 4 digits as well as those that only have the primary 5 digits to begin with.\nOne solution in this case is to take advantage of the fact that zip codes — minus the sometimes extra 4 digits — should be regular: 5 digits. If want the sub-part of a string and that sub-part is always in the same spot, we can use the function, str_sub(), which takes a string or column name first, and has arguments for the starting and ending character that mark the sub-string of interest.\nIn our case, we want the first 5 digits so we should start == 1 and end == 5:\n\n## pull out first 5 digits of zip code\ndf &lt;- df |&gt;\n    mutate(zip5 = str_sub(zip, start = 1, end = 5))\n\n## show (use select() to subset so we can set new columns)\ndf |&gt;\n    select(unitid, zip, zip5)\n\n# A tibble: 7,052 × 3\n   unitid zip        zip5 \n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 100636 36112-6613 36112\n 2 100654 35762      35762\n 3 100663 35294-0110 35294\n 4 100690 36117-3553 36117\n 5 100706 35899      35899\n 6 100724 36101-0271 36101\n 7 100733 35401      35401\n 8 100751 35487-0166 35487\n 9 100760 35010      35010\n10 100812 35611      35611\n# ℹ 7,042 more rows\n\n\nA quick visual inspection of the first few rows shows that our str_sub() function performed as expected (for a real analysis, you’ll want to do more formal checks).\n\n\nReplace using regular expressions: str_replace()\nWe can also use a more sophisticated regex pattern with the function str_replace(). The pieces of our regex pattern, \"([0-9]+)(-[0-9]+)?\", are translated as this:\n\n[0-9] := any digit, 0 1 2 3 4 5 6 7 8 9\n+ := match the preceding one or more times\n? := match the preceding 0 or more times\n() := subexpression\n\nPut together, we have:\n\n([0-9]+) := first, look for 1 or more digits\n(-[0-9]+)? := second, look for a hyphen and one or more digits, but you may not find any of that\n\nBecause we used parentheses, (), to separate our subexpressions, we can call them using their numbers (in order) in the last argument of str_replace():\n\n\"\\\\1\" := return the first subexpression\n\nSo what’s happening? If given a zip code that is \"32605\", the regex pattern will collect each digit — \"3\" \"2\" \"6\" \"0\" \"5\" — into the first subexpression because it never sees a hyphen. That first subexpression, \"\\\\1\", is returned: \"32605\". That’s what we want.\nIf given \"32605-1234\", it will collect the first 5 digits in the first subexpression, but will stop adding characters there when it sees the hyphen. From then on out, it adds everything it sees the second subexpression: \"-\" \"1\" \"2\" \"3\" \"4\". But because str_replace() only returns the first subexpression, we still get the same answer: \"32605\". This is what we want.\nLet’s try it on the data.\n\n## drop last four digits of extended zip code if they exist\ndf &lt;- df |&gt;\n    mutate(zip5_v2 = str_replace(zip, \"([0-9]+)(-[0-9]+)?\", \"\\\\1\"))\n\n## show (use select() to subset so we can set new columns)\ndf |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 7,052 × 4\n   unitid zip        zip5  zip5_v2\n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  \n 1 100636 36112-6613 36112 36112  \n 2 100654 35762      35762 35762  \n 3 100663 35294-0110 35294 35294  \n 4 100690 36117-3553 36117 36117  \n 5 100706 35899      35899 35899  \n 6 100724 36101-0271 36101 36101  \n 7 100733 35401      35401 35401  \n 8 100751 35487-0166 35487 35487  \n 9 100760 35010      35010 35010  \n10 100812 35611      35611 35611  \n# ℹ 7,042 more rows\n\n\n\nQuick exercise\nWhat if you wanted to the get the last 4 digits (after the hyphen)? What bit of two bits of code above would you change so that you can store the last 4 digits without including the hyphen? Make a new variable called zip_plus4 and store these values. HINT Look at the help file for str_replace().\n\nLet’s compare our two versions: do we get the same results?\n\n## check if both versions of new zip column are equal\nidentical(df |&gt; select(zip5), df |&gt; select(zip5_v2))\n\n[1] FALSE\n\n\nNo! Let’s see where they are different:\n\n## filter to rows where zip5 != zip5_v2 (not storing...just looking)\ndf |&gt;\n    filter(zip5 != zip5_v2) |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 4 × 4\n  unitid zip        zip5  zip5_v2   \n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     \n1 108199 90015--350 90015 90015--350\n2 113953 92113--191 92113 92113--191\n3 431707 06360--709 06360 06360--709\n4 435240 551012595  55101 551012595 \n\n\n\nQuick exercise\nWhat happened? In this scenario, which string subsetting technique worked better?\n\nDepending on the task, regular expressions can either feel like a blessing or a curse. To be honest, I’ve spent more time cursing than thanking them. That said, regular expressions are often the only way to perform a data wrangling task on unstructured string data. They are also a cornerstone of natural language processing techniques, which are increasingly of interest to education researchers.\nWe’ve only scratched the surface of what regular expressions can do. If you face string data in the future, taking a little time to craft a regular expression can be well worth it.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-ii-working-with-dates",
    "href": "08-wrangle-iii.html#part-ii-working-with-dates",
    "title": "III: Working with strings & dates",
    "section": "Part II: Working with dates",
    "text": "Part II: Working with dates\n\nMuch like names, dates are often saved as text/strings, and can be messy and formatted differently\n\nFor example, “2026-07-04”, “July 4th 2026”, “4th July 26”, and “04/07/26” all refer to the United State’s upcoming 250th birthday, but how do we make the computer understand they all mean that?\n\nWhat’s trickier is that even once we have dealt with the formatting, we sometimes need to be able to make calculations with dates\n\nFor example, if we have students SAT scores from multiple attempts, somone might be interested in knowing how many days passed between attempts\n\nThis involves knowing how many days are in each month (something I can never remember), if that year was a leap year, etc.\n\n\nIn our IPEDS data see which institutions closed in 2007 and 2008 in the closedat column\n\n-2 means the institution didn’t close in this period, so let’s drop them\n\n\n\n## subset to schools who closed during this period\ndf &lt;- df |&gt;\n  filter(closedat != -2) |&gt;\n  select(unitid, instnm, closedat)\n\ndf\n\n# A tibble: 83 × 3\n   unitid instnm                                                  closedat\n    &lt;dbl&gt; &lt;chr&gt;                                                   &lt;chr&gt;   \n 1 103440 Sheldon Jackson College                                 6/29/07 \n 2 104522 DeVoe College of Beauty                                 3/29/08 \n 3 105242 Mundus Institute                                        Sep-07  \n 4 105880 Long Technical College-East Valley                      3/31/07 \n 5 119711 New College of California                               Jan-08  \n 6 136996 Ross Medical Education Center                           7/31/07 \n 7 137625 Suncoast II the Tampa Bay School of Massage Therapy LLC 5/31/08 \n 8 141583 Hawaii Business College                                 Sep-07  \n 9 150127 Ball Memorial Hospital School of Radiologic Technology  May-07  \n10 160144 Pat Goins Shreveport Beauty School                      3/1/08  \n# ℹ 73 more rows\n\n\n\nparse-ing String into Dates\n\nSo, we can see closedat is a &lt;chr&gt; or string variable\n\nFrom a combination of looking at the values and knowing this is a US document, we can see most of these dates are month/day/year\n\nWe are really only going to use one function from lubridate the parse_date_time() function\n\nThere are other ways you can handle dates, some in base R, some other in lubridate, but this is a way that works well in most situations\nThe function takes two main arguments\n\nx: The string you are trying to turn into a date\norders: The format(s) the string date is written in\n\nThis take one or more date formats to try\nSince we think most of our dates are “month, day, year” this will be \"mdy\"\nYou can see a full list of options on the lubridate reference page\nIt tries these in order, seeing if it can “parse” or “figure out” the date looking for anything that divides dates up (” “,”/“,”-“) and/or spelt out date elements like”August”\n\nIf it can work out the date with the the first format, it will move on, if not, it will try and others you gave it (in order) and then if it can’t make the string fit into any of those formats, it will “fail to parse”\n\n\n\nIt then outputs a &lt;dttm&gt; (date time) type variable, which we are going to assign to a new variable clean_date\n\nThis involves a lot of well thought out code on the back-end, but makes our lives so much easier\nIn case you’re interested, R actually stores every date and time as time since 00:00:00 on January 1st 1970 behind the scenes\n\n\nLet’s give this a go!\n\n\n## create a new clean_date column \ndf &lt;- df |&gt;\n    mutate(clean_date = parse_date_time(closedat,\n                                        orders = \"mdy\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `clean_date = parse_date_time(closedat, orders = \"mdy\")`.\nCaused by warning:\n!  35 failed to parse.\n\n## show\ndf\n\n# A tibble: 83 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 104522 DeVoe College of Beauty                   3/29/08  2008-03-29 00:00:00\n 3 105242 Mundus Institute                          Sep-07   NA                 \n 4 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 5 119711 New College of California                 Jan-08   NA                 \n 6 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 7 137625 Suncoast II the Tampa Bay School of Mass… 5/31/08  2008-05-31 00:00:00\n 8 141583 Hawaii Business College                   Sep-07   NA                 \n 9 150127 Ball Memorial Hospital School of Radiolo… May-07   NA                 \n10 160144 Pat Goins Shreveport Beauty School        3/1/08   2008-03-01 00:00:00\n# ℹ 73 more rows\n\n\n\nOkay, that worked for the majority of our colleges, but we some like Sep-2007 that it didn’t like\n\n\nQuick Dicussion\nWhy wasn’t it able to “parse” Sep-2007?\nDo we know enough about when this institution closed? Why or why not?\nWhat might we be able to do with the information we have?\n\n\nNow, remember orders can take more than one value, so maybe we could try another date format that can pick up some of these dates\n\nLooking through the lubridate reference page, I can see that \"my\" (month year) is a format it will take as well, so let’s try that\n\n\n\n## Try adding another date format\ndf &lt;- df |&gt;\n    mutate(clean_date = parse_date_time(closedat,\n                                        orders = c(\"mdy\", \"my\")))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `clean_date = parse_date_time(closedat, orders = c(\"mdy\",\n  \"my\"))`.\nCaused by warning:\n!  7 failed to parse.\n\n## show\ndf\n\n# A tibble: 83 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 104522 DeVoe College of Beauty                   3/29/08  2008-03-29 00:00:00\n 3 105242 Mundus Institute                          Sep-07   2007-09-01 00:00:00\n 4 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 5 119711 New College of California                 Jan-08   2008-01-01 00:00:00\n 6 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 7 137625 Suncoast II the Tampa Bay School of Mass… 5/31/08  2008-05-31 00:00:00\n 8 141583 Hawaii Business College                   Sep-07   2007-09-01 00:00:00\n 9 150127 Ball Memorial Hospital School of Radiolo… May-07   2007-05-01 00:00:00\n10 160144 Pat Goins Shreveport Beauty School        3/1/08   2008-03-01 00:00:00\n# ℹ 73 more rows\n\n\n\nOkay, this time only 7 failed to parse , so that’s a lot better\nTake a look at what it did with Sep-2007\n\nDo we like this or not? Is it trustworthy?\n\nLet’s take a look at the 7 that didn’t go through this time\n\n\ndf |&gt;\n  filter(is.na(clean_date))\n\n# A tibble: 7 × 4\n  unitid instnm                                     closedat clean_date\n   &lt;dbl&gt; &lt;chr&gt;                                      &lt;chr&gt;    &lt;dttm&gt;    \n1 200794 Akron Machining Institute Inc              07/200   NA        \n2 231572 Braxton School                             2007     NA        \n3 262013 New York Institute of Technology-Central … 2007     NA        \n4 381343 CET-Reno                                   2007     NA        \n5 394846 CET-Santa Ana                              2007     NA        \n6 404532 Sharps Academy of Hair Styling             2007     NA        \n7 436793 Warren Woods Vocational Adult Education    2007     NA        \n\n\n\nHmm, those ones might be a little hard to approximate, so let’s drop them\n\n\ndf &lt;- df |&gt;\n  drop_na(clean_date)\n\n\n\nWorking with parse-ed Date Objects\n\nWe have successfully turn as many of the close dates into date objects that R can understand, but why?\n\nWe can now make comparisons and calculations so much more easily\n\nAs we go through these example, just imagine having to do this by hand\n\n\n\n\nNumerical Calculations\n\nLet’s say we wanted to quickly find the earliest date\n\nWe wouldn’t have got far with the &lt;chr&gt; string date, but our &lt;dttm&gt; object R can understand and tell us the min() value of it\n\n\n\ndf |&gt; filter(clean_date == min(clean_date))\n\n# A tibble: 1 × 4\n  unitid instnm           closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;    &lt;dttm&gt;             \n1 365912 City College Inc 3/23/07  2007-03-23 00:00:00\n\n\n\nQuick Exercise\nFind the school with the most recent closure date\n\n\n\n\nComparing to Reference Dates\n\nOr, lets say we want to see how many schools closed before Christmas Day 2007\n\nWe can use parse_date_time() again to store December 25th 2007 as a date time object christmas_07\nThen we can just filter schools whose date is less than that\n\n\n\nchristmas_07 &lt;- parse_date_time(\"Dec 25 2007\", \"mdy\")\n\ndf |&gt; filter(clean_date &lt; christmas_07)\n\n# A tibble: 60 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 105242 Mundus Institute                          Sep-07   2007-09-01 00:00:00\n 3 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 4 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 5 141583 Hawaii Business College                   Sep-07   2007-09-01 00:00:00\n 6 150127 Ball Memorial Hospital School of Radiolo… May-07   2007-05-01 00:00:00\n 7 161624 Accutech Career Institute                 Sep-07   2007-09-01 00:00:00\n 8 170824 Marquette General Hospital                Aug-07   2007-08-01 00:00:00\n 9 180124 College of Coiffure Art Ltd               6/25/07  2007-06-25 00:00:00\n10 186849 Harrison Career Institute-Vineland        Sep-07   2007-09-01 00:00:00\n# ℹ 50 more rows\n\n\n\nWhat about within 30 days of Christmas\n\nFor this we need to use interval(clean_date, christmas_07) to look between two dates\nThen time_length(, \"day\") converts that to the number of days\nThen abs() to get the absolute value (otherwise dates long after Christmas 2007 will be kept as the interval is -63 days)\nThis gets a little nested, so we can use more pipes |&gt; inside our filter() statement if that’s easier\n\n\n\n## Nested version\ndf |&gt; filter(abs(time_length(interval(clean_date, christmas_07), \"day\")) &lt; 30)\n\n# A tibble: 9 × 4\n  unitid instnm                             closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;    &lt;dttm&gt;             \n1 119711 New College of California          Jan-08   2008-01-01 00:00:00\n2 191870 Interboro Institute                12/21/07 2007-12-21 00:00:00\n3 225867 Austin Business College            12/28/07 2007-12-28 00:00:00\n4 243902 Gaither and Company Beauty College 12/5/07  2007-12-05 00:00:00\n5 415561 Cortiva Institute-Colorado         12/31/07 2007-12-31 00:00:00\n6 436748 New Hampshire Career Institute     12/20/07 2007-12-20 00:00:00\n7 445586 Banner Institute-Chicago           Dec-07   2007-12-01 00:00:00\n8 446871 The Bryman School-East             Dec-07   2007-12-01 00:00:00\n9 447500 Salter School-Cambridge Campus     12/31/07 2007-12-31 00:00:00\n\n## Internal pipes version\ndf |&gt; filter(interval(clean_date, christmas_07) |&gt;\n               time_length(\"day\") |&gt;\n               abs() &lt; 30)\n\n# A tibble: 9 × 4\n  unitid instnm                             closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;    &lt;dttm&gt;             \n1 119711 New College of California          Jan-08   2008-01-01 00:00:00\n2 191870 Interboro Institute                12/21/07 2007-12-21 00:00:00\n3 225867 Austin Business College            12/28/07 2007-12-28 00:00:00\n4 243902 Gaither and Company Beauty College 12/5/07  2007-12-05 00:00:00\n5 415561 Cortiva Institute-Colorado         12/31/07 2007-12-31 00:00:00\n6 436748 New Hampshire Career Institute     12/20/07 2007-12-20 00:00:00\n7 445586 Banner Institute-Chicago           Dec-07   2007-12-01 00:00:00\n8 446871 The Bryman School-East             Dec-07   2007-12-01 00:00:00\n9 447500 Salter School-Cambridge Campus     12/31/07 2007-12-31 00:00:00\n\n\n\nQuick Exercise\n\nDid any schools close within a week of Christmas?\n\n\n\n\nExtracting Info From Dates\n\nWhat is we want to see which fiscal quarter more schools closed in?\n\nThe handy quarter() function tell us that\n\n\n\ndf |&gt; \n  mutate(quarter = quarter(clean_date)) |&gt;\n  count(quarter)\n\n# A tibble: 4 × 2\n  quarter     n\n    &lt;int&gt; &lt;int&gt;\n1       1    11\n2       2    20\n3       3    32\n4       4    13\n\n\n\nWhat about the day of the week they closed on?\n\nSimilarly wday() can tell us that\n\nWe need to say label = TRUE to get “Sun” instead of 1\n\n\n\n\ndf |&gt;\n  mutate(day = wday(clean_date, label = TRUE)) |&gt;\n  count(day)\n\n# A tibble: 7 × 2\n  day       n\n  &lt;ord&gt; &lt;int&gt;\n1 Sun       6\n2 Mon       9\n3 Tue       7\n4 Wed       7\n5 Thu       6\n6 Fri      15\n7 Sat      26\n\n\n\nQuick Question\n\nCan we 100% trust these week day counts? Why or why not?",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#summary",
    "href": "08-wrangle-iii.html#summary",
    "title": "III: Working with strings & dates",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson we’ve looked at working with both generic strings and with dates\n\nFor both of these we have only begun to scratch the surface, but it should have given you enough of an idea to go out and get your hands dirty\nIf you found this interesting, Dr. Jinnie Shin teaches a class on Natural Language Processing in the Fall, which gets into how make the computer begin to understand text\n\nThese are both messy types of data and you’re often going to have to make subjective decisions (e.g., how to handle Sep-2007)\n\nWhat’s most important is that you document/comment what you did and why you did it\n\nAs you will see over the next few weeks, while this stuff can be tricky, working with strings efficiently can be really powerful in more advanced programming",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#questions",
    "href": "08-wrangle-iii.html#questions",
    "title": "III: Working with strings & dates",
    "section": "Questions",
    "text": "Questions\nNB To answer the questions, you will need to join the two IPEDS data sets using the common unitid key. Note that column names in hd2007.csv are uppercase (UNITID) while those in ic2007mission.csv are lowercase (unitid). There are a few ways to join when the keys don’t exactly match. One is to set all column names to the same case. If you want to use left_join() starting with hd2007.csv, you can first use the the dplyr verb rename_all(tolower) in your chain to lower all column names. See the help file for left_join() for other ways to join by different variable names.\nI also find these cheat sheets extremely helpful\n\nCheat sheet for stringr and regex\nCheat sheet for lubridate\nYou can find all posit cheat sheets here\n\n\nHow many chief administrator names start with “Dr.”?\nNB Many chief administrators are listed on more than one line due to branch campuses. Make sure to take this into account by keeping only distinct names.\nBONUS How many chief administrator names end with the title “PH.D.” or some variant?\nAmong those schools that give their mission statement:\n\nHow many repeat their institutional name in their mission statement?\n\nHow many use the word civic?\nWhich top 3 states have the most schools with mission statements that use the word future?\n\nWhich type of schools (public, private-non-profit, private-for-profit) are most likely to use the word skill in their mission statement?\n\nAmong the schools that closed in 2007 or 2008 and give a date with at least a month and year:\n\nWhich has been closed for the longest time? How many months has it been from its close date to the beginning of this current month (1 February 2020)?\n\nHow many days were there between the first school to close and the last?\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the lesson). Assignments will be graded on the following Monday (time permitting) in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#solution",
    "href": "08-wrangle-iii.html#solution",
    "title": "III: Working with strings & dates",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Wrangling III Solution]\n##' [INIT: March 18 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf_hd &lt;- read_csv(\"data/hd2007.csv\")\ndf_mission &lt;- read_csv(\"data/ic2007mission.csv\")\n\n\ndf &lt;- df_hd |&gt;\n  left_join(df_mission, by = c(\"UNITID\" = \"unitid\")) # Could also just rename column to lower case\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\ndf |&gt;\n  mutate(chfnm_lower = str_to_lower(CHFNM)) |&gt; # lower case the name\n  filter(str_detect(chfnm_lower, \"(^|\\\\s)dr(a)?(,|\\\\.|\\\\s)\")) |&gt; # keep anything that after either the start (^) or a \" \" has \"dr\" then maybe \"a\" (for Spanish dra) followed by either \",\" \".\" or \" \" \n  group_by(chfnm_lower, STABBR) |&gt; # group by unique chief admin names (due to branch campuses) also group by state, to minimize chance of two different people with the same name being counted as one. Is there a better way to check for branch campuses?\n  slice(1) |&gt; # slice the one row of these (to remove duplicates)\n  ungroup() |&gt; # remove the grouping as we only wanted slice() to be grouped\n  select(chfnm_lower) |&gt; # keep only the chief admin name\n  count(chfnm_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(sum(n)) # count up the number ones\n\n## Below is how I compared our in class regex with Ben's and my final answer\n\ninclass_names &lt;- df |&gt;\n  mutate(chfnm_lower = str_to_lower(CHFNM)) |&gt; # lower case the name\n  filter(str_detect(chfnm_lower, \"dr\\\\.?\\\\s\")) |&gt; # keep anything \"dr\" maybe a \".\" then \" \"\n  pull(chfnm_lower)\n\nbens_names &lt;- df |&gt;\n  mutate(chfnm_lower = str_to_lower(CHFNM)) |&gt; # lower case the name\n  filter(str_detect(chfnm_lower, \"^dr\\\\.?[^ew]\")) |&gt; # keep anything that starts (^) \"dr\" maybe a \".\" then anything but \"ew\" (to exclude drew or andrew)\n  pull(chfnm_lower)\n\nbens_names[!bens_names %in% inclass_names]\n\nmatts_names &lt;- df |&gt;\n  mutate(chfnm_lower = str_to_lower(CHFNM)) |&gt; # lower case the name\n  filter(str_detect(chfnm_lower, \"(^|\\\\s)dr(a)?(,|\\\\.|\\\\s)\")) |&gt; # keep anything that after either the start (^) or a \" \" has \"dr\" then maybe \"a\" (for Spanish dra) followed by either \",\" \".\" or \" \"\n  pull(chfnm_lower)\n\nmatts_names[!matts_names %in% bens_names]\n\n## The differences between Ben's and my answer get to an important\n## research point, technically Ben's answer is what was asked, but in many situations\n## you were given that question, they would want \"rev. dr\" and \"rabbi dr\" included\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf |&gt;\n  mutate(chfnm_lower = str_to_lower(CHFNM)) |&gt; # lower case the name\n  filter(str_detect(chfnm_lower, \"ph\\\\.?\\\\s?d\\\\.?\\\\s?$\")) |&gt; # keep only if there's \"dr\" maybe a \".\", then a space\n  group_by(chfnm_lower, STABBR) |&gt; # group by unique chief admin names (due to branch campuses) also group by state to minimize chance of two different people with the same name (ideally, you'd be more sophisticated and check for branch campuses directly)\n  slice(1) |&gt; # slice the one row of these (to remove duplicates)\n  ungroup() |&gt; # remove the grouping as we only wanted slice() to be grouped\n  count(chfnm_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(sum(n)) # count up the number ones\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n##'[i]\n\ndf |&gt;\n  mutate(mission_lower = str_to_lower(mission), # lower case the mission\n         instnm_lower = str_to_lower(INSTNM)) |&gt; # lower case the institution name\n  filter(str_detect(mission_lower, instnm_lower)) |&gt; # search for the name in the mission\n  count(mission_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(sum(n)) # count up the number ones\n  \n##'[ii]\n\ndf |&gt;\n  mutate(mission_lower = str_to_lower(mission)) |&gt; # lower case the mission\n  filter(str_detect(mission_lower, \"civic\")) |&gt; # look for the word \"civic\n  count(mission_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(sum(n)) # count up the number ones\n\n##'[iii]\n\ndf |&gt;\n  mutate(mission_lower = str_to_lower(mission)) |&gt; # lower case mission\n  filter(str_detect(mission_lower, \"future\")) |&gt; # look for the word \"future\"\n  group_by(STABBR) |&gt; # before we count this time, group by state\n  count(mission_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(n = sum(n)) |&gt; # count up the number ones\n  arrange(desc(n)) |&gt; # arrange in descending order of the count\n  slice_head(n = 3) # keep the top three rows\n\n##'[iv]\n\ndf |&gt;\n  mutate(mission_lower = str_to_lower(mission)) |&gt; # lower case the mission\n  filter(str_detect(mission_lower, \"skill\")) |&gt; # look for the word skill\n  group_by(CONTROL) |&gt; # before we count, group by control\n  count(mission_lower) |&gt; # get counts of the names (should be a column of ones)\n  summarize(n = sum(n)) |&gt; # count up the number ones\n  arrange(desc(n)) # arrange in descending order\n\n## ---------------------------\n##' [Q4]\n## ---------------------------\n\ndf_close &lt;- df |&gt;\n  select(UNITID, INSTNM, CLOSEDAT) |&gt; # Keep only the columns we need\n  filter(CLOSEDAT != \"-2\") |&gt; # Get rid of any colleges that haven't closed\n  mutate(clean_date = parse_date_time(CLOSEDAT, ## Create clean_date by turning CLOSEDAT into a date_time object\n                                      orders = c(\"mdy\", \"my\"))) ## Try month/day/year format first, then just month/year (assumes 1st of month)\n\n## Print out the 7 that \"failed to parse\"\ndf_close |&gt;\n  filter(is.na(clean_date))\n\n## After inspection, we can't approximate the 7 that failed to parse accurately, so drop them\ndf_close &lt;- df_close |&gt;\n  drop_na(clean_date)\n\n## Now the clean_date is a lubridate object, we can just treat it like a numeric variable\n\n##'[i]\n##' Option One: Use the date written\ndf_close |&gt;\n  mutate(time_from_today = parse_date_time(\"Feb 1 2020\", \"mdy\") - clean_date) |&gt; # same logic as above, create a date_time object for Feb 1 2020 (format is month/day/year) get the difference from clean_date \n  slice_max(time_from_today) # slice off the row with the most days from today\n\n## Option Two: Use the date/time right now\ndf_close |&gt;\n  mutate(time_from_today = now() - clean_date) |&gt; # now() just gets date/time when run\n  slice_max(time_from_today) # slice off the row with the most days from today\n\n##'[ii]\ndf_close |&gt;\n  summarize(answer = max(clean_date) - min(clean_date)) # We can also just use max() and min() like any other variable\n\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data",
    "href": "10-viz-iii.html#reading-in-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nSpatial data comes in many formats:\n\nShapefiles .shp: A traditional format from ESRI (the makers of ArcGIS)\n\nVery widely used, by far the most common format\nComes as a folder containing multiple files with the same name but different extensions\n\nThe primary file (the one you actually read in) ends in .shp , reading one into R looks something like this\n\ndf_my_map &lt;- read_sf(\"data/my-map-data/my-map-data\")\n\n\nSerious limitations for working with attached data, the most annoying of which is the variables can only be 7 characters long\n\nOther modern, better, but (sadly) rarer formats include\n\nGeoDatabase .gdb: A modern format from ESRI\nGeoJSON .geojson: A plain text format\nGeoPackage .gpkg: An open-source driven single file format\nSQL Databses: Many SQL variants can store/handle spatial data\n\nThe sf package we are going to use is based on PostGIS which is the spatial accompaniment to PostgreSQL open source SQL\n\n\nIf you get into spatial data, you’ll want to get familiar with at least Shapefiles at first, then play around with some of the other formats\nWhat’s great about sf is that it can read pretty much any format, and does so in the same way\n\nread_sf(\"file/path.extension\")\n\n\n\n\nQuick Exercise\nThere’s so much spatial data out there, using Google see if you can find a Shapefile for something you’re interested in, download it, and read it in to R using read_sf()\nHint: US Governments of all levels provide free to use spatial data. If you’re stuck, check out Alachua County GIS Portal\n\nFor today’s lesson, however, we are going to use an API to directly download our spatial data",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#apis-setting-up-tidycensus",
    "href": "10-viz-iii.html#apis-setting-up-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "APIs & Setting up tidycensus",
    "text": "APIs & Setting up tidycensus\n\nSo what exactly is an API?\n\nIn short, think of it as a way of R going to a website/database and pulling data directly from the server-side or back end, without our having to ever interact with the website directly\nThis has two main advantages\n\nWe don’t have to store the large dataset on our computer\nOur work becomes instantly more reproducible\n\nNote from BS: We avoid point-click at all costs! We are going to use the API tidycensus today, but all APIs operate on the same basic idea\n\n\n\ntidycensus is, in my opinion, one of the easiest APIs to get set up and use in R\nMost APIs require that you use some kind of key that identifies you as an authorized user\n\nTypically you need to set up the key the first time you use the API, but helpfully, it’s usually possible to store the key on your computer for all future use\nMany API keys are free to obtain and use\n\nIf you were using an API to access a private database such as Google Maps, you might need to pay for your key to have access depending on how much you use it\nBut because we are using Census data, which is freely available to the public, there’s no charge\n\n\n\n\nGetting Census API Key\n\nGetting your Census API key is extremely easy\n\nSimply go here\nEnter your organization name (University of Florida)\nEnter your UF email\n\nYou will quickly receive an email with your API key, which you will need below\n\n\n\n\n\nTo set up tidycensus for the first time, we first need to set our API key\nThe tidycensus package makes this much easier than many APIs by having a built-in function that you can use to save your API key to your computer\n\nSimply place your API key in the &lt;key&gt; of the code below\nThe install option means it will save the API key for future use, so you will not need to worry about this step again\n\n\n\n## ---------------------------\n##' [set API key]\n## ---------------------------\n\n## you only need to do this once: replace everything between the\n## quotes with the key in the email you received\n##\n## eg. census_api_key(\"XXXXXXXXXXXXXXXXXX\", install = T)\ncensus_api_key(\"&lt;key&gt;\", install = T)\n\n\nNow that this is set up, we are ready to start using tidycensus — yay!",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "href": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in data with tidycensus",
    "text": "Reading in data with tidycensus\n\nThere are multiple main tidycensus functions that you can use to call in data, with each calling data from a different source operated by the US Census Bureau\n\nget_acs()\nget_decennial()\nget_estimates()\nget_flows()\nget_pop_groups()\nget_pums()\n\nYou can see more information about these on the tidycensus reference page\n\n\n\nFor today’s lesson we are going to use get_acs()\n\nThis collects data from the American Community Survey (regular sampled surveys of demographic data across the US)\n\nWe are going to assign &lt;- the data we pull down into the object df_census:\n\n\n## ---------------------------\n##' [Get ACS Data]\n## ---------------------------\n\ndf_census &lt;- get_acs(geography = \"county\",\n                     state = \"FL\",\n                     year = 2021,\n                     variables = \"DP02_0065PE\", # Pop &gt;=25 with Bachelors\n                     output = \"wide\",\n                     geometry = TRUE)\n\nLet’s walk through each element of this command in turn:\n\ngeography = \"county\"\n\nTelling the function to get estimates (and spatial data later) at the county level\n\nthis could also be \"state\", for example, to get state level data\n\n\nstate = \"FL\"\n\nTelling the function to get data only for the state of Florida\n\nYou could put a group of states with c()\n\nuse full state names, or use FIPS codes — tidycensus is flexible\n\nIf you want a narrower set of data, you could also add county =, which works in a similar way\n\nFor example, if you added county = \"Alachua\", you would only get county-level data for Alachua County, Florida.\n\n\n\nyear = 2021\n\nTelling the function to pull data for the survey year 2021\n\nFor ACS, this will be the survey set ending in that year\n\nKeep in mind that some data are not available for every year\n\nFor example, data from the full decennial census are only available for 2010 or 2020.\n\n\nvariables = \"DP02_0065PE\"\n\nTelling the function to pull the variable coded \"DP02_0065PE\"\n\nWhich is the percentage of the population older than 25 with a Bachelor’s degree\n\nThis is the only tricky part of using tidycensus: understanding Census API’s variable names\n\nLet me breakdown what we are calling here:\n\nDP02_0065\n\nThis is the main variable code the census uses\nYou can find these by using the load_variables() command, but doing so creates a massive table in R that is hard to navigate through\nAn easier way is to go the census API’s list of variables for the dataset you are using, which for the 2021 ACS is here (change the years/data sources as needed for other surveys)\n\nIn here you can crtl-f or cmd-f search for the variable you are looking for\nFor this variable we could search “bachelor,” which will highlight all the variables that have “bachelor” in the title\nFind the variable you want and copy the name.\n\n\nPE\n\nYou will notice there are multiple DP02_0065 variables\n\nThese are the same underlying variable, but in different forms.\n\nThe common endings are E or PE\n\nWhich stand for Estimate and Percentage Estimate respectively\n\nFor today’s lesson, we want the percentage estimate PE\n\nIf you want the total count instead, select E\n\n\nso we will select DP02_0065PE,\n\nThis will give us\n\nDP02_0065PEthe percent estimate of Bachelor’s degree attainment for those 25 years old and above\nDP02_0065PM which is the margin of error for the percentage (hence the M at the end)\n\nWe don’t need this for our mapping, but it downloads automatically and is useful for some statistical calcualtions\n\n\n\n\n\n\noutput = \"wide\"\n\nTelling it we want the data in a wide format\nThink back to Data Wrangling II: wide data means having a separate column for each variable whereas long data would be in two columns, one with the variable name and one with the variable value\nFor ease of plotting/mapping the variables, we are going to want it in wide format\n\ngeometry = TRUE\n\nTelling the function we want to download geometry (a kind of spatial data) to go with our census data\nBy default, this is FALSE , which just downloads the Census data\nThis saves us having to deal with finding, loading, and joining a shapefile to make our map\n\n\n\nQuick Excercise\nUsing the API variable dictionary we used above, add another variable to your get_acs() using c()\n\nOkay, let see what the top of our new data looks like.\n\n## show header of census data\nhead(df_census)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -82.57599 ymin: 27.64324 xmax: -80.73292 ymax: 30.14312\nGeodetic CRS:  NAD83\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4\n                        geometry\n1 MULTIPOLYGON (((-81.65856 2...\n2 MULTIPOLYGON (((-82.57599 2...\n3 MULTIPOLYGON (((-81.95616 2...\n4 MULTIPOLYGON (((-81.6809 29...\n5 MULTIPOLYGON (((-82.1062 28...\n6 MULTIPOLYGON (((-82.31133 2...\n\n\n\nIt looks a bit different than a normal data frame\n\nFor now, let’s not worry too much about the first few lines which give a summary of the spatial aspects of the our downloaded data\nIf you look underneath those lines, from GEOID to DP02_0065PM, you’ll see something that looks more like the tibbles we are familiar with\nThen, in the last column, we get to our spatial data in the geometry column\nIf you open df_census in the viewer, it looks like a normal data frame ending with this slightly different column called geometry\n\nNote: I wouldn’t recommend often looking through spatial data in the file viewer as the the spatial data is often huge and can make it slow/laggy\n\nIf you need to dig into the data that way, use st_drop_geometry() to remove the spatial features, then either print or view it\n\n\n\n\n\n## view data frame without geometry data\ndf_census_view &lt;- df_census |&gt;\n  st_drop_geometry()\n\nhead(df_census_view)\n\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "href": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "A (Very) Brief Overview of Spatial Data",
    "text": "A (Very) Brief Overview of Spatial Data\n\nWe do not have time to really get into all the complexities of spatial data in this class, so, unfortunately, it will have to remain a bit of black box for now\n\nFor those interested in more depth, this is a nice intro to the types of spatial data\n\n\n\nVectors vs Rasters\n\nWhen looking online for spatial data, you might see how spatial data can be either in vector or raster format\n\nFor our purpose today (and most of the time outside certain use cases) everything is going to be vector\n\nSimilar to a vector in R (a column or list of data)\n\nJust a collection of data points that represent something\n\nOnly this time it’s representing something spatial\nThink of it like instructions to draw a shape\n\n\n\nRaster data, on the other hand, is a grid with information assigned to each square\n\nThink of it like a big paint-by-numbers grid and the shape you see is where some squares are filled in\nCommonly used for satellite imagery analysis\n\nDoesn’t scale well for mapping\n\n\n\n\n\n\nSpatial Data Vectors\n\nNo matter the format, working with spatial data involves two things\n\nThe data itself (the numbers in the geometry column)\nHow that data is projected (the CRS, which we will cover below)\n\nThe data in a spatial vector come in three primary formats\n\npoints (think dots on a map)\nlines (think a line connecting two points on a map)\npolygons (think a collection of lines on a map that create a closed shape)\n\nYou can also have multi-polygons or multi-lines, which are just multiple shapes that represent that observation\n\nUltimately though, it all comes back to point data, everything else is made up of those\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this lesson we are going to use both point and polygon data (you won’t see line data as often in the wild)\nIf this sounds complicated, fear not! It is much simpler than it sounds right now!\n\n\n\nCoordinate Reference Systems (CRS)\n\nFor purposes of this lesson, the only internal workings of spatial data we need to be aware of is something called the Coordinate Reference System or CRS\n\nOur earth is not flat, but rather is a curved three-dimensional object (Note from Ben: this is most likely true)\nSince we don’t want to carry around globes, we take this 3D object and squish it into two dimensions on a map\nThis process necessarily involves some kind of transformation, compromise, or projection\n\nIn a nutshell, this is a very simplified explanation of what a CRS decides\n\nit’s how we are deciding to twist, pull, squish a 3D Earth surface into a flat surface\nTurns out this matters a lot depending on what purpose your map is for\n\nDo you want your results to have the correct areas?\nOr maybe correct distances?\nOr maybe straight lines of bearing (particularly important if you are sailing and don’t want those trips to take any longer than necessary)?\n\nThere are, as you can see by this list, also a lot of localized CRS projections\n\nThe more local a projection, the less it has to twist, pull, stretch, or squish data\n\nThis means it can be more accurate, with less compromise, for that specific area\n\nBut if you use a localized projection for the wrong area, you’re going to be way off\nIf you’re doing spatial analysis (distance calculations etc.) it’s best to the use most local projection you can that is still valid for your whole study area\n\n\n\n\nHere’s a (somewhat old) pop culture look at this issue from one of my favorite shows…\n\n\n\nThis is a relatively complicated process we are not going to go into here. If you’re interested here’s a nice introduction to CRS by QGIS\nFor our class we are going to use the CRS EPSG 4326\n\nIn essence a projection that\n\nMakes east/west run straight left/right\nMakes north/south run straight up/down\nKeeps all latitude and longitude degrees equally spaced apart (much to the dismay of our friends from the OCSE)\n\nAll different CRS have their advantages and disadvantages\n\nThis is nice and simple for quick descriptive maps, but distorts shapes in ways that might be harmful, particularly if you are going to do any distance calculations, etc.\n\n\n\n\nNote from Ben: If you are going to do spatial work in education research (other than just making maps for display), you really need to know what your projection is doing. Even if you are just making maps for display, some projections are, IMNSHO, more aesthetically pleasing that others in different situations.\n\n\nKeep an eye out for crs = 4326 as we go through some examples plotting spatial data below.\n\n\n\nHow R Stores All This Spatial Data\n\nAs we saw above, there is a column on the end of our data called geometry\n\nThis is not technically a column like we are used to\n\nE.g., you can’t join(df_one, df_two, by = geometry) or filter(geometry == x) like we do with other variables in our data frames\n\nIf you want to spatially join or filter data, you have to use\n\nst_filter()\nst_join()\n\nUsing these is bit beyond where we will get in this class, but, note this for future work, it can be really useful\n\n\nInstead, think of it as a special attachment R places on each observation\n\n\n\n\nSummary of Spatial Data Basics\nIn short, what you need to know about spatial data for this lesson is this:\n\nR stores spatial data in something called geometry attached to each observation/row\n\nTo handle these spatial aspects of the data, you can’t just filter() it like normal; instead you have to use functions from a spatial data package such as sf or tigris\n\nThe CRS (coordinate reference system) is how we choose to account for the earth being curved\n\nCrucial for mapping is that everything we use on the plot is using the same CRS\nUsing crs = 4326 will give a nice simple flat projection\n\nThis projection has drawbacks, but is easy to work with and so is what we will use for now",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#lets-make-a-map-finally",
    "href": "10-viz-iii.html#lets-make-a-map-finally",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Let’s Make a Map (finally)!",
    "text": "Let’s Make a Map (finally)!\nIf we have made it this far, things are about to get much more interesting and hands-on!\n\nWe are going to make an education-focused map based on template I used for a real consulting project Summer 2022 as part of my GA-ship\nThis template is really adaptable for a lot the kind of maps we might want educational research and reports\nSo let’s get started!\n\nWe are going to have two layers\n\nA base map with the census data we already downloaded\nA layer of points on top representing colleges\n\n\n\n\nLayer One: Base Map\n\nBefore we plot anything, particularly since we are going to have multiple layers, we want to check our CRS\n\n\n## ---------------------------------------------------------\n##' [Making a map (finally)]\n## ---------------------------------------------------------\n## ---------------------------\n##' [Layer one: base map]\n## ---------------------------\n\n## show CRS for dataframe\nst_crs(df_census)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nThat isn’t our simple flat EPSG 4326, so we are going to st_transform() to set that.\n\n## transform the CRS to 4326\ndf_census &lt;- df_census |&gt;\n  st_transform(crs = 4326)\n\nThen we can check again…\n\n## show CRS again; notice how it changed from NAD93 to EPSG:4326\nst_crs(df_census) \n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nLooks good!\n\nOkay, with our CRS now set, let’s plot our base map.\n\nWe actually use the familiar ggplot() to make our maps because there is a special geom_* that works with spatial data\n\ngeom_sf()\nEverything works in a similar way to our normal plots, so this should be familiar. Luckily all the tricky spatial aspects are handled by ggplot for us.\n\n\nThe below code will make our base map, and store in an object called base_map\n\n\n## create base map\nbase_map &lt;- ggplot() +\n  geom_sf(data = df_census,\n          aes(fill = DP02_0065PE),\n          color = \"black\",\n          size = 0.1) +\n  labs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) +\n  scale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\") +\n  theme_minimal()\n\nLet’s go through each line of the geom_sf() as we did for get_acs() above:\n\ndata = df_census\n\nAll we need to do take make our spatial plot is call a data frame with a geometry attachment. geom_sf() will handle how to plot that for us.\n\naes(fill = DP02_0065PE)\n\nMuch like we would with a box plot, we are simply telling ggplot to fill the shapes (in our case, Florida’s counties) based on that variable\nSo here we are filling Florida’s counties based on the percent of the population over 25 with a Bachelor’s degree (the variable we chose from tidycensus)\n\ncolor = \"black\"\n\nRemember since this is outside the aes() argument it will applied consistenly across the plot. We are telling it to make all the lines black.\n\nsize = 0.1\n\nSimilarly, we are telling to make the lines 0.1 thickness (thinner than the default)\n\n\nThen we have added two visual alterations like we covered in the second plotting lesson. For a quick reminder:\n\nlabs(fill = str_wrap(\"Percent Population with Bachelor's\", 20))\n\nIs saying to give the legend for fill this title\n\nThe new function, str_wrap() says to make a newline (wrap) when there are more than 20 characters\n\n\nscale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\")\n\nIs telling fill with a color gradient starting at with light slate blue and finishing with a dark slate blue\n\nInstead of color names, we’re using hex color codes\n\n\nNow, let’s call our base_map object to see what this looks like\n\n\n## call base map by itself\nbase_map\n\n\n\n\n\n\n\n\nWoohoo! We have made a map!\n\nData Viz II Throwback\n\nRemember in the second lesson on data visualization we spent the entire time customizing the appearance of a single plot?\ngeom_sf() is just another type of ggplot, so, we can use all the same things we learned\n\n\nQuick Exercise\n\nPlay around with the scale_fill_gradient to use your favorite color\nPlay around with the theme argument, can you find the theme that gets rid of all grid lines?\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to make it more interesting with one more layer…\n\n\n\nLayer Two: Institution Points\n\nA lot of education data comes with a latitude and longitude for the institution\nToday we are going to use IPEDS, but you can certainly get these for K-12 schools and a whole lot more besides\n\nCaution: I have noticed in my own work that IPEDS coordinates are not always the most consistent (an issue we have seen with other IPEDS data through the course)\n\nIf you’re wanting to do something that where precision really matters the Department of Homeland Security has open data that, in my experience, is more consistent with Google Maps locations\n\nIt also provides polygons of larger college campuses rather than just coordinate points, which might be useful for some of your research (I’m using them at the moment)\n\n\n\nFor today, we are going to read in some library data from IPEDS that I cleaned and merged earlier\n\nThis is a combination of HD and AL data files for 2021\n\n\n\n## ---------------------------\n##' [Layer Two: Institutions]\n## ---------------------------\n\n## read in IPEDS data\ndf_ipeds &lt;- read_csv(\"data/mapping-api-data.csv\")\n\nLet’s take a look at our data\n\n## show IPEDS data\nhead(df_ipeds)\n\n# A tibble: 6 × 78\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LATITUDE LONGITUD\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089     34.8    -86.6\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073     33.5    -86.8\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101     32.4    -86.2\n4 100706 Unive…       1       1 AL         1 Madison…     1089     34.7    -86.6\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101     32.4    -86.3\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125     33.2    -87.5\n# ℹ 68 more variables: LEXP100K &lt;dbl&gt;, LCOLELYN &lt;dbl&gt;, XLPBOOKS &lt;chr&gt;,\n#   LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;, LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;,\n#   LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;, LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;,\n#   LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;, LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;,\n#   LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;, LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;,\n#   LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;, LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;,\n#   LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;, LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, …\n\n\n\nWe see a normal data frame for colleges with bunch of variables\n\nReminder: You can use IPEDS dictionaries to unpack unknown variable names\n\nMost importantly for right now, we see latitude and longitude\n\nLatitude and longitude represent something spatial, but they’re not quite spatial data like R knows\nLet’s change that!\n\n\n\n## convert coordinates columns into a true geometry column; this is\n## much more reliable than simply plotting them as geom_points as it\n## ensures the CRS matches etc.\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_as_sf(coords = c(\"LONGITUD\", \"LATITUDE\"))\n\n\nAbove we call st_as_sf()\n\nthen tell it the coordinates, coords =, are in columns name LONGITUD and LATITUDE\nThe order these go is very important (and changes between some systems) but for sf it’s longitude then latitude as we follow the usual x, y order\n\nLongitude tells you were you are east/west on the globe, it translates to the x axis\nLatitude gives you north/south direction, it translates to the y axis\n\n\nIf we look at our data again, we are going to see that spatial summary again as R has attached some point geometry to our college data based on the coordinates\n\n\n## show IPEDS data again\nhead(df_ipeds)\n\nSimple feature collection with 6 features and 76 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -87.54598 ymin: 32.36261 xmax: -86.17401 ymax: 34.78337\nCRS:           NA\n# A tibble: 6 × 77\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LEXP100K LCOLELYN\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089        1        2\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073        1        2\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101        1        2\n4 100706 Unive…       1       1 AL         1 Madison…     1089        1        2\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101        1        2\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125        1        2\n# ℹ 67 more variables: XLPBOOKS &lt;chr&gt;, LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;,\n#   LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;, LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;,\n#   LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;, LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;,\n#   LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;, LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;,\n#   LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;, LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;,\n#   LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;, LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;,\n#   LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, LTCRCLT &lt;dbl&gt;, LILLDYN &lt;dbl&gt;, …\n\n\n\nQuick Question\nSomething’s not quite right, can you tell me what?\nHint: It’s something we just talked about being very important\n\n\nThis means R will not be able to turn that spatial data into a map\n\nBasically, R knows we have spatial data, but it doesn’t know how we want to put it onto a 2D surface (how to project it)\n\nTo be sure, let’s check the it directly\n\n\n## check CRS for IPEDS data\nst_crs(df_ipeds)\n\nCoordinate Reference System: NA\n\n\nYep, NA… Luckily the fix for this is simple\n\n## add CRS to our IPEDS data\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_set_crs(4326) # When you first add coordinates to geometry, it doesn't know\n                   # what CRS to use, so we set to 4326 to match our base map data\n\nOkay, let’s have another look…\n\n## check CRS of IPEDS data again\nst_crs(df_ipeds)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAnd we see we have our nice CRS back!\n\nOkay, now the hard work is done, we just need to call our base_map, add a layer representing the colleges as points, and store it into a new object point_map:\n\n\npoint_map &lt;- base_map +\n  geom_sf(data = df_ipeds |&gt; filter(FIPS == 12), # Only want to plot colleges in FL\n          aes(size = LPBOOKS),\n          alpha = 0.8,\n          shape = 23, # Get the diamond shape which stands out nicely on the map\n          fill = \"white\", # This shape has a fill and color for the outline\n          color = \"black\") + # FYI 21 is a circle with both fill and color\n  labs(size = \"Number of Books in Library\")\n\nAs we have done all lesson, we can take a quick look through our second geom_sf() function line by line:\n\ndata = df_ipeds |&gt; filter(FIPS == 12):\n\nFor this layer we are using our df_ipeds data, which covers the country, but since our base map is Florida, we only want colleges located in the Sunshine State (which is FIPS code 12).\n\naes(size = LPBOOKS)\n\nIs saying we want to change the size of point based on LPBOOKS, which is the total number of books in the college’s library collection. More books, bigger point!\n\nshape = 23\n\nThis is purely aesthetic, instead of usual circles, this is a diamond shape\n\nfill = \"white\" and color = \"black\"\n\nChange the inside of the diamonds white and the lines around the edge to black\n\nNote, this shape (another others) have an inside to fill and a border to color, the basic point shape only has color\n\n\nalpha = 0.5\n\nIs outside the aes() so we are making it all 50% transparent.\n\nlabs(size = \"Number of Books in Library\")\n\nTo change the legend title to “Number of Books in Library”\n\n\nPhew! Last thing, let’s call our new point_map object and take a look at what we created!\n\n## show new map\npoint_map\n\n\n\n\n\n\n\n\n\nThere we go!\nWe now have a map that shows us county bachelor’s degree attainment and the number of books in a college’s library.\n\nIf you notice, UF has most books out of all Florida colleges, Go Gators!\n\n\n\nQuick Exercise\n\nPlay around the with shape argument and see what other options there are\nTime permitting: See if you can get rid of the scientific numbering (e.g., 1e+06) so it looks the plot below\n\nThere’s a few ways, Google and StackOverflow have some helpful answers\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously, this may not be the most useful map in the world, but the template is very adaptable to a lot of educational situations\n\nUsing tidycensus we can swap out the base map geography to have most common US geographies and/or swap out any variable available in from the Census Bureau\n\nYou will also see below a way of getting shapes without Census data using tigris\nSee the example on school districts below\n\nEqually, we can swap out the point data to represent anything we have coordinate points for and change the aesthetics to represent any data we have for those points",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "href": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Supplemental Material: US transformations & tigris basics",
    "text": "Supplemental Material: US transformations & tigris basics\n\nFor the sake of time, I left this until the end as we don’t need it for the assignment. But it may be useful if you are looking to make any maps in your final assignment or the future.\n\ntigris is a package that offers a direct way of downloading US spatial data that is not tied to census data.\n\nNote: it’s actually used by tidycensus behind the scenes to get your spatial data\nIf you get spatial data from tigris it won’t come with any additional data to plot per say, but it comes with identifying variables you could use to pair up with external data using something like left_join()\n\n\nMapping School Districts\n\nIf we wanted to plot Census data about the population in these school districts we could get these from tidycensus with geography = \"school district (unified)\"\nIf instead we have school district data we want to plot, or we are only interested in the boundary, we might want to download the spatial data for school districts directly\n\nIn that case, it might be easier to use tigris directly to get the blank shapefiles.\nThe function names for tigris are really simple.\n\nschool_districts() for example retrieves a shapefile for US school districts\n\n\n\n\nggplot() +\n  geom_sf(data = df_school_dist_tx,\n          aes())\n\n\n\n\n\n\n\n\n\nYou’ll notice we actually left aes() blank, as geom_sf() will automatically make the spatial elements for us and right now we have no additional info to add to plot elements like fill\nThere is one new argument in there\n\ncb = TRUE\n\nStands for cartographic boundary, basically, a less detailed line that makes for faster and easier mapping\n\nIf the border detail was really important to your analysis, you might want to set to false, but for mapping it’s usually the best option",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#states-and-crs-transformations",
    "href": "10-viz-iii.html#states-and-crs-transformations",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "50 States and CRS Transformations",
    "text": "50 States and CRS Transformations\n\nFinally, we are going to look at how CRS projections changes national maps\n\nFirst, we need to download some basic spatial data for the 50 states\n\nLike I said before, tigris function names are really simple\n\nstates() downloads spatial data for the states\n\nSimilarly to cb = TRUE we discussed above resolution = \"20m\" sacrifices a little detail for speed and efficiency for mapping\n\n\n\n\n\nLike we did before, let’s take a peak at our newly downloaded data.\n\n## look at head of state data\nhead(df_st)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 24.49813 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n  STATEFP  STATENS    AFFGEOID GEOID STUSPS      NAME LSAD        ALAND\n1      22 01629543 0400000US22    22     LA Louisiana   00 1.119153e+11\n2      02 01785533 0400000US02    02     AK    Alaska   00 1.478943e+12\n3      24 01714934 0400000US24    24     MD  Maryland   00 2.515199e+10\n4      55 01779806 0400000US55    55     WI Wisconsin   00 1.402923e+11\n5      12 00294478 0400000US12    12     FL   Florida   00 1.389617e+11\n6      13 01705317 0400000US13    13     GA   Georgia   00 1.494866e+11\n        AWATER                       geometry\n1  23736382213 MULTIPOLYGON (((-94.04305 3...\n2 245378425142 MULTIPOLYGON (((179.4813 51...\n3   6979074857 MULTIPOLYGON (((-76.04621 3...\n4  29343646672 MULTIPOLYGON (((-86.93428 4...\n5  45972570361 MULTIPOLYGON (((-81.81169 2...\n6   4418360134 MULTIPOLYGON (((-85.60516 3...\n\n\n\nSimilar to before, we have\n\nA spatial summary at the top\nA set of normal looking columns with different ID codes and names\nAn attached geometry for each row\n\nIf we simply plot this with no aesthetics, we get the outline of all states, but there is something about it that makes it less ideal for a quick data visualization…\n\n\n## quick plot of states\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\n\nAs we can see, while the map is geographically accurate, there is a lot of open ocean on the map due to the geographic structure of the US\n\nThe tail of Alaska crosses the 180 degree longitude line, so it wraps to the other side of the map\n\nOften when we see maps of the US, such as on election night, Alaska and Hawaii are moved to make it easier to read\n\nTigris offers an easy way of doing this\n\nshift_geometry()\n\nshould work on any spatial data with Alaska, Hawaii, and Puerto Rico\n\n\n\n\n\n## replotting with shifted Hawaii and Alaska\nggplot() +\n  geom_sf(data = shift_geometry(df_st),\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\n\nAlthough not always a good idea, if you’re looking to plot the 50 states in an easy-to-read manner, this can be a really useful tool\n\nNote: it does not move other U.S. territories like Guam by default, so you may have colleges and schools out there you need to deal with\nNote: this can re-project your data, so you need to make sure your CRS is what you want after this (as we will do in our final examples)\n\nNever do spatial analysis on data you’ve done this to, it will be severely off\n\nI recommend only ever using it inside geom_sf()\n\nThis way you never change your data\n\nYou don’t want to accidentally say Hawaii is closer to Austin than Oklahoma City is…\n\n\n\nFinally, to re-illustrate what a CRS does, let’s plot this two more times\n\nFirst, putting it onto our simple EPSG 4326 CRS from earlier\nThen, using the Peters Projection referenced in the video clip at the start of class\n\n\n\n## change CRS to what we used for earlier map\nggplot() +\n  geom_sf(data = shift_geometry(df_st) |&gt; st_transform(4326),\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\n\nSee how the line are now a perfect grid, but the shapes of states (look at Montana) are a little different?\n\nThat’s the power of a CRS!\n\n\n\nQuick Exercise\n\nPick a different projection from the list of EPSG projections and see what changes\n\nPerhaps try a projection for a different part of the world\n\nBelow I tried EPSG 6684 for Tokyo, Japan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s please the Organization of Cartographers for Social Equality and look at the Peters projection\n\nNote: while this projection is great for showing comparably accurate area across the globe, it does that by other trade offs not acknowledged by Dr. Fallow from OCSE\nNo projection is universally better, each is better for the task it was designed for\n\nThat’s the key with CRS, find the best one for the task you’re doing.\n\n\n\n\n## change CRS to requirements for Peters projection\n## h/t https://gis.stackexchange.com/questions/194295/getting-borders-as-svg-using-peters-projection\npp_crs &lt;- \"+proj=cea +lon_0=0 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n\nggplot() +\n  geom_sf(data = shift_geometry(df_st) |&gt; st_transform(pp_crs),\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\n\nSee how to the gap between 45 and 50 degrees north is much smaller than between 20 and 25 degrees north?\n\nThat’s the projection at work\n\nThink about how this reflects how the globe is shaped and how that affects area (the focus of the Peters projection map)",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#solution",
    "href": "10-viz-iii.html#solution",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Viz III Solution]\n##' [INIT: 02 April 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Step one: pivot spending types to long\ndata_census &lt;- get_acs(geography = \"school district (unified)\",\n                       state = \"TX\",\n                       year = 2021,\n                       variables = \"DP02_0153PE\", # % Households with a computer\n                       output = \"wide\",\n                       geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n## Step two: Change CRS of data_census\ndata_census &lt;- data_census |&gt;\n  st_transform(crs = 4326)\n\n## Step three: Plot the base map\nplot_base_map &lt;- ggplot() +\n  geom_sf(data = data_census,\n          aes(fill = DP02_0153PE),\n          color = \"black\",\n          size = 0.1) +\n  labs(fill = str_wrap(\"% Homes with a Computer\", 20)) +\n  scale_fill_distiller(palette = \"Oranges\") +\n  theme_void()\n\n## Step three: Load IPEDS and transform coordinates using st_as_sf()\ndata_ipeds &lt;- read_csv(\"data/mapping-api-data.csv\") |&gt;\n  st_as_sf(coords = c(\"LONGITUD\", \"LATITUDE\")) |&gt;\n  st_set_crs(4326)\n\n## Step four: Add the points to plot_base_map\nplot_base_map +\n  geom_sf(data = data_ipeds |&gt; filter(STABBR == \"TX\",\n                                      LCOLELYN == 1), # Plot only colleges with purely digital libraries\n          aes(), # No aes() needed as we don't need anything to change with data, we are only plotting schools we are interested in\n          size = 9,\n          shape = 4,\n          stroke = 1.5,\n          alpha = 0.8) +\n  labs(title = str_wrap(\"Colleges with Purely-Digitial Libraries vs Household Computer Access\", 40)) +\n  theme_void()\n\n\n\n\n\n\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "12-pro-model.html#data-preparation",
    "href": "12-pro-model.html#data-preparation",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nBefore we can run any kind of models, we need to make sure our data is prepared\nThis involves using skills from our data wrangling lessons such as\n\nData Wrangling I\n\nHandling missing data\nMaking sure our data is the right format (numeric, factor, character, etc.)\nPerforming basic calculations (e.g., percentages, differences, etc.)\n\nData Wrangling II\n\nJoining multiple data sets together\nPivoting data wider and/or longer\n\nData Wrangling III\n\nCleaning up text data\nTransforming dates into\n\nData Wrangling IV\n\nPerforming any of the above tasks across() multiple columns\ncoalesce()-ing multiple columns into one variable\n\n\nFor the purpose of today’s lesson, we are going to focus on two of these tasks, dealing with missing data, and making sure our data is in the right format\n\n\nHandling Missing Data\n\nWhen modeling, by default, R will simply drop any rows that have an NA in any variable you are modeling on (this is a little different to the cautious R we ran into in Data Wrangling I)\n\nIn real world applications, you need to think carefully about how you handle these…\n\nShould I impute the missing data? If so, using what method?\nShould I use this variable at all if it’s missing for a bunch of observations?\n\nFor this lesson, however, we are just going to drop NA values so we can focus on the main content\n\nThe below code uses the combines the logic we use for making NAs in Data Wrangling I with the ability to work across multiple columns in Data Wrangling IV\n\nFirst, we read our data and select() the columns we want to use\n\n\n\ndata &lt;- read_csv(\"data/hsls-small.csv\") |&gt;\n      select(stu_id, x1sex, x1race, x1txmtscor, x1paredu, x1ses, x1poverty185, x1paredexpct)\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n-   Second we use a combination of `!,` `filter(),` and `if_any()` to say, to say\n    -   \"If a row...\"\n        -   \"has a -8 or -9\"\n            -   `.fns = ~ . %in% c(-8, -9)`\n        -   \"in any columns\"\n            -   `.cols = everything()`\n        -   \"do NOT keep it\"\n            -   `filter(!)`\n\ndata &lt;- data |&gt;\n      filter(! if_any(.cols = everything(),\n                      .fns = ~ . %in% c(-8, -9)))\n\n\n\nMaking Sure Our Data is the Right Format\n\nIn our Data Viz I and Data Viz II lessons, we saw that for R to accurately plot categorical variables, we had to convert them into factor()s\n\nThe same is true for using categorical variables in models\n\nThose more familiar with stats may know that you have to “dummy code” categorical variables as 0 and 1 with one category serving as the “reference level” and all other categories getting their own binary variable\n\nThe wonderful thing is that R handles that all for us if we tell it to treat the variable as a factor()\n\nThe below code combines the logic of turning variables into a factor() from Data Viz I with working across multiple columns for Data Wrangling IV to sat\n\n“Modify”\n\nmutate()\n\n“Each of these columns”\n\nacross(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185)\n\n“Into a factor”\n\n.fns = ~ factor(.)\n\n\n\n\ndata &lt;- data |&gt;\n  mutate(across(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185),\n                .fns = ~ factor(.)))\n\n\nWith that, our data is ready for some basic analysis!\n\nNote: In most real-world projects your data preparation will be much more thorough, usually taking up the vast majority of the lines of code in your entire project, this is just the bare minimum to have to models run",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "12-pro-model.html#t-tests-with-t.test",
    "href": "12-pro-model.html#t-tests-with-t.test",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "t-tests with t.test()",
    "text": "t-tests with t.test()\n\nOne of the first inferential statistical tests you will have learned (or will learn) is the t-test\n\nFor those unfamiliar, the basic concept of a t-test if variance between two groups (i.e., the difference between treatment and control) is greater than the variance within those groups (i.e., random variance between people within the same group)\n\nIf that between-group-variance is great enough compared to the within-group-variance, the t-test will be “statistically significant”\n\nThis means we are (most often) 95% confident that the there is a genuine difference between the groups\n\n\nThere are also a handful of statistical assumptions we have to satisfy, which are beyond our scope here, but hopefully the general concept will hope those of you yet to take your stats foundations follow along\n\n\n\nt.test(x1txmtscor ~ x1sex, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  x1txmtscor by x1sex\nt = 0.38555, df = 16321, p-value = 0.6998\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.2455338  0.3657777\nsample estimates:\nmean in group 1 mean in group 2 \n       52.14309        52.08297 \n\n\n\nLuckily, the code for t.test() is actually very simple (as is the case for regression too)\n\nThe first argument is a forumla, which for a t-test is just outcome ~ group where group must only have 2 levels\n\nIn this case, we are looking at math score as our outcome and sex as our group\n\nThe second argument is data = which we supply our prepared data frame\n\nNote: the pipe |&gt; doesn’t play as nicely with models as it does other commands it’s usually easier to just specify data = in a new line (don’t pipe anything in)\n\nThis code simply prints out our t.test() result\n\nAs our p-value is above 0.05, our result is not significant\n\nThis indicates there is not a significant difference between male and female math scores in our sample",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "12-pro-model.html#regression-with-lm",
    "href": "12-pro-model.html#regression-with-lm",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "Regression with lm()",
    "text": "Regression with lm()\n\nThe problem with t-tests for our research, is that they don’t provide any ability to control for external variables\n\nThey work great in experimental setting with random-treatment-assignment, but in the messy world of educational research, that’s rarely what we have\n\nWhat we far more commonly use is a regression (or more advanced methods that build off regression) which allows use to control for other variables\nThe basic premise of regression very much builds off the logic of t-tests, testing if the variance associated with our treatment variable is great enough compared to a) residual/random variance and b) variance associated with our control variables, to say with confidence that there is a significant difference associated with our treatment\nOverall, this looks relatively similar to our code above, with three main differences\n\nWe use lm() (which stands for linear model) instead of t.test()\nInstead of our formula just being x1txmtscor ~ x1sex we have added + x1poverty185 + x1paredu to “control” for these variables\nWe assigned &lt;- our lm() results to an object rather than just spitting them out\n\n\nThat’s because the summary() function is much more useful for lm() objects, plus, we are going to explore the lm() object more in the next steps\n\n\n\nregression &lt;- lm(x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\nsummary(regression)\n\n\nCall:\nlm(formula = x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.748  -5.715   0.160   6.136  30.413 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    49.0485     0.3379 145.139  &lt; 2e-16 ***\nx1sex2         -0.0412     0.1429  -0.288    0.773    \nx1poverty1851  -3.0224     0.1729 -17.483  &lt; 2e-16 ***\nx1paredu2       1.3419     0.3234   4.149 3.36e-05 ***\nx1paredu3       2.4834     0.3584   6.929 4.38e-12 ***\nx1paredu4       6.1227     0.3507  17.460  &lt; 2e-16 ***\nx1paredu5       8.1726     0.3814  21.426  &lt; 2e-16 ***\nx1paredu7      10.7669     0.4294  25.074  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.161 on 16421 degrees of freedom\nMultiple R-squared:  0.1605,    Adjusted R-squared:  0.1601 \nF-statistic: 448.4 on 7 and 16421 DF,  p-value: &lt; 2.2e-16\n\n\n\nOur results show that, sex still had no significant association with math scores, but, our control variables of poverty and parental education seem to have some very strong associations\n\n\nQuick Question\n\nYou may notice we actually have more variables in the regression table than we put in, why? What do they represent?",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "12-pro-model.html#creating-pretty-regression-output-tables",
    "href": "12-pro-model.html#creating-pretty-regression-output-tables",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "Creating Pretty Regression Output Tables",
    "text": "Creating Pretty Regression Output Tables\n\nRunning regressions in R is all well and good, but the output you see here isn’t exactly “publication ready”\nWe saw in Intro to Quarto lesson that we can turn data frames and summary tables into pretty tables using kable()\n\nNow, we could do the same thing with regression tables, create a nice tibble or dataframe containing the information we want to display, then use kable() to turn that into a table\n\nYou could (and I know Dr. Skinner does) create your own function() that creates a custom regression output table\n\nHowever, for simplicity and consistency, today we are going to us a very popular package that creates these tables in a single line stargazer\n\n\n\nstargazer(regression, type = \"text\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            x1txmtscor         \n-----------------------------------------------\nx1sex2                        -0.041           \n                              (0.143)          \n                                               \nx1poverty1851                -3.022***         \n                              (0.173)          \n                                               \nx1paredu2                    1.342***          \n                              (0.323)          \n                                               \nx1paredu3                    2.483***          \n                              (0.358)          \n                                               \nx1paredu4                    6.123***          \n                              (0.351)          \n                                               \nx1paredu5                    8.173***          \n                              (0.381)          \n                                               \nx1paredu7                    10.767***         \n                              (0.429)          \n                                               \nConstant                     49.049***         \n                              (0.338)          \n                                               \n-----------------------------------------------\nObservations                  16,429           \nR2                             0.160           \nAdjusted R2                    0.160           \nResidual Std. Error     9.161 (df = 16421)     \nF Statistic         448.368*** (df = 7; 16421) \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\nPredictions with lm()\n\nWhen you fit a regression model in R, there is a lot more saved than you see with summary()\nSince we have our lm() object saved as lm_sex, let’s start by taking a look inside it by clicking on the object in our environment (top right) panel\nConfusing, right?\n\nMost statistical models look something like this, it’s basically a collection of lists and tables containing different information about the model\n\nThere are functions such as summary() that are great at pulling out the most commonly needed information without having to go manually digging through the model object, but sometimes, it can be useful to know it’s there\nAnother great function is predict() which extracts estimated values of the outcome variable based on the predictor variables (some other models use fitted() for the same purpose)\n\nFor those more familiar with stats, you’ll know predicted values are often compared against the true values to see how strong the model is\n\n\n\n\nTo start, let’s save a full set of predictions to a new columns in our data frame\n\n\ndata &lt;- data |&gt;\n  mutate(prediction = predict(regression))\n\ndata\n\n# A tibble: 16,429 × 9\n   stu_id x1sex x1race x1txmtscor x1paredu   x1ses x1poverty185 x1paredexpct\n   &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1 10001  1     8            59.4 5         1.56   0                       6\n 2 10002  2     8            47.7 3        -0.370  1                       6\n 3 10003  2     3            64.2 7         1.27   0                      10\n 4 10004  2     8            49.3 4         0.550  0                      10\n 5 10005  1     8            62.6 4         0.150  0                      10\n 6 10006  2     8            58.1 3         1.06   0                       8\n 7 10007  2     8            49.5 2        -0.43   0                      11\n 8 10008  1     8            54.6 7         1.51   0                       6\n 9 10009  1     8            53.2 2        -0.310  0                      11\n10 10010  2     8            63.8 3         0.0451 0                       6\n# ℹ 16,419 more rows\n# ℹ 1 more variable: prediction &lt;dbl&gt;\n\n\n\nNext, we can compare these to our actual results using a simple plot (no formatting) from Data Viz I\n\nThe only new thing we add here is coord_obs_pred() which is from the tidymodels package\n\nThis fixes the axes so that the predictions and observed values are plotted on the same scale\n\n\n\n\nQuick Excercise\n\nTry removing the final line coord_obs_pred() and see what happens. Which plot do you think is better?\n\n\n\nggplot(data,\n       aes(x = prediction,\n           y = x1txmtscor)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_obs_pred()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n(Easier) Quick Question\n\nWhat do we think about our model? Does it look like it’s doing a great job of predicting? Why/why not?\n\n\n\n(Harder) Quick Question\n\nYou’ll notice our plot looks kind of clumped together, why do you think that it? What about the model would lead to that?\n\n\n\n(Not So) Quick (Group) Excercise\n\nGiven what we just discussed, can we change one of the variables we are using in the model to make it less “clumpy” but caputre the same information?\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nQuick Question\n\nDoes that look better? What else is odd about our predictions?\n\n\n\nWe can also use predict() to estimate potential outcome values for new students who don’t have the outcome for\nThis is a common way you evaluate machine learning models\nIf you think you’re model is a really good predictor (which ours is not) you may feel comfortable using something like this to help your office predict student outcomes/identify students in need of additional help\n\n\n\nTo demonstrate this, we are first going to split out 10% of our data using slice_sample() and drop the math score from it\n\n\ndata_outcome_unknown &lt;- data |&gt;\n  slice_sample(prop = 0.1) |&gt;\n  select(-x1txmtscor)\n\n\nThen, we can use anti_join() which is basically the opposite of the joins we used in Data Wrangling II\n\nIt looks for every row in x that isn’t in y and keeps those\n\n\n\ndata_outcome_known &lt;-  anti_join(x = data, y = data_outcome_unknown, by = \"stu_id\")\n\n\nNow, we can fit one more lm() using our data we “know” the outcome for\n\n\nregression_3 &lt;- lm(x1txmtscor ~ x1sex + x1ses + x1paredu, data = data_outcome_known)\n\n\nFinally, we can predict() outcomes for the data we “don’t know” the outcome for\n\nWe add the regression_3 we just fitted as the model, same as before\nBut we also add newdata = data_outcome_unknown to say predict the outcome for this new data, instead of extract the predictions the model originally made\n\n\n\ndata_outcome_unknown &lt;- data_outcome_unknown |&gt;\n  mutate(prediction_3 = predict(regression_3, newdata = data_outcome_unknown))\n\n\nLastly, let’s see how similar our predictions we made using our model without the outcome were to those made when the outcome was known for everyone using cor() to get the correlation\n\n\ncor(data_outcome_unknown$prediction_2, data_outcome_unknown$prediction_3)\n\n[1] 0.9999769\n\n\n\nPretty close!\n\n\n\nChecking Residuals\n\nMany of the assumptions relating to regression are tested by looking at the residuals\n\nWe aren’t going to go over those assumptions, again, this is not a stats class\nBut it might be useful to see how to get them out of a model object\nLet’s start by viewing the lm object again (environment, top right panel), then clicking on the little white box on the right hand side of the screen for the row “residuals”\n\nThat is a magic tip, if you ever want to get something specific out of a model object, often they’ll be something you can click on to generate the code needed to access it in the console\n\nFor residuals, it is regression_2[[\"residuals\"]]\n\n\n\ndata &lt;- data |&gt;\n  mutate(residual = regression_2[[\"residuals\"]])\n\n\nNow, again, not to get too deep into assumptions, but one of the key things to check is that your residuals have a normal distribution\n\nSo let’s revisit some Data Visualization I content and make a simple ggplot() histogram to of them\n\n\n\nggplot(data) +\n  geom_histogram(aes(x = residual),\n                 color = \"black\",\n                 fill = \"skyblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nWow, that is almost a perfect normal distribution!\n\nBonus points: can anyone remember/think of something about the variable x1txmtscor that made this result quite likely? Think about what kind of score it is",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "12-pro-model.html#formula-objects",
    "href": "12-pro-model.html#formula-objects",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "formula() Objects",
    "text": "formula() Objects\n\nThe second from last thing is really simple, but, it can be a time & error saver if you want to get more advanced like our final step\n\nAbove, we simply put our formula into the t.test() or lm() command\n\nInstead, we can actually specify it as a formula object first, then call that object, which has two advantages\n\nIf we run multiple tests with the same formula, we only have to change it once in our code for updates\n\n\nHere, we will run both standard lm() and lm_robust() from the estimatr package\n\n\nIf we want to run multiple tests in a loop like below, it makes that possible too\n\n\n\n\n\n\nTo demonstrate this, we will fit the same model using standard lm() and lm_robust() which for those versed in stats, is one option we can use when we have a violation of heteroskedasticity\n\n\nregression_formula &lt;- formula(x1txmtscor ~ x1sex + x1ses + x1paredu)\n\nregression_4 &lt;- lm(regression_formula, data = data)\nsummary(regression_4)\n\n\nCall:\nlm(formula = regression_formula, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.781  -5.703   0.180   6.129  30.996 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.82144    0.35585 142.819  &lt; 2e-16 ***\nx1sex2      -0.03767    0.14199  -0.265    0.791    \nx1ses        3.77674    0.16341  23.112  &lt; 2e-16 ***\nx1paredu2   -0.04729    0.33298  -0.142    0.887    \nx1paredu3   -0.15566    0.39027  -0.399    0.690    \nx1paredu4    2.04812    0.42368   4.834 1.35e-06 ***\nx1paredu5    2.57885    0.49454   5.215 1.86e-07 ***\nx1paredu7    2.67655    0.60923   4.393 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.099 on 16421 degrees of freedom\nMultiple R-squared:  0.1718,    Adjusted R-squared:  0.1714 \nF-statistic: 486.5 on 7 and 16421 DF,  p-value: &lt; 2.2e-16\n\nregression_robust &lt;- lm_robust(regression_formula, data = data, se_type = \"stata\")\nsummary(regression_robust)\n\n\nCall:\nlm_robust(formula = regression_formula, data = data, se_type = \"stata\")\n\nStandard error type:  HC1 \n\nCoefficients:\n            Estimate Std. Error  t value   Pr(&gt;|t|) CI Lower CI Upper    DF\n(Intercept) 50.82144     0.3443 147.6058  0.000e+00  50.1466  51.4963 16421\nx1sex2      -0.03767     0.1420  -0.2653  7.908e-01  -0.3160   0.2407 16421\nx1ses        3.77674     0.1651  22.8741 5.068e-114   3.4531   4.1004 16421\nx1paredu2   -0.04729     0.3196  -0.1479  8.824e-01  -0.6738   0.5792 16421\nx1paredu3   -0.15566     0.3767  -0.4132  6.795e-01  -0.8941   0.5828 16421\nx1paredu4    2.04812     0.4167   4.9150  8.963e-07   1.2313   2.8649 16421\nx1paredu5    2.57885     0.4891   5.2725  1.363e-07   1.6201   3.5376 16421\nx1paredu7    2.67655     0.6206   4.3130  1.620e-05   1.4601   3.8929 16421\n\nMultiple R-squared:  0.1718 ,   Adjusted R-squared:  0.1714 \nF-statistic: 490.1 on 7 and 16421 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "12-pro-model.html#modeling-programatically-with-loops",
    "href": "12-pro-model.html#modeling-programatically-with-loops",
    "title": "Bringing It All Together feat. Basic Models",
    "section": "Modeling Programatically with Loops",
    "text": "Modeling Programatically with Loops\n\nFinally, we can also bring in content from Functions & Loops and fit regression models using loops\nThis is kind of thing you might want to do if you are testing the same model on a set of outcomes\n\n\nQuick Question\n\nThinking back to that lesson, why might we want to go through the hassle of fitting regressions using loops? What are the advantages of using loops vs coding it all out separately?\n\n\n\nFor example, we might be interested in modeling both a students math score and their parental education expectation\n\nwe make a list containing our outcome variables (x1txmtscor and x1paredexpct)\nUse a for() loop to loop through these outcomes, which paste()s i (which takes on the name of each outcome variable) into the formula and then runs the model\n\n\n\noutcomes &lt;- c(\"x1txmtscor\", \"x1paredexpct\")\n\nfor(i in outcomes) {\n  \n  print(i)\n  \n  loop_formula &lt;- formula(paste0(i, \"~ x1sex + x1ses + x1paredu\"))\n  \n  loop_lm &lt;- lm(loop_formula, data = data)\n  \n  print(summary(loop_lm))\n  \n}\n\n[1] \"x1txmtscor\"\n\nCall:\nlm(formula = loop_formula, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.781  -5.703   0.180   6.129  30.996 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.82144    0.35585 142.819  &lt; 2e-16 ***\nx1sex2      -0.03767    0.14199  -0.265    0.791    \nx1ses        3.77674    0.16341  23.112  &lt; 2e-16 ***\nx1paredu2   -0.04729    0.33298  -0.142    0.887    \nx1paredu3   -0.15566    0.39027  -0.399    0.690    \nx1paredu4    2.04812    0.42368   4.834 1.35e-06 ***\nx1paredu5    2.57885    0.49454   5.215 1.86e-07 ***\nx1paredu7    2.67655    0.60923   4.393 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.099 on 16421 degrees of freedom\nMultiple R-squared:  0.1718,    Adjusted R-squared:  0.1714 \nF-statistic: 486.5 on 7 and 16421 DF,  p-value: &lt; 2.2e-16\n\n[1] \"x1paredexpct\"\n\nCall:\nlm(formula = loop_formula, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5279 -1.6021 -0.0548  2.2533  4.6503 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.17797    0.10020  71.638  &lt; 2e-16 ***\nx1sex2       0.47245    0.03998  11.817  &lt; 2e-16 ***\nx1ses        0.28497    0.04601   6.193 6.03e-10 ***\nx1paredu2   -0.39488    0.09376  -4.212 2.55e-05 ***\nx1paredu3   -0.28136    0.10989  -2.560   0.0105 *  \nx1paredu4   -0.07679    0.11930  -0.644   0.5198    \nx1paredu5    0.29117    0.13925   2.091   0.0365 *  \nx1paredu7    0.85235    0.17155   4.969 6.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.562 on 16421 degrees of freedom\nMultiple R-squared:  0.04934,   Adjusted R-squared:  0.04894 \nF-statistic: 121.8 on 7 and 16421 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Programming",
      "Bringing It All Together feat. Basic Models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Student Folder to Clone/Fork\nStudent Folder to Download as .zip\nR-Script Template",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#class-overview",
    "href": "index.html#class-overview",
    "title": "Homepage",
    "section": "Class Overview",
    "text": "Class Overview\nContemporary research in higher education (and other disciplines) should be both rigorous and reproducible. This is class will teach you the fundamentals of data management and quantitative research workflow with emphasis on rigor and reproducibility.\nOften referred to informally as “the R class”, you will get an introduction to coding using the R programming language, but many of the skills are directly transferable to future work in Python, Stata, or other software.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#credit-to-dr.-skinner",
    "href": "index.html#credit-to-dr.-skinner",
    "title": "Homepage",
    "section": "Credit to Dr. Skinner",
    "text": "Credit to Dr. Skinner\nFirst and foremost, credit for the structure and vast majority of the content on this site goes to Dr. Benjamin T. Skinner who designed this course before leaving UF to take a data scientist position at the National Endowment for the Humanities. I took this class with Dr. Skinner in Spring 2022, then was a Teaching Assistant for this class with him during Spring 2023. In building this class website, I’ve tried to keep the majority of the content consistent whilst making the class work for the format Dr. Tanner and I are going to teach it in.\nIf you’re interested in seeing Dr. Skinner’s versions of the class, here are links to his course site\n\nEDH 7916 Spring 2023\nEDH 7916 Spring 2022",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#a-note-on-uncertainty-tolerance",
    "href": "index.html#a-note-on-uncertainty-tolerance",
    "title": "Homepage",
    "section": "A Note on “Uncertainty Tolerance”",
    "text": "A Note on “Uncertainty Tolerance”\nOne of my close friends is full-time application developer for UF, and he likes to talk with his new hires about the need for “uncertainty tolerance”. In essence, this is the ability to be okay with not knowing if something is going to work as expected and having the patience to play around until it does. For some, this comes naturally, for others this is one of the biggest hurdles to overcome. Unfortunately, whichever category you fall into, to learn to code you will need to build up some level of “uncertainty tolerance”. There are going to be times you code doesn’t run or work as expected, and every single programmer I know solves these issues the same way, playing around until it works. At first this may be wildly frustrating, particularly for some of you, but it will come over time, just bear with it as best as you can!",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#the-im-stuck-workflow",
    "href": "index.html#the-im-stuck-workflow",
    "title": "Homepage",
    "section": "The “I’m Stuck” Workflow",
    "text": "The “I’m Stuck” Workflow\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nMatt’s office hours or email\n\nTrying the above steps first really helps me help you\n\nI’d probably start by going through them anyway\n\nI rarely will give direct answers, I just help you think through the issue\n\n\nNote: As one of the main purposes of this class is to teach you the basics of R programming, the use of AI-based coding tools (such as ChatGPT, GitHub Co-Pilot, Google Bard, etc.) is not permitted.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#useful-rstudio-keyboard-shortcuts",
    "href": "index.html#useful-rstudio-keyboard-shortcuts",
    "title": "Homepage",
    "section": "Useful RStudio Keyboard Shortcuts",
    "text": "Useful RStudio Keyboard Shortcuts\n\nRun selected code: Command Return (mac) Ctrl Enter (windows)\nComment/un-comment selected lines: Shift Command C (mac) Ctrl Shift C (windows)\nAuto-format code indentations: Command I (mac) Ctrl I (windows)\nNew code chunk (in .qmd files): Option Command I (mac) Ctrl Alt I\nToggle full screen code window: Control Shift 1 (mac) Ctrl Shift 1 (windows)\nMulti-line cursor: Alt then Drag-the-mouse\nTypical shortcuts such as Command/Ctrl C for copy also work\n\nSee more here",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Homepage",
    "section": "Useful Links",
    "text": "Useful Links\n Email Matt\n Email Melvin\n Stack Overflow R Questions\n See Examples of Matt’s Code on Github\n See Examples of Dr. Skinner’s Code on Github",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#good-luck-in-the-class",
    "href": "index.html#good-luck-in-the-class",
    "title": "Homepage",
    "section": "Good luck in the class!",
    "text": "Good luck in the class!\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "x-01-git.html#installing-git",
    "href": "x-01-git.html#installing-git",
    "title": "I: Git & GitHub",
    "section": "Installing git",
    "text": "Installing git\n\nThe first thing you are going to need to do is install git\n\nFrom the git website http://git-scm.com/\n\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\n\n\nIf you’re using a Mac, there’s a decent chance git is already installed, you may already have git on Windows if you’ve used something that installed it in the past\n\nTo check if you have git copy this command into the terminal (note: not the console, the terminal which is next to console in RStudio)\n\nOnce installed, you could keep using the terminal for git, but RStudio has a much more beginner friendly point-and-click system we will use instead\n\n\n\nwhich git\nIf it provides a file path to something called git, you have git, move on!\n\nIf you need to install git\n\nThere’s a great resource “Happy git with R” by Jenny Bryan which cover a whole range of git topics beyond what we need today\n\n“Happy git with R”’s installation page has pretty clear instructions for installing git\nNote: You should always choose “Option 1” unless you\n\n\n\n\nHopefully with those instructions you managed to get git installed if it wasn’t already, which is honestly the hardest part of this lesson!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-github-account",
    "href": "x-01-git.html#creating-a-github-account",
    "title": "I: Git & GitHub",
    "section": "Creating a GitHub Account",
    "text": "Creating a GitHub Account\n\ngit is a language which handles the version control, if you just wanted to use version control and store is all locally, git is all you need\nHowever, the real advantage of git is that you keep the version control both on your computer and in a “remote” repository (similar to OneDrive etc.)\nThere are plenty of git clients that offer this service, but by far the biggest is GitHub, which is what we will use\nSo you need to create an account, which is just like signing up for any other online account\n\n\nGo to https://github.com\nClick “sign up” and follow the prompts on screen…\nDone!\n\n\nFYI: If you plan to use GitHub regularly, students are eligible for free GitHub Pro, which I have, find our more here. This will allow you to create private GitHub repos, by default, all GitHub repos are public (which is how all open source stuff like R works)",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-new-github-repo",
    "href": "x-01-git.html#creating-a-new-github-repo",
    "title": "I: Git & GitHub",
    "section": "Creating a New GitHub Repo",
    "text": "Creating a New GitHub Repo\n\nThere are two ways you can create a new GitHub repo\n\nCreate a repo on your computer, then start tracking it with git, then link it to GitHub\nCreate a repo on GitHub and clone it to your computer\n\n\nIMHO, this is far easier, so it’ what we will do!\n\n\n\nStep One: Create Repo on GitHub\n\nWhen on https://github.com\n\nNavigate to “Your Repositories”\nClick the green “new” button\nChoose a name for the new repo\nUnder “Add .gitignore” select the R template\nEverything else is optional, so don’t worry about it for now\n\n\nNote: If you signed up for GitHub pro, you can make the repo private\n\nIf you do, make sure to add me @ttalVlatt so I can see it to give you credit\n\n\n\nYou should be taken to your shiny new repo, yay!\n\n\n\n\nStep Two: Setup GitHub SSH Access\n\nThis is how your computer will have access to edit your repo\nIt sounds scary, but luckily RStudio make it easy-peasy!\n\nIn RStudio go to “Tools” and then “Global Options”\nSelect “Git/SVN” from the left hand menu\nUnder “SSH Key” select “Create SSH Key”\nLeave the optional pass phrase boxes blank and click “create”\nClose the pop-up box that appears\nBack on the “Git/SVN” page select “View Public Key”\nCopy that to your clipboard\nGo to GitHub.com\nGo to “Settings” on the menu under your profile icon\nSelect “SSH and GPG Keys” from the left-hand menu\nSelect the green “New SSH Key”\nGive the key a name\n\n\nIf you’re using RStudio on your computer, this will be set for a while, so just call it “MacBook Pro” or something similar\nIf you’re using RStudio Cloud, you need a new key for each project, so name it accordingly\n\n\nPaste the SSH Key you copied from RStudio into the “key” box\nLeave it set as “authentication key”\nClick “Add New SSH Key” and you’re done!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "href": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "title": "I: Git & GitHub",
    "section": "Cloning a Repo Down to Your Computer",
    "text": "Cloning a Repo Down to Your Computer\n\nThis step is a little different depending on if you’re using RStudio on your computer or the cloud, so I will outline each separately\n\n\nRStudio on Your Computer\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” select “SSH”\nCopy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nClick on the blue cube in the top right (where we set up projects before)\nClick on “New Project” then “Version Control”\nPaste what you copied from GitHub as the URL\nChoose a file name and location that make sense (this is where the repo will be kept)\nDone!\n\n\n\nposit.cloud\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” and keep it on “HTTPS”\nCopy the address that should look like https://github.com/&lt;Username&gt;/&lt;Repo&gt;.git\nOn posit.cloud select “New Project” and then “From GitHub Repository”\nPaste the URL you copied in the URL box and select a name for the project\nGo back to your repo on GitHub and reselect the green “Code” button\nThis time select “SSH” and copy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nOnce the project is opened, go to the terminal (next to the console)\nType git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git replacing &lt;Username&gt; and &lt;Repo&gt; with the correct names (you can copy from the block below)\nDone!\n\n\ngit remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\n\n\nOkay, with git and GitHub set up, the hard part is over! Now we will just go over how to use what we set up\n\nKeep in mind, we are just going to cover one purpose git, this is just the beginning",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#using-git-for-version-control-and-backup",
    "href": "x-01-git.html#using-git-for-version-control-and-backup",
    "title": "I: Git & GitHub",
    "section": "Using git for Version Control and Backup",
    "text": "Using git for Version Control and Backup\n\nGetting a change from your computer to GitHub has three steps\n\n“Stage” the change, which tells git to pay attention to the change\n“Commit” the change, which saves it to your local (on computer) version of git\n“Push” the change, which save it to your remote (GitHub) version of git\n\n\n\nLet’s see what that looks like in RStudio\n\nIn the top right corner panel of RStudio (same area as the “Environment”) there’s a “Git” tab, select it\nYou’ll see a few things here\n\nAlong the top are some buttons for the core git commands of “Commit”, “Pull”, and “Push”\nRight now, there is probably nothing in the main area of the panel\n\nGo ahead and make a new .R script (doesn’t need to be anything in it) and save it in the project folder\n\nNow you’ll see it in the main area of the “Git” panel\n\nAny changes you make to the repo will appear here, new files, changed files, deleted files, etc.\n\n\n\nTo backup these changes to GitHub, follow these steps 1. Click the white square box left of the file in the “Git” main panel - This “stages” the change, i.e., tells git to pay attention to it 2. Click “Commit” - This will open a new box/window - In the top right hand box you can (and should) add an informative message about the change you made - E.g. “Created a test script” - Then hit the “Commit” button right underneath that 3. Finally, hit “Push” - You can do this in the same window, or at the top of the Git panel, it doesn’t matter - This “pushes” the changes you just “committed” up to GitHub - The very first time you do this, you may get a warning that the key isn’t know - Type “yes” as your response, you won’t see this again unless you make a new key\n\nYou can stage, commit, and push lots of change at once, or one by one\n\nThe big difference is that the less each individual commit and push does, the less you have to reverse\n\nFor that reason, always push up things you’re sure about first, then things your not, in separate commits\n\n\nThis process may seem like a lot, but, it will become second nature once you start using it\n\nThe ability to version control your code and easily track back to specific points is alone more than worth it\n\nThat’s not to mention this is only an intro, git can do so much more as you get familiar with it\n\nPlus, if you can use git you will stand out from the crowd in serious data management jobs\n\n\n\n\nThe Need to .gitignore\n\nWhen something appears in the RStudio “Git” panel that you don’t want to push you can right-click and hit the “ignore” option\n\nThis will add that file to a .gitignore file in your repo, and means git will never try and track that file again\nYou can also add file names and/or patterns directly to the .gitignore file\n\nThis is useful for anything you don’t want sharing (as GitHub repos are public by default), or anything too large for git (big data sets etc.)\n\n\n\nSidenote: The Need to pull\n\nAnother git command that is super common is pull, this will just check for any changes in the GitHub copy of your repo and pull them down\n\nIf you keep things simple and only push changes to this project from one computer, there should never be anything to pull down\n\nYou can always hit the button if you’re curious, it will just say “already up to date”\n\nIf you set this up on more than one computer, or start collaborating with someone else, you’ll need to commit and push when you’re finished working then pull from before you start work\n\nNow we’ve covered some of the basics, I just to suggest a few rules you stick by with git",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#git-ground-rules",
    "href": "x-01-git.html#git-ground-rules",
    "title": "I: Git & GitHub",
    "section": "git Ground Rules",
    "text": "git Ground Rules\n\nGenerally, git is best suited for plain text based files, like .R scripts and .qmd files\n\n\ngit can and will track other files, but it’s primarily meant for code, that is where version control is most powerful\nParticularly is a non-code file is large, it is best to ignore it with .gitignore which we will talk about below\n\n\npush regularly and often\n\n\nWhenever you finish something, it’s generally a good idea to push those change up to GitHub\nThis makes each version git stores more granular, so you can undo one thing without undoing a bunch of things. That will make more sense over time, but for, just push\n\n\npull at the start of each work session\n\n\nThis isn’t important if you’re using git in the simple way we are, but the second you start collaberating with git or even using on multiple computers, always pull first\nThis will add any changes that have been push-ed to your files before you edit them, avoiding conflicts and making everyone’s lives easier\n\n\nWrite useful commit messages\n\n\nEverytime you commit then push you have to write a message, if we have to go back in time, this is how you will find the point to go back to, so don’t say make them descriptive\n\n\nDon’t panic\n\n\nSometimes, git can get messed up, particularly when collaborating with others\nThe beuaty is that with version control, we can always go back and fix things\nIf you run into git issues, I am happy to help, and if I can’t I know plenty of people who can!\n\n\nNever, ever, ever, put private or restricted information in git or GitHub\n\n\nBy default GitHub repos are public, and even if they’re private, they are not approved places for private or restricted data\nEven if you’re just backing up code that uses restricted data, you should check-in with your data security/IT team to make sure you’re following institutional rules",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#summary",
    "href": "x-01-git.html#summary",
    "title": "I: Git & GitHub",
    "section": "Summary",
    "text": "Summary\n\nIf you’ve made it to here, congratulations, you’re now officially a git user\n\nI encourage you to keep at it, the more you use it, the easier it becomes\n\nUsing git as a version control and backup for a single computer is the simplest way to use git and more than enough for a lot of people\n\nIf you get comfortable with git, it can do so much more\n\nWorking across multiple computers\nCollaborating with other researchers\nCreating branches of work to try out new approaches\nfork-ing existing repos to make a new version of something someone else did\npull request-ing something you fork-ed and improved/fixed to get your change added to the main project\nHosting websites with gh-pages (like this one!)\nA whole lot more!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "href": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\nNOTE: Since we’re not using the vanilla R, we don’t need to load any packages\n\n## ---------------------------\n## libraries\n## ---------------------------\n\n## NONE",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#check-working-directory",
    "href": "x-03-vanilla.html#check-working-directory",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Check working directory",
    "text": "Check working directory\n\nBefore we get started, make sure your working directory is set to your class folder\n\n\n## Check working directory is correct\nsetwd(this.path::here())",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#read-in-data",
    "href": "x-03-vanilla.html#read-in-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Read in data",
    "text": "Read in data\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n## input\n## ---------------------------\n\n## data are CSV, so we use read.csv(), which is base R function\ndf &lt;- read.csv(file.path(\"data\", \"hsls-small.csv\"))\n\n\nUnlike the read_csv() function we’ve used before, read.csv() doesn’t print anything\n\nnotice the difference: a . instead of an _\n\nSo that we can see our data, well print to the console. BUT before we do that…\n\n-read.csv() returns a base R data.frame() rather than the special data frame or tibble() that the tidyverse uses. - It’s mostly the same, but one difference is that whereas R will only print the first 10 rows of a tibble, it will print the entire data.frame - We don’t need to see the whole thing, so we’ll use the head() function to print only the first 10 rows.\n\n## show first 10 rows\nhead(df, n = 10)\n\n   stu_id x1sex x1race x1stdob x1txmtscor x1paredu x1hhnumber x1famincome\n1   10001     1      8  199502    59.3710        5          3          10\n2   10002     2      8  199511    47.6821        3          6           3\n3   10003     2      3  199506    64.2431        7          3           6\n4   10004     2      8  199505    49.2690        4          2           5\n5   10005     1      8  199505    62.5897        4          4           9\n6   10006     2      8  199504    58.1268        3          6           5\n7   10007     2      8  199409    49.4960        2          2           4\n8   10008     1      8  199410    54.6249        7          3           7\n9   10009     1      8  199501    53.1875        2          3           4\n10  10010     2      8  199503    63.7986        3          4           4\n   x1poverty185   x1ses x1stuedexpct x1paredexpct x1region x4hscompstat\n1             0  1.5644            8            6        2            1\n2             1 -0.3699           11            6        1            1\n3             0  1.2741           10           10        4            1\n4             0  0.5498           10           10        3            1\n5             0  0.1495            6           10        3            1\n6             0  1.0639           10            8        3           -8\n7             0 -0.4300            8           11        1            1\n8             0  1.5144            8            6        1            1\n9             0 -0.3103           11           11        3            1\n10            0  0.0451            8            6        1           -8\n   x4evratndclg x4hs2psmos\n1             1          3\n2             1          3\n3             1          4\n4             0         -7\n5             0         -7\n6            -8         -8\n7             1          2\n8             1          3\n9             1          8\n10           -8         -8",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#select-variables-columns",
    "href": "x-03-vanilla.html#select-variables-columns",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\n\nData frames are like special matrices\n\nThey have rows and columns\nYou can access these rows and columns using square bracket notation ([])\nBecause data frames have two dimensions, you use a comma inside the square brackets to indicate what you mean ([,]):\n\ndf[&lt;rows&gt;,&lt;cols&gt;]\n\n\nAt it’s most basic, you can use numbers to represent the index of the cell or cells you’re interested in\n\nFor example, if you want to access the value of the cell in row 1, column 4, you can use:\n\n\n\n## show value at row 1, col 4\ndf[1, 4]\n\n[1] 199502\n\n\n\nBecause data frames have column names (the variable names in our data set), we can also refer to them by name\n\nThe fourth column is the student date of birth variable, x1stdob\n\nWe can use that instead of 4 (notice the quotation marks \"\"):\n\n\n\n\n## show value at row 1, x1stdob column\ndf[1, \"x1stdob\"]\n\n[1] 199502\n\n\n\nIf we want to see more than one column, we can put the names in a concatenated vector using the c() function:\n\n\n## show values at row 1, stu_id & x1stdob column\ndf[1, c(\"stu_id\", \"x1stdob\")]\n\n  stu_id x1stdob\n1  10001  199502\n\n\n\nSo far, we’ve not assigned these results to anything, so they’ve just printed to the console.\n\nHowever, we can assign them to a new object\nIf we want to slice our data so that we only have selected columns, we can leave the rows section blank (meaning we want all rows) and include all the columns we want to keep in our new data frame object.\n\n\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- df[, c(\"stu_id\", \"x1stuedexpct\", \"x1paredexpct\", \"x1region\")]\n\n## show 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region\n1   10001            8            6        2\n2   10002           11            6        1\n3   10003           10           10        4\n4   10004           10           10        3\n5   10005            6           10        3\n6   10006           10            8        3\n7   10007            8           11        1\n8   10008            8            6        1\n9   10009           11           11        3\n10  10010            8            6        1",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#mutate-data-into-new-forms",
    "href": "x-03-vanilla.html#mutate-data-into-new-forms",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\n\nChanging existing variables (columns)\n\nTo conditionally change a variable, we’ll once again use the bracket notation to target our changes\nThis time, however, we do a couple of things differently:\n\ninclude square brackets on the LHS of the assignment\nuse conditions in the &lt;rows&gt; part of the bracket\n\nAs before, we need to account for the fact that our two expectation variables, x1stuedexpct and x1paredexpct, have values that need to be converted to NA: -8, -9, and 11\n\nSee the first data wrangling lesson for the rationale behind these changes.\n\nFirst, let’s look at the unique values using the table() function\n\nThis somewhat similar to count() in tidyverse\n\nSo that we see any missing values, we’ll include an extra argument useNA = \"ifany\"\n\nThis just means we will see counts for NAs if there are any\n\n\n\n## -----------------\n## mutate\n## -----------------\n\n## see unique values for student expectation\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n  -8    1    2    3    4    5    6    7    8    9   10   11 \n2059   93 2619  140 1195  115 3505  231 4278  176 4461 4631 \n\n## see unique values for parental expectation\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n  -9   -8    1    2    3    4    5    6    7    8    9   10   11 \n  32 6715   55 1293  149 1199  133 4952   76 3355   37 3782 1725 \n\n\n\nNotice that we use a dollar sign, $, to call the column name from the data frame\n\nUnlike with the tidyverse, we cannot just use the column name\n\nBase R will look for that column name not as a column in a data frame, but as its own object\nIt probably won’t find it (or worse, you’ll have another object in memory that it will find and you’ll get the wrong thing!).\n\n\nTo modify a variable when it’s a certain value, we can use the [] square brackets in a more advanced way\n\nStart by identifying the column you’d like\n\ne.g, df_tmp$x1stuedexpct -Then add the [] square brackets and inside them\n\nInside them we can add a condition to them, such as when a column is equal to -8\n\ne.g., df_tmp$x1stuedexpct == -8\n\nThink of this a bit like filter() from the tidyverse\n\n\nIf we just print this, you’ll see a load of -8, not that useful…\n\n\n\n## This will just print a bunch of -8s\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8]\n\n   [1] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [25] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [49] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [73] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [97] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [121] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [145] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [169] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [193] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [217] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [241] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [265] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [289] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [313] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [337] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [361] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [385] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [409] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [433] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [457] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [481] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [505] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [529] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [553] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [577] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [601] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [625] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [649] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [673] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [697] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [721] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [745] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [769] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [793] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [817] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [841] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [865] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [889] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [913] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [937] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [961] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [985] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1009] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1033] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1057] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1081] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1105] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1129] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1153] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1177] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1201] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1225] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1249] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1273] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1297] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1321] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1345] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1369] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1393] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1417] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1441] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1465] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1489] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1513] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1537] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1561] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1585] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1609] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1633] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1657] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1681] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1705] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1729] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1753] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1777] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1801] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1825] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1849] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1873] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1897] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1921] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1945] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1969] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1993] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2017] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2041] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n\n\n\nBut, instead of printing it, we can assign NA to it, which will replace all those -8s with NA\n\nWe can do the same for 11 to while we are at it\n\n\n\n## replace student expectation values\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8] &lt;- NA\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == 11] &lt;- NA\n\n\nIf you think back to our previous lesson, we can be a little more slick than this though\n\nIf we change the statement to %in% c(-8, -9, 11) it will do it all at once\n\n\n\n## replace parent expectation values\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct %in% c(-8, -9, 11)] &lt;- NA\n\nLet’s confirm using table() again. The values that were in -8, -9, and 11 should now be summed under NA.\n\n## see unique values for student expectation (confirm changes)\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  93 2619  140 1195  115 3505  231 4278  176 4461 6690 \n\n## see unique values for parental expectation (confirm changes)\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  55 1293  149 1199  133 4952   76 3355   37 3782 8472 \n\n\n\n\nAdding new variables (columns)\n\nAdding a new variable to our data frame is just like modifying an existing column\nThe only difference is that instead of putting an existing column name after the first $ sign, we’ll make up a new name\nThis tells R to add a new column to our data frame\nAs with the tidyverse version, we’ll use the ifelse() function to create a new variable that is the higher of student or parental expectations\n\n\n## add new column\ndf_tmp$high_expct &lt;- ifelse(df_tmp$x1stuedexpct &gt; df_tmp$x1paredexpct, # test\n                            df_tmp$x1stuedexpct,                       # if TRUE\n                            df_tmp$x1paredexpct)                       # if FALSE\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1         NA\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1         NA\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nJust like in the original lesson, it doesn’t handle NA values how we want it to\n\nLook at student 10002 in the second row:\n\nWhile the student doesn’t have an expectation (or said “I don’t know”), the parent does.\n\nHowever, our new variable records NA. Let’s fix it with this test:\n\n\n\n\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\n\n## NB: We have to include [is.na(df_tmp$high_expct)] each time so that\n## everything lines up\n\n## step 1 student\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]), \n    ## if TRUE do this...\n    df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n## step 2 parent\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1paredexpct[is.na(df_tmp$high_expct)]),\n    ## if TRUE do this...\n    df_tmp$x1paredexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n\nThat’s a lot of text!\nWhat’s happening is that we are trying to replace a vector of values with another vector of values, which need to line up and be the same length\n\nThat’s why we start with df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]\n\nWhen our high_expct column has missing values, we want to replace with non-missing x1stuedexpct values in the same row\n\nThat means we also need to subset that column to only include values in rows that have missing high_expct values\n\nBecause we must do this each time, our script gets pretty long and unwieldy.\n\n\n\n\n\nLet’s check to make sure it worked as intended.\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#filter-observations-rows",
    "href": "x-03-vanilla.html#filter-observations-rows",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\n\nLet’s check the counts of our new variable:\n\n\n## -----------------\n## filter\n## -----------------\n\n## get summary of our new variable\ntable(df_tmp$high_expct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  71 2034  163 1282  132 4334  191 5087  168 6578 3463 \n\n\n\nSince we’re can’t use the missing values we’ll drop those observations from our data frame\nJust like when we selected columns above, we’ll use the [] square brackets notation\n\nAs with dplyr’s filter(), we want to filter in what we want (i.e., when it’s not NA)\n\nSince we want to filter rows, we set this condition before the comma in the square brackets\n\nBecause we want all the columns, we leave the space after the comma blank\n\n\n\n## filter in values that aren't missing\ndf_tmp &lt;- df_tmp[!is.na(df_tmp$high_expct),]\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n10  10010            8            6        1          8\n11  10011            8            6        3          8\n\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values)\nSince we haven’t removed rows until now, to double check, we can compare the number of rows in the original data frame, df, to what we have now\n\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column\n\nWhile not a formal test, it does support what we expected\n\nIn other words, if the number were different, we’d definitely want to go back and investigate",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#summarize-data",
    "href": "x-03-vanilla.html#summarize-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. For an overall average, we can just use the mean() function.\n\n## -----------------\n## summarize\n## -----------------\n\n## get average (without storing)\nmean(df_tmp$high_expct)\n\n[1] 7.272705\n\n\n\nOverall, we can see that students and parents have high post-secondary expectations on average: to earn some graduate credential beyond a bachelor’s degree\nHowever, this isn’t what we want. We want the values across census regions.\n\n\n## check our census regions\ntable(df_tmp$x1region, useNA = \"ifany\")\n\n\n   1    2    3    4 \n3128 5312 8177 3423 \n\n\n\nWe’re not missing any census data, which is good!\nTo calculate our average expectations, we need to use the aggregate function\nThis function allows to compute a FUNction by a group\n\nWe’ll use it to get our summary.\n\n\n\n## get average (assigning this time)\ndf_tmp &lt;- aggregate(df_tmp[\"high_expct\"],                # var of interest\n                    by = list(region = df_tmp$x1region), # by group\n                    FUN = mean)                          # function to run\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n2      2   7.168110\n3      3   7.357833\n4      4   7.125329\n\n\n\nSuccess! Expectations are similar across the country, but not the same by region.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#write-out-updated-data",
    "href": "x-03-vanilla.html#write-out-updated-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Write out updated data",
    "text": "Write out updated data\n\nWe can use this new data frame as a table in its own right or to make a figure\nFor now, however, we’ll simply save it using the opposite of read.csv() — write.csv()\n\n\n## write with useful name\nwrite.csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\n\nAnd with that, we’ve met our task: we can show average educational expectations by region\nTo be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region\n\nThis caveat doesn’t necessarily make our analysis less useful, but rather sets its scope.\n\nFurthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#questions",
    "href": "x-03-vanilla.html#questions",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nWhat is the average standardized math test score?\nHow does this differ by gender?\n\n\n\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (include what that category represents)\n\n\n\n\nOf the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nHow does this differ by region?\n\n\n\n\n\n\n\nWhat percentage of students ever attended a post-secondary institution by February 2016?\nOptional: Give the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35k within each region\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the final lesson). Good faith efforts (as determined by the instructor) at extra credit assignments will earn full credit if submitted on time.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  }
]