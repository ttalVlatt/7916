[
  {
    "objectID": "x-04-scrape.html#inspect-the-web-site",
    "href": "x-04-scrape.html#inspect-the-web-site",
    "title": "IV: Web Scraping",
    "section": "Inspect the web site",
    "text": "Inspect the web site\nFirst, let’s check out the table we want to scrape. The table we see looks like a regularly formatted table, much like we would see in a paper document. But unlike a printed document, a web page relies on hidden-from-the-user code to generate what we see. By doing it this way instead of serving a static image, websites can adjust to the wide array of user screen sizes, devices, and operating systems. Instructions that tell the user device how to generate the page are also smaller than sending a preformatted image, so bandwidth and time to load are also reduced.\nBut as web scrapers, we don’t need this. We need the underlying HTML/CSS/XML code used to generate the page. To see it, you’ll need to use a web site inspector. With Firefox and Chrome, you should be able to right-click the page and see the underlying code (you may need to turn on developer tools first). With Safari, you will have to enable the developer tools first.\nThe top code of the page should look something like this:\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;!-- Current year pub navigation function --&gt;\nMoving further down, we find the table data, but in a very different format (first row):\n...\n&lt;tr&gt;\n  &lt;th class=\"TblCls009\" scope=\"row\" nowrap=\"nowrap\"&gt;1960 &lt;/th&gt;\n  &lt;td class=\"TblCls010\"&gt;1,679&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(44.5)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;756&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(32.3)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;923&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(30.1)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;45.1&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(2.16)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;54.0&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(3.23)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;37.9&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(2.85)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n  &lt;td class=\"TblCls010\"&gt;&mdash;&lt;/td&gt;\n  &lt;td class=\"TblCls011\"&gt;(&dagger;)&lt;/td&gt;\n&lt;/tr&gt;\n...\nThe task is to convert these data into a data frame that we can then store or use in tables and figures. This is what the rvest helps us do.",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#read-web-site",
    "href": "x-04-scrape.html#read-web-site",
    "title": "IV: Web Scraping",
    "section": "Read web site",
    "text": "Read web site\nThe first step is to read the web page code into an object using the read_html() function.\n\n## set site\nurl &lt;- \"https://nces.ed.gov/programs/digest/d17/tables/dt17_302.10.asp\"\n\n## get site\nsite &lt;- read_html(url)\n\nShowing our object, we can see that the basic structure of the web page is stored.\n\n## show\nsite\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body bgcolor=\"#ffffff\" text=\"#000000\"&gt;\\r\\n\\t\\r\\n\\t&lt;!-- Main NCES Header  ...",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#select-nodes",
    "href": "x-04-scrape.html#select-nodes",
    "title": "IV: Web Scraping",
    "section": "Select nodes",
    "text": "Select nodes\nRight now, we have a structured, but not particularly useful object holding our web page data. To pull out specific data, we use the html_nodes() function. Selecting a node is somewhat akin to using dplyr’s filter() on a data frame.\nGreat…but what’s a node and how do I know which ones to use? First, a node is a particular element that is comprised of some information stored between, for example, HTML tags like &lt;p&gt;...&lt;/p&gt; or &lt;h1&gt;...&lt;/h2&gt;. Good web design says that information on page should be organized by its purpose and similarity to other data. For example, major headers should be wrapped in &lt;h1&gt; tags and similar page sections should be given the same CSS class. We can use CSS ids and classes with the html_nodes() function to pull the exact data we need.\nGreat!…but what are the classes that we need? Well, we could just inspect the web page manually and guess. For some pages, that works great. But it certainly looks like a chore for this page. Luckily, there’s a great tool that will help us.\n\nSelectorGadget\nSelectorGadget is a (now very old) plugin that allows you to click on a web page and, through process of elimination, get the exact combination of HTML tags and CSS ids and classes you need to pull only the data you need.\nYou can get it from the link above or by following the instructions here.\nThe SelectorGadget page has instructions, but briefly, this is the process:\n\nOn the first click, SelectorGadget will make its best guess about what you want based on the item you clicked (e.g., table column). The particular element you clicked will be green. The other elements it assumed you want will turn yellow. Sometimes it’s right and you’re finished!\n\nOften, it will select something you don’t want. In that case, click on the yellow item you don’t want. Again, SelectorGadget will make and informed guess. Sometimes it will drop all extraneous elements and sometimes you will need to click multiple times. These elements will be red.\nOn the other hand, SelectorGadget may not have given you everything you want. Keep clicking on new elements (and dropping the extra) until only what you want is highlighted in either green or yellow.\n\nAs you’re clicking, you’ll see a box with a string of element ids and classes changing. When you’re finished, copy this string. This is your node you’ll use in the html_nodes() function!\n\nQuick exercise\nGet the SelectorGadget plugin and play with it for a few minutes. See if you can select only a specific column then only a specific row.",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#first-column-of-data",
    "href": "x-04-scrape.html#first-column-of-data",
    "title": "IV: Web Scraping",
    "section": "First column of data",
    "text": "First column of data\nAs a first step, let’s get the first column of data in Table 302.10: the total number of recent high school graduates. Using SelectorGadget, I see that the node string I should use is '.tableBracketRow td:nth-child(2)'. After selecting the node, we use html_text() to convert the data into a vector like we’re used to seeing.\n\n## subset to just first column\ntot &lt;- site %&gt;%\n  html_nodes(\".tableBracketRow td:nth-child(2)\") %&gt;%\n  html_text()\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \" \"     \"2,659\" \"2,612\" \"2,525\"\n[10] \"2,606\" \"2,842\" \" \"     \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \" \"    \n[19] \"3,185\" \"2,986\" \"3,141\" \"3,163\" \"3,160\" \" \"     \"3,088\" \"3,056\" \"3,100\"\n[28] \"2,963\" \"3,012\" \" \"     \"2,668\" \"2,786\" \"2,647\" \"2,673\" \"2,450\" \" \"    \n[37] \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \" \"     \"2,599\" \"2,660\" \"2,769\"\n[46] \"2,810\" \"2,897\" \" \"     \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\" \" \"    \n[55] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \" \"     \"3,160\" \"3,079\" \"3,203\"\n[64] \"2,977\" \"2,868\" \" \"     \"2,965\" \"3,137\"\n\n\nSo far so good, but we can see a few problems. First, the blank rows in the table show up in our data. While those blank table spaces are good for the eyes, they aren’t good in our data set. Let’s try to remove them using the trim = TRUE option.\n\n## ...this time trim blank spaces\ntot &lt;- site %&gt;%\n  html_nodes(\".tableBracketRow td:nth-child(2)\") %&gt;%\n  html_text(trim = TRUE)\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \"\"      \"2,659\" \"2,612\" \"2,525\"\n[10] \"2,606\" \"2,842\" \"\"      \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \"\"     \n[19] \"3,185\" \"2,986\" \"3,141\" \"3,163\" \"3,160\" \"\"      \"3,088\" \"3,056\" \"3,100\"\n[28] \"2,963\" \"3,012\" \"\"      \"2,668\" \"2,786\" \"2,647\" \"2,673\" \"2,450\" \"\"     \n[37] \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \"\"      \"2,599\" \"2,660\" \"2,769\"\n[46] \"2,810\" \"2,897\" \"\"      \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\" \"\"     \n[55] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \"\"      \"3,160\" \"3,079\" \"3,203\"\n[64] \"2,977\" \"2,868\" \"\"      \"2,965\" \"3,137\"\n\n\nBetter, but the empty elements are still there. We can use str_subset() from the stringr library (loaded with tidyverse) to remove them.\n\n## remove blank values; str_subset removes pattern (\"\")\ntot &lt;- tot %&gt;% str_subset(pattern = \".+\")\n\n## show\ntot\n\n [1] \"1,679\" \"1,763\" \"1,838\" \"1,741\" \"2,145\" \"2,659\" \"2,612\" \"2,525\" \"2,606\"\n[10] \"2,842\" \"2,758\" \"2,875\" \"2,964\" \"3,058\" \"3,101\" \"3,185\" \"2,986\" \"3,141\"\n[19] \"3,163\" \"3,160\" \"3,088\" \"3,056\" \"3,100\" \"2,963\" \"3,012\" \"2,668\" \"2,786\"\n[28] \"2,647\" \"2,673\" \"2,450\" \"2,362\" \"2,276\" \"2,397\" \"2,342\" \"2,517\" \"2,599\"\n[37] \"2,660\" \"2,769\" \"2,810\" \"2,897\" \"2,756\" \"2,549\" \"2,796\" \"2,677\" \"2,752\"\n[46] \"2,675\" \"2,692\" \"2,955\" \"3,151\" \"2,937\" \"3,160\" \"3,079\" \"3,203\" \"2,977\"\n[55] \"2,868\" \"2,965\" \"3,137\"\n\n\nGetting closer. Next, let’s convert our numbers to actual numbers, which R thinks are strings at the moment. To do this, we need to get rid of the commas. The str_replace() function is perfect for this. Regular expressions can become complicated, but our use here is simple:\n\n## remove commas, replacing with empty string\ntot &lt;- tot %&gt;% str_replace(pattern = \",\", replacement = \"\")\n\n## show\ntot\n\n [1] \"1679\" \"1763\" \"1838\" \"1741\" \"2145\" \"2659\" \"2612\" \"2525\" \"2606\" \"2842\"\n[11] \"2758\" \"2875\" \"2964\" \"3058\" \"3101\" \"3185\" \"2986\" \"3141\" \"3163\" \"3160\"\n[21] \"3088\" \"3056\" \"3100\" \"2963\" \"3012\" \"2668\" \"2786\" \"2647\" \"2673\" \"2450\"\n[31] \"2362\" \"2276\" \"2397\" \"2342\" \"2517\" \"2599\" \"2660\" \"2769\" \"2810\" \"2897\"\n[41] \"2756\" \"2549\" \"2796\" \"2677\" \"2752\" \"2675\" \"2692\" \"2955\" \"3151\" \"2937\"\n[51] \"3160\" \"3079\" \"3203\" \"2977\" \"2868\" \"2965\" \"3137\"\n\n\nNow we’re ready to convert to a number.\n\n## convert to numeric\ntot &lt;- tot %&gt;% as.integer()\n\n## show\ntot\n\n [1] 1679 1763 1838 1741 2145 2659 2612 2525 2606 2842 2758 2875 2964 3058 3101\n[16] 3185 2986 3141 3163 3160 3088 3056 3100 2963 3012 2668 2786 2647 2673 2450\n[31] 2362 2276 2397 2342 2517 2599 2660 2769 2810 2897 2756 2549 2796 2677 2752\n[46] 2675 2692 2955 3151 2937 3160 3079 3203 2977 2868 2965 3137\n\n\nFinished!",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#add-year",
    "href": "x-04-scrape.html#add-year",
    "title": "IV: Web Scraping",
    "section": "Add year",
    "text": "Add year\nSo that these numbers make sense, let’s grab the years column and create and data frame so that we can make a figure of long term high school completer totals. Again, the first step is to use SelectorGadget to get the node string. This time, it’s \"tbody th\".\n\n## get years column\nyears &lt;- site %&gt;%\n  html_nodes(\"tbody th\") %&gt;%\n  html_text(trim = TRUE)\n\n## remove blank spaces like before\nyears &lt;- years %&gt;% str_subset(pattern = \".+\")\n\n## show\nyears\n\n [1] \"1960\"  \"1961\"  \"1962\"  \"1963\"  \"1964\"  \"1965\"  \"1966\"  \"1967\"  \"1968\" \n[10] \"1969\"  \"1970\"  \"1971\"  \"1972\"  \"1973\"  \"1974\"  \"1975\"  \"1976\"  \"1977\" \n[19] \"1978\"  \"1979\"  \"1980\"  \"1981\"  \"1982\"  \"1983\"  \"1984\"  \"1985\"  \"1986\" \n[28] \"1987\"  \"1988\"  \"1989\"  \"1990\"  \"1991\"  \"1992\"  \"1993\"  \"1994\"  \"1995\" \n[37] \"1996\"  \"1997\"  \"1998\"  \"1999\"  \"2000\"  \"2001\"  \"2002\"  \"2003\"  \"2004\" \n[46] \"2005\"  \"2006\"  \"2007\"  \"2008\"  \"2009\"  \"20103\" \"20113\" \"20123\" \"20133\"\n[55] \"20143\" \"20153\" \"20163\"\n\n\nWe’ve gotten rid of the blank items, but now we have a new problem: the footnotes in the last few years has just be added to the year. Instead of 2010, we have 20103, and so on through 2016. Since the problem is small (it’s easy to see all the bad items) and regular (always extra 3 as the 5th digit), we can fix it using str_sub().\n\n## trim footnote that's become extra digit\nyears &lt;- years %&gt;% str_sub(start = 1, end = 4)\n\n## show\nyears\n\n [1] \"1960\" \"1961\" \"1962\" \"1963\" \"1964\" \"1965\" \"1966\" \"1967\" \"1968\" \"1969\"\n[11] \"1970\" \"1971\" \"1972\" \"1973\" \"1974\" \"1975\" \"1976\" \"1977\" \"1978\" \"1979\"\n[21] \"1980\" \"1981\" \"1982\" \"1983\" \"1984\" \"1985\" \"1986\" \"1987\" \"1988\" \"1989\"\n[31] \"1990\" \"1991\" \"1992\" \"1993\" \"1994\" \"1995\" \"1996\" \"1997\" \"1998\" \"1999\"\n[41] \"2000\" \"2001\" \"2002\" \"2003\" \"2004\" \"2005\" \"2006\" \"2007\" \"2008\" \"2009\"\n[51] \"2010\" \"2011\" \"2012\" \"2013\" \"2014\" \"2015\" \"2016\"\n\n\nFixed! Now we bind together with our high school completers total. Because we want to make a time period line graph, we’ll also convert the years to a date format. We’ll use ymd from the lubridate library. Since we only have years, we’ll include the argument truncated = 2L, which means that we have an incomplete date (no month or day).\nNB Since we dropped blank elements in each vector separately, it’s important to check that all the data line up properly now that we’ve bound them together. If we wanted to be safer, we could have bound the data first, then dropped the rows with double missing values.\n\n## put in data frame\ndf &lt;- bind_cols(years = years, total = tot) %&gt;%\n  mutate(years = ymd(years, truncated = 2L))\n\n## show\ndf\n\n# A tibble: 57 × 2\n   years      total\n   &lt;date&gt;     &lt;int&gt;\n 1 1960-01-01  1679\n 2 1961-01-01  1763\n 3 1962-01-01  1838\n 4 1963-01-01  1741\n 5 1964-01-01  2145\n 6 1965-01-01  2659\n 7 1966-01-01  2612\n 8 1967-01-01  2525\n 9 1968-01-01  2606\n10 1969-01-01  2842\n# ℹ 47 more rows\n\n\nYou can see that the date format adds a month and day (January 1st by default). While these particular dates probably aren’t right, we won’t use them later when graphing so they can stay.\nLet’s plot our trends.\n\n## plot\ng &lt;- ggplot(df, mapping = aes(x = years, y = total)) +\n  ## line for the main estimate\n  geom_line() +\n  ## make x-axis look nice\n  ## major breaks: every 5 years, from min year to max year\n  ## minor breaks: every 1 year, from min year to max year\n  ## labels: formate to only show year (\"%Y\")\n  scale_x_date(breaks = seq(min(df$years),\n                            max(df$years),\n                            \"5 years\"),\n               minor_breaks = seq(min(df$years),\n                                  max(df$years),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## nice labels and titles\n  labs(x = \"Year\",\n       y = \"High school completers (1000s)\",\n       title = \"Total number of high school completers: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\")\ng\n\n\n\n\n\n\n\n\n\nQuick exercise\nPull in total percentage of enrollment (column 5), add to data frame, and plot against year.",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#scrape-entire-table",
    "href": "x-04-scrape.html#scrape-entire-table",
    "title": "IV: Web Scraping",
    "section": "Scrape entire table",
    "text": "Scrape entire table\nNow that we’ve pulled two columns, let’s try to grab the entire table. Once again, we’ll use SelectorGadget to get our node string.\n\n## save node\nnode &lt;- paste0(\".TblCls002 , td.TblCls005 , tbody .TblCls008 , \",\n               \".TblCls009 , .TblCls011 , .TblCls010\")\n\n## save more dataframe-friendly column names that we\n## get from looking at the table online\nnms &lt;- c(\"year\",\"hs_comp_tot\", \"hs_comp_tot_se\",\n         \"hs_comp_m\", \"hs_comp_m_se\",\n         \"hs_comp_f\", \"hs_comp_f_se\",\n         \"enr_pct\", \"enr_pct_se\",\n         \"enr_pct_2\", \"enr_pct_2_se\",\n         \"enr_pct_4\", \"enr_pct_4_se\",\n         \"enr_pct_m\", \"enr_pct_m_se\",\n         \"enr_pct_2_m\", \"enr_pct_2_m_se\",\n         \"enr_pct_4_m\", \"enr_pct_4_m_se\",\n         \"enr_pct_f\", \"enr_pct_f_se\",\n         \"enr_pct_2_f\", \"enr_pct_2_f_se\",\n         \"enr_pct_4_f\", \"enr_pct_4_f_se\")\n\n## whole table\ntab &lt;- site %&gt;%\n  ## use nodes\n  html_nodes(node) %&gt;%\n  ## to text with trim\n  html_text(trim = TRUE)\n\n## show first few elements\ntab[1:30]\n\n [1] \"1960\"   \"1,679\"  \"(44.5)\" \"756\"    \"(32.3)\" \"923\"    \"(30.1)\" \"45.1\"  \n [9] \"(2.16)\" \"—\"      \"(†)\"    \"—\"      \"(†)\"    \"54.0\"   \"(3.23)\" \"—\"     \n[17] \"(†)\"    \"—\"      \"(†)\"    \"37.9\"   \"(2.85)\" \"—\"      \"(†)\"    \"—\"     \n[25] \"(†)\"    \"1961\"   \"1,763\"  \"(46.7)\" \"790\"    \"(33.7)\"\n\n\nOkay. It looks like we have it, but it’s all in single dimension vector. Since we eventually want a data frame, let’s convert to a matrix.\n\n## convert to matrix\ntab &lt;- tab %&gt;%\n  ## we know the size by looking at the table online\n  matrix(., ncol = 25, byrow = TRUE)\n\n## dimensions\ndim(tab)\n\n[1] 68 25\n\n## show first few columns using base R [&lt;rows&gt;,&lt;cols&gt;] notation\ntab[1:10,1:5]\n\n      [,1]   [,2]    [,3]     [,4]    [,5]    \n [1,] \"1960\" \"1,679\" \"(44.5)\" \"756\"   \"(32.3)\"\n [2,] \"1961\" \"1,763\" \"(46.7)\" \"790\"   \"(33.7)\"\n [3,] \"1962\" \"1,838\" \"(44.3)\" \"872\"   \"(32.0)\"\n [4,] \"1963\" \"1,741\" \"(44.9)\" \"794\"   \"(32.6)\"\n [5,] \"1964\" \"2,145\" \"(43.6)\" \"997\"   \"(32.3)\"\n [6,] \"\"     \"\"      \"\"       \"\"      \"\"      \n [7,] \"1965\" \"2,659\" \"(48.5)\" \"1,254\" \"(35.7)\"\n [8,] \"1966\" \"2,612\" \"(45.7)\" \"1,207\" \"(34.4)\"\n [9,] \"1967\" \"2,525\" \"(38.5)\" \"1,142\" \"(28.9)\"\n[10,] \"1968\" \"2,606\" \"(38.0)\" \"1,184\" \"(28.7)\"\n\n\n\nQuick exercise\nWhat happens if you don’t use byrow = TRUE in the matrix command?\n\nIt’s getting better, but now we have a lot of special characters that we need to clean out. This section relies more heavily on regular expressions, but the idea is the same as above.\n\n## clean up table\ntab &lt;- tab %&gt;%\n  ## convert to tibble, leaving name repair as minimal for now\n  as_tibble(.name_repair = \"minimal\") %&gt;%\n  ## rename using names above\n  set_names(nms) %&gt;%\n  ## remove commas\n  mutate(across(everything(), ~ str_replace(., \",\", \"\"))) %&gt;%\n  ## remove dagger and parentheses\n  mutate(across(everything(), ~ str_replace_na(., \"\\\\(\\U2020\\\\)\"))) %&gt;%\n  ## remove hyphens\n  mutate(across(everything(), ~ str_replace_na(., \"\\U2014\"))) %&gt;%\n  ## remove parentheses, but keep any content that was inside\n  mutate(across(everything(), ~ str_replace(., \"\\\\((.*)\\\\)\", \"\\\\1\"))) %&gt;%\n  ## remove blank strings (^ = start, $ = end, so ^$ = start to end w/ nothing)\n  mutate(across(everything(), ~ str_replace_na(., \"^$\"))) %&gt;%\n  ## fix years like above\n  mutate(year = str_sub(year, 1, 4)) %&gt;%\n  ## convert to numbers, suppressing warnings about NAs b/c we know\n  mutate(across(everything(), ~ suppressWarnings(as.numeric(.)))) %&gt;%\n  ## drop rows with missing year (blank online)\n  drop_na(year)\n\n## show\ntab\n\n# A tibble: 57 × 25\n    year hs_comp_tot hs_comp_tot_se hs_comp_m hs_comp_m_se hs_comp_f\n   &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1  1960        1679           44.5       756         32.3       923\n 2  1961        1763           46.7       790         33.7       973\n 3  1962        1838           44.3       872         32         966\n 4  1963        1741           44.9       794         32.6       947\n 5  1964        2145           43.6       997         32.3      1148\n 6  1965        2659           48.5      1254         35.7      1405\n 7  1966        2612           45.7      1207         34.4      1405\n 8  1967        2525           38.5      1142         28.9      1383\n 9  1968        2606           38        1184         28.7      1422\n10  1969        2842           36.6      1352         27.3      1490\n# ℹ 47 more rows\n# ℹ 19 more variables: hs_comp_f_se &lt;dbl&gt;, enr_pct &lt;dbl&gt;, enr_pct_se &lt;dbl&gt;,\n#   enr_pct_2 &lt;dbl&gt;, enr_pct_2_se &lt;dbl&gt;, enr_pct_4 &lt;dbl&gt;, enr_pct_4_se &lt;dbl&gt;,\n#   enr_pct_m &lt;dbl&gt;, enr_pct_m_se &lt;dbl&gt;, enr_pct_2_m &lt;dbl&gt;,\n#   enr_pct_2_m_se &lt;dbl&gt;, enr_pct_4_m &lt;dbl&gt;, enr_pct_4_m_se &lt;dbl&gt;,\n#   enr_pct_f &lt;dbl&gt;, enr_pct_f_se &lt;dbl&gt;, enr_pct_2_f &lt;dbl&gt;,\n#   enr_pct_2_f_se &lt;dbl&gt;, enr_pct_4_f &lt;dbl&gt;, enr_pct_4_f_se &lt;dbl&gt;\n\n\nGot it!",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#reshape-data",
    "href": "x-04-scrape.html#reshape-data",
    "title": "IV: Web Scraping",
    "section": "Reshape data",
    "text": "Reshape data\nWe could stop where we are, but to make the data more usable in the future, let’s convert to a long data frame. This takes a couple of steps, but the idea is to have each row represent a year by estimate, with a column for the estimate value and a column for the standard error on that estimate. It may help to run the code below one line at a time, checking the progress at each step.\n\n## gather for long data\ndf &lt;- tab %&gt;%\n  ## pivot_longer estimates, leaving standard errors wide for the moment\n  pivot_longer(cols = -c(year, ends_with(\"se\")),\n               names_to = \"group\",\n               values_to = \"estimate\") %&gt;%\n  ## pivot_longer standard errors\n  pivot_longer(cols = -c(year, group, estimate),\n               names_to = \"group_se\",\n               values_to = \"se\") %&gt;% \n  ## drop \"_se\" from standard error estimates\n  mutate(group_se = str_replace(group_se, \"_se\", \"\")) %&gt;%\n  ## filter where group == group_se\n  filter(group == group_se) %&gt;%\n  ## drop extra column\n  select(-group_se) %&gt;%\n  ## arrange\n  arrange(year) %&gt;%\n  ## drop if missing year after reshaping\n  drop_na(year)\n\n## show\ndf\n\n# A tibble: 684 × 4\n    year group       estimate    se\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1  1960 hs_comp_tot   1679   44.5 \n 2  1960 hs_comp_m      756   32.3 \n 3  1960 hs_comp_f      923   30.1 \n 4  1960 enr_pct         45.1  2.16\n 5  1960 enr_pct_2       NA   NA   \n 6  1960 enr_pct_4       NA   NA   \n 7  1960 enr_pct_m       54    3.23\n 8  1960 enr_pct_2_m     NA   NA   \n 9  1960 enr_pct_4_m     NA   NA   \n10  1960 enr_pct_f       37.9  2.85\n# ℹ 674 more rows",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-04-scrape.html#plot-trends",
    "href": "x-04-scrape.html#plot-trends",
    "title": "IV: Web Scraping",
    "section": "Plot trends",
    "text": "Plot trends\nLet’s look at overall college enrollment percentages for recent graduates over time. Because our data are nicely formatted, it’s easy to subset the full table to data to only those estimates we need as well as generate 95% confidence intervals.\n\n## adjust data for specific plot\nplot_df &lt;- df %&gt;%\n  filter(group %in% c(\"enr_pct\", \"enr_pct_m\", \"enr_pct_f\")) %&gt;%\n  mutate(hi = estimate + se * qnorm(.975),\n         lo = estimate - se * qnorm(.975),\n         year = ymd(as.character(year), truncated = 2L),\n         group = ifelse(group == \"enr_pct_f\", \"Women\",\n                        ifelse(group == \"enr_pct_m\", \"Men\", \"All\")))\n\n## show\nplot_df\n\n# A tibble: 171 × 6\n   year       group estimate    se    hi    lo\n   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1960-01-01 All       45.1  2.16  49.3  40.9\n 2 1960-01-01 Men       54    3.23  60.3  47.7\n 3 1960-01-01 Women     37.9  2.85  43.5  32.3\n 4 1961-01-01 All       48    2.12  52.2  43.8\n 5 1961-01-01 Men       56.3  3.14  62.5  50.1\n 6 1961-01-01 Women     41.3  2.81  46.8  35.8\n 7 1962-01-01 All       49    2.08  53.1  44.9\n 8 1962-01-01 Men       55    3     60.9  49.1\n 9 1962-01-01 Women     43.5  2.84  49.1  37.9\n10 1963-01-01 All       45    2.12  49.2  40.8\n# ℹ 161 more rows\n\n\nFirst, let’s plot the overall average. Notice that we use the filter() function in the ggplot() function to remove the subgroup estimates for men and women.\n\n## plot overall average\ng &lt;- ggplot(plot_df %&gt;% filter(group == \"All\"),\n            mapping = aes(x = year, y = estimate)) +\n  ## create shaded ribbon for 95% CI\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"grey70\") +\n  ## line for main estimate\n  geom_line() +\n  ## make x-axis look nice\n  ## major breaks: every 5 years, from min year to max year\n  ## minor breaks: every 1 year, from min year to max year\n  ## labels: formate to only show year (\"%Y\")\n  scale_x_date(breaks = seq(min(plot_df$year),\n                            max(plot_df$year),\n                            \"5 years\"),\n               minor_breaks = seq(min(plot_df$year),\n                                  max(plot_df$year),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## good labels and titles\n  labs(x = \"Year\",\n       y = \"Percent\",\n       title = \"Percent of recent high school completers in college: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\")    \n\n## show\ng\n\n\n\n\n\n\n\n\nAfter a small dip in the early 1970s enrollment trends have steadily risen over time.\nNow let’s compare enrollments over time between men and women (dropping the overall average so our plot is clearer).\n\n## plot comparison between men and women\ng &lt;- ggplot(plot_df %&gt;% filter(group %in% c(\"Men\",\"Women\")),\n            ## add colour == group to separate between men and women\n            mapping = aes(x = year, y = estimate, colour = group)) +\n  ## ribbon for 95% CI, but lower alpha so more transparent\n  geom_ribbon(aes(ymin = lo, ymax = hi, fill = group), alpha = 0.2) +\n  ## primary estimate line\n  geom_line() +\n  ## neat x-axis breaks as before\n  scale_x_date(breaks = seq(min(plot_df$year),\n                            max(plot_df$year),\n                            \"5 years\"),\n               minor_breaks = seq(min(plot_df$year),\n                                  max(plot_df$year),\n                                  \"1 years\"),\n               date_labels = \"%Y\") +\n  ## good labels and titles\n  labs(x = \"Year\",\n       y = \"Percent\",\n       title = \"Percent of recent high school completers in college: 1960 to 2016\",\n       caption = \"Source: NCES Digest of Education Statistics, 2017, Table 302.10\") +\n  ## set legend title, drop legend for colour since it's redundant with fill\n  guides(fill = guide_legend(title = \"Group\"),\n         colour = \"none\") +\n  ## position legend so that it sits on plot face, in lower right-hand corner\n  theme(legend.position = c(1,0), legend.justification = c(1,0))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n## show\ng\n\n\n\n\n\n\n\n\nThough a greater proportion of men enrolled in college in the 1960s and early 1970s, women have been increasing their enrollment percentages faster than men since the 1980s and now have comparatively higher rates of college participation.",
    "crumbs": [
      "Extra Credit",
      "IV: Web Scraping"
    ]
  },
  {
    "objectID": "x-02-dload-ipeds.html",
    "href": "x-02-dload-ipeds.html",
    "title": "II: Automating Data Retrieval in Reproducible Report",
    "section": "",
    "text": "LessonAssignment\n\n\nIn Data Wrangling IV we discussed a wide range of automated data retrieval methods.\nFor this extra credit, you simply need to successfully use some form of automated data retrieval in your final project.\nAs a reminder, here a few of the methods we covered in class\n\nIPEDtaS for IPEDS Data\ntidycensus for Census Data\nUrban Institute’s EducationData for Multiple Education Data Sources\nOr simply downloading .csv files in your R code using the url\n\nBut that isn’t the limit, there are countless packages, APIs, and other tools to automatically download data\nWe aren’t providing much instruction here, as the purpose of this assignment is to push you a little into playing with resources you find in the wild! There’s a lot of documentation out there for these tools, and it’s a useful skill to be able to use it.\n\n\nTo get complete this extra credit, utilize automatic data retrieval in your final reproducible report (i.e., we shouldn’t need to go and download your data files).\n\nHint: If you use an extra script, you either to want to source() the .R script from your .qmd script, or, provide clear instructions on when to run it\n\nYour submission for this extra credit is your final project, no additional submission required. Good faith efforts (as determined by the instructor) at extra credit assignments will earn full credit if submitted on time.",
    "crumbs": [
      "Extra Credit",
      "II: Automating Data Retrieval in Reproducible Report"
    ]
  },
  {
    "objectID": "99-final.html",
    "href": "99-final.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project for this class is to create a truly “reproducible report” on a topic of your choosing related to higher education\n\nThe topic can really be almost anything of interest related to higher ed, so long as you can find public data to use\n\nYour report should be 3-5 pages including multiple graphs and visual elements (i.e., not too much text)\n\nYour goal is something like what you might hand a senior administrator at your university to summarize a trend/issue/topic\n\nYou will likely only have a handful of citations\nYou should devote around half your page space to data visualizations and tables\n\n\nThe primary focus of this report is reproducibility\n\nYour data must be publicly available with no IRB restrictions, as you will not submit it, I will go and collect it (unless you download it as part of the project code)\n\n\n\nProposal (5 points)Initial Analyses (10 points)Presentation (5 Points)Final Report (20 Points)\n\n\nThis assignment should be submitted as a text entry directly on Canvas consisting of;\n\nA paragraph describing your project:\n\nWhat will you be investigating/exploring/predicting?\nWhy is it interesting?\n\nA description of where you will find this data.\nA few lines describing your main outcome variable in detail\n\nHow is it coded/what scale is it on?\nHow can you interpret it?\n\n\nThis assignment is worth 5 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\nNOTE: For your initial analyses, the most important thing is that you submit code that sources/renders in full, this assignment will not be successfully completed until that happens, you may have to resubmit multiple times.\nSubmit the following in either a cleanly formatted R (.R) script or Quarto (.qmd) file:\n\nComments (if an R script) or text (if a Quarto script) that describe\n\nWhere your data is from (a link is preferable)\nHow to download it\nWhere to save it in order for your code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nCode that:\n\nReads in the data set that contains (at least) your dependent variable (must read in EXACTLY what downloads from following your instructions above)\nIf appropriate\n\nConverts missing values to NA\nReshapes the data wider or longer\nJoins in additional data files\n\n\nCode that creates at least three of the following:\n\nA plot that shows the overall distribution of your dependent/outcome variable\n\nHint: A histogram or density plot might be a good option here\n\nA plot that shows the distribution of your dependent/outcome variable grouped by a variable in your data\n\nHint: A histogram or density plot with fill might be a good option here\n\nA plot that shows the median, interquartile range, and potential outliers of your dependent/outcome variable grouped by a variable in your data\n\nHint: A box plot with x and/or fill might be a good option here\n\nA plot that shows how your dependent/outcome variable changes by another continuous variable in your data\n\nHint: A scatter plot might be a good option here\n\n\n\nThis assignment is worth 10 points, full points will be awarded once satisfactorily completed, multiple re-submissions may be required.\nThis should be submitted to Canvas by the due date listed.\n\n\n\nYou will present the results of your report in class during the penultimate week of the semester (see date in Canvas)\nThe presentation format is up to you, previous students have\n\nPresented an image of one figure they created\nCreated a short PowerPoint presentation\nCreated presentations using Quarto\n\nThe primary rule for this presentation is that it is to be 3-5 mins long (read min 3 mins, max 5 mins, ideal 4 mins)\n\nAs this is meant to replicate the you presenting your report to senior administrators, this is a hard time-limit, you will be stopped if you go over 5 mins\n\n\nThis assignment is worth 5 points, your grade will be determined by:\n\n3 points: Did you present a plot/table/finding from your report that tells a story about your topic?\n1 point: Did you present the information in a professional and engaging manner?\n1 point: Did you finish within the allotted time limit of 3-5 mins?\n\n\n\n\nYour final report is a 3-5 page (single-spaced, not including citations) document that summarizes the analysis you have done using plenty of figures and summary tables along the way\n\nThis is NOT a traditional academic paper, it is meant to be concise report intended for a university administrator or policymaker\n5 pages is a hard limit, anything beyond the 5th page won’t be graded\nNOTE: You can submit a draft (for feedback only) by the due date on Canvas\n\nYou will submit the report as Quarto (.qmd) file\n\nThe output format: should be either docx, pdf (traditional way using LaTeX), or typst (brand new way to create a .pdf)\n\nI would strongly recommend docx for most students\n\nSee the Quarto documentation for word output\n\nIf you’re feeling more adventurous, have a go with typst\n\nSee the Quarto documentation for typst output\nYou’ll need Quarto 1.4, which came out after the start of the class, see me if you need help installing it\n\nIf you want to cause yourself unnecessary misery and frustration by doing it the old fashioned way, use pdf\n\nSee the Quarto documentation for pdf output\n\n\nOptional: If your analysis code becomes long, you might want to submit accompanying .R scripts that are source()-ed as discussed in the Quarto Lesson\n\n\n\nRequired Report Content\n\nBefore the introduction of your document, a section called “Instructions to Run” that states\n\nWhere your data is from (a link if preferable)\nHow to download it\nWhere to save it in order for the code to run\nE.g., For this project I used two .csv data files from IPEDS survey year 2019, institutional characteristics HD2019 and public finance F1819_F1A. These can be downloaded from here by clicking on the named files name under “Data Files”. To run this code, save these files in a sub-folder called “data” that sits in the same folder as this .qmd file.\n\nWell commented code (either in the .qmd file or a source()-ed .R script) that:\n\nReads in all your raw data (must read in EXACTLY what downloads from following your instructions above)\nPerforms all data wrangling tasks to clean, join, and reshape your data as necessary for your project\nCreates:\n\nRequired: 3 or more plots with ggplot2\nRequired: 1 or more overall descriptive statistics table(s) made with summarize\nRequired: 1 or more other summary table(s) made with summarize\nOptional: Basic statistics like t.test() or lm()\n\n\nWritten text that should be clearly structured with subheadings and describe:\n\nWhy this is interesting and/or important\n\nThis should be a single concise but convincing paragraph, think an “elevator pitch” argument as to why this matters\nThere should NOT be any lengthy literature review in this assignment, this is it\n\nWhy your chose your data source and what the data represents\nWhat analysis you did and why, in layman’s terms (not an R or stats expert)\nWhat each individual plot and table shows\nWhat you found overall\nAny limitations or future research\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\nReport Element\nCriteria\nPoints\n\n\n\n\nDoes it run?\n\nIf I hit render, does the code run and produce the output report without errors?\n\n4\n\n\nData Wrangling: Reading & Cleaning\n\nAre the “instructions to run” provided and correct?\nIs the data read in correctly?\nIs the data joined and/or reshaped correctly where necessary?\nIs the data cleaned where necessary with reasonable justification provided for subjective decisions?\n\n4\n\n\nData Wrangling: Analysis\n\nIs there at least 1 overall descriptive statistics summarize table in the report?\nIs there at least 1 additional summarize table in the report?\nDo the summary tables show interesting and relevant information to the report topic?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nData Visualization\n\nAre there at least 3 ggplot2 plots in the report?\nDo the plots show interesting and relevant information to the report topic?\nAre the plots aesthetically pleasing with good plot design, color choices, and labels?\nIs the code to produce the summary tables free of mistakes?\n\n4\n\n\nWritten Content\n\nDoes the written content address the points outlined above?\nDoes the report make a convincing argument?\nIs the text of the report generally well written and in layman’s terms?\nIs the text of the report well structured with subheadings?\n\n4",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "11-pro-functions.html#review-of-assignment--",
    "href": "11-pro-functions.html#review-of-assignment--",
    "title": "Functions & Loops",
    "section": "Review of Assignment <-",
    "text": "Review of Assignment &lt;-\n\nIn essence, both of these skills are built off something we have been doing this whole class, assignment\nWe have assigned data\n\n\ndf &lt;- haven::read_dta(\"data/hsls-small.dta\")\n\n\nWe have assigned plots\n\n\nplot &lt;- ggplot(df) +\n  geom_histogram(aes(x = x1txmtscor))\n\n\nWe have assigned summary tables\n\n\ndf_sum &lt;- df |&gt;\n  summarize(mean = mean(x1txmtscor, na.rm = T))\n\n\nWe even assigned results\n\n\nuf_age &lt;- 2024 - 1853\n\n\nEverything we are going to cover today comes back to this basic principle, things being assigned names\n\nWe will be working with more than one assigned or named thing at once, which gets confusing at first, but it always comes back to this idea",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#why-write-loops-functions-dry-vs-wet-code",
    "href": "11-pro-functions.html#why-write-loops-functions-dry-vs-wet-code",
    "title": "Functions & Loops",
    "section": "Why Write Loops & Functions: DRY vs WET Code",
    "text": "Why Write Loops & Functions: DRY vs WET Code\nThe watchwords for this lesson are DRY vs WET:\n\nDRY: Don’t repeat yourself\nWET: Write every time\n\nLet’s say you have a three-step analysis process for 20 files (read, lower names, add a column). Under a WET programming paradigm in which each command gets its own line of code, that’s 60 lines of code. If the number of your files grows to 50, that’s now 150 lines of code — for just three tasks! When you write every time, you not only make your code longer and harder to parse, you also increase the likelihood that your code will contain bugs while simultaneously decreasing its scalability.\nIf you need to repeat an analytic task (which may be a set of commands), then it’s better to have one statement of that process that you repeat, perhaps in a loop or in a function. Don’t repeat yourself — say it once and have R repeat it for you!\nThe goal of DRY programming is not abstraction or slickness for its own sake. That runs counter to the clarity and replicability we’ve been working toward. Instead, we aspire to DRY code since it is more scalable and less buggy than WET code. To be clear, a function or loop can still have bugs, but the bugs it introduces are often the same across repetitions and fixed at a single point of error. That is, it’s typically easier to debug when the bug has a single root cause than when it could be anywhere in 150 similar but slightly different lines of code.\nAs we work through the lesson examples, keep in the back of your mind:\n\nWhat would this code look like if I wrote everything twice (WET)?\nHow does this DRY process not only reduce the number of lines of code, but also make my intent clearer?",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#setup",
    "href": "11-pro-functions.html#setup",
    "title": "Functions & Loops",
    "section": "Setup",
    "text": "Setup\nWe’ll use a combination of nonce data and the school test score data we’ve used in a past lesson. We won’t read in the school test score data until the last section, but we’ll continue following our good organizational practice by setting the directory paths at the top of our script.",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#for-loops",
    "href": "11-pro-functions.html#for-loops",
    "title": "Functions & Loops",
    "section": "for() Loops",
    "text": "for() Loops\n\nThe idea of loops if relatively simple\n\nTake a list of things\n\nfor(i in matts_list) {\n\nDo a set of things\n\nprint(i) }\n\n\nWait, but what’s i\n\ni is the most common word to use here, but we could call it anything\n\nIt is just the name we are assigning to the item (think i for item) in the list\n\n\nOkay, but what’s { and }\n\nSince we are doing one or more things for each i in the list\n\n\n\nmatts_list &lt;- c(\"Let's\", \"go\", \"Gators\", \"!\")\n\nfor(i in matts_list) { print(i) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nBut, we can use anything we want instead of i\n\n\nfor(word in matts_list) { print(word) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nLiterally anything\n\n\nfor(gator_egg in matts_list) { print(gator_egg) }\n\n[1] \"Let's\"\n[1] \"go\"\n[1] \"Gators\"\n[1] \"!\"\n\n\n\nAll we are doing is assigning a name to the item in the list\nWe can do the name thing with numbers\n\n\ngators_points_23 &lt;- c(11, 49, 29, 22, 14, 38, 41, 20, 36, 35, 31, 15)\n\nfor(i in gators_points_23) { print(i) }\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nAgain, we can literally use anything as the name\n\n\nfor(billy_napier in gators_points_23) { print(billy_napier) }\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nQuick exercise\nCreate a list of the names of every school you’ve attended, then use a for loop to print them out",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#adding-if-and-else-to-loops",
    "href": "11-pro-functions.html#adding-if-and-else-to-loops",
    "title": "Functions & Loops",
    "section": "Adding if() and else() to Loops",
    "text": "Adding if() and else() to Loops\n\nLoops that print things are all well and good, but really we want to be able to do a little more than that\nWe are going to use if() and else() to that\n\nRemember ifelse() from Data Wrangling?\nThis is just splitting that up, if() something if true, do this, else() do that\n\nLet’s just start with an if()\n\n\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    print(i)\n  }\n}\n\n[1] 49\n[1] 38\n[1] 41\n[1] 36\n[1] 35\n[1] 31\n\n\n\nNotice we only got scores if they were above 30\nNext, we can add an else() to say what to do if the score was not above 30\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    print(i)\n  } else {\n    print(i)\n  }\n}\n\n[1] 11\n[1] 49\n[1] 29\n[1] 22\n[1] 14\n[1] 38\n[1] 41\n[1] 20\n[1] 36\n[1] 35\n[1] 31\n[1] 15\n\n\n\nQuick Question: Is that the same list we had before? Why or why not?\n\n\nLet’s see how we can make it different\n\nWe are going to use a new command paste() which combines strings, then print() that\n\nTop tip: You’re going to want to use paste() in your homework as well\n\n\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    paste(\"Yay the Gators scored\", i, \"points, which is more than 30!\") |&gt; print()\n  } else {\n    print(i)\n  }\n}\n\n[1] 11\n[1] \"Yay the Gators scored 49 points, which is more than 30!\"\n[1] 29\n[1] 22\n[1] 14\n[1] \"Yay the Gators scored 38 points, which is more than 30!\"\n[1] \"Yay the Gators scored 41 points, which is more than 30!\"\n[1] 20\n[1] \"Yay the Gators scored 36 points, which is more than 30!\"\n[1] \"Yay the Gators scored 35 points, which is more than 30!\"\n[1] \"Yay the Gators scored 31 points, which is more than 30!\"\n[1] 15\n\n\n\nThen we can extend that same logic to the else() statement\n\n\nfor(i in gators_points_23) {\n  if(i &gt; 30) {\n    paste(\"Yay, the Gators scored\", i, \"points, which is more than 30!\") |&gt; print()\n  } else {\n    paste(\"Sad times, the Gators only scored\", i, \"points...\") |&gt; print()\n  }\n}\n\n[1] \"Sad times, the Gators only scored 11 points...\"\n[1] \"Yay, the Gators scored 49 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 29 points...\"\n[1] \"Sad times, the Gators only scored 22 points...\"\n[1] \"Sad times, the Gators only scored 14 points...\"\n[1] \"Yay, the Gators scored 38 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 41 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 20 points...\"\n[1] \"Yay, the Gators scored 36 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 35 points, which is more than 30!\"\n[1] \"Yay, the Gators scored 31 points, which is more than 30!\"\n[1] \"Sad times, the Gators only scored 15 points...\"\n\n\n\nThis may seem a little silly right now, but this fun example was just meant to show the basics of we are doing\nWe will cover these in a more serious way at the end of the lesson",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#writing-your-own-functions",
    "href": "11-pro-functions.html#writing-your-own-functions",
    "title": "Functions & Loops",
    "section": "Writing Your Own Functions",
    "text": "Writing Your Own Functions\n\nFunctions work much the same way as loops, whatever we say inside { } is done\nThe difference is that instead of doing it for each item in a list, we do it for a single input\nWe also have to use the function for it to work\n\nWe have been using functions all semester filter(), summarize(), mutate() are all functions just like the one we are going to make\n\nTo demonstrate, let’s make a function that prints a welcome message for students arriving at UF\n\n\nwelcome &lt;- function() { print(\"Welcome to UF!\") }\n\nwelcome()\n\n[1] \"Welcome to UF!\"\n\n\n\nTo do this, we need some data, so I am just going to make some up\n\ntribble() is just a way of making a tidyverse data frame, don’t worry about it for now, it’s not the main idea for the lesson\n\n\n\nfake_data &lt;- tribble(~ufid, ~name, ~dorm, ~first_class, ~meal_plan, ~roommate,\n                     1853, \"Jack\", \"Cyprus\", \"BIO-1001\", 1, \"Mike\",\n                     1854, \"Hailey\", \"Simpson\", \"BIO-1001\", 0, \"Jessica\",\n                     1855, \"Tamika\", \"Simpson\", \"CHEM-1002\", 1, \"Hannah\",\n                     1856, \"Jessica\", \"Simpson\", \"ARCH-1003\", 1, \"Hailey\",\n                     1857, \"Mike\", \"Cyrpus\", \"STA-1002\", 0, \"Jack\",\n                     1858, \"Hannah\", \"Simpson\", \"EDF-1005\", 1, \"Tamika\")\n\n\nFor our function to be able to work, it needs to be able to take an input, in this case UFID\n\nYou can imagine a much more sophisticated version of this could be used for automated dorm check in\n\nLet’s run our same function again, but adding ufid to the brackets, saying that it takes ufid as the only input\n\n\nwelcome &lt;- function(id) { print(\"Welcome to UF!\") }\n\nwelcome()\n\n[1] \"Welcome to UF!\"\n\n\n\nQuick question: It ran, but why did this not change anything?\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  print(student)\n  \n}\n\nwelcome(1853)\n\n# A tibble: 1 × 6\n   ufid name  dorm   first_class meal_plan roommate\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;   \n1  1853 Jack  Cyprus BIO-1001            1 Mike    \n\n\n\nOkay so that ran, but it spat a data frame, how can we make it more of a welcome message?\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  name &lt;- student |&gt; pull(name)\n  \n  paste(\"Welcome to UF\", name)\n  \n}\n\nwelcome(1853)\n\n[1] \"Welcome to UF Jack\"\n\n\n\nOkay, now we’re getting somewhere!\nLet’s add a bit more info to say where they live and what their first class will be\n\n\nwelcome &lt;- function(id) {\n  \n  student &lt;- fake_data |&gt; filter(ufid == id)\n  \n  name &lt;- student |&gt; pull(name)\n  dorm &lt;- student |&gt; pull(dorm)\n  first_class &lt;- student |&gt; pull(first_class)\n  \n  paste(\"Welcome to UF\", name, \"you will be living in\", dorm, \"and your first class is\", first_class)\n  \n}\n\nwelcome(1853)\n\n[1] \"Welcome to UF Jack you will be living in Cyprus and your first class is BIO-1001\"\n\n\n\nQuick exercise: Add to the above block of code, to also say who their roommate is",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#practical-example-batch-reading-files",
    "href": "11-pro-functions.html#practical-example-batch-reading-files",
    "title": "Functions & Loops",
    "section": "Practical Example: Batch Reading Files",
    "text": "Practical Example: Batch Reading Files\n\nRemember, to use a loop, we need a list to loop through\nTo get that, let’s use the list.files() function\n\nBy default, this will list files in the current working directory\nIf we want to list files in a different folder, we need to say where using a relative path from the working directory\n\nFor this, we want to list the by school files we used in Data Wrangling II, so we give it a path to that folder\n\nWe also include the argument full.names = TRUE\n\n\n\nQuick Exercise\nTry removing the full.names = TRUE see what the differences are, and think why we need to include it\n\n\nfiles &lt;- list.files(\"data/sch-test/by-school\",\n                    full.names = T)\n\n\nAs a starting point, let’s print out what this list looks like using a loop just like before\n\n\nfor(i in files) {\n  print(i)\n}\n\n[1] \"data/sch-test/by-school/bend-gate-1980.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1981.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1982.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1983.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1984.csv\"\n[1] \"data/sch-test/by-school/bend-gate-1985.csv\"\n[1] \"data/sch-test/by-school/east-heights-1980.csv\"\n[1] \"data/sch-test/by-school/east-heights-1981.csv\"\n[1] \"data/sch-test/by-school/east-heights-1982.csv\"\n[1] \"data/sch-test/by-school/east-heights-1983.csv\"\n[1] \"data/sch-test/by-school/east-heights-1984.csv\"\n[1] \"data/sch-test/by-school/east-heights-1985.csv\"\n[1] \"data/sch-test/by-school/niagara-1980.csv\"\n[1] \"data/sch-test/by-school/niagara-1981.csv\"\n[1] \"data/sch-test/by-school/niagara-1982.csv\"\n[1] \"data/sch-test/by-school/niagara-1983.csv\"\n[1] \"data/sch-test/by-school/niagara-1984.csv\"\n[1] \"data/sch-test/by-school/niagara-1985.csv\"\n[1] \"data/sch-test/by-school/spottsville-1980.csv\"\n[1] \"data/sch-test/by-school/spottsville-1981.csv\"\n[1] \"data/sch-test/by-school/spottsville-1982.csv\"\n[1] \"data/sch-test/by-school/spottsville-1983.csv\"\n[1] \"data/sch-test/by-school/spottsville-1984.csv\"\n[1] \"data/sch-test/by-school/spottsville-1985.csv\"\n\n\n\nOkay, that looks like what we want, a list of files to read in\nNow let’s try reading these\n\n\nfor(i in files) {\n  read_csv(i)\n}\n\n\nOkay, that read the files, but we didn’t save them anywhere, we to assign them\nNow, this gets a little tricky, as anything we assign with &lt;- in a loop only exists in a loop, so we two extra steps\n\nFirst, we read it in mostly like normal, into something called file , which is temporary and only exists within the loop\nSecond, we make another temporary object, which is the name we want to assign the data frame to\n\nFor now, let’s just use df_&lt;name of the file&gt;\n\nThird, we use assign(), which basically does what &lt;- would do in normal circumstances, but keep the object after the loop\n\nname &lt;- read_csv() becomes file &lt;- read_csv() then assign(name, file)\n\n\n\n\nfor(i in files) {\n  file &lt;- read_csv(i)\n  name &lt;- paste0(\"df_\", i)\n  assign(name, file)\n}\n\n\nCool! That seems to have worked, but, our df_ names are really really long, which probably isn’t that useful for future analysis\nSo, instead of simply pasting together df_ and i, let’s use our friend regular expressions to get something more usable\n\nGet the school name (anything that matches the options we give)\nGet the year (any digits)\nPaste those together with df_ to get our nicer object names\n\n\n\nfor(i in files) {\n  school &lt;- str_extract(i, \"niagara|bend|east|spot\")\n  year &lt;- str_extract(i, \"\\\\d+\")\n  name &lt;- paste0(\"df_\", school, year)\n  file &lt;- read_csv(i)\n  assign(name, file)\n}\n\n\nMuch better!\nOne last thing, instead of reading each of those into a new data frame, we could bind them all together\n\nNote: This is only appropriate here due to the nature of the school data, with each data frame having the school name and year in it, other times we may want to join data instead. Review Data Wrangling II for a refresher on how to bind and join data appropriately\n\nTo do this, we first need to set up an empty data frame (a.k.a., “tibble”), as otherwise we will get an error the first time through the loop, as we would be attempting to bind something that doesn’t exist\nThen, we simply run the loop like before, but use bind_rows() to stack each file onto the existing list\n\n\ndf_bind &lt;- tibble()\n\nfor(i in files) {\n  file &lt;- read_csv(i)\n  df_bind &lt;- bind_rows(df_bind, file)\n}\n\n\nGreat, that worked!\nFinally, what if we wanted to this only for certain schools?\n\n\nHint: this might be useful in the homework\n\n\nWith a loop, we do something for each item in the list\nSo do something only for certain schools, we want to change the list, not the loop\n\nIn this case, we can add a pattern to our list.files() function saying to only list files that match that pattern\n\nThis could be some fancy regex, but in our case, we just need any files that have the word “niagara” it their name\n\n\n\n\nfiles_niagara &lt;- list.files(\"data/sch-test/by-school\",\n                            full.names = T,\n                            pattern = \"niagara\")\n\ndf_niagara &lt;- tibble()\n\nfor(i in files_niagara) {\n  file &lt;- read_csv(i)\n  df_niagara &lt;- bind_rows(df_niagara, file)\n}\n\n\nLet’s see what our final output looks like\n\n\nprint(df_niagara)\n\n# A tibble: 6 × 5\n  school   year  math  read science\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Niagara  1980   514   292     787\n2 Niagara  1981   499   268     762\n3 Niagara  1982   507   310     771\n4 Niagara  1983   497   301     814\n5 Niagara  1984   483   311     818\n6 Niagara  1985   489   275     805\n\n\n\nPerfect!",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#summary",
    "href": "11-pro-functions.html#summary",
    "title": "Functions & Loops",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson we have mostly print()-ed our output in this lesson, because it’s one of the easiest ways to see what’s going on\nBut, you can use functions and loops for other things too, like modifying variables, reading data, etc.\n\nThis said, whenever I am writing a loop, I usually start by print()-ing what I am looping through, then make is more sophisticated from there\n\nLoops and Functions were something that I didn’t fully grasp until I had been using R for about a year, so I don’t expect you to get everything\n\nWhile confusing, they are super useful in the real world\n\nYou might see some variations of the loops we learned today such as\n\nwhile() loops\nfor() loops that use the index (line) number of the list instead\nWhat’s important is that the underlying logic remains the same\n\nTime permitting, let’s take a look a few examples of my code that uses loops and functions!",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#question-one",
    "href": "11-pro-functions.html#question-one",
    "title": "Functions & Loops",
    "section": "Question One",
    "text": "Question One\na) Using a for() loop, read in the school test files from data/sch-test/by-school/.\nb) Do the same, but, only for Bend Gate and Niagara\n\nHint: Looking inside the list.files() help file for somewhere you might use some regex\n\nc) Do it one more time, but include some code that add a column called file_path which contains the location where each row’s information is coming from\nd) Bind the data sets from part c) together",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#question-two",
    "href": "11-pro-functions.html#question-two",
    "title": "Functions & Loops",
    "section": "Question Two",
    "text": "Question Two\na) Read in hsls-small.dta\nb) Write a function that\n\nTakes a stu_id as the input\npaste()s back a sentence which says if() they ever attended college else() whether their parent expected them to go to college\n\n\nIf a student went to college it should return something like “this student went to college”\nIf they didn’t, it should return something like “this student did not go to college, their students did/did not expect them to”\n\n\nOptional: if() the student went to college, add a second sentence that says how many months they had between high school and going to college\nSuper-Optional: Following from that, edit the second sentence to include how that student’s delay compares to the average delay (you can pick, mean or median)",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "11-pro-functions.html#submission",
    "href": "11-pro-functions.html#submission",
    "title": "Functions & Loops",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Functions & Loops"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html#section-one-reproducibility-replicability",
    "href": "09-wrangle-iv.html#section-one-reproducibility-replicability",
    "title": "IV: Advanced tidyverse & Data Retrieval",
    "section": "Section One: Reproducibility & Replicability",
    "text": "Section One: Reproducibility & Replicability\nIn this first section we will discuss what reproducibility is, why is matters, and how automating data retrieval can help achieve it.\n\nWhat is reproducibility?\nThe “reproducibiity crisis” began to gather steam in STEM fields in the mid-2010s\n\nSpecific to educational research, a joint report from the National Science Foundation (NSF) and Institute of Education Sciences (IES) define reproducibility as\n\n“the ability to achieve the same findings as another investigator using extant data from a prior study”.\n\nIn other words, can another researcher get the same results as you if they start with the same data?\n\nQuick Question\nBased on the video we just watched and your prior knowledge, why are we concerned about reproducibility?\n\nAccording to the NSF/IES report, what can authors do to enhance reproducibility?\n\nMinimum/baseline level\n\nYour methods section should describe the analyses should describe the analyses you performed in enough detail that someone could broadly copy what you did\n\nHowever, rarely are you going to be able to describe enough to achieve exact reproducibility\n\nImagine detailing all your data cleaning decisions, specific model settings, etc. in the paper’s text\n\n\n\n\n\nTrue reproducibility\n\nIdeally, you use a public repositories to store data and code\n\nGitHub, Havard Dataverse, AERA’s openICPSR\n\n“Data used to support claims in publications should be made available in public repositories along with data processing and cleaning methods, relevant statistical analyses, codebooks as well as analytic code”\n\n\n\nCommon data sharing issues\n\nHowever, sharing data in public repos can be tricky, even if the data is public\n\nSize constraints (GitHub)\nLicense and right to share concerns\nAlso, you might accidentally upload an edited version\n\nIf you can’t share data yourself, you could provide clear instructions on how to retrieve the data (e.g., “go to this site”, “click on this option”, “select these 4 variables”, “click download”)\n\nThis can be time-consuming and error-prone\n\nThis is where automating data retrieval can help (more on that later)\n\n\n\n\nReproducibility vs Replicability\n\nYou may also here the word “replicability” used in the same context.\nSometimes the words are used interchangeably depending on the context.\nThe NSF/IES joint report suggest replication is a “higher bar” than reproducibility, where the same results can be achieved by entirely new studies.\n\n\nTo enhance replicability\n\nResults need to be truly generalizable to your study population\n\nSomewhat hard to know with a single study\n\nBest step you can take is to pre-register your study\n\nBecoming the gold standard in medical trials\nBefore you run your analysis (or even collect/explore your data) you make your analysis plan available\n\nEither publicly online or get it pre-approved for publication with a journal\n\nPrevents “p-hacking” and other temptations during the statistical analysis which can lead to non-replicable results\nAERA Open list “Registered Reports” as an option in their submission guidelines\n\nIn this workshop we will focus on a specific step towards achieving reproducibility: automating data retrieval?\n\n\nSmall Group Discussion Exercise (5 mins)\nFind of a recent piece of published work you’re already familiar with\n\nIf you wanted to, do you think you would be able to reproduce the results?\n\nIf you couldn’t, what would the author need to do to make it reproducible?\nIf you could, how easy do you think it would be?\n\n\n\n\n\n\nWhy does reproducibility matter?\nCited in the NSF/IES joint report, the a Subcommittee on Replicability and Science suggest reproducibility is the\n\n“Minimum necessary condition for a finding to be believable and informative”.\n\n\nResearch (at least from a post-positivist world view) should always be about trying to move us closer to understanding the truth about the world\nBeing able to reproduce research helps us in this goal by raising the level of trust we can have in results\n\nHonest mistakes are made in research, making it reproducible helps us know if there are any\nLess-than-honest findings can be hidden behind when work isn’t reproducible\n\nA common one that may seem harmless is only reporting partial results, those that fit the narrative\n\n\n\n\n\nIf I do all this, will anyone ever actually reproduce my work?\n\nA 2014 Educational Researcher article by Makel & Plucker shows significantly less educational research is devoted to reproduction and replication than in other fields, only 0.13%.\nWe are seeing more meta-analyses in education\n\nThese involve some replication, however, they serve a totally different purpose\n\nMeta-analyses aim to synthesize overall findings across multiple studies\nReproduction and replication aim to test the quality and trustworthiness of a single study\n\n\nMore reproduction and replication studies would help raise the credibility of education research among scientific researchers\n\nEven if we don’t want to go out and start reproducing and replicating other studies, we can help the cause by ensuring our studies are as reproducible as possible.\n\n\nSmall Group Discussion Exercise (5 mins)\nOkay, we get it, reproducibility is awesome… But sometimes it isn’t possible\n\nThink of a specific study or scenario (real or fictional) where true open reproducibility (e.g. sharing data) is either not possible or not practical\nIn that scenario, what can you do to aid reproducibility and/or replication?\n\n\n\n\nHow does automating data retrieval achieve reproducibility?\n\nAbove we discussed how reproducibility is “the ability to achieve the same findings as another investigator using extant data from a prior study”\nWe also discussed how in order to make work reproducible researchers should publish code and data where possible\nAutomating data retrieval makes this process easier and gets around common issues in sharing secondary data\nAll we have to provide a researcher is our code and the data is “automagically” downloaded for them\n\nHelps us ensure the reproducing researcher has the correct data\n\nNo worries about file names, sharing restrictions, or size limits\n\nProvides additional trust to the researcher that the data is coming directly from the source and hasn’t been edited/doctored\n\nIn next two sections of the workshop we are going to cover tools that we can use to automate data retrieval in our work",
    "crumbs": [
      "Data Wrangling",
      "IV: Advanced `tidyverse` & Data Retrieval"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html#section-two-using-ipedtas-to-automagically-retrive-labelled-ipeds-data-in-stata-r",
    "href": "09-wrangle-iv.html#section-two-using-ipedtas-to-automagically-retrive-labelled-ipeds-data-in-stata-r",
    "title": "IV: Advanced tidyverse & Data Retrieval",
    "section": "Section Two: Using IPEDtaS to Automagically Retrive Labelled IPEDS Data in Stata & R",
    "text": "Section Two: Using IPEDtaS to Automagically Retrive Labelled IPEDS Data in Stata & R\nFor this section we will walk through the tutorial page for IPEDtaS which can be found at capaldi.info/IPEDtaS/tutorial",
    "crumbs": [
      "Data Wrangling",
      "IV: Advanced `tidyverse` & Data Retrieval"
    ]
  },
  {
    "objectID": "09-wrangle-iv.html#section-three-other-tools-packages-for-automated-data-retrieval",
    "href": "09-wrangle-iv.html#section-three-other-tools-packages-for-automated-data-retrieval",
    "title": "IV: Advanced tidyverse & Data Retrieval",
    "section": "Section Three: Other Tools & Packages for Automated Data Retrieval",
    "text": "Section Three: Other Tools & Packages for Automated Data Retrieval\nOkay, hopefully you will find the IPEDtaS script useful in future research, but, IPEDS is not the only common source of open data for education research (there are some other ways to automatically retrieve IPEDS too).\nIn the remaining session time we are going to cover a handful of API’s the can be used to download data from common educational research sources.\n\nUS Census APIs & tidycensus\n\nWe’re going to use this API in our final Data Viz lesson at the end of the semester, so time dependent, we may save this for this for then!\n\n\nSo what exactly is an API?\n\nIn short, think of it as a way of R going to a website/database and pulling data directly from the server-side or back end, without our having to ever interact with the website directly\nThis has two main advantages\n\nWe don’t have to store the large dataset on our computer\nOur work becomes instantly more reproducible\n\n\nMost APIs require that you use some kind of key that identifies you as an authorized user\n\nTypically you need to set up the key the first time you use the API, but helpfully, it’s usually possible to store the key on your computer for all future use\nMany API keys are free to obtain and use\n\nIf you were using an API to access a private database such as Google Maps, you might need to pay for your key to have access depending on how much you use it\nBut because we are using Census data, which is freely available to the public, there’s no charge\n\n\n\n\nGetting Census API Key\n\nGetting your Census API key is extremely easy\n\nSimply go here\nEnter your organization name (e.g., University of Florida)\nEnter your school/work email\n\nYou will quickly receive an email with your API key, which you will need in a moment\n\n\nThere are a number of R packages that access the Census API\nPersonally, I think tidycensus is the easiest to use, particularly if you generally use a tidyverse version R\nYou can check out the full documentation website for tidycensus here\n\n\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\n\n\nNext, we want to set our census API key in R, this is a convenient feature that you only have to do once\n\n\ncensus_api_key(\"&lt;key&gt;\", install = T)\n\n\nOkay, with that, we are ready to download our data!\n\nNote, the process is generally similar for a lot of APIs; get a key, set a key, go!\n\nFor this example, we are just going to pull down the % of 25+ population with a bachelor’s degree and the % of families below the poverty line for every county in the great state of Minnesota\n\nThese variables are measured in the Census Bureau’s American Community Survey, which are survey responses collected over 3 and 5 year intervals\n\nMore current and have more variables than the decennial Census\n\n\nFor the sake of time, I have already found the variable names\n\nIf you want to look them up in the future, use this Census API Variable List\nYou can change the year in url to access different years\n\nDP02_0065PE is % of 25+ population with a bachelor’s degree\nDP03_0119PE is % of families below the poverty line\nThere are multiple main tidycensus functions that you can use to call in data, with each calling data from a different source operated by the US Census Bureau\n\nget_acs()\nget_decennial()\nget_estimates()\nget_flows()\nget_pop_groups()\nget_pums()\n\nYou can see more information about these on the tidycensus reference page\nFor today’s example we are going to use get_acs()\n\nThis collects data from the American Community Survey (regular sampled surveys of demographic data across the US)\n\nWe are going to assign &lt;- the data we pull down into the object data:\n\n\ndata &lt;- get_acs(geography = \"tract\",\n                state = \"MN\",\n                year = 2022,\n                survey = \"acs5\",\n                variables = c(\"DP02_0065PE\", \"DP03_0119PE\"), # Pop &gt;=25 with Bachelors, Pop below poverty line\n                output = \"wide\",\n                geometry = FALSE)\n\nNow, we could look at these in a quick plot\n\nggplot(data) +\n  geom_point(aes(x = DP03_0119PE,\n                 y = DP02_0065PE))\n\n\n\n\n\n\n\n\nBy using either of these packages for research with Census data, you get a convinient way to access Census data, plus, you make your work infinitely more reproducible in the process\n\n\n\nUrban Institute Education Data Explorer\n\nAnother API that might be useful is the Urban Institute’s Education Data Explorer\n\nThis actually has access to a bunch of education datasets including;\n\nCommon Core of Data\nThe Civil Rights Data Collection\nSmall Area Income and Poverty Estimates\nEDFacts\nIntegrated Postsecondary Education Data System\nCollege Scorecard\nNational Historical Geographic Information System\nFederal Student Aid\nNational Association of College and University Business Officers\nNational Center for Charitable Statistics\nModel Estimates of Poverty in Schools (MEPS)\nEquity in Athletics Data (EADA)\nCampus Safety and Security\n\n\nNow, the Urban Institute have done a lot of work behind the scenes to make this data easier to work with\n\nIn a lot of cases, this will be nice, but as you will see below, it has a couple of issues associated with it\n\nTo demonstrate the pros and cons of this, let’s download the same data we just used in the IPEDtaS section of the workshop\n\nTo use educationdata in R, we first will install their package\n\nYou can consult the reference page for the educationdata R package\n\n\ninstall.packages(\"educationdata\")\nlibrary(educationdata)\n\nThere is no API key for education data, so, we can go straight into downloading data\n\ndata_info &lt;- get_education_data(level = \"college-university\",\n                           source = \"ipeds\",\n                           topic = \"directory\",\n                           filters = list(year = 2021),\n                           add_labels = TRUE)\n\ndata_aid &lt;- get_education_data(level = \"college-university\",\n                           source = \"ipeds\",\n                           topic = \"sfa-by-tuition-type\",\n                           filters = list(year = 2021),\n                           add_labels = TRUE)\n\nNow, if we look\n\nnrow(data_info)\n\n[1] 6289\n\nnrow(data_aid)\n\n[1] 16206\n\n\nOur data_aid appears to be in long format, which if you inspect the data frame, you’ll see if due to tuition_type being in long format\nTo get the out of state tuition percentage we need to either reshape of filter the data down\n\ndata_aid &lt;- data_aid |&gt;\n  filter(tuition_type == \"Out of state\")\n\nnrow(data_aid)\n\n[1] 1596\n\n\n\nHmm… So… Now our data is too short\n\nWhy is that?\nAll we know for sure is that some colleges have been dropped\n\nIt could be that all private colleges didn’t get a value for out of state tuition\nIt could be that any college that didn’t report out of state tuition didn’t get a value\nIt could be any other reason\n\n\nFor some, this might seem nice, they’re only giving you the data they have in a clean format\n\nFor me, it’s dangerous\n\nWe don’t know why these data are missing, if they simply weren’t reported that’s fine, but, a researcher needs to know what is missing and why\n\nSimply deleting missing values is often a terrible idea, for reasons beyond the scope of this workshop\nThis data tool makes a decision on that behind the scenes\n\nThere is probably documentation on this somewhere on the website, but, I don’t like the fact they do it at all\n\n\n\n\nBeyond the deletion issues, another thing I really don’t agree with was their decision to change the names of the variables away from the original IPEDS names\n\nThey did this with the intent of making the data easier to work with\nHowever, there are a number of drawbacks to this approach\n\nIt’s harder/requires guesswork to check the dictionary to see details of a variable\nWith similar variables, especially things they have derivied, it’s not always clear which variables from the main IPEDS files you’d need to use to get the same values\nMost researchers who use IPEDS use the complete data files, so, those are the variable names people are familiar with\nYou can’t easily integrate it with existing code\nBy keeping the original names but adding variable labels, the IPEDtaS data gives us more information about the variable too\n\n\nIt’s also worth noting (as you can see above) some of the data is long in format, which can be a little extra work\nLastly, a subtle but important difference is educationdata is a secondary API, meaning the Urban Institute have a collect the data, reformat it, then upload to their servers\n\nA big drawback is the data is not kept up to date (e.g., directory information for 2023 is not yet available 11/20/2024, when it is available directly from IPEDS)\nAs we talked about earlier, some IPEDS files are reported a year ahead/behind others depending on what the information is (characteristics of that current school year vs retention rate from previous school year), so, this also becomes an issue that intersects with the lack of clear dictionary to check\n\n\n\n\nAIR’s EdSurvey Package\nThe final API I’d like to cover today is the EdSurvey package, which is produced by the American Institute of Research (a large research organization who are responsible for a number of education projects including NAEP)\nThis is only available as a R package (as far as I’m aware) and has two broad purposes 1. Automate retrieval of various NCES survey and assessment data files 2. Modeling functions designed specifically for these data sets (e.g., defaults and settings that work best with the sampling designs/weights)\nLike most packages, the first step is to install and load the package\n\ninstall.packages(\"EdSurvey\")\nlibrary(EdSurvey)\n\nA lot of the surveys in the EdSurvey package might be less common in higher education research (ECLS_K, NAEP, etc.), but, the public version of HSLS has some information on college access, so, let’s look at how we’d download that\nThis is actually quite a limited function, as you can only download the whole thing (which is 325mb), so it take a bit of time and space\n\ndownloadHSLS(\".\")\n\n\nOnce it’s downloaded and saved, you can open the file using the readHSLS function\n\nThis isn’t actually reading in data you can use yet, it’s a prepatory step designed prevent hogging your RAM by storing every variable in your data frame\n\n\n\nhsls &lt;- readHSLS(\"HSLS/2009\")\n\n\nThe final step is to retrieve the variables you want out of this data\n\nFor instance, if all I wanted to keep was whether a student ever attended college alongside their own and parental expectations about college, I would do this\n\n\n\ndata_hsls &lt;- getData(hsls, \n                     varnames = c(\"x4evratndclg\", \"x1paredexpct\"))\n\n\nNow we can look at distributions of these\n\n\ndata_hsls_plot &lt;- data_hsls |&gt;\n  group_by(x4evratndclg) |&gt;\n  count(x1stuedexpct) |&gt;\n  mutate(sum = sum(n),\n         perc = n/sum*100) |&gt;\n  select(x4evratndclg, x1stuedexpct, perc)\n\nggplot(data_hsls_plot) +\n  geom_col(aes(y = perc, x = x1stuedexpct, fill = x4evratndclg),\n           position = \"dodge\") +\n  scale_fill_manual(values = c(\"pink2\", \"navy\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, size = 6)) +\n  labs(x = \"Student Educational Expectations in Wave 1\",\n       y = \"% of Students Respondants\",\n       fill = str_wrap(\"Did the Student Ever Attend College by Wave 4\", 30))\n\n\n\n\n\n\n\n\n\n\nDownloading Data Using a URL\nLastly, if you’re working with data that isn’t part of one of these large datasets with an API, that doesn’t mean you can’t necessarily automate your data retrieval\n\nThe first step IPEDtaS takes is to simply download a file using the url copied from the IPEDS complete data files page\n\nThis can be done easily in both Stata and R, so long as you’re able to obtain the url that actually triggers the download (sometimes this can involve immediately hitting ctrl/c or cmnd/c when a download starts)\n\nSince we are in Minneapolis, home of Target, I found this data set of Target store locations\n\n“https://www.kaggle.com/api/v1/datasets/download/ben1989/target-store-dataset”\n\n\nTo download a URL in R, you simply have to add a URL and a name for the file once it’s downloaded (most often, you will be downloading a zip file)\n\ndownload.file(url = \"https://www.kaggle.com/api/v1/datasets/download/ben1989/target-store-dataset\",\n              destfile = \"data/target-data.zip\")\n\nYou then need to unzip it\n\nunzip(\"data/target-data.zip\", exdir = \"data/\")\n\nYou can then read it in like any other csv file\n\ndata_target &lt;- read_csv(\"data/targets.csv\")\n\nAnd perform analyses with it\n\ndata_target |&gt;\n  count(SubTypeDescription)\n\n# A tibble: 4 × 2\n  SubTypeDescription     n\n  &lt;chr&gt;              &lt;int&gt;\n1 City                   9\n2 SuperTarget          242\n3 TargetExpress         56\n4 &lt;NA&gt;                1522\n\n\nThat concludes the section of the class on automating data retrieval!\nI hope you gained an appriciation for why we should try to automate data retrieval when possible and some understanding of a handful of tools available to do so!\nThe final bonus section of this lesson is an exploration of the links between tidyverse and an extremely improtant data retrieval language SQL. This is all bonus content, but, if you’re planning on a career in institutional research, SQL is a must-have skills, so I encourage you to take a look and explore some of the other resources listed in the next tab!",
    "crumbs": [
      "Data Wrangling",
      "IV: Advanced `tidyverse` & Data Retrieval"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#quarto-documentation",
    "href": "07-quarto-intro.html#quarto-documentation",
    "title": "Reproducable Reports with Quarto",
    "section": "Quarto Documentation",
    "text": "Quarto Documentation\n\nQuarto also comes with one of the best sets of documentation I’ve come across on their website, check it out here\n\nFun fact, the whole website is a massive Quarto project, which is kind of meta!\n\nRather than trying to re-explain something they put so clearly, we will look to this site for guidance throughout this lesson",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#why-quarto-over-word",
    "href": "07-quarto-intro.html#why-quarto-over-word",
    "title": "Reproducable Reports with Quarto",
    "section": "Why Quarto over Word?",
    "text": "Why Quarto over Word?\n\nUp until now, you’ve likely written most of your assignments using a Microsoft Word or something similar\nThere are a few major reasons Quarto has most replaced Word for 90% of my work\n\nIntegration with code/analysis\n\n\nThis is the main one\nAs you’ll see in this lesson, Quarto seamlessly integrates text with code output\n\nSo any tables and plots you create will be integrated into the report\n\nNo more spending hours copying your output from one screen to another and likely typing some of it wrong!\n\nYou can even have the text you type update\n\nAs a result, if when reviewing your final paper you spot an error right at the start of your analysis, as you often will (I did with my first publication), you just correct it and everything else automatically updates\n\n\nReproducibility/Transparancy\n\n\nAs the code and text are integrated, it makes it easier to share and reproduce your results, there’s no way someone could fake a figure/table without it being obvious\n\n\nZotero integration\n\n\nThe way Quarto’s visual editor integrates with Zotero on your laptop is superb, Word does integrate with Zotero as well, but it’s not as smooth\nZotero is an open source reference manager\n\nIf you aren’t using a reference manager, you 110% should be\n\nZotero is a fantastic choice, as it’s open-source and free, check it out here\n\n\nIf you have zotero on your computer, to cite anything in your Zotero library, you just type @ and you can search and add any article you have saved\n\n\nVersion control with git\n\n\nAs Quarto .qmd files are plain text/code files, they’re very small and work well with git for version control\n\nSee the git extra credit lesson for more information on git\n\n\n\nOne file, variety of formats\n\n\nSometimes, you need to share your work in a variety of formats\n\nWord and PDF is pretty easy, you can just save as…\nBut what if you’re asked to publish your research to a website?\nWhat if you want to work on HiPerGator so need your work in as .ipynb Jupyter Notebook?\n\nQuarto enables you to take the same work and switch formats easily and even publish to multiple formats at the same time\nYou can also make presentations from as PowerPoint, PDF, or my new favorite, RevealJS\n\nAgain, if you fix a mistake, all your tables and plots will be corrected automatically, a lifesaver 2 hours before you go on stage at ASHE!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#text-portions-of-a-quarto-report",
    "href": "07-quarto-intro.html#text-portions-of-a-quarto-report",
    "title": "Reproducable Reports with Quarto",
    "section": "Text Portions of a Quarto Report",
    "text": "Text Portions of a Quarto Report\n\nNo matter how good your data viz skills are, the majority of most reports are still going to be text\nText in Quarto reports is written as markdown text (that’s what the md part of .qmd stands for)\nMarkdown is a relatively simple plain text way of formatting text\nQuarto provide a great overview of markdown basics on their site here, let’s take a look\n\n\n\nQuarto Visual Editor\n\nHowever, Quarto also has a visual editor, which looks like a simplified Microsoft Word, with point and click options for everything\n\nYou can toggle between source and visual modes at the top left of any .qmd file in RStudio\nAll the visual editor does is write actual markdown based on what you clicked\n\nIf you want to learn markdown, flipping between the two is a great way to do that\n\n\nIn my day to day life - If you start using Quarto on a regular basis, you’ll inevitably become familiar with markdown basics, as it is so much faster!\n\nIf I’m writing extended passages of text, I usually use the visual editor (primarily for the Zotero referencing feature I metioned above)\n\nIf you know markdown syntax (e.g., headings, bold, etc.), you can type it into the visual editor rather than clicking on options\n\nIf I’m writing, I generally prefer the source editor\n\nFor this class, you should be able to use the visual editor for most of your needs, no need to learn markdown!\n\n\nQuick Excercise\n\nInside the Quarto template you downloaded at the top of the lesson write a few sentances descrbing your proposal for your reproducible report\nAdd an appropriate header (e.g., “Introduction”)\nAdd bold face around your dependent variable\nMake your data source name italic",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#code-portions-of-a-quarto-document",
    "href": "07-quarto-intro.html#code-portions-of-a-quarto-document",
    "title": "Reproducable Reports with Quarto",
    "section": "Code Portions of a Quarto Document",
    "text": "Code Portions of a Quarto Document\n\nThe text is all well and good, but, the real power of Quarto comes from the ability to integrate code into work\nThere are two main ways of doing this\n\nInline code chunks\nSourcing .R scripts\n\n\n\nInline Code Chunks\n\nFor shorter chunks of code such as printing tables and plots, or short calculations, you can create code chunks in the .qmd file\n\nThese are like you cut an .R script up into little pieces and put them in between text\nThey still run like one continuous .R script, from top to bottom\n\nAnything you &lt;- assign in an earlier chunk will be available in later chunks\n\nThe results/output of code in a chunk will print where the chunk is in the text in the rendered document\nYou can add a code chunk by clicking on the insert code button along the top, or, by keyboard shortcut\n\ncommand + option + I on Mac\nctrl + alt + I on PC\n\nOnce inserted, you can name chunks for cross-referencing and set executation options to change if/how they output results, both of which are beyond the scope of this class\n\n\n\nLet’s start out by reading in data, then making a simple ggplot of math test scores\n\nSince we want to see the results, we will just call it rather than assigning it to an object\nNote: We need to load packages in our quarto script every time\n\nUnless they are loaded by a script we source (see below)\n\n\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\nggplot(data = df) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nQuick Excercise\n\nAdd a new code chunk to your Quarto document\nRead in the all-schools.csv from the sch-test folder\nMake a line ggplot for math scores with colored lines for each school\n\n\n\n\n\n\n\n\n\n\n\n\nOkay, those are printing, but last week we put so much work into creating a pretty plot, do I really have to copy all that code?\n\nThe answer is no!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#sourcing-scripts",
    "href": "07-quarto-intro.html#sourcing-scripts",
    "title": "Reproducable Reports with Quarto",
    "section": "Sourcing scripts",
    "text": "Sourcing scripts\n\nTechnically, you could do everything in code chunks, from data joining and cleaning right through to analysis\n\nBut, this will get really long, and make it harder to skip around and navigate the text of your document while you’re editing\n\nA better solution is to source() plain R scripts that have already been written\nWhen you do this, any objects created in those scripts will then be in the environment and able to be added seamlessly to your Quarto document\nTo demonstrate, let’s source() our R script from last week’s lesson\n\nThere are two things that need to be right for this to work\n\nThe script has to be able to be run top to bottom with no errors\n\nAnything that creates an error needs to be ## commented out\n\nYou need to know where the file is\n\nIn our case, because we used a slightly messy kitchen approach (see our setup lesson on reading data for a reminder) all our scripts should be in the top level of your project folder, so you should be able to source them by name\n\nIf they’re somewhere else, you can use file.path() like we use for data etc.\n\n\n\n\n\n\nsource(\"06-viz-ii.R\")\n\n\nNow, with that script sourced, we can call objects we created in that script, and they will appear in our Quarto document\n\nFor example, if we source() a script that joins and cleans data and leaves it assigned to an object df_clean, we can then call df_clean in a code chunk and it will be waiting\nRemember right at the end of last week we saved our final plot to patch and I said why was a surprise, this is why!\n\nWe simply call patch and our fancy patchwork will print out beneath the chunk\n\nNote, since we loaded the patchwork library in the script we sourced, we don’t need to load it here\n\n\n\n\n\npatch\n\n\n\n\n\n\n\n\n\nThis logic is really useful to keep long streams of code out of your Quarto document\nYou also probably have lot’s of code already written in an .R script, and this is much easier than copy and pasting it all\n\n\nQuick Exercise\n\nsource() 04-wrangle-ii.R\nprint out the data object we made called data_long\n\n\n\nprint(data_long)\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# ℹ 62 more rows\n\n\n\nThat covers the basics of quarto document content! No matter what output you choose, this all stays the same!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#yaml-header-options",
    "href": "07-quarto-intro.html#yaml-header-options",
    "title": "Reproducable Reports with Quarto",
    "section": "YAML Header Options",
    "text": "YAML Header Options\n\nThe final thing to be aware of is the YAML header, that stuff at the top of the .qmd file\nIn your template, I have set some basic things up that you can change\n\nThere are so, so, so, many options, many of which change with the format you are using\nSome basic ones to be aware of for now are\n\ntitle: kind of self-explanatory\nauthor: again, kind of self-explanatory\ndate: this can take a specific date, or, it “today” will use the current date when you render the document\neditor: visual defaults to the visual editor, easier for now\neditor_options:\n\nchunk_output_type: console Just means that when editing the document and running code chunks, the output will appear in the console (like it always has with R scripts) rather than appearing as a preview in the script\n\nThis is a personal preference, feel free to play around!\n\n\nformat: this is power of Quarto, there are so many options which will be discussed next\nexecute: these are the default way we want code chunks to output\n\necho: FALSE means don’t print out the code (e.g., this site use echo: TRUE)\nmessage: FALSE means don’t print out information like when tidyverse reads in data\n\nThere’s a bunch more of these options, setting them in the YAML Header makes that the default behavior for the document, you can set them at the chunk level to only apply to that chunk\nYou don’t need to alter these for this class, but if you want to learn more, see executation options documentation\n\n\n\n\n\n\nQuick Exercise\n\nChange the title to something appropriate for your final report\nChange the author to your name",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#quarto-output-formats",
    "href": "07-quarto-intro.html#quarto-output-formats",
    "title": "Reproducable Reports with Quarto",
    "section": "Quarto Output Formats",
    "text": "Quarto Output Formats\n\nQuarto has so many different output formats you can choose from\n\nThe majority fall into three main categories\n\nhtml\npdf\nMicrosoft Office (docx and pptx)\n\nWe will mostly focus of office output for this class, but let’s do a quick overview of the others first\n\n\n\nhtml\n\nhtml is the file format that web browsers read for almost every website you visit\n\nFun fact: this entire website is built with Quarto as html output\n\nClick on the little GitHub icon in the lower right corner to see to .qmd files\n\nYou can include fancy content like interactive graphics\n\nUnfortunately, the majority of work in academia (for now at least) still relies on traditional paper-based document formats, so we can’t justify spending much time on this today\nIf you want to learn more about html output, start with the Quarto documentation on Quarto Websites and Reveal.js Presentations\n\n\n\npdf\n\nThe first paper-based document format is straight to .pdf\nTraditionally, this has been dominated by LaTeX\n\nStudents under Dr. Skinner’s version of the class used this\nPersonally, I find LaTeX slow, unintuitive, and inflexible\n\nIn Quarto 1.4 (the version that came out a couple of weeks ago), support for a new much more user-friendly pdf generator typst has been added\n\nI haven’t had chance to play around with this yet, but from first impressions I’m hopefull!\n\nYou can also create pdfs straight from Quarto via word using an R script like this which I use for the syllabus\n\n\n\ndocx\n\ndocx output is, IMHO, the best starting place\nWhen you render your Quarto document, the result is a .docx file that you can then open in word or send to a supervisor/advisor\n\nFor example, if I’m running the data analysis for a project, I can create all the plots and tables, write up the methods section, then pass to my advisor to fill in the literature sections\n\nSure, straight to .pdf is nice is everyone is working in Quarto, but often (sadly) they won’t be\n\n.docx is a just more practical option a lot of the time\n\nYou still get a document rendered with all the tables and plots which will update if the data changes upstream\n\n\nPlus, even if you craft the entire document in Quarto, it can be nice to have the option to run Grammarly on it in word, or tweak one little layout feature you can’t figure out how to get right in Quarto\nThis is the output format I’d like you to use for your reproducible reports (unless you can convince me you need a different output)\n\nSo let’s take a look through the Quarto documentation for .docx output\n\n\n\nAs I say, the Quarto documentation is fantastic, I’m not asking you to do anything beyond the default docx output for the class - But, if you want to, docx output is formatted by a template docx file, read more about those in the Word templates documentation",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#rendering-quarto-reports",
    "href": "07-quarto-intro.html#rendering-quarto-reports",
    "title": "Reproducable Reports with Quarto",
    "section": "Rendering Quarto Reports",
    "text": "Rendering Quarto Reports\n\nTo see the magic happen, we simply have to hit the “Render” button at the top of the Quarto file\n\nThis will go from top to bottom of our file turning the text/markdown sections into formatted text, and the code output to\n\n\n\nA Common Trap when Rendering Quarto Reports\n\nOne thing that people always get tripped up on is when something runs while editing the Quarto document, and doesn’t when rendering/running it\n\nThe main thing to be aware of is that when it starts to render, it will work with an entirely clean environment\n\nIf you have anything (a plot, a dataframe) saved in your environment you can access that when running the chunks one by one\n\nWhen it renders, anything you call needs to created, either by source()-ing a script that makes it, or by making it within an earlier code chunk\nIf you move chunks around, make sure you don’t try and call something before it’s been assigned!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#basic-tables-and-kables",
    "href": "07-quarto-intro.html#basic-tables-and-kables",
    "title": "Reproducable Reports with Quarto",
    "section": "Basic Tables and Kable()s",
    "text": "Basic Tables and Kable()s\n\nOne of the most common things you need when writing reports and/or papers are tables, whether than be descriptive statistics, results of a regression model, or even qualitative information about participants\nThere are many ways of creating tables in R\n\nIf you want to get fancy with customization, gt and gtsummary offer more advanced customization options\nIf you’re looking for easy and consistent output from regressions and other models stargazer might be what you’re looking for\nBut for now, we will focus on basic markdown tables and kable\n\nkableextra is also an option if you want to customize kables outside what the basic package allows\n\n\n\n\nBasic Markdown Tables\n\nBasic markdown tables are somewhat like tables in word, you manually fill them in\n\nYou could use inline code in one, but you will see why that would rarely be necessary when we look at kable\n\nBefore Quarto’s visual editor, making basic markdown tables was a real pain\n\nThey basically involve using a series of ————, ::::, and |||| to craft a table\n\nIf you’re interested you can see more on the Quarto table documentation\n\nWith the visual editor, however, you can just click on “Table” from the drop down menu, and write out the contents of your table\n\nThis is great for certain things, such as a table where we describe something qualitatively, such as syllabus for this class (which uses a markdown table).\n\n\n\nTables from data frames with kable\n\nSo far in this class, there have been plenty of times we have used tables to answer questions, when we have had them print out the console, e.g.\n\n\n## Read in using .dta so we have nice labels\ndf &lt;- haven::read_dta(\"data/hsls-small.dta\") |&gt;\n  drop_na(x1txmtscor)\n\ndf |&gt;\n  summarize(mean(x1txmtscor))\n\n# A tibble: 1 × 1\n  `mean(x1txmtscor)`\n               &lt;dbl&gt;\n1               51.1\n\n\n\nThis is kind of a table, but, when we render our Quarto document it doesn’t look great\nLuckily, the fix is really easy\n\nFirst, we load the knitr package\nSecond, we just pipe our output into kable()\n\n\n\nlibrary(knitr)\n\ndf |&gt;\n  summarize(mean(x1txmtscor)) |&gt;\n  kable()\n\n\n\n\nmean(x1txmtscor)\n\n\n\n\n51.10957\n\n\n\n\n\n\nkable() is relatively basic in terms of customization, but, it can do most things you’re going to need\nFor now, we will address the two most obvious issues with this table, the column names and the rounding (or lack thereof)\n\nWe just need to add two simple arguments to our kable()\n\n\n\ndf |&gt;\n  summarize(mean(x1txmtscor)) |&gt;\n  kable(col.names = c(\"Mean of Math Score\"),\n        digits = 2)\n\n\n\n\nMean of Math Score\n\n\n\n\n51.11\n\n\n\n\n\n\nkable() will turn any data you pass to it into a table, let’s make a slightly more interesting summary table\n\nNotice, we will use as_factor() to get the labels to show up\n\nfactor() allows us to make a factor and apply our own labels\nas_factor() works with haven labeled data and gets the labels out\n\n\n\n\ndf |&gt; \n  group_by(as_factor(x1region), as_factor(x1sex)) |&gt;\n  summarize(mean = mean(x1txmtscor),\n            median = median(x1txmtscor),\n            min = min(x1txmtscor),\n            max = max(x1txmtscor)) |&gt;\n  kable(col.names = c(\"Region\", \"Sex\", \"Mean\", \"Median\", \"Min\", \"Max\"),\n        digits = 2,\n        caption = \"Math Score by Region and Sex\")\n\n\nMath Score by Region and Sex\n\n\nRegion\nSex\nMean\nMedian\nMin\nMax\n\n\n\n\nNortheast\nMale\n52.13\n51.97\n24.9468\n82.1876\n\n\nNortheast\nFemale\n52.22\n52.00\n25.5304\n78.9298\n\n\nMidwest\nMale\n51.23\n51.18\n24.0999\n82.1876\n\n\nMidwest\nFemale\n51.22\n50.94\n24.7966\n82.1876\n\n\nSouth\nMale\n51.01\n51.02\n24.0180\n82.1876\n\n\nSouth\nFemale\n51.07\n50.86\n24.5797\n81.0926\n\n\nWest\nMale\n50.04\n49.80\n24.0744\n82.1876\n\n\nWest\nFemale\n50.26\n50.19\n25.0437\n78.9298\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#advanced-descriptive-statistics-tables-with-gtsummary",
    "href": "07-quarto-intro.html#advanced-descriptive-statistics-tables-with-gtsummary",
    "title": "Reproducable Reports with Quarto",
    "section": "Advanced Descriptive Statistics Tables with gtsummary",
    "text": "Advanced Descriptive Statistics Tables with gtsummary\n\nOften, a simple summarize() and kable() will produce the descriptive statistics output we want\nHowever, the gtsummary package has a tbl_summary() which I really like to create some more complicated/customized table\n\nIn particular, I like how it can handle continuous and categorical variables differently within the same table\n\nLet’s take a look a smaller descriptive statistics table looking at Math scores and region\n\nAt it’s most simple, we just select the columns we want to summarize and pipe |&gt; into tbl_summary()\n\n\n\ndf |&gt;\n  select(x1txmtscor, x1region) |&gt;\n  tbl_summary()\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 21,4441\n\n\n\n\nX1 Mathematics standardized theta score\n51 (45, 58)\n\n\nX1 School geographic region\n\n\n\n\n    1\n3,331 (16%)\n\n\n    2\n5,695 (27%)\n\n\n    3\n8,705 (41%)\n\n\n    4\n3,713 (17%)\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\n\nBy default, you can see it provides median and interquartile range for continuous variables, and counts with percentages for categorical variables\n\nThis alone would be hard to produce by hand\n\nNow, let’s be more specific about what kind of statistics we want to see\n\n\ndf |&gt;\n  select(x1txmtscor, x1region) |&gt;\n  tbl_summary(type = all_continuous() ~ \"continuous2\",\n              statistic = c(all_continuous() ~ c(\"{mean}\",\n                                                 \"{sd}\",\n                                                 \"{min} to {max}\")))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 21,4441\n\n\n\n\nX1 Mathematics standardized theta score\n\n\n\n\n    Mean\n51\n\n\n    SD\n10\n\n\n    Min to Max\n24 to 82\n\n\nX1 School geographic region\n\n\n\n\n    1\n3,331 (16%)\n\n\n    2\n5,695 (27%)\n\n\n    3\n8,705 (41%)\n\n\n    4\n3,713 (17%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\ntype = all_continuous() ~ \"continuous2\"\n\nThis just means I want continuous variables to be given more than one line in the table\n\nstatistic = c(all_continuous() ~ c(\"{mean}\", \"{sd}\", \"{min} to {max}\")\n\nThis spells out exactly what statistics I want and how to lay them out\n\nall_continuous() ~ means for continuous variables, do this\n\nSimilarly, if you want to change how categorical variables are described you would use all_categorical() ~\n\nYou’ll then see than anything in {} is a statistic I want, and line breaks, words, and punctuation outside the {} are included as typed\n\n{mean} gives the mean and {sd} gives the standard deviation\n{min} to {max} gives the minimum value, the word “to”, then the maximum value\n\n\n\n\n\n\nLastly, by default gtsummary produces pretty html based tables, these play nicely with some foramts, but not others\nIf you’re having trouble getting the table to appear in the format you’re rendering to, gtsummary has a collection of functions to convert the table to different formats\n\nHere, we will use as_kable() to print the same table as above, but as a kable (just like we would make)\n\nThis will sacrifice some of the details and styling, but it’s a good option if you want need simplicity/compatibility, or, just want it match other kable()s you already made\n\n\n\n\ndf |&gt;\n  select(x1txmtscor, x1region) |&gt;\n  tbl_summary(type = all_continuous() ~ \"continuous2\",\n              statistic = c(all_continuous() ~ c(\"{mean}\",\n                                                 \"{sd}\",\n                                                 \"{min} to {max}\"))) |&gt;\n  as_kable()\n\n\n\n\nCharacteristic\nN = 21,444\n\n\n\n\nX1 Mathematics standardized theta score\n\n\n\nMean\n51\n\n\nSD\n10\n\n\nMin to Max\n24 to 82\n\n\nX1 School geographic region\n\n\n\n1\n3,331 (16%)\n\n\n2\n5,695 (27%)\n\n\n3\n8,705 (41%)\n\n\n4\n3,713 (17%)\n\n\n\n\n\n\nThis is very much beyond the expectation for summary tables in your final reports, but I wanted to show how you might want to think about descriptive tables for future publishable work\nThere is so much more to gtsummary than this, we will see this package again in our Bringing It All Together (Feat. Basic Models) Lesson\nIf you’re interested in learning more check out gtsummary’s reference website",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#tips-for-technical-writing-citations",
    "href": "07-quarto-intro.html#tips-for-technical-writing-citations",
    "title": "Reproducable Reports with Quarto",
    "section": "Tips for Technical Writing & Citations",
    "text": "Tips for Technical Writing & Citations\n\nOne of the most frustrating things in Microsoft Word is technical aspects of writing such as equations needed for quantitative research articles\n\nBy no means do you need to write equations in your project for this class, but this is something you’ll need in future stats classes\n\nAs always, the Quarto documentation does a great job of explaining the basics of technical writing, so let’s take a look!\n\nI mentioned this above, but this page also discusses the Zotero integration in a bit more depth as well",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#question-one-to-be-done-in-class",
    "href": "07-quarto-intro.html#question-one-to-be-done-in-class",
    "title": "Reproducable Reports with Quarto",
    "section": "Question One (to be done in class)",
    "text": "Question One (to be done in class)\na) Write a few sentences describing your proposal for your reproducible report\nb) Add an appropriate section header (e.g. “Overview”)\nc) Add bold face around your dependent variable\nd) Make your data source name italic\ne) Add an appropriate title and author name to the YAML header",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#question-two-to-be-done-after-class",
    "href": "07-quarto-intro.html#question-two-to-be-done-after-class",
    "title": "Reproducable Reports with Quarto",
    "section": "Question Two (to be done after class)",
    "text": "Question Two (to be done after class)\na) Read in at least one data file you intend to use for your final project\nb) Write a few sentences describing where you got the data from\nc) Add an appropriate header (e.g., “Data”)\nd) Summarize one or more continuous variable including\n\nmean\nstandard deviation\nminimum\nmaximum\n\ne) Summarize one or more categorical variables including\n\ncount for each category\npercentage of count for each category\n\n\nHint: Remember how to get percentages from assignment 2\n\nf) Save the file as reproducible-report.qmd\nCongratulations, you’ve officially started the Quarto document for your final project!",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "07-quarto-intro.html#submission",
    "href": "07-quarto-intro.html#submission",
    "title": "Reproducable Reports with Quarto",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Reproducible Reports with Quarto"
    ]
  },
  {
    "objectID": "05-viz-i.html#progress-check-in",
    "href": "05-viz-i.html#progress-check-in",
    "title": "I: Basics",
    "section": "Progress Check-In",
    "text": "Progress Check-In\n\nIn keeping with the theme of today’s lesson, we made a little plot that represents roughly the breakdown of “core R content” covered in each lesson (as in, the basic knowledge you need to go off and do your own data work)\n\nCongratulations, after the first two data-wrangling lessons, you’re already half-way done!\nOver the next three weeks, we will cover how to communicate the results of these basic skills using visualization with ggplot2 and report generation with quarto\nThen, we will get into the final series of lessons, each of which gives you a little taste of some more sophisticated and/or niche applications of R\nThroughout all this, we will continue to become familiar with the basics of data wrangling, as that really is the key to it all\n\nAny questions or concerns?",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#setup",
    "href": "05-viz-i.html#setup",
    "title": "I: Basics",
    "section": "Setup",
    "text": "Setup\n\nOne key part of understanding your data and presenting your analyses lies in making plots, which we will cover through this section of the class\n\n\nChoosing a Plot\n\nThe main problem does not lie in the creation of a chart. R can do (almost) any chart you can dream up. The diffculty consists in thinking hard about what is it that you want to show to the reader. What is your message? What do you want to show?\n\nWhat types of variables do we want to visualize?\nHow many variables?\n\n\n\n\n\n# variables\nType of variables\nPlot type\n\n\n\n\nOne\nCategorical\nbar chart\n\n\n\nContinuous\nhistogram\n\n\n\n\ndensity plot\n\n\n\n\nbox plot\n\n\nTwo\nCat - Cat\ngrouped bar chart\n\n\n\nCat - Con\ngrouped box plot\n\n\n\nCon - Con\nscatterplot\n\n\n\n\nline plot\n\n\n\n\nMany more graphs. Feel free to explore R graph gallery.\nThere are multiple graphing systems in R, but we are going to focus on two (primarily one);\n\nbase R plots\nggplot2\n\n\n\n\nBase R Plots\n\nBase R (i.e., R without any packages loaded) can create some basic plots\n\nThey aren’t the prettiest, so we wouldn’t recommend them in reports and papers\n\nBut, they’re super-easy and quick to create, so they’re perfect for quickly checking your data during the exploration phase\n\n\n\n\n\nggplot2 Plots\n\nggplot or ggplot2 (ggplot was originally a different library, but doesn’t exist anymore)\nThe gg stands for grammar of graphics\n\nThis is a whole world of detail to dive into if you want\nThe basic idea is that the graphs are made up of layers\n\nThis allows us to create some really cool and detailed plots\n\nStrongly recommend ggplot2 for making plots you want to share\n\nEven stata users admit the plots can’t match ggplot2\n\n\n\n\n\n\n\nLibraries\n\nWe’re using two libraries today (plus the base R plot functions)\nggplot2\nThe ggplot2 library is part of the tidyverse\n\nWe don’t need to load it separately (we can just use library(tidyverse) as always)\n\n\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n\nWe’re also going to use haven,\nHaven allows us to read in data files from other software such as SPSS, SAS, and Stata\n\nWe’ll use it to read in a Stata (*.dta) version of the small HSLS data we’ve used before\n\nThe Stata version, unlike the plain .csv version, has labels for the variables and values, which will be useful when plotting\n\n\nHaven is also part of the tidyverse but not loaded by default\nWe could load the package with library(haven), but, if we only need one function from a package, it’s often easier to call it directly (plus this is useful trick to know)\n\nlibrary(&lt;package&gt;) pre-loads all the functions so that we can easily call them with just the function name\nIf you only need one function from a package, or, you want to use a function from a package with a conflict (i.e., two packages with functions with the same name) we can specify where the function should come from like this\n\n&lt;package&gt;::&lt;function&gt;\nI.e., haven::read_dta()\n\n\n\n\n## ---------------------------\n##' [Input data]\n## ---------------------------\n\n## read_dta() ==&gt; read in Stata (*.dta) files\ndata_hs &lt;- haven::read_dta(\"data/hsls-small.dta\")\n## read_csv() ==&gt; read in comma separated value (*.csv) files\ndata_ts &lt;- read_csv(\"data/sch-test/all-schools.csv\")\n\n\nNote that since we have two data files this lesson, we gave them unique names instead of the normal data:\ndata_hs := hsls-small.dta\ndata_ts := all-schools.csv",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-base-r",
    "href": "05-viz-i.html#plots-using-base-r",
    "title": "I: Basics",
    "section": "Plots using base R",
    "text": "Plots using base R\n\nEven though new graphics libraries have been developed, the base R graphics system remains powerful\nThe base system is also very easy to use in a pinch\n\nGood for a quick visual of a data distribution that’s just for ourselves\n\nNote that for the next few plots, let’s not worry too much about how they look\n\nSpecifically, the axis labels won’t look very nice\n\nAlso note that we’ll have to switch to using the base R data frame $ notation to pull out the columns we want\n\nIn short, to reference a column in a data frame in base R, you say dataframe$column\nIf you need some more information on using $ notation, check out the supplemental lesson on data wrangling with base R.\n\n\n\nHistogram\n\nFor continuous variables, a histogram is a useful plot\nThough the hist() function has many options to adjust how it looks\n\nThe default settings work really well if you just want a quick look at the distribution.\n\n\n\n## histogram of math scores (which should be normal by design)\nhist(data_hs$x1txmtscor)\n\n\n\n\n\n\n\n\n\nQuick exercise\nCheck the distribution of the students’ socioeconomic score (SES).\n\n\n\nDensity\n\nDensity plots are also really helpful for checking the distribution of a variable\nBase R doesn’t have formal density plot function, but you can get a density plot with a trick\n\nplot() the density() of a continuous variable\n\n\n\nQuick question: What does the na.rm = TRUE and why might we need it?\n\n\n## density plot of math scores\ndensity(data_hs$x1txmtscor, na.rm = TRUE) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\nQuick exercise\nFirst, plot the density of SES Then, add the col argument in plot() to change the color of the line to \"red\"\n\n\n\nBox plot\n\nA box plot will let you see the distribution of a continuous variable at specific values of a categorical variable\n\nFor example, test scores ranges at each student expectation level\n\nCall a box plot using the boxplot() function\n\nThis one is a little trickier because it uses the R formula construction to set the continuous variable against the discrete variable\nThe formula uses a tilde, ~, and should be constructed like this:\n\n&lt;continuous var&gt; ~ &lt;discrete var&gt;\nWe will talk in more detail about formulas in the Programming: Modeling Basics lesson\n\nAs we are using a formula, notice how we can use the data = data_hs argument instead of adding data_hs$ in front of the variable names, which saves some typing\n\n\n\n## box plot of math scores against student expectations\nboxplot(x1txmtscor ~ x1stuedexpct, data = data_hs)\n\n\n\n\n\n\n\n\nFrom the boxplot, we can see that math test scores tend to increase as students’ educational expectations increase (remember that 11 means “I don’t know [how far I’ll go in school]”), though there’s quite a bit of overlap in the marginal distributions.\n\n\nScatterplot\n\nPlot two continuous variables against one another using the base plot() function\nThe main way to make a scatter plot is plot(x, y)\n\nThe x is the variable that will go on the x-axis and y the one that will go on the y-axis\n\n\n\n## scatter plot of math against SES\nplot(data_hs$x1ses, data_hs$x1txmtscor)\n\n\n\n\n\n\n\n\nFrom the scatter plot we see the data seem to show a positive correlation between socioeconomic status and math test score, there’s also quite a bit of variation in that association (notice that the cloud-like nature of the circles).\n\nQuick exercise\nRerun the above plot, but this time store it in an object, plot_1, then get the plot to print out",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#plots-using-ggplot2",
    "href": "05-viz-i.html#plots-using-ggplot2",
    "title": "I: Basics",
    "section": "Plots using ggplot2",
    "text": "Plots using ggplot2\n\nggplot2 is R users’ primary system for making plots\nIt is based on the idea of a grammar of graphics\n\nJust as we can use finite rules of a language grammar to construct an endless number of unique sentences, so too can we use a few graphical grammatical rules to make an endless number of unique figures.\n\nThe ggplot2 system is too involved to cover in all of its details\n\nBut that’s kind of the point of the grammar of graphics\nOnce you see how it’s put together, you can anticipate the commands you need to build your plot.\n\nWe’ll start by covering the same plots as above.\n\n\nHistogram\nAs the main help site says, all ggplot2 plots need three things:\n\n[data]: The source of the variables you want to plot\n[aesthetics]: How variables in the data map onto the plot (e.g., what’s on the x-axis? what’s on the y-axis?)\n[geom]: The geometry of the figure or the kind of figure you want to make (e.g., what do you want to do with those data and mappings? A line graph? A box plot?…)\nWe’ll start by making a histogram again\nTo help make these pieces clearer, we’ll use the argument names when possible\n\nAs you become familiar, you probably will stop naming some of these core arguments\n\nThe first function, which initializes the plot is ggplot()\n\nIts first argument is the data, which want to use data_hs\n\n\n\n## init ggplot \nggplot(data = data_hs)\n\n\n\n\n\n\n\n\n\n…nothing! Well, not nothing, but no histogram.\n\nThat’s because the plot knows the data but doesn’t know what do with it. What do we want?\n\nSince we want a histogram, we add the geom_histogram() function to the existing plot object with a plus sign(+). Once we do that, we’ll try to print the plot again…\nThe aesthetic mappings, that is, which variables go where or how they function on the plot, go inside the aes() function.\n\nSince we only have one variable, x1txmtscor, it is assigned to x.\n\n\n\n## add histogram instruction (notice we can add pieces using +)\nggplot(data = data_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\nSuccess!\n\nQuick excercise: try assigning the historgram to an object, then getting it to print out\n\n\nAs you can see, the code to make a ggplot2 figure looks a lot like what we’ve seen with other tidyverse libraries, e.g. dplyr.\nThe key difference between ggplot2 and our previous code, however, is that\n\nUp to now we have used the pipe (|&gt;) to pass output to the next function\nggplot2 uses a plus sign (+) add new layers to the plot\n\nLayers is exactly how you want to think about ggplots\n\nThe ggplot() is the base layer of the graph\n\nAnything you place in here will become the default for every other layer\n\nFor example, if we say data = data in ggplot(), that will be the default data for every layer\n\nGenerally we will specify the data here, and mapping = aes() in the specific plots\n\n\n\n\n\n\nQuick question(s): Why might that make sense? Which is more likely to change? Are there times you might want to set an aesthetic for the whole plot?\n\n\n\nDensity\n\nUnlike the base R graphics system, ggplot2 does have a density plotting command geom_density()\n\nThe rest of the code remains the same as for geom_histogram()\n\n\n\n## density\nggplot(data = data_hs) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nQuick exercise\nIf we wanted to see the histogram and density plot on top of each other, what might we do? Give it a go. Why didn’t it work?\n\n\nggplot(data = data_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor)) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nThe issue is that the histogram y scale is much much larger than the density\n\nTo fix that, let’s modify the geom_histogram() aesthetic to use the density function rather than the raw counts\n\nWe use the after_stat() function, which basically means after ggplot calculates the statistics, it converts them to density\n\n\n\n\n## histogram with density plot overlapping\nggplot(data = data_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor, y = after_stat(density))) +\n  geom_density(mapping = aes(x = x1txmtscor))\n\n\n\n\n\n\n\n\n\nIt worked, but it’s not the greatest visual since the colors are the same and the density plot is thin with no fill.\nAdding to what came before, the geom_histogram() and geom_density() both take on new arguments that change the defaults\nNow the resulting plot should look nicer and be easier to read\n\n\n## histogram with density plot overlapping (add color to see better)\nggplot(data = data_hs) +\n  geom_histogram(mapping = aes(x = x1txmtscor, y = after_stat(density)),\n                 color = \"black\",\n                 fill = \"white\") +\n  geom_density(mapping = aes(x = x1txmtscor),\n               fill = \"red\",\n               alpha = 0.2)\n\n\n\n\n\n\n\n\n\nQuick exercise\nTry changing some of the arguments in the last plot. What happens when you change alpha (keep the value between 0 and 1)? What does the color argument change? And fill? What happens if you switch the geom_*() functions, call geom_histogram() after you call geom_density()?\n\n\nA critical thing to note, in the previous plot color, fill, and alpha were all outside the aes()\n\nThis means they take a single value and apply it uniformly, it should portray no information and just change the appearance\nTo use these elements to portray information, we need to place the arguments inside aes() like we will do in the next plot\n\n\n\n\nTwo-way\n\nPlotting the difference in a continuous distribution across groups is a common task\nLet’s see the difference between student math scores between students with parents who have any postsecondary degree and those without.\nSince we’re using data that was labeled in Stata, we’ll see the labels when we use count()\n\n\n## see the counts for each group\ndata_hs |&gt; count(x1paredu)\n\n# A tibble: 7 × 2\n  x1paredu                                         n\n  &lt;dbl+lbl&gt;                                    &lt;int&gt;\n1  1 [Less than high school]                    1010\n2  2 [High school diploma or GED]               5909\n3  3 [Associate's degree]                       2549\n4  4 [Bachelor's degree]                        4102\n5  5 [Master's degree]                          2116\n6  7 [Ph.D/M.D/Law/other high lvl prof degree]  1096\n7 NA                                            6721\n\n\n\nWe can see that all values of x1paredu greater than 2 represent parents with some college credential\n\nSince we want only two distinct groups, we can use mutate, ifelse and the operator &gt;= to make a new 0/1 binary variable. - If a value of x1paredu is above 3, then the new indicator pared_coll will be 1; if not, 0.\n\n\nNOTE that in the Stata version of hsls_small, all the missing values, which are normally negative numbers, have already been properly converted to NA values. That’s why we see a count column for NA and not labels for missingness that we might have expected based on prior lessons.\n\nThe ggplot() function doesn’t need to use our full data\nIn fact, our data needs to be set up a bit differently to make this plot\n\nThis is a common thing people forget when plotting, data wrangling lessons one and two are your friend here\n\nWe’ll make a new temporary data object that only has the data we need.\nNotice, after we create pared_coll, we use the factor() command to make it a factor type of variable\n\nThis is R’s built in way of handling categorical variables (i.e., so that is doesn’t think it’s continuous)\nCreating factors is really useful for plotting, and later on for statistical models\n\n\n\n## need to set up data\nplot_data &lt;- data_hs |&gt;\n  ## select the columns we need\n  select(x1paredu, x1txmtscor) |&gt;\n  ## can't plot NA so will drop\n  drop_na() |&gt;\n  ## create new variable that == 1 if parents have any college, then make it a factor\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0),\n         pared_coll = factor(pared_coll)) |&gt;\n  ## drop (using negative sign) the original variable we don't need now\n  select(-x1paredu) \n\n## show\nhead(plot_data)\n\n# A tibble: 6 × 2\n  x1txmtscor pared_coll\n  &lt;dbl+lbl&gt;  &lt;fct&gt;     \n1 59.4       1         \n2 47.7       1         \n3 64.2       1         \n4 49.3       1         \n5 62.6       1         \n6 58.1       1         \n\n\n\nTo plot against the two groups we’ve made, we need to add it to the aesthetic feature, aes()\nThe math score, x1txmtscor, is still mapped to x\nSince we want two side-by-side histograms, we set the fill aesthetic to our new indicator variable\n\n\n## two way histogram\nggplot(plot_data) +\n  geom_histogram(aes(x = x1txmtscor,\n                     fill = pared_coll),\n                 alpha = 0.5,\n                 color = \"black\")\n\n\n\n\n\n\n\n\n\nBy assigning pared_coll to the fill aesthetic, we can see a difference in the distribution of math test scores between students whose parents have at least some college and those whose parents do not\n\nNote: there are more students with no parental college education, so that whole histogram is bigger\n\nIf we want to compare the shape of distribution more easily, we should use geom_density()\n\n\n\n\n## two way density\nggplot(plot_data) +\n  geom_density(aes(x = x1txmtscor,\n                   fill = pared_coll),\n               alpha = 0.5,\n               color = \"black\")\n\n\n\n\n\n\n\n\n\nQuick question\nWhy does the color = \"black\" not mean we have two black density/histogram plots? What happens if you remove it? Can you make it &lt;something else&gt; = \"black\" to get rid of the colors?\n\n\n\nBox plot\n\nBy this point, you’re hopefully seeing the pattern in how ggplot2 figures are put together\nTo make a box plot, we need to add a y mapping to the aes() in addition to the x mapping\nWe’ve also added the same variable to fill as we did to x\n\nWe do this so that in addition to having different box and whisker plots along the x-axis, each plot is given its own color\n\nNotice: this time, we just threw factor() around the variable in the plot, rather than using mutate to change the data\n\n\nQuick question: What do you think the pros and cons of using factor() in the plot over mutating the data might be?\n\n\n## box plot using both factor() and as_factor()\nggplot(data = data_hs,\n       mapping = aes(x = factor(x1paredu),\n                     y = x1txmtscor,\n                     fill = factor(x1paredu))) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nIn a way, this plot is similar to the dual histogram above\n\nBut since we want to see the distribution of math scores across finer-grained levels of parental education, the box and whisker plot is clearer than trying to overlap seven histograms.\n\n\n\nQuick exercise\nWe will get more into making things look pretty in Data Vizualization II, but, what is a real problem with this graph? Does it even need to be there?\n\n\n\nScatterplot\n\nTo make a scatter plot, make sure that the aes() has mappings for the x axis and y axis and then use geom_point() to plot.\nTo make things easier to see (remembering the over-crowded cloud from the base R plot above), we’ll reduce the data to 10% of the full sample using sample_frac() from dplyr\nWe’ll also limit our 10% to those who aren’t missing information about student education expectations\n\n\n## sample 10% to make figure clearer\ndata_hs_10 &lt;- data_hs |&gt;\n  ## drop observations with missing values for x1stuedexpct\n  drop_na(x1stuedexpct) |&gt;\n  ## sample\n  sample_frac(0.1)\n\n## scatter\nggplot(data = data_hs_10) +\n  geom_point(mapping = aes(x = x1ses, y = x1txmtscor))\n\n\n\n\n\n\n\n\n\nNow that we have our scatter plot, let’s say that we want to add a third dimension\n\nSpecifically, we want to change the color of each point based on whether a student plans to earn a Bachelor’s degree or higher\n\nThat means we need a new dummy variable that is 1 for those with BA/BS plans and 0 for others.\n\n\n\nWe can look at the student base year expectations with count():\n\n## see student base year plans\ndata_hs |&gt;\n  count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct                                     n\n   &lt;dbl+lbl&gt;                                    &lt;int&gt;\n 1  1 [Less than high school]                      93\n 2  2 [High school diploma or GED]               2619\n 3  3 [Start an Associate's degree]               140\n 4  4 [Complete an Associate's degree]           1195\n 5  5 [Start a Bachelor's degree]                 115\n 6  6 [Complete a Bachelor's degree]             3505\n 7  7 [Start a Master's degree]                   231\n 8  8 [Complete a Master's degree]               4278\n 9  9 [Start Ph.D/M.D/Law/other prof degree]      176\n10 10 [Complete Ph.D/M.D/Law/other prof degree]  4461\n11 11 [Don't know]                               4631\n12 NA                                            2059\n\n\n\nWe see that x1stuedexpct &gt;= 6 means a student plans to earn a Bachelor’s degree or higher.\nBut since we need to account for the fact that 11 means “I don’t know”, we need to make sure our test includes x1stuedexpct &lt; 11\nRemember from a prior lesson that we can connect these two statements together with the operator &\nLet’s create our new variable\n\nNotice this time when we create the factor, we specify levels and labels\n\nThis applies labels much like the haven version of our data has, which will print out in our plot\n\n\n\n\n## create variable for students who plan to graduate from college\ndata_hs_10 &lt;- data_hs_10 |&gt;\n  mutate(plan_col_grad = ifelse(x1stuedexpct &gt;= 6 & x1stuedexpct &lt; 11,\n                                1,        # if T: 1\n                                0),       # if F: 0\n         plan_col_grad = factor(plan_col_grad,\n                                levels = c(0, 1),\n                                labels = c(\"No\", \"Yes\")))      \n\n\nNow that we have our new variable plan_col_grad, we can add it the color aesthetic, aes() in geom_point().\n\n\n## scatter\nggplot(data = data_hs_10,\n       mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = plan_col_grad), alpha = 0.5)\n\n\n\n\n\n\n\n\n\nQuick exercise\nRemake the plot so the variables on each axis are flipped\n\n\n\nFitted lines\n\nIt’s often helpful to plot fitted lines against a scatter plot to help see the underlying trend\n\nThere are a number of ways to do this with the geom_smooth() function\n\n\n\nLinear fit\n\nSetting method = lm in geom_smooth() will fit a simple straight line of best fit with 95% confidence interval shaded around it.\nSince we want the points and the line to share the same x and y aesthetics, let’s put them in the ggplot() base layer\n\n\n## add fitted line with linear fit\nggplot(data = data_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n  geom_smooth(method = lm, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nLoess\n\nFinally, we can skip trying to adjust a linear line and just fit a LOESS curve, which is a smooth line produced by fitting a large number of local polynomial regressions on subsets of the data.\n\n\n## add fitted line with loess\nggplot(data = data_hs_10, mapping = aes(x = x1ses, y = x1txmtscor)) +\n  geom_point(mapping = aes(color = factor(plan_col_grad)), alpha = 0.5) +\n  geom_smooth(method = loess, color = \"black\")\n\n\n\n\n\n\n\n\n\nTo be clear, these semi-automated lines of best fit should not be used to draw final conclusions about the relationships in your data\nYou will want to do much more analytic work to make sure any correlations you observe aren’t simply spurious and that fitted lines are telling you something useful\nThat said, fitted lines via ggplot2 can be useful when first trying to understand your data or to more clearly show observed trends.\n\n\n\n\nLine graph\n\nWhen you want to show changes in one variable as a function of another variable\n\ne.g., changes in test scores over time\n\nA line graph is often a good choice.\nSince our hsls-small data is cross-sectional, we’ll shift to using our school test score data data_ts from Data Wrangling II\n\n\nQuick question: What does cross-sectional mean? What is the opposite of it?\n\n\nAs a reminder, here’s what the schools data looks like\n\n\n## show test score data\ndata_ts\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\nSimple Line-Graph\n\nTo keep it simple for our first line plot, we’ll filter our plot data to keep only scores for one school\n\nNotice how we can do that directly with pipes inside the ggplot() function\n\nWe want to see changes in test scores over time, so we’ll map\n\nyear to the x axis\nmath to the y axis\n\nTo see a line graph, we add geom_line().\n\n\n## line graph\nggplot(data = data_ts |&gt; filter(school == \"Spottsville\"),\n       mapping = aes(x = year, y = math)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nQUICK EXERCISE\nChange the school in filter() to “East Heights” and then “Bend Gate”.\n\n\n\nMultiple-Line Graphs\n\nEasy enough, but let’s say that we want to add a third dimension — to show math scores for each school in the same plot area. -To do this, we can map a third aesthetic to school. Looking at the help file for geom_line(), we see that lines (a version of a path) can take color, which means we can change line color based on a variable.\n\nThe code below is almost exactly the same as before less two things:\n\nWe don’t filter data_ts this time, because we want all schools\nWe add color = school inside aes()\n\n\n## line graph for math scores at every school over time\nggplot(data = data_ts,\n       mapping = aes(x = year, y = math, color = school)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nThis is nice (though maybe a little messy at the moment) because it allows us to compare math scores across time across schools.\nBut we have two more test types — reading and science — that we would like to include as well\nOne approach that will let us add yet another dimension is to use facets\n\n\n\n\nFacets\n\nWith facets, we can put multiple plots together, each showing some subset of the data\nFor example, instead of plotting changes in math scores across schools over time on the same plot area (only changing the color), we can use facet_wrap() to give each school its own little plot.\nCompared to the code just above, notice how we’ve removed color = school from aes() and included facet_wrap(~school)\n\nThe tilde (~) works like the tilde in plot(y ~ x) above: it means “plot against or by X”. In this case, we are plotting math test scores over time by each school.\n\n\n\n## facet line graph\nggplot(data = data_ts,\n       mapping = aes(x = year, y = math)) +\n  facet_wrap(~ school) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nIs this faceted plot better than the color line plot before it?\nWhether you use the first or the second would largely depend on your specific data and presentation needs.\nFaceting has a clearer advantage, however, when you want to include the fourth level of comparison:\n\nscores across\ntime across\nschools\ndifferent tests.\n\nTo make this comparison, we first need to reshape our data, which is currently long in year, to be long in test, too.\n\nAs we saw in Data Wrangling II, we’ll use pivot_longer() to place each test type in its own column (test) with the score next to it.\n\n\n\n## reshape data long\ndata_ts_long &lt;- data_ts |&gt;\n  pivot_longer(cols = c(\"math\",\"read\",\"science\"), # cols to pivot long\n               names_to = \"test\",                 # where col names go\n               values_to = \"score\")               # where col values go\n\n## show\ndata_ts_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQUICK EXERCISE\nIf we have 4 schools, 6 years, and 3 tests, how many observations should data_ts_long have in total? Does it?\n\n\nWith our reshaped data frame, we now reintroduce color into the aes(), this time set to test\nWe make one other change: y = score now, since that’s the column for test scores in our reshaped data\nAll else is the same.\n\n\n## facet line graph, with colour = test and ~school\nggplot(data = data_ts_long) +\n  geom_line(mapping = aes(x = year, y = score, color = test)) +\n  facet_wrap(~school)\n\n\n\n\n\n\n\n\n\nHmm, currently, each test score is on its own scale, which means this plot isn’t super useful\n\nThe difference between the types of score is so much greater than the variance within the test scores, we have 3 pretty flat lines\n\nBut maybe what we really want to know is how they’ve changed relative to where they started\n\nYou can imagine a superintendent who took over in 1980 would be keen to know how scores have changed during their tenure.\n\nTo see this, we need to standardize the test scores\n\nOften in statistics, you will mean-standardize a variable\n\nDifference between current score and the mean of the score, divided by the standard deviation of the score\n\nYou’ve probably heard of this as a “Z-score” in stats classes\n\n\n\\[\n\\frac{x_i - \\bar{x}}{\\sigma}\n\\]\n\nIn simple terms, this puts all scores on the same scale\nTo get the graph we want, we are going to do something very similar, but instead of using the mean of the score, we are going to take the score in 1980\n\nTherefore, every subsequent year will show the change since 1980, and since it’s standardized, all test scores will be on the same scale\n\n\n\ndata_ts_long_std &lt;- data_ts_long |&gt;\n  group_by(test, school) |&gt;\n  arrange(year) |&gt; \n  mutate(score_year_one = first(score),\n         ## note that we're using score_year_one instead of mean(score)\n         score_std_sch = (score - score_year_one) / sd(score)) |&gt;\n  ungroup()\n\nprint(data_ts_long_std, n = 13)\n\n# A tibble: 72 × 6\n   school        year test    score score_year_one score_std_sch\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Bend Gate     1980 math      515            515          0   \n 2 Bend Gate     1980 read      281            281          0   \n 3 Bend Gate     1980 science   808            808          0   \n 4 East Heights  1980 math      501            501          0   \n 5 East Heights  1980 read      318            318          0   \n 6 East Heights  1980 science   782            782          0   \n 7 Niagara       1980 math      514            514          0   \n 8 Niagara       1980 read      292            292          0   \n 9 Niagara       1980 science   787            787          0   \n10 Spottsville   1980 math      498            498          0   \n11 Spottsville   1980 read      288            288          0   \n12 Spottsville   1980 science   813            813          0   \n13 Bend Gate     1981 math      503            515         -1.07\n# ℹ 59 more rows\n\n\nLet’s walk through that code\n\nStart with our data_ts_long and assign the output to data_ts_long_std\nGroup the data by both test and school\n\n\nThis is for us to be able to get a starting point for each test at each school\n\n\nArrange the data in order of date (it already was, but since we are reliant on that, it’s best to check)\nCreate a new variable which is the first() score (since we are grouped by test and school, it will do it for each test/school combo)\nCreate another new variable score_std_sch using the score_year_one as zero\nUngroup the data since we are done with the calculation\n\n\n## facet line graph, with colour = test and ~school\nggplot(data = data_ts_long_std) +\n  geom_line(mapping = aes(x = year, y = score_std_sch, color = test)) +\n  facet_wrap(~school)\n\n\n\n\n\n\n\n\nAnd there we have a really informative plot, showing how test scores of all types have changed since 1980 across all four schools. Hopefully this shows that good plots and good data wrangling (and sometimes a little bit of stats) go hand in hand!",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#question-one",
    "href": "05-viz-i.html#question-one",
    "title": "I: Basics",
    "section": "Question One",
    "text": "Question One\na) How does student socioeconomic status differ between students who ever attended college and those who did not?",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#question-two",
    "href": "05-viz-i.html#question-two",
    "title": "I: Basics",
    "section": "Question Two",
    "text": "Question Two\na) How do educational expectations (of both students and parents) differ by high school completion status?\n\nHint: You will want to borrow some code from last week’s assignment here",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#question-three",
    "href": "05-viz-i.html#question-three",
    "title": "I: Basics",
    "section": "Question Three",
    "text": "Question Three\na) What is the relationship between student socioeconomic status and math test score?",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "05-viz-i.html#submission",
    "href": "05-viz-i.html#submission",
    "title": "I: Basics",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Visualization",
      "I: Basics"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "href": "03-wrangle-i.html#re-urgent-data-question-from-the-provost",
    "title": "I: Enter the tidyverse",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\n\nThroughout this lesson (and class), we are going to lean heavily on the tidyverse collection of packages\n\nIf you don’t already have this installed, use install.packages(\"tidyverse\")\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#check-working-directory",
    "href": "03-wrangle-i.html#check-working-directory",
    "title": "I: Enter the tidyverse",
    "section": "Check working directory",
    "text": "Check working directory\n\nThis script — like the one from the organizing lesson — assumes that the class folder is the working directory and that the required data file is in the data sub-directory\nIf you need a refresher on setting the working directory, see the prior lesson.\nNotice that we’re not setting (i.e. hard coding) the working directory in the script. That would not work well for sharing the code. Instead, we rely on relative paths once you know where you need to be and have gotten there\n\n\nReading Data in with read_csv()\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\n## data are CSV, so we use read_csv() from the readr library\ndata &lt;- read_csv(\"data/hsls-small.csv\")\n\nRows: 23503 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): stu_id, x1sex, x1race, x1stdob, x1txmtscor, x1paredu, x1hhnumber, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## alternatively, you can also use read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\n\nYou may notice the read_csv() prints out information about the data just read in. Nothing is wrong! The read_csv() function, like many other functions in the tidyverse, assumes you’d rather have more rather than less information and acts accordingly\nHere we assign our data to an object called data\n\nYou can call it whatever you want\n\nUp to now, this should all seem fairly consistent with last week’s lesson. This is just the set up, now it’s time to dive in!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#native-pipe-operator-in-r",
    "href": "03-wrangle-i.html#native-pipe-operator-in-r",
    "title": "I: Enter the tidyverse",
    "section": "Native Pipe Operator |> in R",
    "text": "Native Pipe Operator |&gt; in R\nThe pipe is one of the things that makes R code more intuitive than other programming languages, as it allows us write code in the order we think about it, passing it one from one function to another, rather than nesting it like traditional code would be written\n\nA Brief History of the R Pipe\n\n\n\nbadge\n\n\n\nThe pipe was originally a tidyverse invention, and used %&gt;% symbol, which is probably still a common pipe you see “in the wild”\nThe pipe we are using was brought into the Vanilla version of R a few years ago as |&gt; (the “native pipe”)\nThe reason for the change is some benefits that are beyond the scope of this class, but you just need to know that |&gt; and %&gt;% are essentially the same thing\n\nThe default shortcut for pipe (ctrl + shift + m) will generate %&gt;%, but you can change it to |&gt; by going to Tools &gt; Global Options &gt; Code &gt; Use native pipe operator, |&gt;\n\n\n\n\nHow the Pipe Works\nFor this example don’t worry about the actual processes (we will go over them more below), just look at how much more intuitive the code is with |&gt;s.\nFirst, let’s say we want to take the data we just read in and select the x1txmtscor (math test scores) column\n\n\nWithout |&gt;\n\n## Without |&gt;\nselect(data, x1txmtscor)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndata |&gt; select(x1txmtscor)\n\nNeither is that confusing… But, what if we want to take that output and select only students with a math score above 50?\n\n\nWithout |&gt;\n\n## Without |&gt;\nfilter(select(data, x1txmtscor), x1txmtscor &gt; 50)\n\n\n\nWith |&gt;\n\n## With |&gt;\ndata |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50)\n\nSee how the non-piped version is getting messy? Let’s add one more level to really make the point, creating a new variable that is the square root of the test score\n\n\nWithout |&gt;\n\n## Without |&gt;\nmutate(filter(select(data, x1txmtscor), x1txmtscor &gt; 50), square_root = sqrt(x1txmtscor))\n\n\n\nWith |&gt;\n\n## With |&gt;\ndata |&gt; select(x1txmtscor) |&gt; filter(x1txmtscor &gt; 50) |&gt; mutate(square_root = sqrt(x1txmtscor))\n\nAs we are getting longer, let’s use a new line for each pipe, just to make it even clearer (it makes no difference to R)\n\n## Best to use  a new line for each pipe when code gets longer\ndata |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\nEven though we haven’t covered any of these commands yet, we can see that the |&gt; is pretty easy to know roughly what’s going on. Whereas, the traditional nested way gets really tricky beyond a couple of commands.\nOf course, if you wanted, you could do each step separately, constantly assigning and overwriting an object like below\n\n## Without the |&gt;, we could technically break it down step by step\ntemp &lt;- select(data, x1txmtscor)\ntemp &lt;- filter(temp, x1txmtscor &gt; 50)\ntemp &lt;- mutate(temp, square_root = sqrt(x1txmtscor))\ntemp\n\nBut it’s less intuitive than simply piping the results.\n\nIf we do want to see step-by-step output in a piped command, you can either\n\nRun the code as you write it line-by-line\nHighlight sections of the piped code to run up to a point\n\n\n\n\nAssigning Output from Pipes\n\nAssigning output from pipes is the same as we have covered a few times, we use a &lt;- to pass it backwards to an object (in this case we called that object data_backward_pass)\n\n\n## Always assign backwards\ndata_backward_pass &lt;- data |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor))\n\n\nIf you want to think of it this way, we are effectively continuing to pass it forward like this…\n\n\n## You can think of the assignment as a continuation of the pipe like this\n## but don't write it this way, it's then hard to find what you called something later\ndata |&gt;\n  select(x1txmtscor) |&gt;\n  filter(x1txmtscor &gt; 50) |&gt;\n  mutate(square_root = sqrt(x1txmtscor)) -&gt;\n  data_forward_pass\n\n\nThat’s how pipes and assignment work together, and you can see the outputs are all.equal()\n\n\n## Checking they are the same\nall.equal(data_backward_pass, data_forward_pass)\n\n[1] TRUE\n\n\n\nHowever, you really shouldn’t write code like this, as although it runs, it’s then hard to later find where you created data_forward_pass.\nStarting the string of code with where you store the result is MUCH clearer.\n\nBut hopefully it will help some of you understand how |&gt; and &lt;- work together",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#basic-tidyverse-commands",
    "href": "03-wrangle-i.html#basic-tidyverse-commands",
    "title": "I: Enter the tidyverse",
    "section": "Basic Tidyverse Commands",
    "text": "Basic Tidyverse Commands\n\nNow we have the pipe |&gt; covered, it’s time to dig into some basic data wrangling commands\n\n\nSelecting Variables/Columns with select()\n\nOften data sets contain hundreds, thousands, or even tens of thousands of variables, when we are only interested in a handful. Our first tidyverse command select() helps us deal with this, by, as you may have guessed, select()-ing the variables we want\nSince we are going to pipe our R commands, we start with our data then pipe it into the select() command\nIn the select() command, we list out the variables we want\n\nstu_id, x1stuedexpct, x1paredexpct, x1region\n\nNotice: in this (and most tidyverse) command(s) we don’t have to use “quotes” around the variable names. A common error message when you did need to put something in quotes is Error: object &lt;thing you should have \"quoted\"&gt; not found\n\n\n\n\ndata |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n# ℹ 23,493 more rows\n\n\n\nQuick question: if we want to use this reduced dataset going forward, what should we do?\n\nThat’s right, assign it to an object! Let’s call that data_small\n\ndata_small &lt;- data |&gt; select(stu_id, x1stuedexpct, x1paredexpct, x1region)\n\n\nOur next step is to create a variable that’s the higher of student and parent expectations. Sounds simple enough, but first, we have a problem…\n\nCan anyone guess what it is?\n\n\n\n\nUnderstanding our data\nFirst things first, however, we need to check the code book to see what the numerical values for our two education expectation variables represent. To save time, we’ve copied them here:\n\nx1stuedexpct\nHow far in school 9th grader thinks he/she will get\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n\n\n\nx1paredexpct\nHow far in school parent thinks 9th grader will go\n\n\n\nvalue\nlabel\n\n\n\n\n1\nLess than high school\n\n\n2\nHigh school diploma or GED\n\n\n3\nStart an Associate’s degree\n\n\n4\nComplete an Associate’s degree\n\n\n5\nStart a Bachelor’s degree\n\n\n6\nComplete a Bachelor’s degree\n\n\n7\nStart a Master’s degree\n\n\n8\nComplete a Master’s degree\n\n\n9\nStart Ph.D/M.D/Law/other prof degree\n\n\n10\nComplete Ph.D/M.D/Law/other prof degree\n\n\n11\nDon’t know\n\n\n-8\nUnit non-response\n\n\n-9\nMissing\n\n\n\n\nThe good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression\nThe bad news is that we have three values — -8, -9, and 11 — that we need to deal with so that the averages we compute later represent what we mean\n\n\n\n\nExploring Catagorical Variables with count()\n\nFirst, let’s see how many observations are affected by these values using count()\n\n\nNotice that we don’t assign to a new object; this means we’ll see the result in the console, but nothing in our data or object will change\n\n\n## see unique values for student expectation\ndata_small |&gt; count(x1stuedexpct)\n\n# A tibble: 12 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -8  2059\n 2            1    93\n 3            2  2619\n 4            3   140\n 5            4  1195\n 6            5   115\n 7            6  3505\n 8            7   231\n 9            8  4278\n10            9   176\n11           10  4461\n12           11  4631\n\n## see unique values for parental expectation\ndata_small |&gt; count(x1paredexpct)\n\n# A tibble: 13 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1           -9    32\n 2           -8  6715\n 3            1    55\n 4            2  1293\n 5            3   149\n 6            4  1199\n 7            5   133\n 8            6  4952\n 9            7    76\n10            8  3355\n11            9    37\n12           10  3782\n13           11  1725\n\n\n\nDealing with -8 and -9 is straight forward — we’ll convert it missing.\n\nIn R, missing values are technically stored as NA.\n\nNot all statistical software uses the same values to represent missing values (for example, STATA uses a dot .)\n\nNCES has decided to represent missing values as a limited number of negative values. In this case, -8 and -9 represent missing values\n\nNote: how to handle missing values is a very important topic, one we could spend all semester discussing\n\nFor now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results\nIn real research, a better approach is usually to impute missing values, but that is beyond our scope right now\n\nDeciding what to do with 11 is a little trickier. While it’s not a missing value per se, it also doesn’t make much sense in its current ordering, that is, to be “higher” than completing a professional degree\n\nFor now, we’ll make a decision to convert these to NA as well, effectively deciding that an answer of “I don’t know” is the same as missing an answer\n\nSo first step: convert -8, -9, and 11 in both variables to NA. For this, we’ll use the mutate() and ifelse() functions\n\n\n\nConditional Values with ifelse()\n\nifelse() is a really common command in R and has three parts\n\nstatement that can be TRUE or FALSE\nWhat to return if the statement is TRUE\nelse what to return when the statement is FALSE\n\n\n\n\nModifying an Existing Variable with mutate()\n\nWhen we want to add variables and change existing ones, we can use the mutate() function\n\nThe basic idea of mutate commands is mutate(&lt;where to go&gt; = &lt;what to go there&gt;)\n\nThis is probably the trickiest function we cover today to understand\n\nNew variables are created if you provide &lt;where to go&gt; a new variable name (or nothing)\nVariables are modified if you provide &lt;where to go&gt; an existing variable name\n&lt;what to go there&gt; can as simple as a single number all the way to a chain of piped functions, so long as there’s a clear answer for every row\n\nIn this case, we want to modify x1stuedexpct to be NA when x1stuedexpct is -8, -9, or 11\n\nNow, we have three values we want to covert to NA, so we could do them one-at-a-time, like below\n\ndata_small &lt;- data_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct == -8, NA, x1stuedexpct))\n\n\nLet’s walk through this code\n\n\nAssign the results back to data_small (which will overwrite our previous data_small)\nTake data_small and pipe |&gt; it into mutate()\nInside mutate() assign our results to x1stuedexpct (which will modify the existing variable)\nModify x1stuedexpct with an ifelse() statement, which remember has 3 parts\n\n\n\nstatement which is asking “is x1stuedexpct == -8”? - Notice == means “is equal to”, while = means “assign to”. Yes it’s confusing, but you’ll get it over time!\nif that statement is true, make it NA\nelse (if the statement is false) return the original variable x1stuedexpct\n\nOkay, make sense? Let’s see what we just did (look at row 26)\n\nprint(data_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           11            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           11        1\n 8  10008            8            6        1\n 9  10009           11           11        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           11           11        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           11           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           -8        2\n19  10020            8            8        4\n20  10021            8           11        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           -8        4\n24  10026            7           10        3\n25  10027           11            6        1\n26  10028           NA           -8        3\n# ℹ 23,477 more rows\n\n\n\nThis is fine, but we have to do it 3 times for both parent and student expectation\n\nInstead, can anyone think (not in R code, just in terms of logic) how we could change our statement piece of the ifelse() to be more efficient?\n\n\n\n\nBeing Efficient with %in% and c()\n\nWhat we can do, is group -8, -9, and 11 together into a list using c()\n\nc() is a very common function in R used to create a list\n\nThen, we can use the %in% operator to ask if that result is any of the numbers in that list\n\nThis keeps our code shorter and easier to read\n\n\n\ndata_small &lt;- data_small |&gt;\n  mutate(x1stuedexpct = ifelse(x1stuedexpct %in% c(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% c(-8, -9, 11), NA, x1paredexpct))\n\n\nThe code now works just as above, but instead of asking if x1stuedexpct is equal to -8, it asks if it’s in the list of -8, -9, and 11, then does the same for parental expectations!\n\nLet’s view those first 26 rows again to see what we did\n\n\n\nprint(data_small, n = 26)\n\n# A tibble: 23,503 × 4\n   stu_id x1stuedexpct x1paredexpct x1region\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2\n 2  10002           NA            6        1\n 3  10003           10           10        4\n 4  10004           10           10        3\n 5  10005            6           10        3\n 6  10006           10            8        3\n 7  10007            8           NA        1\n 8  10008            8            6        1\n 9  10009           NA           NA        3\n10  10010            8            6        1\n11  10011            8            6        3\n12  10012           NA           NA        2\n13  10013            8           10        3\n14  10014            2            6        3\n15  10015           NA           10        3\n16  10016            4            6        2\n17  10018            6            7        2\n18  10019            8           NA        2\n19  10020            8            8        4\n20  10021            8           NA        2\n21  10022           10            8        4\n22  10024            6            8        2\n23  10025            8           NA        4\n24  10026            7           10        3\n25  10027           NA            6        1\n26  10028           NA           NA        3\n# ℹ 23,477 more rows\n\n\n\nJust to be doubly-sure, lets check count() again\n\n\ndata_small |&gt; count(x1stuedexpct) \n\n# A tibble: 11 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    93\n 2            2  2619\n 3            3   140\n 4            4  1195\n 5            5   115\n 6            6  3505\n 7            7   231\n 8            8  4278\n 9            9   176\n10           10  4461\n11           NA  6690\n\ndata_small |&gt; count(x1paredexpct)\n\n# A tibble: 11 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    55\n 2            2  1293\n 3            3   149\n 4            4  1199\n 5            5   133\n 6            6  4952\n 7            7    76\n 8            8  3355\n 9            9    37\n10           10  3782\n11           NA  8472\n\n\nSuccess!\n\n\nCreating a New Variable with mutate()\n\nSo, with that tangent out of the way, let’s get back to our original task, creating a new variable that is the highest of parental and student expectations\nTo make a new variable which is the highest of two variables, we can use our friends mutate() and ifelse() some more\n\n\ndata_small &lt;- data_small |&gt;\n  mutate(high_exp = ifelse(x1stuedexpct &gt; x1paredexpct, x1stuedexpct, x1paredexpct))\n\n\nThat code is almost what we want to do\n\nIf x1stuedexpct is higher then take that, if not, take x1paredexpct\n\nThere’s two things we haven’t fully accounted for though…\n\nOne doesn’t actually matter here, but might in other circumstances\nOne definitely matters here\n\nWithout scrolling past the duck, does anyone know what they might be?\n\n\n\n\n\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.\n\n\n\nSloppy Mistake 1 (doesn’t matter here)\n\nWe were a little sloppy with the statement piece, we just asked if x1stuedexpct was greater than x1paredexpct or not\n\nIf we were being more careful, we might have said “greater than or equal to”\n\nWhy doesn’t this matter in this context, and when might it matter?\n\n\n\n\n\nSloppy Mistake 2 (does matter here)\n\nNow let’s check our data frame to see the one that does matter\n\n\nprint(data_small, n = 26)\n\n# A tibble: 23,503 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10002           NA            6        1       NA\n 3  10003           10           10        4       10\n 4  10004           10           10        3       10\n 5  10005            6           10        3       10\n 6  10006           10            8        3       10\n 7  10007            8           NA        1       NA\n 8  10008            8            6        1        8\n 9  10009           NA           NA        3       NA\n10  10010            8            6        1        8\n11  10011            8            6        3        8\n12  10012           NA           NA        2       NA\n13  10013            8           10        3       10\n14  10014            2            6        3        6\n15  10015           NA           10        3       NA\n16  10016            4            6        2        6\n17  10018            6            7        2        7\n18  10019            8           NA        2       NA\n19  10020            8            8        4        8\n20  10021            8           NA        2       NA\n21  10022           10            8        4       10\n22  10024            6            8        2        8\n23  10025            8           NA        4       NA\n24  10026            7           10        3       10\n25  10027           NA            6        1       NA\n26  10028           NA           NA        3       NA\n# ℹ 23,477 more rows\n\n\n\nHmm, that seems odd, why would R consider NA to be greater than 6?\n\nAny thoughts?\n\nGenerally, R is overly-cautious when dealing with NAs to ensure you don’t accidentally drop them without realizing it\n\nFor example, if you were asked what the mean(c(5, 6, 4, NA)) would be, you’d probably say 5, right?\n\nR is never going to just ignore the NA values like that unless we tell it to\n\n\n\n\nmean(c(5, 6, 4, NA))\n\n[1] NA\n\n\n\nSee, what have to explicitly tell it to remove the NA values\n\n\nmean(c(5, 6, 4, NA), na.rm = T)\n\n[1] 5\n\n\n\nSo in our case of trying to get the highest expectation, R doesn’t want us to forget we have NA values, so it throws them at us.\nDealing with missing values is a huge topic in data analysis, and there are many ways to handle them, which is beyond the scope of this lesson\n\nFor now, let’s remove rows that have NA values in either x1stuedexpct or x1paredexpct or both\n\n\n\n\n\nDealing With Missing Values with is.na(), &, and !\n\nTo do this, we will add another couple of code helpers -is.na(x1stuedexpct) simply asks if the x1stuedexpct is NA or not\n\nR doesn’t let you just say x1stuedexpct == NA -! is really helpful tool, which can be used to negate or invert a command\n\n!is.na(x1stuedexpct) just returns the opposite of is.na(x1stuedexpct) so it tells us that the column is not NA -& can be useful inside conditional statements, as it means both must be TRUE (FYI: | means or)\n\n\n\n\ndata_small_cut &lt;- data_small |&gt;\n  filter(!is.na(x1stuedexpct) & !is.na(x1paredexpct))\n\n\nSo !is.na(x1stuedexpct) & !is.na(x1paredexpct) makes sure that both x1stuedexpct and x1paredexpct are not NA\nNow what does filter() do here?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "href": "03-wrangle-i.html#keeping-rows-based-on-a-condition-with-filter",
    "title": "I: Enter the tidyverse",
    "section": "Keeping Rows Based on a Condition with filter()",
    "text": "Keeping Rows Based on a Condition with filter()\n\nThe filter() command from tidyverse works by only keeping observations that meet the condition(s) we set\n\nAs in, to make it through the filter, a row must answer “yes” to “does it meet this condition?”\n\nSo in this case, we are keeping all rows where x1stuedexpct and x1paredexpct are not NA\n\nNotice, instead of overwriting data_small we assigned this to a new object data_small_cut\n\nGenerally, when making substantial changes to a data set like dropping observations, we might want to be able to double check what we did, which is easier if we make a new data\n\n\n\n\nQuick Question: A common confusion from this lesson is between filter() and select(). Can someone explain when you’d use select() over filter()?\n\n\nLet’s check the counts of our x1stuedexpct and x1paredexpct again to see if filter() worked\n\n\ndata_small_cut |&gt; count(x1stuedexpct) \n\n# A tibble: 10 × 2\n   x1stuedexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    39\n 2            2  1483\n 3            3    83\n 4            4   769\n 5            5    78\n 6            6  2539\n 7            7   170\n 8            8  3180\n 9            9   127\n10           10  3336\n\ndata_small_cut |&gt; count(x1paredexpct)\n\n# A tibble: 10 × 2\n   x1paredexpct     n\n          &lt;dbl&gt; &lt;int&gt;\n 1            1    41\n 2            2   911\n 3            3   105\n 4            4   825\n 5            5    97\n 6            6  3761\n 7            7    66\n 8            8  2746\n 9            9    30\n10           10  3222\n\n\n\nOkay, so no NAs, perfect!\nJust to be extra sure we only removed NAs, we can check the difference in how many rows our original data_small has with data_small_cut\n\n\n## what's the difference between original # of rows and current # or rows?\nnrow(data_small) - nrow(data_small_cut)\n\n[1] 11699\n\n\n\nnrow() does what you’d expect, counts the number of rows\n\nYou could also just check by looking at the dataframes in the environment tab, but this way leaves no room for mental math errors\n\nNow let’s check our high_exp variable in the new data_small_cut\n\n\nprint(data_small_cut, n = 26)\n\n# A tibble: 11,804 × 5\n   stu_id x1stuedexpct x1paredexpct x1region high_exp\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  10001            8            6        2        8\n 2  10003           10           10        4       10\n 3  10004           10           10        3       10\n 4  10005            6           10        3       10\n 5  10006           10            8        3       10\n 6  10008            8            6        1        8\n 7  10010            8            6        1        8\n 8  10011            8            6        3        8\n 9  10013            8           10        3       10\n10  10014            2            6        3        6\n11  10016            4            6        2        6\n12  10018            6            7        2        7\n13  10020            8            8        4        8\n14  10022           10            8        4       10\n15  10024            6            8        2        8\n16  10026            7           10        3       10\n17  10029            2            2        2        2\n18  10030            6            8        3        8\n19  10036            8            6        2        8\n20  10037            8            8        3        8\n21  10038            6            4        4        6\n22  10039            8           10        3       10\n23  10041            3            1        3        3\n24  10044           10           10        3       10\n25  10045            4            4        2        4\n26  10049            8            8        4        8\n# ℹ 11,778 more rows\n\n\n\nTo be more straightforward, let’s compare the counts of high_exp in data_small and dataf_small_cut\n\n\ndata_small |&gt; count(high_exp)\n\n# A tibble: 11 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1     3\n 2        2   516\n 3        3    62\n 4        4   482\n 5        5    59\n 6        6  2177\n 7        7   120\n 8        8  3380\n 9        9   112\n10       10  4893\n11       NA 11699\n\ndata_small_cut |&gt; count(high_exp)\n\n# A tibble: 10 × 2\n   high_exp     n\n      &lt;dbl&gt; &lt;int&gt;\n 1        1     3\n 2        2   516\n 3        3    62\n 4        4   482\n 5        5    59\n 6        6  2177\n 7        7   120\n 8        8  3380\n 9        9   112\n10       10  4893\n\n\n\nYay, all NAs are gone!\nIs there any other way to remove NAs?\n\nYou know what, there is a drop_na() function!\n\n\n\nSimply Dropping NAs\n\nThe tidyr package in tidyverse has a handy function drop_na()\n\nWithout arguments in (), it will remove any row that has NA in any column\nSpecify columns to remove rows that have NA in the designated columns\n\n\n\ndata_small_drop &lt;- data_small |&gt; \n  drop_na(x1stuedexpct, x1paredexpct)\n\n\nLet’s compare our two ways of removing NAs and see if they end up being the same\n\nWhat’s the function to check if things are equal (we used it when introducing |&gt; )?\n\n\n\n## compare two ways of dropping NAs\nall.equal(data_small_cut, data_small_drop)\n\n[1] TRUE\n\n\n\n\nSummarizing Data with summarize()\n\nOkay, so we have our data selected, we made our high_exp variable, and we’ve done some work to handle missing data\nFor our final task today, we are going to make some summary tables using summarize() from the tidyverse\nsummarize() allows us to apply a summary statistic (mean, sd, median, etc.) to a column in our data\nsummarize() takes an entire data frame as an input, and spits out a small data frame with the just the summary variables\n\nNote: for this reason, you rarely ever want to assign &lt;- the output of summarize() back to the main data frame object, as you’ll overwrite it\n\nYou can either spit the summary tables out into the console without assignment (which we will do) or if you need to use them for something else, assign them to a new object\n\n\n\n\n## get average (without storing)\ndata_small_cut |&gt; summarize(mean(high_exp))\n\n# A tibble: 1 × 1\n  `mean(high_exp)`\n             &lt;dbl&gt;\n1             7.99\n\n\n\nSee, the output is a 1x1 table with the mean expectation mean(high_exp) of 7.99, almost about completing a master’s degree\n\nNote: if we want to name the summary variable, we can name it just like we did earlier in mutate() with a single =\n\n\n\ndata_small_cut |&gt; summarize(mean_exp = mean(high_exp))\n\n# A tibble: 1 × 1\n  mean_exp\n     &lt;dbl&gt;\n1     7.99\n\n\n\nBut, that wasn’t quite the question we were asked\n\nWe were asked if it varied by region…\n\nFor time’s sake, we’re going to tell you the region variable is x1region and splits the US in 4 Census regions\n\n\n\n\n\nGrouping Data with group_by()\n\nThe group_by() function, following the tidyverse principle of intuitive naming, groups the data and outputs by the variable(s) you say\n\nSo, since we want to calculate the average high expectation by region, we group_by(x1region)\n\nSince we just want it for our summarize(), we just add it to the pipe\n\nIf you wanted to save the data in it’s group_by()-ed state, you could assign it to something\n\n\n\n\n\n## get grouped average\ndata_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp))\n\n# A tibble: 4 × 2\n  x1region mean_exp\n     &lt;dbl&gt;    &lt;dbl&gt;\n1        1     8.13\n2        2     7.88\n3        3     8.06\n4        4     7.86\n\n\n\nSuccess! While expectations are similar across the country, there’s some variance by region\n\nWhile there are few things we could do to make this a little fancier (e.g., changing the region numbers to names, formatting the table, etc.) we have answered our question, and have clear documentation of how we got here, so let’s call that a win!\n\n\n\n\nSaving Data with write_csv()\n\nSometimes we want to be able to access objects from scripts without having to re-run the whole thing\n\nRemember: one of the main advantages of R is the data we read in is untouched\n\nTo do this, we want to write_ a new csv() file, containing our modified data\n\nunlike read_csv() which only needed a file name/path, write_csv() needs to know what you’re trying to save and the file name/path you want to save it to\n\nThe only way you can overwrite or change the original data is by saving to the same file name as the original data, so NEVER do that!\n\n\nSince we didn’t assign our summary table to anything, we can just add write_csv() to the end of the pipe and add a path to where you want to save it\n\nIf you want to save a data frame you already assigned to an object write_csv(&lt;object&gt;, \"path\") would work just fine!\n\n\n\n## write with useful name\n\ndata_small_cut |&gt;\n  group_by(x1region) |&gt;\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  write_csv(\"data/region-expects.csv\")\n\nPhew!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#appendix-all-at-once",
    "href": "03-wrangle-i.html#appendix-all-at-once",
    "title": "I: Enter the tidyverse",
    "section": "Appendix: All at Once",
    "text": "Appendix: All at Once\nWe went through that piece by piece to demonstrate each function, but, there’s no reason we can’t just |&gt; pipe it all together\n\n## Let's redo the analysis above, but with a fully chained set of\n## functions.\n\n## start with original dataset\ndata |&gt;\n  ## select columns we want\n  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |&gt;\n  ## If expectation is -8, -9. or 11, make it NA\n  mutate(x1stuedexpct = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),\n         x1paredexpct = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |&gt;\n  ## Make a new variable called high_exp that is the higher or parent and student exp\n  mutate(high_exp = ifelse(x1stuedexpct &gt; x1paredexpct, x1stuedexpct, x1paredexpct)) |&gt;\n  ## Drop if either or both parent or student exp is NA\n  filter(!is.na(x1stuedexpct) & !is.na(x1paredexpct)) |&gt; \n  ## Group the results by region\n  group_by(x1region) |&gt;\n  ## Get the mean of high_exp (by region)\n  summarize(mean_exp = mean(high_exp)) |&gt;\n  ## Write that to a .csv file\n  write_csv(\"data/region-expects-chain.csv\")\n\nTo double check, let’s just check these are the same…\n\nnon_chain &lt;- read_csv(\"data/region-expects.csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchain &lt;- read_csv(\"data/region-expects-chain.csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x1region, mean_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nall.equal(non_chain, chain)\n\n[1] TRUE\n\n\nWooHoo!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#final-notes",
    "href": "03-wrangle-i.html#final-notes",
    "title": "I: Enter the tidyverse",
    "section": "Final notes",
    "text": "Final notes\n\nThis rather lengthy lesson has thrown you in the (medium) deep end of the coding pool\n\nBy no means are you expected to get everything we just did\n\nWe will continue to revisit all these commands throughout the class, by the end of the semester, they will be second nature!\n\n\nWe also saw how to use code to answer a realistic question we might be asked in a data management job, a translation skill that will prove invaluable later on!\n\nWe had to plan out steps and then make some adjustments along the way (e.g., our NA issues), that’s all part of the process!",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-one",
    "href": "03-wrangle-i.html#question-one",
    "title": "I: Enter the tidyverse",
    "section": "Question One",
    "text": "Question One\na) What is the average (mean) standardized math score?\nb) How does this differ by gender?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-two",
    "href": "03-wrangle-i.html#question-two",
    "title": "I: Enter the tidyverse",
    "section": "Question Two",
    "text": "Question Two\na) Among those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (Include a description what that category represents)",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-three",
    "href": "03-wrangle-i.html#question-three",
    "title": "I: Enter the tidyverse",
    "section": "Question Three",
    "text": "Question Three\na) Of the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nb) How does this differ by region?",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#question-four",
    "href": "03-wrangle-i.html#question-four",
    "title": "I: Enter the tidyverse",
    "section": "Question Four",
    "text": "Question Four\na) What percentage of students ever attended a post-secondary institution? (as of the data collection in february 2016)\nb) Give the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35,000 within each region\n\n\nHint: group_by() can be given more than one group",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#submission",
    "href": "03-wrangle-i.html#submission",
    "title": "I: Enter the tidyverse",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the PDF rendered to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "03-wrangle-i.html#solution",
    "href": "03-wrangle-i.html#solution",
    "title": "I: Enter the tidyverse",
    "section": "Solution",
    "text": "Solution\n R Solution Code\n\n## -----------------------------------------------------------------------------\n##\n##' [PROJ: EDH 7916]\n##' [FILE: Data Wrangling I Solution]\n##' [INIT: Jan 28 2024]\n##' [AUTH: Matt Capaldi] @ttalVlatt\n##\n## -----------------------------------------------------------------------------\n\n## note to matt\n\nsetwd(this.path::here())\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\ndf &lt;- read_csv(file.path(\"data\", \"hsls-small.csv\"))\n\n## ---------------------------\n##' [Q1]\n## ---------------------------\n\n## Part One\ndf |&gt;\n  filter(!x1txmtscor %in% c(-8, -9)) |&gt;\n  summarize(mean = mean(x1txmtscor))\n\n## Part Two\ndf |&gt;\n  filter(!x1txmtscor %in% c(-8, -9),\n         x1sex != -9) |&gt;\n  group_by(x1sex) |&gt;\n  summarize(mean = mean(x1txmtscor))\n\n## ---------------------------\n##' [Q2]\n## ---------------------------\n\ndf |&gt;\n  filter(x1poverty185 == 1,\n         !x1famincome %in% c(-8,-9)) |&gt;\n  summarize(med_inc_cat = median(x1famincome))\n\nprint(\"Median income category in 2, which represents Family income &gt; $15,000 and &lt;= $35,000\")\n\n## ---------------------------\n##' [Q3]\n## ---------------------------\n\n## Simplified\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  count(x4hscompstat) |&gt;\n  mutate(perc = n / sum(n) * 100)\n\n## Part One\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  summarize(ged = sum(x4hscompstat == 2),\n            total = sum(x4hscompstat %in% c(1,2)),\n            perc = ged/total*100)\n\n## Part Two\n## Simplified\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  group_by(x1region) |&gt;\n  count(x4hscompstat) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  filter(x4hscompstat == 2)\n\ndf |&gt;\n  filter(x4hscompstat %in% c(1,2)) |&gt;\n  group_by(x1region) |&gt;\n  summarize(ged = sum(x4hscompstat == 2),\n            total = sum(x4hscompstat %in% c(1,2)),\n            perc = ged/total*100)\n\n## ---------------------------\n##' [Q4]\n## ---------------------------\n\n## Part One\ndf |&gt;\n  filter(x4evratndclg != -8) |&gt;\n  count(x4evratndclg) |&gt;\n  mutate(perc = n / sum(n) * 100)\n\n\ndf |&gt;\n  filter(x4evratndclg != -8) |&gt;\n  summarize(college = sum(x4evratndclg == 1),\n            total = sum(x4evratndclg %in% c(0, 1)),\n            perc = college/total*100)\n\n## Part Two\ndf |&gt;\n  filter(x4evratndclg != -8,\n         !x1famincome %in% c(-8, -9)) |&gt;\n  mutate(below_35k = ifelse(x1famincome %in% c(1,2), 1, 0)) |&gt;\n  group_by(x1region, below_35k) |&gt;\n  count(x4evratndclg) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  filter(x4evratndclg == 1)\n\n\n\ndf |&gt;\n  filter(x4evratndclg != -8,\n         !x1famincome %in% c(-8, -9)) |&gt;\n  mutate(below_35k = ifelse(x1famincome %in% c(1,2), 1, 0)) |&gt;\n  group_by(x1region, below_35k) |&gt;\n  summarize(college = sum(x4evratndclg == 1),\n            total = sum(x4evratndclg %in% c(0, 1)),\n            perc = college/total*100)\n\n## -----------------------------------------------------------------------------\n##' *END SCRIPT*\n## -----------------------------------------------------------------------------",
    "crumbs": [
      "Data Wrangling",
      "I: Enter the tidyverse"
    ]
  },
  {
    "objectID": "01-set-install.html#getting-started",
    "href": "01-set-install.html#getting-started",
    "title": "I: Installing R & RStudio",
    "section": "Getting started",
    "text": "Getting started\nThe primary pieces of software you are going to need for this class are\n\nR\nRStudio\nMicrosoft Office\n\nAssuming you already have this, but we can meet to install it if not, it is free for UF students\n\n\nThere are also a few optional pieces of software you’ll need for extra credit lessons, but we will cover when needed.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-r",
    "href": "01-set-install.html#installing-r",
    "title": "I: Installing R & RStudio",
    "section": "Installing R",
    "text": "Installing R\n\nR is fantastic (hopefully you will see that throughout the course), but sometimes it can make things seem more complicated than they need to\n\nThe first time it does this is when trying to install it, there’s a bunch of options called “mirrors”\n\nThese are basically to reduce strain on the servers that you download from by using the closest location\n\nThe good news, however, is that a URL that automates this whole process for you came out recently\nThe even better news is that we’ve set up a little portal to that URL here, so you can download it without leaving this page\n\n\n\n\nClick the option for the OS you have (Windows/Mac/Linux)\nThen under the “latest release”\n\n\n\nFor windows users, select “base” then “Download R…” (the top options)\nFor mac users, select either apple silicon or intel options depending on how new your mac is\n\nIf you need to check which kind your mac is, hit the apple logo in the top left of your screen, then “About This Mac.” On Mac computers with Apple silicon, it will show an item labeled Chip, followed by the name of the chip. On Mac computers with an Intel processor, it will show an item labeled Processor, followed by the name of an Intel processor\n\n\n\n\nR will then download, double click on the download when it’s finished and then follow the on-screen prompts\nR is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#installing-rstudio",
    "href": "01-set-install.html#installing-rstudio",
    "title": "I: Installing R & RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nTechnically, R is all you need to do all of our analyses. However, to make it accessible and usable, we also need a “development environment”\n\nThe reason of this, unlike a computer program like Stata or SAS, R is a programming language (same as Python, C++, etc.), that’s what we just installed\nThe easiest way to use programming languages is through a “development environment”\n\nThere are multiple “development environments” you can use for R. VSCode is a great option by Microsoft for using a variety of languages, but, the best option for R is RStudio as it is purpose built for the language (it also works with Python too)\n\n\nTo install RStudio, let’s do the following:\n\n\nGo to this site and click the “Download RStudio Desktop for…” button underneath “2: Install RStudio” (we already did step 1 Install R)\n\n\nThis is simpler than installing R, Posit have a more sophisticated website which will automatically download the right version for your computer\n\n\nRStudio will then download, double click on the download when it’s finished and then follow the on-screen prompts\n\n\nFor mac users, this will just be drag n’ drop RStudio into your Applications folder\n\n\nRStudio is now (hopefully) installed!\n\n\nNote: If you’re using a work computer you may run into issues with administrator privileges, we can work on this individually",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#lets-see-what-we-just-installed",
    "href": "01-set-install.html#lets-see-what-we-just-installed",
    "title": "I: Installing R & RStudio",
    "section": "Let’s See What We Just Installed",
    "text": "Let’s See What We Just Installed\n\nHopefully, you should now be able to open RStudio on your computer (it should be the same place all your software is kept)\n\nGo ahead and open it up!\n\n\nBy default, RStudio has 3-4 main frames:\n\nTop left: Script window (will be closed at first if you don’t have any scripts open)\nBottom left: Console\nTop right: Environment / History / Connections\nBottom right: Files / Plots / Packages / Help / Viewer\n\nFor today, we are mostly going to explore some basic features of R using the console, copying and pasting commands from the website rather than saving them in a script (which we will set up next week). All an .R script does is save your code and pass it line-by-line to the console. After today, anything we want to save will be done through a script, anything we just need to run one time will be done in console",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#basic-r-commands",
    "href": "01-set-install.html#basic-r-commands",
    "title": "I: Installing R & RStudio",
    "section": "Basic R Commands",
    "text": "Basic R Commands\nFirst, let’s try the traditional first command!\n\nprint(\"Hello, World!\")\n\n[1] \"Hello, World!\"\n\n\nWe can also use R like a basic calculator\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#assignment",
    "href": "01-set-install.html#assignment",
    "title": "I: Installing R & RStudio",
    "section": "Assignment",
    "text": "Assignment\n\nThe first two commands we ran simply spat the output out in the console\n\nThis can be useful if you want to check something quickly or if we have our final output\n\nMore often, though, we want to save the output to our R Environment (top right panel)\nTo do this, we need to assign the output to an object\n\nR is a type of object-oriented programming environment. This means that R thinks of things in its world as objects, which are like virtual boxes in which we can put things: data, functions, and even other objects.\n\nIn R (for quirky reasons), the primary means of assignment is the arrow, &lt;-, which is a less than symbol, &lt;, followed by a hyphen, -.\n\nYou can use = (which is more common across other programming languages), and you may see this “in the wild”\nBut R traditionalists prefer &lt;- for clarity and readability, and let’s try to use &lt;- in this class\n\n\n\n## assign value to object x using &lt;-\nx &lt;- 1\n\n\nBut where’s the output?\n\nCheck out the “Environment” tab on the top left panel\n\nWe see something called x has a value of 1\n\nNow let’s call that object\n\n\n\n\n## what's in x?\nx\n\n[1] 1\n\n\nNote: the [1] is just the index (order) number, if we had more than 1 thing in our object, that would be more useful\n\nQuick exercise\nUsing the arrow, assign the output of 1 + 1 to x. Next subtract 1 from x and reassign the result to x. Show the value in x.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#comments",
    "href": "01-set-install.html#comments",
    "title": "I: Installing R & RStudio",
    "section": "Comments",
    "text": "Comments\nFor this section, let’s just open a blank .R script in RStudio (again, all these commands will be in a script in your class folder we set up next week)\n\nComments in R are set off using the hash or pound character at the beginning of the line: #\nThe comment character tells R to ignore the line\nComments are useful for explaining what your code is doing, why you’re doing it, or for temporarily removing code from your script without deleting it\nYou can also use comments to take notes for this class!\n\n\nQuick exercise\nType the phrase “This is a comment” directly into the R console both with and without a leading “#”. What happens each time?\n\n\nYou may notice sometimes we use two hashes\n\nYou can use only a single # for your comments if you like, R treats them all the same\nIf you’re typing longer comments ##' (two hashes and an apostrophe) is really useful in RStudio, as it automatically comments the next line (although this can be annoying at times too)\n\nLastly, RStudio can comment/uncomment multiple lines of code you’ve already written\n\nOn the top menu bar select “Code” then “Commment/Uncomment Lines”\n\nAlso see the keyboard shortcut next to that option!\n\n\nThis is a big time saver!\n\n\n## Try commenting/uncommenting the below line\n\n# EDH7916 &lt;- \"Hi\"",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#data-types-and-structures",
    "href": "01-set-install.html#data-types-and-structures",
    "title": "I: Installing R & RStudio",
    "section": "Data types and structures",
    "text": "Data types and structures\nR uses variety of data types and structures to represent and work with data. There are many, but the major ones that you’ll use most often are:\n\nlogical\nnumeric (integer & double)\ncharacter\nvector\nmatrix\nlist\ndataframe\n\nLet’s see what type of object x we created earlier is\n\ntypeof(x)\n\n[1] \"double\"\n\n\nWhat if we make it “1”?\n\nx &lt;- \"1\"\ntypeof(x)\n\n[1] \"character\"\n\n\nUnderstanding the nuanced differences between data types is not important right now. Just know that they exist and that you’ll gain an intuitive understanding of them as you become better acquainted with R.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#packages",
    "href": "01-set-install.html#packages",
    "title": "I: Installing R & RStudio",
    "section": "Packages",
    "text": "Packages\n\nUser-submitted packages are a huge part of what makes R great\nYou may hear the phrases “base R” or “vanilla R” during class\n\nThat is the R that comes as you download it with no packages loaded\nWhile it’s powerful in and of itself — you can do everything you need with base R — most of your scripts will make use of one of more contributed packages. These will make your data analytic life much nicer. We’ll lean heavily on the tidyverse suite of packages this semester.\n\n\n\nInstalling packages from CRAN\n\nMany contributed packages are hosted on the CRAN package repository. - What’s really nice about CRAN is that packages have to go through quite a few checks in order for CRAN to approve and host them. Checks include;\n\nMaking sure the package has documentation\nWorks on a variety of systems\nDoesn’t try to do odd things to your computer\n\nThe upshot is that you should feel okay downloading these packages from CRAN\n\nTo download a package from CRAN, use:\n\ninstall.packages(\"&lt;package name&gt;\")\n\nNOTE Throughout this course, if you see something in triangle brackets (&lt;...&gt;), that means it’s a placeholder for you to change accordingly.\nMany packages rely on other packages to function properly. When you use install.packages(), the default option is to install all dependencies. By default, R will check how you installed R and download the right operating system file type.\n\nQuick exercise\nInstall the tidyverse package, which is really a suite of packages that we’ll use throughout the semester. Don’t forget to use double quotation marks around the package name:\n\n\ninstall.packages(\"tidyverse\")\n\n\n\nInstalling packages using the top menu bar\n\nAlternatively, you can install packages by going to “Tools”, then “Install Packages”, then type in the package you want to install\n\n\n\nLoading package libraries\nPackage libraries can loaded in a number of ways, but the easiest it to write:\n\nlibrary(\"&lt;library name&gt;\")\n\nwhere \"&lt;library name&gt;\" is the name of the package/library. You will need to load these before you can use their functions in your scripts. Typically, they are placed at the top of the script file.\nFor example, let’s load the tidyverse library we just installed:\n\n## load library (note quirk that you don't need quotes here)\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that when you load the tidyverse (which, again, is actually loading a number of other libraries), you see a lot of output. Not all packages are this noisy, but the information is useful here because it shows all the libraries that are now loaded and ready for you to use.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#help",
    "href": "01-set-install.html#help",
    "title": "I: Installing R & RStudio",
    "section": "Help",
    "text": "Help\nIt’s almost impossible to have every R function and nuance memorized, and you don’t have to. With all the user-written packages, it would be difficult to keep up. When stuck, there are a few ways to get help.\n\nHelp files\nIn the console, typing a function name immediately after a question mark will bring up that function’s help file (in RStudio, you should see in the bottom right panel):\n\n## get help file for function\n?median\n\nTwo question marks will search for the command name in CRAN packages (again, in the bottom right facet):\n\n## search for function in CRAN\n??median\n\nAt first, using help files may feel like trying to use a dictionary to see how to spell a word — if you knew how to spell it, you wouldn’t need the dictionary! Similarly, if you knew what you needed, you wouldn’t need the help file. But over time, they will become more useful, particularly when you want to figure out an obscure option that will give you exactly what you need.\n\n\nPackage Website\n\nWhile all R packages have to have help files, not all R packages have nice webpages. However, a lot of the main ones do, and they are often much nicer than the CRAN helpfiles\n\nFor example, here’s another magic portal to the tidyverse’s dplyr website (you may spent a good amount of time here this semester)\n\nUsually if you Google something like “&lt;package name&gt; R,” and the website will come up\nYou can find links to all the tidyverse packages here\n\n\nGoogle it!\nGoogle is a coder’s best friend. If you are having a problem, odds are a 1,000+ other people have too and at least one of them has been brave enough (people can be mean on the internet) to ask about it in a forum like StackOverflow, CrossValidated, or R-help mailing list.\nIf you are lucky, you’ll find the exact answer to your question. More likely, you’ll find a partial answer that you’ll need to modify for your needs. Sometimes, you’ll find multiple partial answers that, in combination, help you figure out a solution. It can feel overwhelming at first, particularly if it’s a way of problem-solving that’s different from what you’re used to. But it does become easier with practice.",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "01-set-install.html#useful-packages",
    "href": "01-set-install.html#useful-packages",
    "title": "I: Installing R & RStudio",
    "section": "Useful packages",
    "text": "Useful packages\nWe’re going to use a number of packages this semester. While we may need more than this list — and you almost certainly will in your own future work — let’s install these to get us started.\n\nQuick exercise\nInstall the following packages using the install.packages() function:\n\n\n\ndevtools\nknitr\n`rmarkdown`\nquarto",
    "crumbs": [
      "Getting Started",
      "I: Installing R & RStudio"
    ]
  },
  {
    "objectID": "02-set-data.html#organizing-a-project-folder",
    "href": "02-set-data.html#organizing-a-project-folder",
    "title": "II: Reading Data & IPEDS",
    "section": "Organizing a Project Folder",
    "text": "Organizing a Project Folder\nWe’ll begin with how to organize your course and project files.\n\nThe Kitchen Metaphor (from Dr. Skinner)\n\n“Every data analysis project should have its own set of organized folders. Just like you might organize a kitchen so that ingredients, cookbooks, and prepared food all have a specific cabinet or shelf, so too should you organize your project. We’ll organize our course directory in a similar fashion.”\n\n\nIn Dr. Skinner’s kitchen, think of pristine space where NOTHING sits out on the counter.\nHowever, we are going to have kitchen where some things are stored away in cupboards, but what we regularly work with (the scripts) sits out on the counter\n\nThe kitchen metaphor also works to explain why you might take these approaches\n\nIt’s definitely tidier to keep everything in your kitchen stored away, but, it adds an extra step whenever you want to cook a meal. The same is true here.\nWhy? We will get to that later…\n\n\n\n\n\nEDH 7916 Folder Setup\n\nWith this kitchen metaphor in mind, let’s set up our folder for the class\nWe’ve made an EDH-7916 example folder you can download here which is also available on the class homepage\n\nIn here, you will see\n\nAn .R script template\nA set of numbered .R scripts for our lessons\nA set of numbered .qmd files for our assignments\nA data folder\nA reproducible-report folder (with it’s own data sub-folder)\nA .pdf copy of the syllabus\n\n\nDownload and save this folder wherever you usually keep class folders\n\nHere we use Desktop, but you can use your Documents folder etc. if you wish\nYou can rename the folder if you’d like (but please don’t rename the internal folders)\n\nSee naming guidelines below on how best to name files\n\n\nThroughout the class (and especially in your final project) you may feel the need for other sub-folders for other items (such as one to keep graphs in), but this should be fine for now\n\n\n\nR Project Setup\n\nRStudio has some really helpful features, one of which is creating R Projects easily\n\nAt their very simplest, these can be ways of keeping your RStudio environment saved (especially helpful switching between projects), but also enable more feature like using git (see extra credit lesson)\n\nIt’s pretty easy to set up a project now we have our class folder set up\n\nIn the top right corner of RStudio you’ll see a blue cube with “none” next to it\nClick there, then “new project”\nThen click “from existing directory”\nFind the class folder we just created, select it, and we’re done!\n\nThis is really useful for keeping track of multiple projects, but if this is all you use it for, it will be helpful to keep working directory correct!\n\n\n\nNaming Guidelines\n\nYour class scripts and data files are already named, but there will be numerous files you need to create throughout the class (e.g., everything for your final report, etc.). So it’s best we get on the same page\nAlways name your files something logical\n\nThe file name should always tell something about the purpose of that file or folder\n\nScript numbering\n\nFollowing Hadley Wickham (the founder of RStudio)’s script numbering\n\nBasically start all your script names with the number that they should be run in\n\n01-data-reading\n02-data-cleaning\n03-data-analysis\n\nThis can be especially helpful if you’re keeping scripts in the top level of the project directory to keep things organized\n\n\nGenerally, a good programming tip is to avoid spaces at all costs, use dashes or underscores instead\nIt’s also good to be consistent with capitalization, most traditional programmers will avoid it completely, but if you do it, do it consistently throughout that project\n\nWe used no capitalization through this class\n\nWhatever you do, never (ever, ever) have different versions of files with the same name but different capitalization\n\n\nLastly, try to keep files names as short as possible\n\nLater on we will be in situations where we have to type out file names, so if you go too long, it can become frustrating\n\nHow do we understand the names of our class scripts?\n\nlesson indicating it’s a class script instead of an assignment file\nHadley Wickham’s script numbering corresponding to the order of the lessons\nset, wrangle, viz, quarto, or pro indicate which group of lessons it belongs to\nAnything else is just descriptive, roman numerals for the lesson series, or a descriptive word\n\nWith our class folder now set up, it’s time to go over some other key organization principles\n\n\n\nWorking Directory\n\nThe working directory is almost certainly the most common cause of issues in this class, so this may take a minute to get your head around\nAs a general rule, no matter how you have your folders organized in the future, you usually want your working directory set to where your script is\n\nThat way, you’re always giving directions from the common point of “where we are right now”\n\nThis will then be the same if we move the project folder on our computer, or run it on someone else’s computer\n\n\nBy default, when we open a project in RStudio, RStudio helpfully sets our working directory to the project folder\n\nThis is why we are keeping our scripts out on the counter top so to speak, the default working directory should be the correct one\n\nThat said, there will be times when you need to change your working directory, so, let’s go over the basics of that quickly\n\nFor instance, if you forget to open a project, RStudio will often the leave working directory as your root folder\n\nYou can see the currently working directory path next to the little R icon and version at the top of your console panel\nIf it’s wrong, there are a few ways to change it\n\nFind “session” on the top drop-down menu\n\n\nThen “set working directory”\nThen “To source file location”\n\nThis should be the same as “To project directory” as our scripts are stored at the top level of the project folder\n\n\n\nInstall the this.path package (recall how to do that from last week)\n\n\nWith that installed, call setwd(this.path::here()) at the top of the script\n\nNote: this.path::here() is the same as doing library(this.path) followed by here() but is more efficient if you only want one thing from a package\nAssuming you want the working directory to be the script location, this never hurts to always leave at the top\n\n\n\nNavigate to the desired folder in the files panel (bottom right)\n\n\nSelect the cog symbol\n\nSelect “Set as working directory”\n\nNote: “Go to working directory” can be useful to see what’s in the folder if you navigate away\n\n\n\n\nThe old schoolbase R way setwd(\"&lt;path to your script&gt;\")\n\n\nBut, this really isn’t usually the most efficient\n\n\n\nIf we organize our folder as outlined in this lesson, and use an R project, we shouldn’t need to change this much, but it’s inevitable you will need to change it every now and then\n\n\n\nFile Paths\n\nWhen we are working with R, we (most of the time) need to bring in other items, such as data\n\nIn order to do that, our computer has to find these items, and there are two ways it can do that\n\n\n\nAbsolute Paths\n\nAbsolute paths are directions to what you’re looking for starting from the root of your computer, and list out exactly where a file is. For example, the absolute path of this Quarto script we are now looking is\n\n\"/Users/juewu/Desktop/7916/02-set-data.qmd\"\n\nThis is perfectly fine, assuming two things\n\nWe don’t move the project directory\nIt only needs to run on this computer\n\nOftentimes, we cannot rely on both these assumptions being true\n\nPlus, if we start with these absolute paths and then need to change, it will then become a real pain to update everything\n\nSo, we should ALWAYS use relative paths instead (this is one of the only strict rules for assignments)\n\n\n\n\n\nRelative Paths\n\nImagine you are going to a College of Education cookout, but, you are given directions from my house. That’s only any use if you know where my house is…\n\nInstead, you really want directions from somewhere we all know, like Norman Hall\n\nThat is (basically) how relative paths work, we give directions to to our data from a common point\n\n\nRelative paths are directions to what you’re looking for from where you are right now (a.k.a your “working directory”)\nIf we assume have our working directory set to our shiny new class folder, then, that becomes the starting point for all our directions\n\nTherefore, to access hd2007.csv in out data sub-folder, we just need to say \"data/hd2007.csv\"\n\n\n\n\nWhat If I Need to Go Back a Level?\n\nSometimes we are in a folder, but want to go back a level, i.e. not the folder our current folder is in\n\nThis is very common if with we were using the “pristine kitchen” approach\n\nTo do so is easy, we just add a \"..\" to our relative path\n\nSo, if we are in our class folder on the desktop, and we want to go to another folder on the desktop\n\nwe can do \"../&lt;folder we want&gt;\"",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#script-template",
    "href": "02-set-data.html#script-template",
    "title": "II: Reading Data & IPEDS",
    "section": "Script Template",
    "text": "Script Template\n\nIn our shiny new class folder, you’ll see an r-script-template.R file (thanks to Matt and Ben)\n\nThis a resource for you to use for assignments and other work, feel free to change it to suit your needs\n\nGenerally you can just “Save As” the template everytime you make a new script\n\n\nThe script header block is a useful way to keep more info than a file name can\nThe main reason to use a template is to keep your work organized into sections\n\nThis template has\n\nLibraries to load needed packages\nInput to load data\nPrep to clean the data\nAnalysis to run our analyses\nOutput to save our modified data\n\nHowever, these will not always be the sections you need\n\nIn bigger projects, you might have a whole script for data cleaning\nIn other projects, you might want a section or script just for making plots\nIn your assignments, you’ll likely want a section for each question\n\nThe main point is to ensure you have some kind of sections in your scripts\n\nScripts can be really hard to navigate if you don’t!\n\n\n\n\nQuarto Files\n\nYou should also see a set of .qmd files in your folder\n\nYou will use Quarto for your assignments and final project, and we will go through Quarto in more detail in a few weeks. For now, you only need to know that you can write text along with codes in Quarto, and that you will render it to a PDF file for your assignment submission.",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#reading-in-data",
    "href": "02-set-data.html#reading-in-data",
    "title": "II: Reading Data & IPEDS",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nNext, let’s apply some of this thrilling knowledge about file paths and working directories to read in some data from IPEDS\nTo do this, open lesson-02-set-data.R from your class folder\nFirst up, check your working directory by either\n\nLooking at the top of your console or\nTyping getwd() into the console\n\nThis should be your class folder, but if not, we need to set it there\n\nOn the top drop-down menu, select “Session”, “Set Working Directory,”To Source File Location”\nQuick Question: Without scrolling up, who can remember the other ways of doing this?\n\nOkay, with this set, it’s time to read in our first dataset!\n\nQuick Question 1: Where is our data?\nQuick Question 2: Who remembers how we assign something in R?\n\nWith those questions answered, we have everything we need\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# read_csv() reads in csv files\ndata_ipeds &lt;- read_csv(\"data/hd2007.csv\")\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSuccess!\nWe will cover other ways of reading in data over the course of the class (we can download data directly to R somtimes), but this is most common way, so we are ready for some analysis next week!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#setting-up-github-copilot",
    "href": "02-set-data.html#setting-up-github-copilot",
    "title": "II: Reading Data & IPEDS",
    "section": "Setting up GitHub Copilot",
    "text": "Setting up GitHub Copilot\nOne major change of the course this year is the introduction of GitHub Copilot.\n\nWhat is GitHub Copilot?\nGitHub Copilot is an AI-powered coding assistant that helps write code faster by offering suggestions, autocompleting code, or generating code snippets based on comments or existing code. This is a tool that can be very helpful, but also can be a crutch if not used correctly. We will go over how to set it up and some best practices for using it.\n\n\nHow to set up GitHub Copilot?\n\nFirst, you will need a GitHub account. If you don’t have one, you can sign up one. Feel free to sign up for GitHub Student Developer Pack as a student.\nNext, you will need to enable GitHub Copilot from the Copilot page.\nThen, enable GitHub Copilot Plugin for RStudio.\n\nOn your top menu bar, go to Tools &gt; Global Options &gt; Copilot, and enable it\nYou will be prompted to sign in to your GitHub account and authorize GitHub Copilot to use your account\n\n\n\n\nUsing GitHub Copilot for your projects\nIf you do use GitHub Copilot to help with your codes, please keep your prompts in your comment line and acknowledge with a ## h/t\nThat’s it for R today, phew!\nNow let’s go and explore IPEDS Data Center and see where a lot of contemporary higher education research data comes from!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "href": "02-set-data.html#ipeds-exploration-key-points-for-review",
    "title": "II: Reading Data & IPEDS",
    "section": "IPEDS Exploration Key Points (for review)",
    "text": "IPEDS Exploration Key Points (for review)\n\nIPEDS is an annual federally mandated data collection process (and compliance is a significant portion of many Institutional Researcher jobs)\nThere are a few ways of downloading IPEDS data, if you click through the website you may well find a point-and-click way of selecting specific variables\n\nThis is NOT reproducible and therefore NOT the best practice for research\n\nWe want to use the IPEDS data center to access the complete data files then select and join variables to get our desired data set\n\nFear not, we will go over how to do those things in the first two data wrangling lessons!\n\n\n\n“Data File” vs. “STATA Data File”\n\nFor some NCES data sets, such as HSLS downloaded from NCES DataLab, selecting the Stata file option will download a .dta STATA file format version of the data, which is often nicely labelled\n\nWe can actually read these into R using the haven library from the tidyverse\n\nHowever, for IPEDS, the STATA file option is actually just another .csv file, it’s formatted slightly differently to read into STATA, so there’s no reason to bother with it when using R\n\n\n\nUsing IPEDS Codebooks\n\nTo be able to use most of these big data sets, you need to be able to understand the code. Let’s look together at the codebook for EFFY (headcount enrollment) for 2021\n\nFor IPEDS, the code book is called the dictionary, and is always an Excel file (.xlsx). Other data sources will look different but the general principle will be the same",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "href": "02-set-data.html#other-common-data-sources-for-higher-education-research",
    "title": "II: Reading Data & IPEDS",
    "section": "Other common data sources for higher education research",
    "text": "Other common data sources for higher education research\n\nNational Center for Educational Statistics (also the owner of IPEDS)\n\nLongitudinal Surveys such as HSLS-High School Longitudinal Study and ECLS-Early Childhood Longitudinal Studies\nAdministrative Data (including IPEDS)\nNCES has a good amount of publicly available data, but they also have a LOT of restricted data\n\nTypically publicly available data will be either institution level (school, college, university wide) or fully anonymized. Meanwhile restricted data will often be student level and have some more detailed information\n\nGetting restricted data is tough, but not impossible\n\nYou will need a clear purpose of your study and to know exactly what data you want access to (see available data here)\nYou’ll then need to take this idea to your advisor\n\nFor your final project in this class, your data MUST be publicly available\n\nThis means we must be able to go and download it ourselves, you won’t submit data with your final project submission\n\n\n\n\nCollege Scorecard\n\nDesigned more as a tool for potential college students, college scorecard has data points of interest to this audience, but some things useful for our research too, in particular graduate earning levels\n\nSimilarly to IPEDS, if using College Scorecard, we want to avoid the point-and-click interface and download the entire data files available here\n\n\nNational Bureau of Labor Statistics\n\nLongitudinal surveys, some have educational variables similar to NCES but are often much broader in scope\n\nNational Longitudinal Survey of Youth (NLSY) is one of the most used\n\nThere are publicly available portions of these surveys, but other sections are restricted, see BLS’s accessing data page for more info\n\nCensus & American Community Survey\n\nUseful for population statistics, not student specific\n\nCommon variables for higher ed research include education and income levels for a population\n\nFor example of what is available, see the variables available in the 2019 ACS here\n\nWe will actually do some cool stuff to download ACS data directly to R later in Data Viz III using the tidycensus package\n\n\nMSI Data Project\n\nDetails about MSI classification and funding, includes IPEDS ID numbers to easily link to additional data\n\nMany, many more, have fun exploring!",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#question-one",
    "href": "02-set-data.html#question-one",
    "title": "II: Reading Data & IPEDS",
    "section": "Question One",
    "text": "Question One\na) Create an Excel spreadsheet called r-class-family.xlsx with three columns; name, degree_program, and years_at_uf\nb) Add your information to it\nc) Optional: Add some of your classmates information (recall from class introductions, or, re-introduce yourself after class)\nd) Read this file into R and assign it to an object called data (just like we did in class)\n\nHint: You’ll need to load a new package to read Excel files, between Google and asking your friends, you should be able to figure it out",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#question-two",
    "href": "02-set-data.html#question-two",
    "title": "II: Reading Data & IPEDS",
    "section": "Question Two",
    "text": "Question Two\na) Pick any single data file from IPEDS that peaks your interest from IPEDS complete data files\nb) Download it and save it to your data subfolder in your class folder\nc) Read it into R and assign it to an object called data_ipeds",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "02-set-data.html#submission",
    "href": "02-set-data.html#submission",
    "title": "II: Reading Data & IPEDS",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Getting Started",
      "II: Reading Data & IPEDS"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#data",
    "href": "04-wrangle-ii.html#data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Data",
    "text": "Data\n\nThe data for today’s lesson is all in your data/sch-test folder\n\nIt should look something like this:\n\n\n|__ data/\n    |-- ...\n    |__ sch_test/\n        |-- all_schools.csv\n        |-- all_schools_wide.csv\n        |__ by_school/\n            |-- bend_gate_1980.csv\n            |-- bend_gate_1981.csv\n            |...\n            |-- spottsville_1985.csv\n\nThese fake data represent test scores across three subjects — math, reading, and science — across four schools over six years.\nEach school has a file for each year in the by_school subdirectory.\nThe two files in sch_test directory, all_schools.csv and all_schools_wide.csv, combine the individual files but in different formats.\n\nWe’ll use these data sets to practice appending, joining, and reshaping.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#setup",
    "href": "04-wrangle-ii.html#setup",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Setup",
    "text": "Setup\nAs always, we begin by reading in the tidyverse library.\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\n\n\nAs we did in the past lesson, we will run this script assuming that our working directory is set to the project folder",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#appending-data",
    "href": "04-wrangle-ii.html#appending-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Appending data",
    "text": "Appending data\n\nOur first task is the most straightforward. When appending data, we simply add similarly structured rows to an exiting data frame.\nWhat is similarly structured? Imagine you have a data frame that looks like this:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\n\n\nNow, assume you are given data that look like this:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\n\nThese data are similarly structured: same column names in the same order. If we know that the data came from the same process (e.g., ids represent students in the same classroom with each file representing a different test day), then we can safely append the second to the first:\n\n\n\n\nid\nyear\nscore\n\n\n\n\nA\n2020\n98\n\n\nB\n2020\n95\n\n\nC\n2020\n85\n\n\nD\n2020\n94\n\n\nE\n2020\n99\n\n\nF\n2020\n90\n\n\n\n\nData that are the result of the exact same data collecting process across locations or time may be appended. In education research, administrative data are often recorded each term or year, meaning you can build a panel data set by appending. The NCES IPEDS data files generally work like this\n\nNote on IPEDS: Some of the variables have changed over time, one of the most notable being enrollment data between 2001 and 2002, so always check your dictionary (codebook)!\n\nHowever, it’s incumbent upon you as the researcher to understand your data. Just because you are able to append (R will try to make it work for you) doesn’t mean you always should.\n\nWhat if the score column in our data weren’t on the same scale?\nWhat if the test date mattered but isn’t included in the file?\nWhat if the files actually represent scores from different grades or schools?\n\nIt’s possible that we can account for each of these issues as we clean our data, but it won’t happen automatically — append with care!\n\n\n\n\n\n\nExample\nLet’s practice with an example. First, we’ll read in three data files from the by_school directory.\n\n## read in data, storing in data_*, where * is a unique number\ndata_1 &lt;- read_csv(\"data/sch-test/by-school/bend-gate-1980.csv\")\ndata_2 &lt;- read_csv(\"data/sch-test/by-school/bend-gate-1981.csv\")\ndata_3 &lt;- read_csv(\"data/sch-test/by-school/bend-gate-1982.csv\")\n\n\nLooking at each, we can see that they are similarly structured, with the following columns in the same order: school, year, math, read, science:\n\n\n## show each\ndata_1\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n\ndata_2\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1981   503   312     814\n\ndata_3\n\n# A tibble: 1 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1982   514   316     816\n\n\nFrom the dplyr library, we use the bind_rows() function to append the second and third data frames to the first.\n\n## append files\ndata &lt;- bind_rows(data_1, data_2, data_3)\n\n## show\ndata\n\n# A tibble: 3 × 5\n  school     year  math  read science\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Bend Gate  1980   515   281     808\n2 Bend Gate  1981   503   312     814\n3 Bend Gate  1982   514   316     816\n\n\n\nThat’s it!\n\n\nQuick exercise\nRead in the rest of the files for Bend Gate and append them to the current data frame.\n\n\nQuick exercise: Take Two\nIf bind_rows() stacks tables on top of each other, what do you think would stack them side-by-side? Copy the below code and try to figure how to get them back together\n\n\ndata_split_left &lt;- data[,1:2]\ndata_split_right &lt;- data[,3:5]\n\nprint(data_split_left)\n\n# A tibble: 3 × 2\n  school     year\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Bend Gate  1980\n2 Bend Gate  1981\n3 Bend Gate  1982\n\nprint(data_split_right)\n\n# A tibble: 3 × 3\n   math  read science\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1   515   281     808\n2   503   312     814\n3   514   316     816\n\n## Append them back together side-by-side",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#joining-data",
    "href": "04-wrangle-ii.html#joining-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Joining data",
    "text": "Joining data\n\nMore often than appending your data files, however, you will need to merge or join them.\nWith a join, you add to your data frame new columns (new variables) that come from a second data frame.\nThe key difference between joining and appending is that a join requires a key, that is, a variable or index common to each data frame that uniquely identifies observations.\n\nIt’s this key that’s used to line everything up.\n\n\nFor example, say you have these two data sets,\n\n\n\nid\nsch\nyear\nscore\n\n\n\n\nA\n1\n2020\n98\n\n\nB\n1\n2020\n95\n\n\nC\n2\n2020\n85\n\n\nD\n3\n2020\n94\n\n\n\n\n\n\nsch\ntype\n\n\n\n\n1\nelementary\n\n\n2\nmiddle\n\n\n3\nhigh\n\n\n\n\nYou want to add the school type to the first data set.\nYou can do this because you have a common key between each set: sch.\n\n\nAdd a column to the first data frame called type\nFill in each row of the new column with the type value that corresponds to the matching sch value in both data frames:\n\nsch == 1 --&gt; elementary\nsch == 2 --&gt; middle\nsch == 3 --&gt; high\n\n\nThe end result would then look like this:\n\n\n\nid\nsch\nyear\nscore\ntype\n\n\n\n\nA\n1\n2020\n98\nelementary\n\n\nB\n1\n2020\n95\nelementary\n\n\nC\n2\n2020\n85\nmiddle\n\n\nD\n3\n2020\n94\nhigh\n\n\n\n\nExample\n\nA common join task in education research involves adding group-level aggregate statistics to individual observations, for example;\n\nadding school-level average test scores to each student’s row.\nWith a panel data set (observations across time), we might want within-year averages added to each unit-by-time period row.\n\nLet’s do the second, adding within-year across school average test scores to each school-by-year observation.\n\n\n## read in all_schools data\ndata &lt;- read_csv(\"data/sch-test/all-schools.csv\")\n\n\nLooking at the data, we see that it’s similar to what we’ve seen above, with additional schools.\n\n\n## show\ndata\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\nOur task is two-fold:\n\nGet the average of each test score (math, reading, science) across all schools within each year and save the summary data frame in an object.\nJoin the new summary data frame to the original data frame.\n\n\n1. Get summary\n\n## get test score summary \ndata_sum &lt;- data |&gt;\n    ## grouping by year so average within each year\n    group_by(year) |&gt;\n    ## get mean(&lt;score&gt;) for each test\n    summarize(math_m = mean(math),\n              read_m = mean(read),\n              science_m = mean(science))\n\n## show\ndata_sum\n\n# A tibble: 6 × 4\n   year math_m read_m science_m\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  1980   507    295.      798.\n2  1981   496.   293.      788.\n3  1982   506    302.      802.\n4  1983   500    293.      794.\n5  1984   490    300.      792.\n6  1985   500.   290.      794.\n\n\n\nQuick exercise\nThinking ahead, why do you think we created new names for the summarized columns? Why the _m ending?\n\n\n\n2. Join\n\nWhile one can merge using base R, dplyr (part of the tidyverse package) is what we will be using in this lesson.\nHere are the most common joins you will use:\nleft_join(x, y, by = ): keep all x, drop unmatched y\nright_join(x, y, by = ): keep all y, drop unmatched x\ninner_join(x, y, by = ): keep only matching\nfull_join(x, y, by = ): keep everything\nanti_join(x, y, by = ): keep only obs in x and that are not in y (more useful than you’d think)\nEssentially, all _join() functions takes three main “arguments” and they have always the same meaning\n\nx data one, a.k.a. the “left” data\ny data two, a.k.a. the “right” data\nby the variables to use as a “key”\n\n\n\n\n\nby\n\nWhichever type of _join you are doing, the by argument is just how x and y are being matched up\n\nby has to be at least one variable that is in both x and y and identifies the same observation\n\ne.g., by = \"school_id\"\nMost often this will be some kind of identifying variable (student ID, school ID, city, state, region, etc.)\n\nIf have more than one piece of information to match on, such as school and year, we can specify that with c()\n\ne.g., by = c(\"school_id\", \"year\")\nThis will then find information for each school in each year, and join it in correctly\n\n\nIf you don’t provide any by arguments, R will try to be smart and look for columns names that are the same in both data sets\n\nThis can lead to incorrect joins though, so always best to specify\n\nLastly, if you need to join by columns that have different names in each data set use c(\"name in data_1\" = \"name in data_2\")\n\ne.g., by = c(\"student_id\" = \"id_number\")\n\n\n\n\nConceptual example\n\nFor example, the result of a left join between data frame X and data frame Y will include all observations in X and those in Y that are also in X.\n\nX\n\n\n\nid\ncol_A\ncol_B\n\n\n\n\n001\na\n1\n\n\n002\nb\n2\n\n\n003\na\n3\n\n\n\nY\n\n\n\nid\ncol_C\ncol_D\n\n\n\n\n001\nT\n9\n\n\n002\nT\n9\n\n\n004\nF\n9\n\n\n\nXY (result of left join)\n\n\n\nid\ncol_A\ncol_B\ncol_C\ncol_D\n\n\n\n\n001\na\n1\nT\n9\n\n\n002\nb\n2\nT\n9\n\n\n003\na\n3\nNA\nNA\n\n\n\n\nObservations in both X and Y (001 and 002, above), will have data for the columns that were separately in X and Y before.\nThose in X only (003), will have missing values in the new columns that came from Y because they didn’t exist there.\nObservations in Y but not X (004) are dropped entirely.\n\n\n\nPractice joining\nBack to our example…\n\nSince we want to join a smaller aggregated data frame, data_sum, to the original data frame, data, we’ll use a left_join().\n\nThe join functions will try to guess the joining variable (and tell you what it picked) if you don’t supply one, but we’ll specify one to be clear.\n\n\n\n## start with our dataframe...\ndata_joined &lt;- data |&gt;\n    ## pipe into left_join to join with data_sum using \"year\" as key\n    left_join(data_sum, by = \"year\")\n\n## show\ndata_joined\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nQuick exercise\nLook at the first 10 rows of data_joined. What do you notice about the new summary columns we added?\n\n\n\nUsing the |&gt; pipe\n\nNow, if you remember from Data Wrangling I, the |&gt; makes R code more intuitive by “piping” one thing into the next\n\nThis makes things simpler 99% of the time (no one wants to be writing nested code)\nBut in this case it takes a second to get your head around\n\nBy default, the pipe |&gt; will always go into the first “argument” of a function, which in this case, is x\n\nWe can always specify where it should go with an _ underscore\n\n\n\n## Therefore \nleft_join(x = data,\n          y = data_sum,\n          by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n## Is exactly the same as\ndata |&gt;\n  left_join(x = _, ## If it helps to visualize, the _ is where the |&gt; will go\n            y = data_sum,\n            by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n## Is exactly the same as\ndata |&gt;\n  left_join(data_sum,\n            by = \"year\")\n\n# A tibble: 24 × 8\n   school        year  math  read science math_m read_m science_m\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808   507    295.      798.\n 2 Bend Gate     1981   503   312     814   496.   293.      788.\n 3 Bend Gate     1982   514   316     816   506    302.      802.\n 4 Bend Gate     1983   491   276     793   500    293.      794.\n 5 Bend Gate     1984   502   310     788   490    300.      792.\n 6 Bend Gate     1985   488   280     789   500.   290.      794.\n 7 East Heights  1980   501   318     782   507    295.      798.\n 8 East Heights  1981   487   323     813   496.   293.      788.\n 9 East Heights  1982   496   294     818   506    302.      802.\n10 East Heights  1983   497   306     795   500    293.      794.\n# ℹ 14 more rows\n\n\n\nYou can write joins whichever way you want (piped or not-piped)\n\nEventually, you will find piped is easier and more-efficient, but focus on whichever way makes more sense to you for now\n\n\n\n## Note: if we want to keep the joined data, we should assign it to data_join\ndata_join &lt;- data |&gt;\n  left_join(data_sum,\n            by = \"year\")\n\n\n_join summary\n\nJoining data is one of the most important tasks in educational research\n\nIPEDS Complete Data Files are the perfect example, within each year all the data is stored in separate files that need joining\n\nRemember the same IPEDS files for different years need bind_rows() once you add a year identifier\n\n\nIt takes a while to get your head around, but if you can do it properly, you are well on the way to mastering data wrangling!",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#reshaping-data",
    "href": "04-wrangle-ii.html#reshaping-data",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nReshaping data is a common and important data wrangling task.\nWhether going from wide to long format or long to wide, it can be a painful process.\n\nBut with a little practice, the ability to reshape data will become a powerful tool in your toolbox.\n\n\n\nDefinitions\n\nWhile there are various definitions of tabular data structure, the two you will most often come across are wide and long.\n\nWide data are data structures in which all variable/values are columns.\n\nAt the extreme end, every id will only have a single row:\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nmath_score_2019\nread_score_2019\nmath_score_2020\nread_score_2020\n\n\n\n\nA\n93\n88\n92\n98\n\n\nB\n99\n92\n97\n95\n\n\nC\n89\n88\n84\n85\n\n\n\n\nNotice how each particular score (by year) has its own column?\n\nCompare this to long data in which each observational unit (id test score within a given year) will have a row:\n\n\n\n\n\nid\nyear\ntest\nscore\n\n\n\n\nA\n2019\nmath\n93\n\n\nA\n2019\nread\n88\n\n\nA\n2020\nmath\n92\n\n\nA\n2020\nread\n98\n\n\nB\n2019\nmath\n99\n\n\nB\n2019\nread\n92\n\n\nB\n2020\nmath\n97\n\n\nB\n2020\nread\n95\n\n\nC\n2019\nmath\n89\n\n\nC\n2019\nread\n88\n\n\nC\n2020\nmath\n84\n\n\nC\n2020\nread\n85\n\n\n\n\nThe first wide and second long table present the same information in a different format.\n\nSo why bother reshaping?\n\nThe short answer is that you sometimes need one format and sometimes the other due to the demands of the analysis you want to run, the figure you want to plot, or the table you want to make.\n\n\n\n\nNote: Data in the wild are often some combination of these two types: wide-ish or long-ish. For an example, see our all-schools.csv data below, which is wide in some variables (test), but long in others (year). The point of defining long vs wide is not to have a testable definition, but rather to have a framework for thinking about how your data are structured and if that structure will work for your data analysis needs.\n\n\n\nExample: wide –&gt; long\nTo start, we’ll go back to the all_schools.csv file.\n\n## reading again just to be sure we have the original data\ndata &lt;- read_csv(\"data/sch-test/all-schools.csv\")\n\n## print to see what the data structure looks like\nprint(data)\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\nNotice how the data are wide in test:\n\nEach school has one row per year, but each test (math, read, science) gets its own column.\nWhile this setup can be efficient for storage, it’s not always the best for analysis or even just browsing.\n\nWhat we want is for the data to be long.\n\n\nInstead of each test having its own column, we would like to make the data look like our long data example above, with each row representing a single school, year, test, score:\n\n\n\n\n\n\n\n\n\n\nschool\nyear\ntest\nscore\n\n\n\n\nBend Gate\n1980\nmath\n515\n\n\nBend Gate\n1980\nread\n281\n\n\nBend Gate\n1980\nscience\n808\n\n\n…\n…\n…\n…\n\n\n\n\nAs with joins, you can reshape data frames using base R commands.\nBut again, we’ll use tidyverse functions in the tidyr library.\n\nSpecifically, we’ll rely on the tidyr pivot_longer() and pivot_wider() commands.\n\n\n\npivot_longer()\nThe pivot_longer() function can take a number of arguments, but the core things it needs to know are:\n\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\ncols: the names of the columns that you want to pivot into values of a single new column (thereby making the data frame “longer”)\nnames_to: the name of the new column that will contain the names of the cols you just listed\nvalues_to: the name of the column where the values in the cols you listed will go\nIn our current situation, our cols to pivot are \"math\", \"read\", and \"science\".\n\nSince they are test types, we’ll call our names_to column \"test\" and our values_to column \"score\".\n\n\n\n## wide to long\ndata_long &lt;- data |&gt;\n    ## cols: current test columns\n    ## names_to: where \"math\", \"read\", and \"science\" will go\n    ## values_to: where the values in cols will go\n    pivot_longer(cols = c(\"math\",\"read\",\"science\"),\n                 names_to = \"test\",\n                 values_to = \"score\")\n\n## show\ndata_long\n\n# A tibble: 72 × 4\n   school     year test    score\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 Bend Gate  1980 math      515\n 2 Bend Gate  1980 read      281\n 3 Bend Gate  1980 science   808\n 4 Bend Gate  1981 math      503\n 5 Bend Gate  1981 read      312\n 6 Bend Gate  1981 science   814\n 7 Bend Gate  1982 math      514\n 8 Bend Gate  1982 read      316\n 9 Bend Gate  1982 science   816\n10 Bend Gate  1983 math      491\n# ℹ 62 more rows\n\n\n\nQuick (ocular test) exercise\nHow many rows did our initial data frame data have? How many unique tests did we have in each year? When reshaping from wide to long, how many rows should we expect our new data frame to have? Does our new data frame have that many rows?\n\n\n\n\nExample: long –&gt; wide\n\npivot_wider()\n\nNow that we have our long data, let’s reshape it back to wide format using pivot_wider(). In this case, we’re doing just the opposite from before — here are the main arguments you need to attend to:\ndata: the name of the data frame you’re reshaping (we can use |&gt; to pipe in the data name)\nnames_from: the name of the column that contains the values which will become new column names\nvalues_from: the name of the column that contains the values associated with the values in names_from column; these will go into the new columns.\n\n\n## long to wide\ndata_wide &lt;- data_long |&gt;\n    ## names_from: values in this column will become new column names\n    ## values_from: values in this column will become values in new cols\n    pivot_wider(names_from = \"test\",\n                values_from = \"score\")\n\n## show\ndata_wide\n\n# A tibble: 24 × 5\n   school        year  math  read science\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bend Gate     1980   515   281     808\n 2 Bend Gate     1981   503   312     814\n 3 Bend Gate     1982   514   316     816\n 4 Bend Gate     1983   491   276     793\n 5 Bend Gate     1984   502   310     788\n 6 Bend Gate     1985   488   280     789\n 7 East Heights  1980   501   318     782\n 8 East Heights  1981   487   323     813\n 9 East Heights  1982   496   294     818\n10 East Heights  1983   497   306     795\n# ℹ 14 more rows\n\n\n\n\npivot_()-ing with separation\n\nUnfortunately, it’s not always so clear cut to reshape data.\nIn this second example, we’ll again reshape from wide to long, but with an extra argument that helps when there’s more than one piece of information in the variable name\n\nFirst, we’ll read in a second file all_schools_wide.csv - This file contains the same information as before, but in a very wide format\n\n## read in very wide test score data\ndata &lt;- read_csv(\"data/sch-test/all-schools-wide.csv\")\n\n## show\ndata\n\n# A tibble: 4 × 19\n  school       math_1980 read_1980 science_1980 math_1981 read_1981 science_1981\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Bend Gate          515       281          808       503       312          814\n2 East Heights       501       318          782       487       323          813\n3 Niagara            514       292          787       499       268          762\n4 Spottsville        498       288          813       494       270          765\n# ℹ 12 more variables: math_1982 &lt;dbl&gt;, read_1982 &lt;dbl&gt;, science_1982 &lt;dbl&gt;,\n#   math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;, science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;,\n#   read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;, math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;,\n#   science_1985 &lt;dbl&gt;\n\n\nYou see, each school has only one row and each test by year value gets its own column in the form &lt;test&gt;_&lt;year&gt;.\n\nWe will use pivot_longer() just as we did before\n\nBut instead of one column for names_to we use our friend c() to list two columns we want the information from column name to go to\nThen, we add names_sep = \"_\", which means separate the information from the names at every underscore\n\nI.e., this will put everything before the underscore in the first column test and everything after into the second column year\n\n\n\n\n## wide to long\ndata_long &lt;- data |&gt;\n    ## contains() looks for \"19\" in name: if there, it adds it to cols\n    pivot_longer(cols = contains(\"19\"),\n                 names_to = c(\"test\", \"year\"),\n                 names_sep = \"_\",\n                 values_to = \"score\")\n\n## show\ndata_long\n\n# A tibble: 72 × 4\n   school    test    year  score\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Bend Gate math    1980    515\n 2 Bend Gate read    1980    281\n 3 Bend Gate science 1980    808\n 4 Bend Gate math    1981    503\n 5 Bend Gate read    1981    312\n 6 Bend Gate science 1981    814\n 7 Bend Gate math    1982    514\n 8 Bend Gate read    1982    316\n 9 Bend Gate science 1982    816\n10 Bend Gate math    1983    491\n# ℹ 62 more rows\n\n\n\nQuick exercise(s)\n\nWhat do you think we’d need to change if the column name had 3 pieces of information all separated by an underscore?\nWhat about if the information was separated by a . period?\n\n\n\nNow, if we want to get our data back in to extra long form, we can use a very similar argument in pivot_wider()\n\nWe just use c() to say get the name information from two columns\nnames_sep = \"_\" is identical to before, but this time it’s saying to place and underscore as the separator\n\n\n\n## wide to long\ndata_wide &lt;- data_long |&gt;\n    pivot_wider(values_from = score,\n                names_from = c(test, year),\n                names_sep = \"_\")\n\n## show\ndata_wide\n\n# A tibble: 4 × 19\n  school       math_1980 read_1980 science_1980 math_1981 read_1981 science_1981\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Bend Gate          515       281          808       503       312          814\n2 East Heights       501       318          782       487       323          813\n3 Niagara            514       292          787       499       268          762\n4 Spottsville        498       288          813       494       270          765\n# ℹ 12 more variables: math_1982 &lt;dbl&gt;, read_1982 &lt;dbl&gt;, science_1982 &lt;dbl&gt;,\n#   math_1983 &lt;dbl&gt;, read_1983 &lt;dbl&gt;, science_1983 &lt;dbl&gt;, math_1984 &lt;dbl&gt;,\n#   read_1984 &lt;dbl&gt;, science_1984 &lt;dbl&gt;, math_1985 &lt;dbl&gt;, read_1985 &lt;dbl&gt;,\n#   science_1985 &lt;dbl&gt;",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#final-note",
    "href": "04-wrangle-ii.html#final-note",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Final note",
    "text": "Final note\n\nJust as all data sets are unique, so too are the particular steps you may need to take to append, join, or reshape your data.\n\nEven experienced coders rarely get all the steps correct the first try.\nBe prepared to spend time getting to know your data and figuring out, through trial and error, how to wrangle it so that it meets your analytic needs.\nCode books, institutional/domain knowledge, and patience are your friends here!",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#question-one",
    "href": "04-wrangle-ii.html#question-one",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Question One",
    "text": "Question One\na) Compute the average test score by region\nb) Join back into the full data frame\nc) Compute the difference between each student’s test score and that of the region\nd) Finally, show the mean of these differences by region\n\nHint: If you think about it, this should probably be a very very small number…\n\ne) Optional: Do all of the above steps in one piped chain of commands",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#question-two",
    "href": "04-wrangle-ii.html#question-two",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Question Two",
    "text": "Question Two\na) Compute the average test score by region and family income level\nb) Join that average score back to the full data frame\n\nHint: You can join on more than one key using c()\n\nc) Optional: Do all of the above steps in one piped chain of commands",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#question-three",
    "href": "04-wrangle-ii.html#question-three",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Question Three",
    "text": "Question Three\na) Select the following variables from the full data set\n\nstu_id\nx1stuedexpct\nx1paredexpct\nx4evratndclg\n\nb) From this reduced data frame, reshape the data frame so that it is long in educational expectations\n\nAs in, each observation should have two rows, one for each educational expectation type\n\ne.g. (your column names and values may be different)\n\n\n\nstu_id\nexpect_type\nexpectation\nx4evratndclg\n\n\n\n\n0001\nx1stuedexpct\n6\n1\n\n\n0001\nx1paredexpct\n7\n1\n\n\n0002\nx1stuedexpct\n5\n1\n\n\n0002\nx1paredexpct\n5\n1",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "04-wrangle-ii.html#submission",
    "href": "04-wrangle-ii.html#submission",
    "title": "II: Appending, joining, & reshaping data",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "II: Appending, joining, & reshaping data"
    ]
  },
  {
    "objectID": "06-viz-ii.html#libraries",
    "href": "06-viz-ii.html#libraries",
    "title": "II: Customization",
    "section": "Libraries",
    "text": "Libraries\n\nIn addition to tidyverse, we’ll add a new library, patchwork, that we’ll use toward the end of the lesson.\nIf you haven’t already downloaded it, be sure to install it first using install.packages(\"patchwork\").\n\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n\nFinally, we’ll load the data file we’ll be using, hsls-small.csv\n\n\n## ---------------------------\n##' [Input data]\n## ---------------------------\n\ndata &lt;- read_csv(\"data/hsls-small.csv\")",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#initial-plot-with-no-formatting",
    "href": "06-viz-ii.html#initial-plot-with-no-formatting",
    "title": "II: Customization",
    "section": "Initial plot with no formatting",
    "text": "Initial plot with no formatting\n\nRather than make a variety of plots, we’ll focus on making and incrementally improving a single figure (with some slight variations along the way)\nIn general, we’ll be looking at math test scores via the x1txtmscor data column\nFirst, we need to clean our variable of interest\n\nAs you may recall from an earlier lesson, x1txmtscor is a normed variable with a mean of 50 and standard deviation of 10.\n\nThat means any negative values are missing data, which for our purposes, will be dropped\n\n\n\n\nQuick Question: We didn’t have to do this last week as default behavior when plotting is simply to drop missing values, which worked fine using the .dta version of this file. Why wouldn’t that work with the .csv version of the same data?\n\n\n## -----------------------------------------------------------------------------\n##' [Initial plain plot]\n## -----------------------------------------------------------------------------\n\n## Drop missing values for math test score\ndata &lt;- data |&gt;\n  filter(x1txmtscor != -8)\n\n\nFirst, let’s make a plain histogram with no settings\nLast week we never assigned the plot to an object, we always just had it print out\nThis week, as we are going to be focusing on one plot, editing and adding to it, we are always going to assign the plot to p and then print it out\n\nRemember: ggplots are layered\n\nIf we want to change the base layers, we will have to start again and overwrite p\nIf we want to add more detail to p we can just add those layers like p + &lt;layer&gt;\n\nIf we want to overwrite something that is already in p, we just specify it again\n\nAnything we don’t say will be left as is\n\n\n\n\n\n\n## create histogram using ggplot\np &lt;- ggplot(data = data) +\n  geom_histogram(mapping = aes(x = x1txmtscor))\n\n## show\np\n\n\n\n\n\n\n\n\nSo there it is. Let’s make it better.",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#titles-and-captions",
    "href": "06-viz-ii.html#titles-and-captions",
    "title": "II: Customization",
    "section": "Titles and captions",
    "text": "Titles and captions\n\nThe easiest things to improve on a figure are the title, subtitle, axis labels, and caption\nAs with a lot of ggplot2 commands, there are a few different ways to set these labels, but the most straightforward way is to use the labs() function\n\nThis can be added as another layer to our existing plot p\n\n\n\n## -----------------------------------------------------------------------------\n##' [Titles and captions]\n## -----------------------------------------------------------------------------\n\n## Add placeholder titles/labels/captions\np &lt;- p +\n  labs(title = \"Title\",\n       subtitle = \"Subtitle\",\n       caption = \"Caption\",\n       x = \"X axis label\",\n       y = \"Y axis label\")\n\n## show \np\n\n\n\n\n\n\n\n\n\nRather than accurately labeling the figure, we’ve repeated the arguments in strings so that it’s clearer where every piece goes\n\nThe title is of course on top\nWith the subtitle in a smaller font just below\nThe x and y axis labels go with their respective axes\nThe caption is right-aligned below the figure\n\nYou don’t have to use all of these options for every figure\nIf you don’t want to use one, you have a couple of options:\n\nIf the argument would otherwise be blank (title, subtitle, and caption), you can just leave the argument out of labs()\nIf the argument will be filled, as is the case on the axes (ggplot will use the variable name by default), you can use NULL\n\nTo make our figure nicer, we’ll add a title, axis labels, and caption describing the data source\n\nWe don’t really need a subtitle and since there’s no default value, we’ll just leave it out\nNotice: p right now still has the placeholder labs() arguments, so we can just call it and overwrite labs with a new call\n\n\n\n## create histogram using ggplot\np &lt;- p +\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\")\n\n## show \np\n\n\n\n\n\n\n\n\n\nQuick question: What about these labels is a little silly? Try to fix it so it looks like the below\n\n\n\n\n\n\n\n\n\n\nWith that fixed, now we’ll move to improving the axis scales!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#axis-formatting",
    "href": "06-viz-ii.html#axis-formatting",
    "title": "II: Customization",
    "section": "Axis formatting",
    "text": "Axis formatting\n\nIn general, the default tick mark spacing and accompanying labels are pretty good\nBut sometimes we want to change them, sometimes to have fewer ticks and sometimes to have more\nFor this figure, we could use more ticks on the x axis to make differences in math test score clearer\nWe’ll increase the number of tick marks on the y axis too.\nTo change these values, we need to use scale_&lt; aesthetic &gt;_&lt; distribution &gt; function\n\nThese may seem strange at first, but they follow a logic. Specifically:\n\n&lt; aesthetic &gt;: x, y, fill, colour, etc (what is being changed?)\n&lt; distribution &gt;: is the underlying variable continuous, discrete, or do you want to make a manual change?\n\n\n\nTo change our x and y tick marks we will use:\n\nscale_x_continuous()\nscale_y_continuous()\n\nWe use x and y because those are the aesthetics being adjusted\nwe use continuous in both cases because math_test on the x axis and the histogram counts on the y axis are both continuous variables\n\nThere are a LOT of options within these scale_ functions\nThey depend on what kind of aesthetic you’re adjusting\n\nFor now we will focus on using two\n\nbreaks: where the major lines are going (they get numbers on the axis)\nminor_breaks: where the minor lines are going (they don’t get numbers on the axis)\n\n\nBoth breaks and minor_breaks take a vector of numbers\nWe can put each number in manually using c() (e.g., c(0, 10, 20, 30, 40)), but a better way is to use R’s seq() function:\n\nseq(start, end, by)\n\nNotice that within each scale_*() function, we use the same start and end values for both minor and major breaks, just change the by option\n\nThis will give us axis numbers at spaced intervals with thinner, unnumbered lines between.\n\n\n\n## -----------------------------------------------------------------------------\n##' [Axis formatting]\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np &lt;- p +\n    scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 100),\n                     minor_breaks = seq(from = 0, to = 2500, by = 50))\n\n## show \np\n\n\n\n\n\n\n\n\n\nWe certainly have more lines now. Maybe too many on the y axis, which is a sort of low-information axis\n\ndo we need really that much detail for histogram counts?\n\nLet’s keep what we have for the x axis and increase the by values of the y axis\n\n\np &lt;- p +\n    scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100))\n\n## show\np\n\n\n\n\n\n\n\n\n\nThat seems like a better balance. We’ll stick with that and move on to legend labels",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#legend-labels",
    "href": "06-viz-ii.html#legend-labels",
    "title": "II: Customization",
    "section": "Legend labels",
    "text": "Legend labels\n\nLet’s make our histogram a little more complex by separating math scores by parental education\n\nSpecifically, we’ll use a binary variable that represents, did either parent attend college?\n\nFirst, we need to create a new variable, pared_coll, from the ordinal variable, x1paredu\n\nAlso, we are going make it a factor() so R knows the 0 and 1 don’t mean numbers, they mean cateogories\n\nWe did this last week, but this time we are going add labels too\n\nSpecifically, we want to use levels and labels arguments\n\nThese pair up to make a labelled factor\n\nlevels should be a list of the values you have in the column\n\nIn this case just 0 and 1, so levels = c(0,1)\n\nlabels should be a list of the labels you want to use\n\nThese should be strings (in ““) in the same order as the levels you want to tie them to&lt;&gt;\nIn this case, we can say labels = c(\"No college\",\"College\") to match the levels = c(0,1)\n\n\n\n\n\n\n## -----------------------------------------------------------------------------\n##' [Legend labels]\n## -----------------------------------------------------------------------------\n\n## add indicator that == 1 if either parent has any college\ndata &lt;- data |&gt;\n  mutate(pared_coll = ifelse(x1paredu &gt;= 3, 1, 0),\n         pared_coll = factor(pared_coll,\n                             levels = c(0,1),\n                             labels = c(\"No college\", \"College\")))\n\n\nNow we’ll make our same histogram, but add the fill aesthetic\n\nNote: since we are changing our underlying plot data, it’s best to start again and store the plot in p2\n\n\n\nQuick question: We also added one other new line, who can remember what alpha = 0.66 does?\n\n\np2 &lt;- ggplot(data = data) +\n  geom_histogram(mapping = aes(x = x1txmtscor,\n                               fill = factor(pared_coll)),\n                 alpha = 0.66) +\n  ## Below here is just what we had added to p in previous steps\n  labs(title = \"Math test scores\",\n       caption = \"Data: High School Longitudinal Study, 2009\",\n       x = \"Math score\",\n       y = \"Count\") +\n      scale_x_continuous(breaks = seq(from = 0, to = 100, by = 5),\n                     minor_breaks = seq(from = 0, to = 100, by = 1)) +\n    scale_y_continuous(breaks = seq(from = 0, to = 2500, by = 500),\n                     minor_breaks = seq(from = 0, to = 2500, by = 100))\n\n## show \np2\n\n\n\n\n\n\n\n\n\nClose! But, factor(pared_coll) isn’t the best name for our legend…\n\n\nQuick question: try adding something that will fix this issue and make it look like below",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#color-scales",
    "href": "06-viz-ii.html#color-scales",
    "title": "II: Customization",
    "section": "Color Scales",
    "text": "Color Scales\n\nThis graph still looks a little ugly\n\nMany people are red-green colorblind\n\nR chooses shades that aren’t too bad for this, but it’s still not great\n\nWhen we changed the color of a plot last week, it was purely decorative, so it went inside the geom_ function\nThis time, we want to change the colors associated with aes() element, so, we are changing a scale_\n\nSpecifically scale_fill and we are going to choose manual colors, so scale_fill_manual()\n\nThis takes values = c(&lt;list of colors the same length as factor&gt;)\n\nIn this case our fill variable has two levels, so we need two colors\n\nFor school spirit, let’s pick a shade of orange and blue\n\nTip: You can add 1,2,3, or 4 next to a color in R to make it darker\n\n\n\n\n\n\n\n\n## ---------------------------\n##' [Color Scales]\n## ---------------------------\n\n## create histogram using ggplot\np2 &lt;- p2 +\n  scale_fill_manual(values = c(\"blue4\", \"orange2\"))\n\n## show \np2\n\n\n\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#facet_wrap",
    "href": "06-viz-ii.html#facet_wrap",
    "title": "II: Customization",
    "section": "facet_wrap()",
    "text": "facet_wrap()\n\nNow, what if we want to see how this trend varies by sex?\nRemember from last week, we can use facet_wrap() to split data and make separate plots for each level of a variable\n\nfacet_wrap() uses a ~ tilde to mean “facet by this variable”\n\n\n\n## Add a facet wrap for sex\np2 &lt;- p2 +\n  facet_wrap(~x1sex)\n\n## show \np2\n\n\n\n\n\n\n\n\n\nOkay, not a bunch of difference between sexes here, but this is interesting\n\n\nQuick question: without going back and changing the data (i.e., within the facet_wrap command, turn the x1sex variable into a factor with labels of the correct names)\n\n\nFrom the Codebook: 1 is Male, 2 is Female\n\n\n\n\n\n\n\n\n\n\n\nOkay, really getting close, but now that scale we set earlier is cramped…\n\n\nQuick task: add a new scale_x_continuous() to overwrite the “breaks” we set earlier with a sequence from 0 to 100 by 10, like below\n\n\n\n\n\n\n\n\n\n\n\nOkay, this is starting to come together!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#themes",
    "href": "06-viz-ii.html#themes",
    "title": "II: Customization",
    "section": "Themes",
    "text": "Themes\n\nNow that we have our figure mostly set up, we can adjust some of the overall appearence using theme() arguments\n\n\nPreset Themes\n\nThere are a number of preset themes in ggplot2\n\nFeel free to play around with these for the plots in your report\n\nFirst, let’s take a look at theme_bw()\n\n\n## -----------------------------------------------------------------------------\n##' [Preset themes]\n## -----------------------------------------------------------------------------\n\n## create histogram using ggplot\np2 &lt;- p2 +\n  theme_bw()\n\n## show \np2\n\n\n\n\n\n\n\n\n\nHow about theme_linedraw()?\n\n\np2 &lt;- p2 +\n  theme_linedraw()\n\n## show \np2\n\n\n\n\n\n\n\n\n\nYes, that looks pretty smart now!\n\n\n\nEditing Specific Theme Elements\n\nThis is where you can really dive into the depths of customization\nWe don’t have time to do much of this here, but for demonstration’s sake, let’s change one wildly specific thing\n\nLet’s tinker with the x axis ticks\n\nMake them a little thicker to 0.8 units\n\nggplot units are how you resize most things in ggplot, easiest way to pick a number is to test a value and adjust from there\n\nMake them round-ended pill shapes\n\n\n\n\np2 &lt;- p2 +\n  theme(axis.ticks.x = element_line(linewidth = 0.8,\n                                    lineend = \"round\"))\n\n## show \np2\n\n\n\n\n\n\n\n\n\nEditing theme elements is always the same, you need to play around to learn this on your own, but the general gist is\n\n\nAdd a theme() argument\n\n\nAlways below any preset themes, as those will overwrite anything you set\n\n\nInside theme() there are many many options which you can see listed here\nPick one of more and assign to it = an element_\nelement_ come in different types including element_line() like we used above, element_text(), element_rect(), and others\nEach of these has specific options you can customize as you wish\n\n\nThere is a world of possibilities in customization, depending on how much time you want to dedicate!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#multiple-plots-with-patchwork",
    "href": "06-viz-ii.html#multiple-plots-with-patchwork",
    "title": "II: Customization",
    "section": "Multiple plots with patchwork",
    "text": "Multiple plots with patchwork\n\nIn this final section, we’ll practice putting multiple figures together\nAll the plots we’ve made so far have been one single ggplot object, but, we can put more than one together if we want\nWe use the patchwork library!\nWe technically have our original p plot saved, but we’ve made such progress since then, let’s whip up a nicer second plot and assign it to p3\nThere are some new things here, but it should be mostly familiar\n\nscale_fill_viridis_d() sets color-blind friendly palettes, especially useful when you have a categorical variable\n\nGradient color sets (greyscale, shades of red, etc.) are pretty color-blind friendly, as without any color, they all will look like greyscale\n\nThis is also just useful for printing in black and white!\nTypically though, gradient colors imply increasing/decreasing values of a continuous variable (e.g., darker red means more GOP on election maps)\n\nThe viridis palettes in R have been designed to look like discrete color scales, i.e., they don’t look just like one color getting darker to most readers\n\nBut they also use shading/darkness/lightness of the colors, which means if printed in black and white, or read by someone who can’t see colors, they still work\n\n\nbegin and end just allow you to trim the more extreme ends of the color palette off - This is useful in situations like this when you only have two categories and don’t want them to look totally black and white\n\n\n\n## -----------------------------------------------------------------------------\n##' [Multiple plots with patchwork]\n## -----------------------------------------------------------------------------\n\n## Make a nice looking second plot of math scores by by parental education\np3 &lt;- ggplot(data) +\n  geom_boxplot(mapping = aes(x = pared_coll,\n                             y = x1txmtscor,\n                             fill = pared_coll),\n               alpha = 0.66) +\n  scale_fill_viridis_d(option = \"magma\", begin = 0.2, end = 0.8) +\n  labs(x = NULL,\n       y = \"Math Score\",\n       fill = \"Parental Education\") +\n  theme_linedraw()\n\n## show\np3\n\n\n\n\n\n\n\n\n\nNow that we have our new figure, let’s paste it side by side (left-right) with our previous figure\nOnce we’ve loaded the patchwork library (like we already did at the top of the script), we can use a + sign between out two ggplot objects\n\np2 + p3\n\n\n\n## use plus sign for side by side\np2 + p3\n\n\n\n\n\n\n\n\n\nThis is super squished, and even if we zoom in it’s not great…\n\nLet’s try on top of each other instead\n\nIn patchwork, this is / rather than +\n\n\n\n\np2 / p3\n\n\n\n\n\n\n\n\n\nCool! But there’s still a few things we may not like…\n\nThe orange and blue clashes against the viridis colors\n\nSo let’s edit p2 to also use the magma viridis scale too\n\n\n\n\np2 &lt;- p2 +\n  scale_fill_viridis_d(option = \"magma\", begin = 0.2, end = 0.8)\n\np2 / p3\n\n\n\n\n\n\n\n\n\nThat’s definitely better! If we’re getting picky, the boxplot is a little stretched though, as it’s trying to fill the length of two histograms\n\nIf you want to specify a layout, you can do it using plot_layout() which takes a string of letters and #s\n\nA, B, C, etc. refer to the plots in the order you add them to the patchwork\n# Can be used to add space\nSo let’s create a design the histograms on top take up 5 spaces, and the boxplot takes up 3, with a spacer on either side\n\n\n\n\np2 / p3 + plot_layout(design = \"AAAAA\n                                #BBB#\")\n\n\n\n\n\n\n\n\n\nPretty good, but when can do even better…\n\nWe can add a guide_area() for all legend keys to go\nWe then need to add that as C in our patchwork design\nThen inside plot_layout() specify guides = \"collect\" which will try and put all the legends in one place\n\nIf our legends matched exactly, it would only keep one, but as the boxplot lines are part of the legend, they technically cannot be collapsed\n\n\n\n\np2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\")\n\n\n\n\n\n\n\n\n\nGetting somewhere now… But the caption we added to p2 is getting in the way, the title isn’t that great, and maybe some subtitles for each plot would be useful\n\n\nQuick excercise: Remove the title and caption labels from p2, add subtitles to p2 and p3, then replot the patchwork\n\n\n\n\n\n\n\n\n\n\n\nOkay, now finally, we can add some overall labels to our patchwork with plot_annotation()\n\nThese work very similarly to the ggplot labs()\n\n\n\np2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\") +\n  plot_annotation(title = \"Math Test Scores Differences by Parental Education\",\n                  caption = \"Data: High School Longitudinal Study, 2009\")\n\n\n\n\n\n\n\n\n\nThere we go, now the title better reflects our patchwork, and the caption sits below both plots as it should\nAs a note, we can also assign a whole patchwork to an object if we want\n\nLet’s assign this plot to an object called patch\n\nWhat we are going to do with it is a surprise for next week\n\n\n\n\npatch &lt;- p2 / p3 + guide_area() + plot_layout(design = \"AAAAA\n                                               BBBCC\",\n                                     guides= \"collect\") +\n  plot_annotation(title = \"Math Test Scores Differences by Parental Education\",\n                  caption = \"Data: High School Longitudinal Study, 2009\")\n\n\nGenerally, our plot shows there’s a bit of difference in math test scores by parental education, but that doesn’t differ much by sex",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#summary",
    "href": "06-viz-ii.html#summary",
    "title": "II: Customization",
    "section": "Summary",
    "text": "Summary\n\nThere is infinitely more customization you can do with both ggplot2 and patchwork, but this lesson has touched on a good amount of the basics!\nWe can always do more, of course, but remember that a figure doesn’t need to be complicated to be good\n\nIn fact, simpler is often better\n\nThe main thing is that it is clean and clear and tells the story you want the reader to hear\nWhat exactly that looks like is up to you and your project!",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "06-viz-ii.html#question-one",
    "href": "06-viz-ii.html#question-one",
    "title": "II: Customization",
    "section": "Question One",
    "text": "Question One\na) Recreate the challenge plot using the data provided on the assignment page of the class website\nTo successfully complete the assignment, write ggplot2 code that creates a plot that closely resembles the below plot\n\nFor the full 5 points, the information portrayed needs to be identical with reasonably similar aesthetics\n\n\nHint: the data is already cleaned, but, there are still a couple of data wrangling tasks you’ll need to do before you can make the plot",
    "crumbs": [
      "Data Visualization",
      "II: Customization"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#setup",
    "href": "08-wrangle-iii.html#setup",
    "title": "III: Working with strings & dates",
    "section": "Setup",
    "text": "Setup\nAs before, we’ll continue working within the tidyverse. We’ll focus, however, on using two specific libraries:\n\nstringr for strings\nlubridate for dates\n\nYou may have noticed already that when we load the tidyverse library with library(tidyverse), the stringr library is already loaded. The lubridate library, though part of the tidyverse, is not. We need to load it separately.\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\n## NB: The stringr library is loaded with tidyverse, but\n## lubridate is not, so we need to load it separately\n\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\n\nNB: As we have done in the past few lessons, we’ll run this script assuming that our working directory is set to the scripts directory.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-1-working-with-strings",
    "href": "08-wrangle-iii.html#part-1-working-with-strings",
    "title": "III: Working with strings & dates",
    "section": "Part 1: Working with strings",
    "text": "Part 1: Working with strings\nTo practice working with strings, we’ll use data from Integrated Postsecondary Education Data System (IPEDS):\n\nThe National Center for Education Statistics (NCES) administers the Integrated Postsecondary Education Data System (IPEDS), which is a large-scale survey that collects institution-level data from postsecondary institutions in the United States (50 states and the District of Columbia) and other U.S. jurisdictions. IPEDS defines a postsecondary institution as an organization that is open to the public and has the provision of postsecondary education or training beyond the high school level as one of its primary missions. This definition includes institutions that offer academic, vocational and continuing professional education programs and excludes institutions that offer only avocational (leisure) and adult basic education programs. Definitions for other terms used in this report may be found in the IPEDS online glossary.\nNCES annually releases national-level statistics on postsecondary institutions based on the IPEDS data. National statistics include tuition and fees, number and types of degrees and certificates conferred, number of students applying and enrolled, number of employees, financial statistics, graduation rates, student outcomes, student financial aid, and academic libraries.\n\nYou can find more information about IPEDS here. As higher education scholars, IPEDS data are a valuable resource that you may often turn to.\nWe’ll use one file (which can be found here), that covers institutional characteristics for one year:\n\nDirectory information, 2007 (hd2007.csv)\n\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\n## read in data and lower all names using rename_all(tolower)\ndata &lt;- read_csv(\"data/hd2007.csv\") |&gt;\n    rename_all(tolower)\n\nRows: 7052 Columns: 59\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): INSTNM, ADDR, CITY, STABBR, ZIP, CHFNM, CHFTITLE, EIN, OPEID, WEBA...\ndbl (43): UNITID, FIPS, OBEREG, GENTELE, OPEFLAG, SECTOR, ICLEVEL, CONTROL, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#finding-str_detect",
    "href": "08-wrangle-iii.html#finding-str_detect",
    "title": "III: Working with strings & dates",
    "section": "Finding: str_detect()",
    "text": "Finding: str_detect()\nPreviously, we’ve filtered data using dplyr’s filter() function. When matching a string, we have used == (or != for negative match). For example, if we wanted to limit our data to only those institutions in Florida, we could filter using the stabbr column:\n\n## filter using state abbreviation (not saving, just viewing)\ndata |&gt;\n    filter(stabbr == \"FL\")\n\n# A tibble: 316 × 59\n   unitid instnm    addr  city  stabbr zip    fips obereg chfnm chftitle gentele\n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 132268 Wyotech-… 470 … Ormo… FL     32174    12      5 Stev… Preside… 3.86e12\n 2 132338 The Art … 1799… Fort… FL     3331…    12      5 Char… Preside… 9.54e13\n 3 132374 Atlantic… 4700… Coco… FL     3306…    12      5 Robe… Director 7.54e 9\n 4 132408 The Bapt… 5400… Grac… FL     32440    12      5 Thom… Preside… 8.50e 9\n 5 132471 Barry Un… 1130… Miami FL     3316…    12      5 Sist… Preside… 8.01e 9\n 6 132523 Gooding … 615 … Pana… FL     32401    12      5 Dr. … CRNA Ph… 8.51e 9\n 7 132602 Bethune-… 640 … Dayt… FL     3211…    12      5 Dr T… Preside… 3.86e 9\n 8 132657 Lynn Uni… 3601… Boca… FL     3343…    12      5 Kevi… Preside… 5.61e 9\n 9 132666 Bradento… 5505… Brad… FL     34209    12      5 A. P… CEO      9.42e 9\n10 132675 Bradford… 609 … Star… FL     32091    12      5 Rand… Director 9.05e 9\n# ℹ 306 more rows\n# ℹ 48 more variables: ein &lt;chr&gt;, opeid &lt;chr&gt;, opeflag &lt;dbl&gt;, webaddr &lt;chr&gt;,\n#   adminurl &lt;chr&gt;, faidurl &lt;chr&gt;, applurl &lt;chr&gt;, sector &lt;dbl&gt;, iclevel &lt;dbl&gt;,\n#   control &lt;dbl&gt;, hloffer &lt;dbl&gt;, ugoffer &lt;dbl&gt;, groffer &lt;dbl&gt;, fpoffer &lt;dbl&gt;,\n#   hdegoffr &lt;dbl&gt;, deggrant &lt;dbl&gt;, hbcu &lt;dbl&gt;, hospital &lt;dbl&gt;, medical &lt;dbl&gt;,\n#   tribal &lt;dbl&gt;, locale &lt;dbl&gt;, openpubl &lt;dbl&gt;, act &lt;chr&gt;, newid &lt;dbl&gt;,\n#   deathyr &lt;dbl&gt;, closedat &lt;chr&gt;, cyactive &lt;dbl&gt;, postsec &lt;dbl&gt;, …\n\n\nThis works well because the stabbr column, even though it uses strings, is regular. But what happens when the strings aren’t so regular? For example, let’s look the different titles chief college administrators take.\n\n## see first few rows of distinct chief titles\ndata |&gt;\n    distinct(chftitle)\n\n# A tibble: 556 × 1\n   chftitle          \n   &lt;chr&gt;             \n 1 Commandant        \n 2 President         \n 3 Chancellor        \n 4 Interim President \n 5 CEO               \n 6 Acting President  \n 7 Director          \n 8 President/CEO     \n 9 Interim Chancellor\n10 President/COO     \n# ℹ 546 more rows\n\n\nWe find over 500 unique titles. Just looking at the first 10 rows, we see that some titles are pretty similar — President vs. CEO vs. President/CEO — but not exactly the same. Let’s look again, but this time get counts of each distinct title and arrange from most common to least.\n\n## return the most common titles\ndata |&gt;\n    ## get counts of each type\n    count(chftitle) |&gt;\n    ## arrange in descending order so we see most popular at top\n    arrange(desc(n))\n\n# A tibble: 556 × 2\n   chftitle               n\n   &lt;chr&gt;              &lt;int&gt;\n 1 President           3840\n 2 Director             560\n 3 Chancellor           265\n 4 Executive Director   209\n 5 Owner                164\n 6 Campus President     116\n 7 Superintendent       105\n 8 CEO                   90\n 9 &lt;NA&gt;                  85\n10 Interim President     75\n# ℹ 546 more rows\n\n\n\nQuick exercise\nWhat do you notice about the data frames returned by distinct() and count()? What’s the same? What does count() do that distinct() does not?\n\nGetting our counts and arranging, we can see that President is by far the most common title. That said, we also see Campus President and Interim President (and before we saw Acting President as well).\nIf your research question asked, how many chief administrators use the title of “President”? regardless the various iterations, you can’t really use a simple == filter any more. In theory, you could inspect your data, find the unique versions, get counts of each of those using ==, and then sum them up — but that’s a lot of work and likely to be error prone!\nInstead, we can use the stringr function str_detect(), which looks for a pattern in a vector of strings:\nstr_detect(&lt; vector of strings &gt;, &lt; pattern &gt;)\nGoing item by item in the vector, it compares what it sees to the pattern. If it matches, then it returns TRUE; it not, then FALSE. Here’s a toy example:\n\n## string vector example\nfruits &lt;- c(\"green apple\", \"banana\", \"red apple\")\n\n## search for \"apple\", which should be true for the first and third item\nstr_detect(fruits, \"apple\")\n\n[1]  TRUE FALSE  TRUE\n\n\nWe can use str_detect() inside filter() to select only certain rows in our data frame. In our case, we want only those observations in which the title \"President\" occurs in the chftitle column. Because we’re only detecting, as long as \"President\" occurs anywhere in the title, we’ll get that row back.\n\n## how many use some form of the title president?\ndata |&gt;\n    ## still starting with our count\n    count(chftitle) |&gt;\n    ## ...but keeping only those titles that contain \"President\"\n    filter(str_detect(chftitle, \"President\")) |&gt;\n    ## arranging as before\n    arrange(desc(n))\n\n# A tibble: 173 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 163 more rows\n\n\nNow we’re seeing many more versions. We can even more clearly see a few titles that are almost certainly the same title, but were just inputted differently — President/CEO vs. President and CEO vs. President & CEO.\n\nQuick exercise\nIgnoring the sub-counts of the various versions, how many chief administrators have the word “President” in their title?\n\nSeeing the different versions of basically the same title should have us stopping to think: since it seems that this data column contains free form input (e.g. Input chief administrator title:), maybe we should allow for typos? The easiest: Is there any reason to assume that “President” will be capitalized?\n\nQuick exercise\nWhat happens if we search for “president” with a lowercase “p”?\n\nAh! We find a few stragglers. How can we restructure our filter so that we get these, too? There are at least two solutions.\n\n1. Use regular expressions\nRegular expressions (aka regex) are strings that use a special syntax to create patterns that can be used to match other strings. Some of you may have used them when you try to search for literature in your work. They are very useful when you need to match strings that have some general form, but may differ in specifics.\nWe already used this technique in the a prior lesson when we matched columns in the all_schools_wide.csv with contains(\"19\") so that we could pivot_longer(). Instead of naming all the columns specifically, we recognized that each column took the form of &lt;test&gt;_19&lt;YY&gt;. This is a type of regular expression.\nIn the tidyverse some of the stringr and tidyselect helper functions abstract-away some of the nitty-gritty behind regular expressions. Knowing a little about regular expression syntax, particularly how it is used in R, can go a long way.\nIn our first case, we can match strings that have a capital P President or lowercase p president using square brackets ([]). If we want either “P” or “p”, then we can use the regex, [Pp], in place of the first character: \"[Pp]resident\". This will match either \"President\" or \"president\".\n\n## solution 1: look for either P or p\ndata |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(chftitle, \"[Pp]resident\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 175 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 165 more rows\n\n\nThough we don’t see the new observations in the abbreviated output, we note that the number of rows has increased by two. This means that there are at least two title formats in which \"president\" is lowercase and that we weren’t picking up when we only used the uppercase version of \"President\" before.\n\n\n2. Put everything in the same case and match with that case\nAnother solution, which is probably much easier in this particular case, is to set all potential values in chftitle to the same case and then match using that case. In many situations, this is preferable since you don’t need to guess cases up front.\nWe won’t change the values in chftitle permanently — only while filtering. To compare apples to apples (rather than \"Apples\" to \"apples\"), we’ll wrap our column name with the function str_to_lower(), which will make character lowercase, and match using lowercase \"president\".\n\n## solution 2: make everything lowercase so that case doesn't matter\ndata |&gt;\n    count(chftitle) |&gt;\n    filter(str_detect(str_to_lower(chftitle), \"president\")) |&gt;\n    arrange(desc(n))\n\n# A tibble: 177 × 2\n   chftitle              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 President          3840\n 2 Campus President    116\n 3 Interim President    75\n 4 President/COO        47\n 5 President/CEO        46\n 6 School President     31\n 7 Vice President       29\n 8 President and CEO    17\n 9 College President    15\n10 President & CEO      14\n# ℹ 167 more rows\n\n\nWe recover another two titles when using this second solution. Clearly, our first solution didn’t account for other cases (perhaps “PRESIDENT\"?).\nIn general, it may be a good idea to try a solution like the second one before a more complicated one like the first. But because every problem is different, so too are the solutions. You may find yourself using a combination of the two.\n\nNot-so-quick exercise\nAnother chief title that was high on the list was “Owner.” How many institutions have an “Owner” as their chief administrator? Of these, how many are private, for-profit institutions (control == 3)? How many have the word “Beauty” in their name?\n\n\n\nReplace using string position: str_sub()\nIn addition to filtering data, we sometimes need to create new variables from pieces of exiting variables. For example, let’s look at the zip code values that are included in the file.\n\n## show first few zip code values\ndata |&gt;\n    select(unitid, zip)\n\n# A tibble: 7,052 × 2\n   unitid zip       \n    &lt;dbl&gt; &lt;chr&gt;     \n 1 100636 36112-6613\n 2 100654 35762     \n 3 100663 35294-0110\n 4 100690 36117-3553\n 5 100706 35899     \n 6 100724 36101-0271\n 7 100733 35401     \n 8 100751 35487-0166\n 9 100760 35010     \n10 100812 35611     \n# ℹ 7,042 more rows\n\n\nWe can see that we have both regular 5 digit zip codes as well as those that include the extra 4 digits (ZIP+4). Let’s say we don’t need those last four digits for our analysis (particularly because not every school uses them anyway). Our task is to create a new column that pulls out only the main part of the zip code. It is has to work both for zip values that include the additional hyphen and 4 digits as well as those that only have the primary 5 digits to begin with.\nOne solution in this case is to take advantage of the fact that zip codes — minus the sometimes extra 4 digits — should be regular: 5 digits. If want the sub-part of a string and that sub-part is always in the same spot, we can use the function, str_sub(), which takes a string or column name first, and has arguments for the starting and ending character that mark the sub-string of interest.\nIn our case, we want the first 5 digits so we should start == 1 and end == 5:\n\n## pull out first 5 digits of zip code\ndata &lt;- data |&gt;\n    mutate(zip5 = str_sub(zip, start = 1, end = 5))\n\n## show (use select() to subset so we can see new columns)\ndata |&gt;\n    select(unitid, zip, zip5)\n\n# A tibble: 7,052 × 3\n   unitid zip        zip5 \n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;\n 1 100636 36112-6613 36112\n 2 100654 35762      35762\n 3 100663 35294-0110 35294\n 4 100690 36117-3553 36117\n 5 100706 35899      35899\n 6 100724 36101-0271 36101\n 7 100733 35401      35401\n 8 100751 35487-0166 35487\n 9 100760 35010      35010\n10 100812 35611      35611\n# ℹ 7,042 more rows\n\n\nA quick visual inspection of the first few rows shows that our str_sub() function performed as expected (for a real analysis, you’ll want to do more formal checks).\n\n\nReplace using regular expressions: str_replace()\nWe can also use a more sophisticated regex pattern with the function str_replace(). The pieces of our regex pattern, \"([0-9]+)(-[0-9]+)?\", are translated as this:\n\n[0-9] := any digit, 0 1 2 3 4 5 6 7 8 9\n+ := match the preceding one or more times\n? := match the preceding 0 or more times\n() := subexpression\n\nPut together, we have:\n\n([0-9]+) := first, look for 1 or more digits\n(-[0-9]+)? := second, look for a hyphen and one or more digits, but you may not find any of that\n\nBecause we used parentheses, (), to separate our subexpressions, we can call them using their numbers (in order) in the last argument of str_replace():\n\n\"\\\\1\" := return the first subexpression\n\nSo what’s happening? If given a zip code that is \"32605\", the regex pattern will collect each digit — \"3\" \"2\" \"6\" \"0\" \"5\" — into the first subexpression because it never sees a hyphen. That first subexpression, \"\\\\1\", is returned: \"32605\". That’s what we want.\nIf given \"32605-1234\", it will collect the first 5 digits in the first subexpression, but will stop adding characters there when it sees the hyphen. From then on, it adds everything it sees the second subexpression: \"-\" \"1\" \"2\" \"3\" \"4\". But because str_replace() only returns the first subexpression, we still get the same answer: \"32605\". This is what we want.\nLet’s try it on the data.\n\n## drop last four digits of extended zip code if they exist\ndata &lt;- data |&gt;\n    mutate(zip5_v2 = str_replace(zip, \"([0-9]+)(-[0-9]+)?\", \"\\\\1\"))\n\n## show (use select() to subset so we can see new columns)\ndata |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 7,052 × 4\n   unitid zip        zip5  zip5_v2\n    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  \n 1 100636 36112-6613 36112 36112  \n 2 100654 35762      35762 35762  \n 3 100663 35294-0110 35294 35294  \n 4 100690 36117-3553 36117 36117  \n 5 100706 35899      35899 35899  \n 6 100724 36101-0271 36101 36101  \n 7 100733 35401      35401 35401  \n 8 100751 35487-0166 35487 35487  \n 9 100760 35010      35010 35010  \n10 100812 35611      35611 35611  \n# ℹ 7,042 more rows\n\n\n\nQuick exercise\nWhat if you wanted to the get the last 4 digits (after the hyphen)? What bit of two bits of code above would you change so that you can store the last 4 digits without including the hyphen? Make a new variable called zip_plus4 and store these values. HINT Look at the help file for str_replace().\n\nLet’s compare our two versions: do we get the same results?\n\n## check if both versions of new zip column are equal\nidentical(data |&gt; select(zip5), data |&gt; select(zip5_v2))\n\n[1] FALSE\n\n\nNo! Let’s see where they are different:\n\n## filter to rows where zip5 != zip5_v2 (not storing...just looking)\ndata |&gt;\n    filter(zip5 != zip5_v2) |&gt;\n    select(unitid, zip, zip5, zip5_v2)\n\n# A tibble: 4 × 4\n  unitid zip        zip5  zip5_v2   \n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;     \n1 108199 90015--350 90015 90015--350\n2 113953 92113--191 92113 92113--191\n3 431707 06360--709 06360 06360--709\n4 435240 551012595  55101 551012595 \n\n\n\nQuick exercise\nWhat happened? In this scenario, which string subsetting technique worked better?\n\nDepending on the task, regular expressions can either feel like a blessing or a curse. However, regular expressions are often the only way to perform a data wrangling task on unstructured string data. They are also a cornerstone of natural language processing techniques, which are increasingly of interest to education researchers.\nWe’ve only scratched the surface of what regular expressions can do. If you face string data in the future, taking a little time to craft a regular expression can be well worth it.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#part-ii-working-with-dates",
    "href": "08-wrangle-iii.html#part-ii-working-with-dates",
    "title": "III: Working with strings & dates",
    "section": "Part II: Working with dates",
    "text": "Part II: Working with dates\n\nMuch like names, dates are often saved as text/strings, and can be messy and formatted differently\n\nFor example, “2026-07-04”, “July 4th 2026”, “4th July 26”, and “04/07/26” all refer to the United State’s upcoming 250th birthday, but how do we make the computer understand they all mean that?\n\nWhat’s trickier is that even once we have dealt with the formatting, we sometimes need to be able to make calculations with dates\n\nFor example, if we have students SAT scores from multiple attempts, somone might be interested in knowing how many days passed between attempts\n\nThis involves knowing how many days are in each month, if that year was a leap year, etc.\n\n\nIn our IPEDS data, see which institutions closed in 2007 and 2008 in the closedat column\n\n-2 means the institution didn’t close in this period, so let’s drop them\n\n\n\n## subset to schools who closed during this period\ndata &lt;- data |&gt;\n  filter(closedat != -2) |&gt;\n  select(unitid, instnm, closedat)\n\ndata\n\n# A tibble: 83 × 3\n   unitid instnm                                                  closedat\n    &lt;dbl&gt; &lt;chr&gt;                                                   &lt;chr&gt;   \n 1 103440 Sheldon Jackson College                                 6/29/07 \n 2 104522 DeVoe College of Beauty                                 3/29/08 \n 3 105242 Mundus Institute                                        Sep-07  \n 4 105880 Long Technical College-East Valley                      3/31/07 \n 5 119711 New College of California                               Jan-08  \n 6 136996 Ross Medical Education Center                           7/31/07 \n 7 137625 Suncoast II the Tampa Bay School of Massage Therapy LLC 5/31/08 \n 8 141583 Hawaii Business College                                 Sep-07  \n 9 150127 Ball Memorial Hospital School of Radiologic Technology  May-07  \n10 160144 Pat Goins Shreveport Beauty School                      3/1/08  \n# ℹ 73 more rows\n\n\n\nparse-ing String into Dates\n\nSo, we can see closedat is a &lt;chr&gt; or string variable\n\nFrom a combination of looking at the values and knowing this is a US document, we can see most of these dates are month/day/year\n\nWe are really only going to use one function from lubridate the parse_date_time() function\n\nThere are other ways you can handle dates, some in base R, some other in lubridate, but this is a way that works well in most situations\nThe function takes two main arguments\n\nx: The string you are trying to turn into a date\norders: The format(s) the string date is written in\n\nThis take one or more date formats to try\nSince we think most of our dates are “month, day, year” this will be \"mdy\"\nYou can see a full list of options on the lubridate reference page\nIt tries these in order, seeing if it can “parse” or “figure out” the date looking for anything that divides dates up (” “,”/“,”-“) and/or spelt out date elements like”August”\n\nIf it can work out the date with the the first format, it will move on, if not, it will try and others you gave it (in order) and then if it can’t make the string fit into any of those formats, it will “fail to parse”\n\n\n\nIt then outputs a &lt;dttm&gt; (date time) type variable, which we are going to assign to a new variable clean_date\n\nThis involves a lot of well thought out code on the back-end, but makes our lives so much easier\n\n\nLet’s give this a go!\n\n\n## create a new clean_date column \ndata &lt;- data |&gt;\n    mutate(clean_date = parse_date_time(closedat,\n                                        orders = \"mdy\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `clean_date = parse_date_time(closedat, orders = \"mdy\")`.\nCaused by warning:\n!  35 failed to parse.\n\n## show\ndata\n\n# A tibble: 83 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 104522 DeVoe College of Beauty                   3/29/08  2008-03-29 00:00:00\n 3 105242 Mundus Institute                          Sep-07   NA                 \n 4 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 5 119711 New College of California                 Jan-08   NA                 \n 6 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 7 137625 Suncoast II the Tampa Bay School of Mass… 5/31/08  2008-05-31 00:00:00\n 8 141583 Hawaii Business College                   Sep-07   NA                 \n 9 150127 Ball Memorial Hospital School of Radiolo… May-07   NA                 \n10 160144 Pat Goins Shreveport Beauty School        3/1/08   2008-03-01 00:00:00\n# ℹ 73 more rows\n\n\n\nOkay, that worked for the majority of our colleges, but we see some like Sep-2007 that it didn’t like\n\n\nQuick Dicussion\nWhy wasn’t it able to “parse” Sep-2007?\nDo we know enough about when this institution closed? Why or why not?\nWhat might we be able to do with the information we have?\n\n\nNow, remember orders can take more than one value, so maybe we could try another date format that can pick up some of these dates\n\nLooking through the lubridate reference page, we can see that \"my\" (month year) is a format it will take as well, so let’s try that\n\n\n\n## Try adding another date format\ndata &lt;- data |&gt;\n    mutate(clean_date = parse_date_time(closedat,\n                                        orders = c(\"mdy\", \"my\")))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `clean_date = parse_date_time(closedat, orders = c(\"mdy\",\n  \"my\"))`.\nCaused by warning:\n!  7 failed to parse.\n\n## show\ndata\n\n# A tibble: 83 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 104522 DeVoe College of Beauty                   3/29/08  2008-03-29 00:00:00\n 3 105242 Mundus Institute                          Sep-07   2007-09-01 00:00:00\n 4 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 5 119711 New College of California                 Jan-08   2008-01-01 00:00:00\n 6 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 7 137625 Suncoast II the Tampa Bay School of Mass… 5/31/08  2008-05-31 00:00:00\n 8 141583 Hawaii Business College                   Sep-07   2007-09-01 00:00:00\n 9 150127 Ball Memorial Hospital School of Radiolo… May-07   2007-05-01 00:00:00\n10 160144 Pat Goins Shreveport Beauty School        3/1/08   2008-03-01 00:00:00\n# ℹ 73 more rows\n\n\n\nOkay, this time only 7 failed to parse , so that’s a lot better\nTake a look at what it did with Sep-2007\n\nDo we like this or not? Is it trustworthy?\n\nLet’s take a look at the 7 that didn’t go through this time\n\n\ndata |&gt;\n  filter(is.na(clean_date))\n\n# A tibble: 7 × 4\n  unitid instnm                                     closedat clean_date\n   &lt;dbl&gt; &lt;chr&gt;                                      &lt;chr&gt;    &lt;dttm&gt;    \n1 200794 Akron Machining Institute Inc              07/200   NA        \n2 231572 Braxton School                             2007     NA        \n3 262013 New York Institute of Technology-Central … 2007     NA        \n4 381343 CET-Reno                                   2007     NA        \n5 394846 CET-Santa Ana                              2007     NA        \n6 404532 Sharps Academy of Hair Styling             2007     NA        \n7 436793 Warren Woods Vocational Adult Education    2007     NA        \n\n\n\nHmm, those ones might be a little hard to approximate, so let’s drop them\n\n\ndata &lt;- data |&gt;\n  drop_na(clean_date)\n\n\n\nWorking with parse-ed Date Objects\n\nWe have successfully turn as many of the close dates into date objects that R can understand, but why?\n\nWe can now make comparisons and calculations so much more easily\n\nAs we go through these examples, just imagine having to do this by hand\n\n\n\n\nNumerical Calculations\n\nLet’s say we wanted to quickly find the earliest date\n\nWe wouldn’t have got far with the &lt;chr&gt; string date, but our &lt;dttm&gt; object R can understand and tell us the min() value of it\n\n\n\ndata |&gt; filter(clean_date == min(clean_date))\n\n# A tibble: 1 × 4\n  unitid instnm           closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;    &lt;dttm&gt;             \n1 365912 City College Inc 3/23/07  2007-03-23 00:00:00\n\n\n\nQuick Exercise\nFind the school with the most recent closure date\n\n\n\n\nComparing to Reference Dates\n\nOr, lets say we want to see how many schools closed before Christmas Day 2007\n\nWe can use parse_date_time() again to store December 25th 2007 as a date time object christmas_07\nThen we can just filter schools whose date is less than that\n\n\n\nchristmas_07 &lt;- parse_date_time(\"Dec 25 2007\", \"mdy\")\n\ndata |&gt; filter(clean_date &lt; christmas_07)\n\n# A tibble: 60 × 4\n   unitid instnm                                    closedat clean_date         \n    &lt;dbl&gt; &lt;chr&gt;                                     &lt;chr&gt;    &lt;dttm&gt;             \n 1 103440 Sheldon Jackson College                   6/29/07  2007-06-29 00:00:00\n 2 105242 Mundus Institute                          Sep-07   2007-09-01 00:00:00\n 3 105880 Long Technical College-East Valley        3/31/07  2007-03-31 00:00:00\n 4 136996 Ross Medical Education Center             7/31/07  2007-07-31 00:00:00\n 5 141583 Hawaii Business College                   Sep-07   2007-09-01 00:00:00\n 6 150127 Ball Memorial Hospital School of Radiolo… May-07   2007-05-01 00:00:00\n 7 161624 Accutech Career Institute                 Sep-07   2007-09-01 00:00:00\n 8 170824 Marquette General Hospital                Aug-07   2007-08-01 00:00:00\n 9 180124 College of Coiffure Art Ltd               6/25/07  2007-06-25 00:00:00\n10 186849 Harrison Career Institute-Vineland        Sep-07   2007-09-01 00:00:00\n# ℹ 50 more rows\n\n\n\nWhat about within 30 days of Christmas\n\nFor this we need to use interval(clean_date, christmas_07) to look between two dates\nThen time_length(, \"day\") converts that to the number of days\nThen abs() to get the absolute value (otherwise dates long after Christmas 2007 will be kept as the interval is -63 days)\nThis gets a little nested, so we can use more pipes |&gt; inside our filter() statement if that’s easier\n\n\n\n## Nested version\ndata |&gt; filter(abs(time_length(interval(clean_date, christmas_07), \"day\")) &lt; 30)\n\n# A tibble: 9 × 4\n  unitid instnm                             closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;    &lt;dttm&gt;             \n1 119711 New College of California          Jan-08   2008-01-01 00:00:00\n2 191870 Interboro Institute                12/21/07 2007-12-21 00:00:00\n3 225867 Austin Business College            12/28/07 2007-12-28 00:00:00\n4 243902 Gaither and Company Beauty College 12/5/07  2007-12-05 00:00:00\n5 415561 Cortiva Institute-Colorado         12/31/07 2007-12-31 00:00:00\n6 436748 New Hampshire Career Institute     12/20/07 2007-12-20 00:00:00\n7 445586 Banner Institute-Chicago           Dec-07   2007-12-01 00:00:00\n8 446871 The Bryman School-East             Dec-07   2007-12-01 00:00:00\n9 447500 Salter School-Cambridge Campus     12/31/07 2007-12-31 00:00:00\n\n## Internal pipes version\ndata |&gt; filter(interval(clean_date, christmas_07) |&gt;\n               time_length(\"day\") |&gt;\n               abs() &lt; 30)\n\n# A tibble: 9 × 4\n  unitid instnm                             closedat clean_date         \n   &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;    &lt;dttm&gt;             \n1 119711 New College of California          Jan-08   2008-01-01 00:00:00\n2 191870 Interboro Institute                12/21/07 2007-12-21 00:00:00\n3 225867 Austin Business College            12/28/07 2007-12-28 00:00:00\n4 243902 Gaither and Company Beauty College 12/5/07  2007-12-05 00:00:00\n5 415561 Cortiva Institute-Colorado         12/31/07 2007-12-31 00:00:00\n6 436748 New Hampshire Career Institute     12/20/07 2007-12-20 00:00:00\n7 445586 Banner Institute-Chicago           Dec-07   2007-12-01 00:00:00\n8 446871 The Bryman School-East             Dec-07   2007-12-01 00:00:00\n9 447500 Salter School-Cambridge Campus     12/31/07 2007-12-31 00:00:00\n\n\n\nQuick Exercise\n\nDid any schools close within a week of Christmas?\n\n\n\n\nExtracting Info From Dates\n\nWhat if we want to see which fiscal quarter more schools closed in?\n\nThe handy quarter() function tell us that\n\n\n\ndata |&gt; \n  mutate(quarter = quarter(clean_date)) |&gt;\n  count(quarter)\n\n# A tibble: 4 × 2\n  quarter     n\n    &lt;int&gt; &lt;int&gt;\n1       1    11\n2       2    20\n3       3    32\n4       4    13\n\n\n\nWhat about the day of the week they closed on?\n\nSimilarly wday() can tell us that\n\nWe need to say label = TRUE to get “Sun” instead of 1\n\n\n\n\ndata |&gt;\n  mutate(day = wday(clean_date, label = TRUE)) |&gt;\n  count(day)\n\n# A tibble: 7 × 2\n  day       n\n  &lt;ord&gt; &lt;int&gt;\n1 Sun       6\n2 Mon       9\n3 Tue       7\n4 Wed       7\n5 Thu       6\n6 Fri      15\n7 Sat      26\n\n\n\nQuick Question\n\nCan we 100% trust these week day counts? Why or why not?",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#summary",
    "href": "08-wrangle-iii.html#summary",
    "title": "III: Working with strings & dates",
    "section": "Summary",
    "text": "Summary\n\nIn this lesson we’ve looked at working with both generic strings and with dates\n\nFor both of these we have only begun to scratch the surface, but it should have given you enough of an idea to go out and get your hands dirty\nIf you found this interesting, Dr. Jinnie Shin teaches a class on Natural Language Processing (in python programming language), which gets into how make the computer begin to understand text\n\nThese are both messy types of data and you’re often going to have to make subjective decisions (e.g., how to handle Sep-2007)\n\nWhat’s most important is that you document/comment what you did and why you did it\n\nAs you will see over the next few weeks, while this stuff can be tricky, working with strings efficiently can be really powerful in more advanced programming",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#question-one",
    "href": "08-wrangle-iii.html#question-one",
    "title": "III: Working with strings & dates",
    "section": "Question One",
    "text": "Question One\na) Join together the two datasets\n\nHint: Column names in hd2007.csv are uppercase (UNITID) while those in ic2007mission.csv are lowercase (unitid). There are multiple ways to handle this, but, rename_all(tolower) might be useful…\n\nI also find these cheat sheets extremely helpful\n\nCheat sheet for stringr and regex\nCheat sheet for lubridate\n\nYou can find all posit cheat sheets here",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#question-two",
    "href": "08-wrangle-iii.html#question-two",
    "title": "III: Working with strings & dates",
    "section": "Question Two",
    "text": "Question Two\na) How many chief administrator names start with “Dr.”?\n\nHint: Many chief administrators are listed on more than one line due to branch campuses. Make sure to take this into account by keeping only distinct names.\n\nb) Optional: How many chief administrator names end with the title “Ph.D.” or some variant?",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#question-three",
    "href": "08-wrangle-iii.html#question-three",
    "title": "III: Working with strings & dates",
    "section": "Question Three",
    "text": "Question Three\nAmong schools that provide their mission statement\na) How many repeat their institutional name in their mission statement?\nb) How many use the word civic?\nc) Which top 3 states have the most schools with mission statements that use the word future?\nd) Which type of schools (public, private-non-profit, private-for-profit) are most likely to use the word skill in their mission statement?",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#question-four",
    "href": "08-wrangle-iii.html#question-four",
    "title": "III: Working with strings & dates",
    "section": "Question Four",
    "text": "Question Four\nAmong the schools that closed in 2007 or 2008 (and give a date with at least a month and year)\na) Which has been closed for the longest time?\nb) How many months has it been from that school’s close date to the beginning of this current month (1 February 2025)?\nc) How many days were there between the first school to close and the last?",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "08-wrangle-iii.html#submission",
    "href": "08-wrangle-iii.html#submission",
    "title": "III: Working with strings & dates",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Wrangling",
      "III: Working with strings & dates"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data",
    "href": "10-viz-iii.html#reading-in-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in Data",
    "text": "Reading in Data\n\nSpatial data comes in many formats:\n\nShapefiles .shp: A traditional format from ESRI (the makers of ArcGIS)\n\nVery widely used, by far the most common format\nComes as a folder containing multiple files with the same name but different extensions\n\nThe primary file (the one you actually read in) ends in .shp , reading one into R looks something like this\n\ndf_my_map &lt;- read_sf(\"data/my-map-data/my-map-data\")\n\n\nSerious limitations for working with attached data, the most annoying of which is the variables can only be 7 characters long\n\nOther modern, better, but (sadly) rarer formats include\n\nGeoDatabase .gdb: A modern format from ESRI\nGeoJSON .geojson: A plain text format\nGeoPackage .gpkg: An open-source driven single file format\nSQL Databses: Many SQL variants can store/handle spatial data\n\nThe sf package we are going to use is based on PostGIS which is the spatial accompaniment to PostgreSQL open source SQL\n\n\nIf you get into spatial data, you’ll want to get familiar with at least Shapefiles at first, then play around with some of the other formats\nWhat’s great about sf is that it can read pretty much any format, and does so in the same way\n\nread_sf(\"file/path.extension\")\n\n\n\n\nQuick Exercise\nThere’s so much spatial data out there, using Google see if you can find a Shapefile for something you’re interested in, download it, and read it in to R using read_sf()\nHint: US Governments of all levels provide free to use spatial data. If you’re stuck, check out Alachua County GIS Portal\n\nFor today’s lesson, however, we are going to use an API to directly download our spatial data",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#apis-setting-up-tidycensus",
    "href": "10-viz-iii.html#apis-setting-up-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "APIs & Setting up tidycensus",
    "text": "APIs & Setting up tidycensus\n\nSo what exactly is an API?\n\nIn short, think of it as a way of R going to a website/database and pulling data directly from the server-side or back end, without our having to ever interact with the website directly\nThis has two main advantages\n\nWe don’t have to store the large dataset on our computer\nOur work becomes instantly more reproducible\n\nNote from BS: We avoid point-click at all costs! We are going to use the API tidycensus today, but all APIs operate on the same basic idea\n\n\n\ntidycensus is, in my opinion, one of the easiest APIs to get set up and use in R\nMost APIs require that you use some kind of key that identifies you as an authorized user\n\nTypically you need to set up the key the first time you use the API, but helpfully, it’s usually possible to store the key on your computer for all future use\nMany API keys are free to obtain and use\n\nIf you were using an API to access a private database such as Google Maps, you might need to pay for your key to have access depending on how much you use it\nBut because we are using Census data, which is freely available to the public, there’s no charge\n\n\n\n\nGetting Census API Key\n\nGetting your Census API key is extremely easy\n\nSimply go here\nEnter your organization name (University of Florida)\nEnter your UF email\n\nYou will quickly receive an email with your API key, which you will need below\n\n\n\n\n\nTo set up tidycensus for the first time, we first need to set our API key\nThe tidycensus package makes this much easier than many APIs by having a built-in function that you can use to save your API key to your computer\n\nSimply place your API key in the &lt;key&gt; of the code below\nThe install option means it will save the API key for future use, so you will not need to worry about this step again\n\n\n\n## ---------------------------\n##' [set API key]\n## ---------------------------\n\n## you only need to do this once: replace everything between the\n## quotes with the key in the email you received\n##\n## eg. census_api_key(\"XXXXXXXXXXXXXXXXXX\", install = T)\ncensus_api_key(\"&lt;key&gt;\", install = T)\n\n\nNow that this is set up, we are ready to start using tidycensus — yay!",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "href": "10-viz-iii.html#reading-in-data-with-tidycensus",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Reading in data with tidycensus",
    "text": "Reading in data with tidycensus\n\nThere are multiple main tidycensus functions that you can use to call in data, with each calling data from a different source operated by the US Census Bureau\n\nget_acs()\nget_decennial()\nget_estimates()\nget_flows()\nget_pop_groups()\nget_pums()\n\nYou can see more information about these on the tidycensus reference page\n\n\n\nFor today’s lesson we are going to use get_acs()\n\nThis collects data from the American Community Survey (regular sampled surveys of demographic data across the US)\n\nWe are going to assign &lt;- the data we pull down into the object df_census:\n\n\n## ---------------------------\n##' [Get ACS Data]\n## ---------------------------\n\ndf_census &lt;- get_acs(geography = \"county\",\n                     state = \"FL\",\n                     year = 2021,\n                     variables = \"DP02_0065PE\", # Pop &gt;=25 with Bachelors\n                     output = \"wide\",\n                     geometry = TRUE)\n\nWarning: • You have not set a Census API key. Users without a key are limited to 500\nqueries per day and may experience performance limitations.\nℹ For best results, get a Census API key at\nhttp://api.census.gov/data/key_signup.html and then supply the key to the\n`census_api_key()` function to use it throughout your tidycensus session.\nThis warning is displayed once per session.\n\n\nLet’s walk through each element of this command in turn:\n\ngeography = \"county\"\n\nTelling the function to get estimates (and spatial data later) at the county level\n\nthis could also be \"state\", for example, to get state level data\n\n\nstate = \"FL\"\n\nTelling the function to get data only for the state of Florida\n\nYou could put a group of states with c()\n\nuse full state names, or use FIPS codes — tidycensus is flexible\n\nIf you want a narrower set of data, you could also add county =, which works in a similar way\n\nFor example, if you added county = \"Alachua\", you would only get county-level data for Alachua County, Florida.\n\n\n\nyear = 2021\n\nTelling the function to pull data for the survey year 2021\n\nFor ACS, this will be the survey set ending in that year\n\nKeep in mind that some data are not available for every year\n\nFor example, data from the full decennial census are only available for 2010 or 2020.\n\n\nvariables = \"DP02_0065PE\"\n\nTelling the function to pull the variable coded \"DP02_0065PE\"\n\nWhich is the percentage of the population older than 25 with a Bachelor’s degree\n\nThis is the only tricky part of using tidycensus: understanding Census API’s variable names\n\nLet me breakdown what we are calling here:\n\nDP02_0065\n\nThis is the main variable code the census uses\nYou can find these by using the load_variables() command, but doing so creates a massive table in R that is hard to navigate through\nAn easier way is to go the census API’s list of variables for the dataset you are using, which for the 2021 ACS is here (change the years/data sources as needed for other surveys)\n\nIn here you can crtl-f or cmd-f search for the variable you are looking for\nFor this variable we could search “bachelor,” which will highlight all the variables that have “bachelor” in the title\nFind the variable you want and copy the name.\n\n\nPE\n\nYou will notice there are multiple DP02_0065 variables\n\nThese are the same underlying variable, but in different forms.\n\nThe common endings are E or PE\n\nWhich stand for Estimate and Percentage Estimate respectively\n\nFor today’s lesson, we want the percentage estimate PE\n\nIf you want the total count instead, select E\n\n\nso we will select DP02_0065PE,\n\nThis will give us\n\nDP02_0065PEthe percent estimate of Bachelor’s degree attainment for those 25 years old and above\nDP02_0065PM which is the margin of error for the percentage (hence the M at the end)\n\nWe don’t need this for our mapping, but it downloads automatically and is useful for some statistical calcualtions\n\n\n\n\n\n\noutput = \"wide\"\n\nTelling it we want the data in a wide format\nThink back to Data Wrangling II: wide data means having a separate column for each variable whereas long data would be in two columns, one with the variable name and one with the variable value\nFor ease of plotting/mapping the variables, we are going to want it in wide format\n\ngeometry = TRUE\n\nTelling the function we want to download geometry (a kind of spatial data) to go with our census data\nBy default, this is FALSE , which just downloads the Census data\nThis saves us having to deal with finding, loading, and joining a shapefile to make our map\n\n\n\nQuick Excercise\nUsing the API variable dictionary we used above, add another variable to your get_acs() using c()\n\nOkay, let see what the top of our new data looks like.\n\n## show header of census data\nhead(df_census)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -82.57599 ymin: 27.64324 xmax: -80.73292 ymax: 30.14312\nGeodetic CRS:  NAD83\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4\n                        geometry\n1 MULTIPOLYGON (((-81.65856 2...\n2 MULTIPOLYGON (((-82.57599 2...\n3 MULTIPOLYGON (((-81.95616 2...\n4 MULTIPOLYGON (((-81.6809 29...\n5 MULTIPOLYGON (((-82.1062 28...\n6 MULTIPOLYGON (((-82.31133 2...\n\n\n\nIt looks a bit different than a normal data frame\n\nFor now, let’s not worry too much about the first few lines which give a summary of the spatial aspects of the our downloaded data\nIf you look underneath those lines, from GEOID to DP02_0065PM, you’ll see something that looks more like the tibbles we are familiar with\nThen, in the last column, we get to our spatial data in the geometry column\nIf you open df_census in the viewer, it looks like a normal data frame ending with this slightly different column called geometry\n\nNote: I wouldn’t recommend often looking through spatial data in the file viewer as the the spatial data is often huge and can make it slow/laggy\n\nIf you need to dig into the data that way, use st_drop_geometry() to remove the spatial features, then either print or view it\n\n\n\n\n\n## view data frame without geometry data\ndf_census_view &lt;- df_census |&gt;\n  st_drop_geometry()\n\nhead(df_census_view)\n\n  GEOID                    NAME DP02_0065PE DP02_0065PM\n1 12095  Orange County, Florida        23.0         0.6\n2 12125   Union County, Florida         7.6         2.1\n3 12069    Lake County, Florida        16.0         0.9\n4 12127 Volusia County, Florida        16.8         0.5\n5 12105    Polk County, Florida        14.0         0.5\n6 12119  Sumter County, Florida        19.4         1.4",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "href": "10-viz-iii.html#a-very-brief-overview-of-spatial-data",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "A (Very) Brief Overview of Spatial Data",
    "text": "A (Very) Brief Overview of Spatial Data\n\nWe do not have time to really get into all the complexities of spatial data in this class, so, unfortunately, it will have to remain a bit of black box for now\n\nFor those interested in more depth, this is a nice intro to the types of spatial data\n\n\n\nVectors vs Rasters\n\nWhen looking online for spatial data, you might see how spatial data can be either in vector or raster format\n\nFor our purpose today (and most of the time outside certain use cases) everything is going to be vector\n\nSimilar to a vector in R (a column or list of data)\n\nJust a collection of data points that represent something\n\nOnly this time it’s representing something spatial\nThink of it like instructions to draw a shape\n\n\n\nRaster data, on the other hand, is a grid with information assigned to each square\n\nThink of it like a big paint-by-numbers grid and the shape you see is where some squares are filled in\nCommonly used for satellite imagery analysis\n\nDoesn’t scale well for mapping\n\n\n\n\n\n\nSpatial Data Vectors\n\nNo matter the format, working with spatial data involves two things\n\nThe data itself (the numbers in the geometry column)\nHow that data is projected (the CRS, which we will cover below)\n\nThe data in a spatial vector come in three primary formats\n\npoints (think dots on a map)\nlines (think a line connecting two points on a map)\npolygons (think a collection of lines on a map that create a closed shape)\n\nYou can also have multi-polygons or multi-lines, which are just multiple shapes that represent that observation\n\nUltimately though, it all comes back to point data, everything else is made up of those\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this lesson we are going to use both point and polygon data (you won’t see line data as often in the wild)\nIf this sounds complicated, fear not! It is much simpler than it sounds right now!\n\n\n\nCoordinate Reference Systems (CRS)\n\nFor purposes of this lesson, the only internal workings of spatial data we need to be aware of is something called the Coordinate Reference System or CRS\n\nOur earth is not flat, but rather is a curved three-dimensional object (Note from Ben: this is most likely true)\nSince we don’t want to carry around globes, we take this 3D object and squish it into two dimensions on a map\nThis process necessarily involves some kind of transformation, compromise, or projection\n\nIn a nutshell, this is a very simplified explanation of what a CRS decides\n\nit’s how we are deciding to twist, pull, squish a 3D Earth surface into a flat surface\nTurns out this matters a lot depending on what purpose your map is for\n\nDo you want your results to have the correct areas?\nOr maybe correct distances?\nOr maybe straight lines of bearing (particularly important if you are sailing and don’t want those trips to take any longer than necessary)?\n\nThere are, as you can see by this list, also a lot of localized CRS projections\n\nThe more local a projection, the less it has to twist, pull, stretch, or squish data\n\nThis means it can be more accurate, with less compromise, for that specific area\n\nBut if you use a localized projection for the wrong area, you’re going to be way off\nIf you’re doing spatial analysis (distance calculations etc.) it’s best to the use most local projection you can that is still valid for your whole study area\n\n\n\n\nHere’s a (somewhat old) pop culture look at this issue from one of my favorite shows…\n\n\n\nThis is a relatively complicated process we are not going to go into here. If you’re interested here’s a nice introduction to CRS by QGIS\nFor our class we are going to use the CRS EPSG 4326\n\nIn essence a projection that\n\nMakes east/west run straight left/right\nMakes north/south run straight up/down\nKeeps all latitude and longitude degrees equally spaced apart (much to the dismay of our friends from the OCSE)\n\nAll different CRS have their advantages and disadvantages\n\nThis is nice and simple for quick descriptive maps, but distorts shapes in ways that might be harmful, particularly if you are going to do any distance calculations, etc.\n\n\n\n\nNote from Ben: If you are going to do spatial work in education research (other than just making maps for display), you really need to know what your projection is doing. Even if you are just making maps for display, some projections are, IMNSHO, more aesthetically pleasing that others in different situations.\n\n\nKeep an eye out for crs = 4326 as we go through some examples plotting spatial data below.\n\n\n\nHow R Stores All This Spatial Data\n\nAs we saw above, there is a column on the end of our data called geometry\n\nThis is not technically a column like we are used to\n\nE.g., you can’t join(df_one, df_two, by = geometry) or filter(geometry == x) like we do with other variables in our data frames\n\nIf you want to spatially join or filter data, you have to use\n\nst_filter()\nst_join()\n\nUsing these is bit beyond where we will get in this class, but, note this for future work, it can be really useful\n\n\nInstead, think of it as a special attachment R places on each observation\n\n\n\n\nSummary of Spatial Data Basics\nIn short, what you need to know about spatial data for this lesson is this:\n\nR stores spatial data in something called geometry attached to each observation/row\n\nTo handle these spatial aspects of the data, you can’t just filter() it like normal; instead you have to use functions from a spatial data package such as sf or tigris\n\nThe CRS (coordinate reference system) is how we choose to account for the earth being curved\n\nCrucial for mapping is that everything we use on the plot is using the same CRS\nUsing crs = 4326 will give a nice simple flat projection\n\nThis projection has drawbacks, but is easy to work with and so is what we will use for now",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#lets-make-a-map-finally",
    "href": "10-viz-iii.html#lets-make-a-map-finally",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Let’s Make a Map (finally)!",
    "text": "Let’s Make a Map (finally)!\nIf we have made it this far, things are about to get much more interesting and hands-on!\n\nWe are going to make an education-focused map based on template I used for a real consulting project Summer 2022 as part of my GA-ship\nThis template is really adaptable for a lot the kind of maps we might want educational research and reports\nSo let’s get started!\n\nWe are going to have two layers\n\nA base map with the census data we already downloaded\nA layer of points on top representing colleges\n\n\n\n\nLayer One: Base Map\n\nBefore we plot anything, particularly since we are going to have multiple layers, we want to check our CRS\n\n\n## ---------------------------------------------------------\n##' [Making a map (finally)]\n## ---------------------------------------------------------\n## ---------------------------\n##' [Layer one: base map]\n## ---------------------------\n\n## show CRS for dataframe\nst_crs(df_census)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nThat isn’t our simple flat EPSG 4326, so we are going to st_transform() to set that.\n\n## transform the CRS to 4326\ndf_census &lt;- df_census |&gt;\n  st_transform(crs = 4326)\n\nThen we can check again…\n\n## show CRS again; notice how it changed from NAD93 to EPSG:4326\nst_crs(df_census) \n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nLooks good!\n\nOkay, with our CRS now set, let’s plot our base map.\n\nWe actually use the familiar ggplot() to make our maps because there is a special geom_* that works with spatial data\n\ngeom_sf()\nEverything works in a similar way to our normal plots, so this should be familiar. Luckily all the tricky spatial aspects are handled by ggplot for us.\n\n\nThe below code will make our base map, and store in an object called base_map\n\n\n## create base map\nbase_map &lt;- ggplot() +\n  geom_sf(data = df_census,\n          aes(fill = DP02_0065PE),\n          color = \"black\",\n          size = 0.1) +\n  labs(fill = str_wrap(\"Percent Population with Bachelor's\", 20)) +\n  scale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\") +\n  theme_minimal()\n\nLet’s go through each line of the geom_sf() as we did for get_acs() above:\n\ndata = df_census\n\nAll we need to do take make our spatial plot is call a data frame with a geometry attachment. geom_sf() will handle how to plot that for us.\n\naes(fill = DP02_0065PE)\n\nMuch like we would with a box plot, we are simply telling ggplot to fill the shapes (in our case, Florida’s counties) based on that variable\nSo here we are filling Florida’s counties based on the percent of the population over 25 with a Bachelor’s degree (the variable we chose from tidycensus)\n\ncolor = \"black\"\n\nRemember since this is outside the aes() argument it will applied consistenly across the plot. We are telling it to make all the lines black.\n\nsize = 0.1\n\nSimilarly, we are telling to make the lines 0.1 thickness (thinner than the default)\n\n\nThen we have added two visual alterations like we covered in the second plotting lesson. For a quick reminder:\n\nlabs(fill = str_wrap(\"Percent Population with Bachelor's\", 20))\n\nIs saying to give the legend for fill this title\n\nThe new function, str_wrap() says to make a newline (wrap) when there are more than 20 characters\n\n\nscale_fill_gradient(low = \"#a6b5c0\", high = \"#00254d\")\n\nIs telling fill with a color gradient starting at with light slate blue and finishing with a dark slate blue\n\nInstead of color names, we’re using hex color codes\n\n\nNow, let’s call our base_map object to see what this looks like\n\n\n## call base map by itself\nbase_map\n\n\n\n\n\n\n\n\nWoohoo! We have made a map!\n\nData Viz II Throwback\n\nRemember in the second lesson on data visualization we spent the entire time customizing the appearance of a single plot?\ngeom_sf() is just another type of ggplot, so, we can use all the same things we learned\n\n\nQuick Exercise\n\nPlay around with the scale_fill_gradient to use your favorite color\nPlay around with the theme argument, can you find the theme that gets rid of all grid lines?\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to make it more interesting with one more layer…\n\n\n\nLayer Two: Institution Points\n\nA lot of education data comes with a latitude and longitude for the institution\nToday we are going to use IPEDS, but you can certainly get these for K-12 schools and a whole lot more besides\n\nCaution: I have noticed in my own work that IPEDS coordinates are not always the most consistent (an issue we have seen with other IPEDS data through the course)\n\nIf you’re wanting to do something that where precision really matters the Department of Homeland Security has open data that, in my experience, is more consistent with Google Maps locations\n\nIt also provides polygons of larger college campuses rather than just coordinate points, which might be useful for some of your research (I’m using them at the moment)\n\n\n\nFor today, we are going to read in some library data from IPEDS that I cleaned and merged earlier\n\nThis is a combination of HD and AL data files for 2021\n\n\n\n## ---------------------------\n##' [Layer Two: Institutions]\n## ---------------------------\n\n## read in IPEDS data\ndf_ipeds &lt;- read_csv(\"data/mapping-api-data.csv\")\n\nLet’s take a look at our data\n\n## show IPEDS data\nhead(df_ipeds)\n\n# A tibble: 6 × 78\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LATITUDE LONGITUD\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089     34.8    -86.6\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073     33.5    -86.8\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101     32.4    -86.2\n4 100706 Unive…       1       1 AL         1 Madison…     1089     34.7    -86.6\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101     32.4    -86.3\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125     33.2    -87.5\n# ℹ 68 more variables: LEXP100K &lt;dbl&gt;, LCOLELYN &lt;dbl&gt;, XLPBOOKS &lt;chr&gt;,\n#   LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;, LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;,\n#   LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;, LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;,\n#   LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;, LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;,\n#   LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;, LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;,\n#   LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;, LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;,\n#   LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;, LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, …\n\n\n\nWe see a normal data frame for colleges with bunch of variables\n\nReminder: You can use IPEDS dictionaries to unpack unknown variable names\n\nMost importantly for right now, we see latitude and longitude\n\nLatitude and longitude represent something spatial, but they’re not quite spatial data like R knows\nLet’s change that!\n\n\n\n## convert coordinates columns into a true geometry column; this is\n## much more reliable than simply plotting them as geom_points as it\n## ensures the CRS matches etc.\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_as_sf(coords = c(\"LONGITUD\", \"LATITUDE\"))\n\n\nAbove we call st_as_sf()\n\nthen tell it the coordinates, coords =, are in columns name LONGITUD and LATITUDE\nThe order these go is very important (and changes between some systems) but for sf it’s longitude then latitude as we follow the usual x, y order\n\nLongitude tells you were you are east/west on the globe, it translates to the x axis\nLatitude gives you north/south direction, it translates to the y axis\n\n\nIf we look at our data again, we are going to see that spatial summary again as R has attached some point geometry to our college data based on the coordinates\n\n\n## show IPEDS data again\nhead(df_ipeds)\n\nSimple feature collection with 6 features and 76 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -87.54598 ymin: 32.36261 xmax: -86.17401 ymax: 34.78337\nCRS:           NA\n# A tibble: 6 × 77\n  UNITID INSTNM CONTROL ICLEVEL STABBR  FIPS COUNTYNM COUNTYCD LEXP100K LCOLELYN\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 100654 Alaba…       1       1 AL         1 Madison…     1089        1        2\n2 100663 Unive…       1       1 AL         1 Jeffers…     1073        1        2\n3 100690 Amrid…       2       1 AL         1 Montgom…     1101        1        2\n4 100706 Unive…       1       1 AL         1 Madison…     1089        1        2\n5 100724 Alaba…       1       1 AL         1 Montgom…     1101        1        2\n6 100751 The U…       1       1 AL         1 Tuscalo…     1125        1        2\n# ℹ 67 more variables: XLPBOOKS &lt;chr&gt;, LPBOOKS &lt;dbl&gt;, XLEBOOKS &lt;chr&gt;,\n#   LEBOOKS &lt;dbl&gt;, XLEDATAB &lt;chr&gt;, LEDATAB &lt;dbl&gt;, XLPMEDIA &lt;chr&gt;,\n#   LPMEDIA &lt;dbl&gt;, XLEMEDIA &lt;chr&gt;, LEMEDIA &lt;dbl&gt;, XLPSERIA &lt;chr&gt;,\n#   LPSERIA &lt;dbl&gt;, XLESERIA &lt;chr&gt;, LESERIA &lt;dbl&gt;, XLPCOLLC &lt;chr&gt;,\n#   LPCLLCT &lt;dbl&gt;, XLECOLLC &lt;chr&gt;, LECLLCT &lt;dbl&gt;, XLTCLLCT &lt;chr&gt;,\n#   LTCLLCT &lt;dbl&gt;, XLPCRCLT &lt;chr&gt;, LPCRCLT &lt;dbl&gt;, XLECRCLT &lt;chr&gt;,\n#   LECRCLT &lt;dbl&gt;, XLTCRCLT &lt;chr&gt;, LTCRCLT &lt;dbl&gt;, LILLDYN &lt;dbl&gt;, …\n\n\n\nQuick Question\nSomething’s not quite right, can you tell me what?\nHint: It’s something we just talked about being very important\n\n\nThis means R will not be able to turn that spatial data into a map\n\nBasically, R knows we have spatial data, but it doesn’t know how we want to put it onto a 2D surface (how to project it)\n\nTo be sure, let’s check the it directly\n\n\n## check CRS for IPEDS data\nst_crs(df_ipeds)\n\nCoordinate Reference System: NA\n\n\nYep, NA… Luckily the fix for this is simple\n\n## add CRS to our IPEDS data\ndf_ipeds &lt;- df_ipeds |&gt; \n  st_set_crs(4326) # When you first add coordinates to geometry, it doesn't know\n                   # what CRS to use, so we set to 4326 to match our base map data\n\nOkay, let’s have another look…\n\n## check CRS of IPEDS data again\nst_crs(df_ipeds)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAnd we see we have our nice CRS back!\n\nOkay, now the hard work is done, we just need to call our base_map, add a layer representing the colleges as points, and store it into a new object point_map:\n\n\npoint_map &lt;- base_map +\n  geom_sf(data = df_ipeds |&gt; filter(FIPS == 12), # Only want to plot colleges in FL\n          aes(size = LPBOOKS),\n          alpha = 0.8,\n          shape = 23, # Get the diamond shape which stands out nicely on the map\n          fill = \"white\", # This shape has a fill and color for the outline\n          color = \"black\") + # FYI 21 is a circle with both fill and color\n  labs(size = \"Number of Books in Library\")\n\nAs we have done all lesson, we can take a quick look through our second geom_sf() function line by line:\n\ndata = df_ipeds |&gt; filter(FIPS == 12):\n\nFor this layer we are using our df_ipeds data, which covers the country, but since our base map is Florida, we only want colleges located in the Sunshine State (which is FIPS code 12).\n\naes(size = LPBOOKS)\n\nIs saying we want to change the size of point based on LPBOOKS, which is the total number of books in the college’s library collection. More books, bigger point!\n\nshape = 23\n\nThis is purely aesthetic, instead of usual circles, this is a diamond shape\n\nfill = \"white\" and color = \"black\"\n\nChange the inside of the diamonds white and the lines around the edge to black\n\nNote, this shape (another others) have an inside to fill and a border to color, the basic point shape only has color\n\n\nalpha = 0.5\n\nIs outside the aes() so we are making it all 50% transparent.\n\nlabs(size = \"Number of Books in Library\")\n\nTo change the legend title to “Number of Books in Library”\n\n\nPhew! Last thing, let’s call our new point_map object and take a look at what we created!\n\n## show new map\npoint_map\n\n\n\n\n\n\n\n\n\nThere we go!\nWe now have a map that shows us county bachelor’s degree attainment and the number of books in a college’s library.\n\nIf you notice, UF has most books out of all Florida colleges, Go Gators!\n\n\n\nQuick Exercise\n\nPlay around the with shape argument and see what other options there are\nTime permitting: See if you can get rid of the scientific numbering (e.g., 1e+06) so it looks the plot below\n\nThere’s a few ways, Google and StackOverflow have some helpful answers\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously, this may not be the most useful map in the world, but the template is very adaptable to a lot of educational situations\n\nUsing tidycensus we can swap out the base map geography to have most common US geographies and/or swap out any variable available in from the Census Bureau\n\nYou will also see below a way of getting shapes without Census data using tigris\nSee the example on school districts below\n\nEqually, we can swap out the point data to represent anything we have coordinate points for and change the aesthetics to represent any data we have for those points",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "href": "10-viz-iii.html#supplemental-material-us-transformations-tigris-basics",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Supplemental Material: US transformations & tigris basics",
    "text": "Supplemental Material: US transformations & tigris basics\n\nFor the sake of time, I left this until the end as we don’t need it for the assignment. But it may be useful if you are looking to make any maps in your final assignment or the future.\n\ntigris is a package that offers a direct way of downloading US spatial data that is not tied to census data.\n\nNote: it’s actually used by tidycensus behind the scenes to get your spatial data\nIf you get spatial data from tigris it won’t come with any additional data to plot per say, but it comes with identifying variables you could use to pair up with external data using something like left_join()\n\n\nMapping School Districts\n\nIf we wanted to plot Census data about the population in these school districts we could get these from tidycensus with geography = \"school district (unified)\"\nIf instead we have school district data we want to plot, or we are only interested in the boundary, we might want to download the spatial data for school districts directly\n\nIn that case, it might be easier to use tigris directly to get the blank shapefiles.\nThe function names for tigris are really simple.\n\nschool_districts() for example retrieves a shapefile for US school districts\n\n\n\n\nggplot() +\n  geom_sf(data = df_school_dist_tx,\n          aes())\n\n\n\n\n\n\n\n\n\nYou’ll notice we actually left aes() blank, as geom_sf() will automatically make the spatial elements for us and right now we have no additional info to add to plot elements like fill\nThere is one new argument in there\n\ncb = TRUE\n\nStands for cartographic boundary, basically, a less detailed line that makes for faster and easier mapping\n\nIf the border detail was really important to your analysis, you might want to set to false, but for mapping it’s usually the best option",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#states-and-crs-transformations",
    "href": "10-viz-iii.html#states-and-crs-transformations",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "50 States and CRS Transformations",
    "text": "50 States and CRS Transformations\n\nFinally, we are going to look at how CRS projections changes national maps\n\nFirst, we need to download some basic spatial data for the 50 states\n\nLike I said before, tigris function names are really simple\n\nstates() downloads spatial data for the states\n\nSimilarly to cb = TRUE we discussed above resolution = \"20m\" sacrifices a little detail for speed and efficiency for mapping\n\n\n\n\n\nLike we did before, let’s take a peak at our newly downloaded data.\n\n## look at head of state data\nhead(df_st)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 24.49813 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n  STATEFP  STATENS    AFFGEOID GEOID STUSPS      NAME LSAD        ALAND\n1      22 01629543 0400000US22    22     LA Louisiana   00 1.119153e+11\n2      02 01785533 0400000US02    02     AK    Alaska   00 1.478943e+12\n3      24 01714934 0400000US24    24     MD  Maryland   00 2.515199e+10\n4      55 01779806 0400000US55    55     WI Wisconsin   00 1.402923e+11\n5      12 00294478 0400000US12    12     FL   Florida   00 1.389617e+11\n6      13 01705317 0400000US13    13     GA   Georgia   00 1.494866e+11\n        AWATER                       geometry\n1  23736382213 MULTIPOLYGON (((-94.04305 3...\n2 245378425142 MULTIPOLYGON (((179.4813 51...\n3   6979074857 MULTIPOLYGON (((-76.04621 3...\n4  29343646672 MULTIPOLYGON (((-86.93428 4...\n5  45972570361 MULTIPOLYGON (((-81.81169 2...\n6   4418360134 MULTIPOLYGON (((-85.60516 3...\n\n\n\nSimilar to before, we have\n\nA spatial summary at the top\nA set of normal looking columns with different ID codes and names\nAn attached geometry for each row\n\nIf we simply plot this with no aesthetics, we get the outline of all states, but there is something about it that makes it less ideal for a quick data visualization…\n\n\n## quick plot of states\nggplot() +\n  geom_sf(data = df_st,\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\n\nAs we can see, while the map is geographically accurate, there is a lot of open ocean on the map due to the geographic structure of the US\n\nThe tail of Alaska crosses the 180 degree longitude line, so it wraps to the other side of the map\n\nOften when we see maps of the US, such as on election night, Alaska and Hawaii are moved to make it easier to read\n\nTigris offers an easy way of doing this\n\nshift_geometry()\n\nshould work on any spatial data with Alaska, Hawaii, and Puerto Rico\n\n\n\n\n\n## replotting with shifted Hawaii and Alaska\nggplot() +\n  geom_sf(data = shift_geometry(df_st),\n          aes(),\n          size = 0.1) # keep the lines thin, speeds up plotting processing\n\n\n\n\n\n\n\n\n\nAlthough not always a good idea, if you’re looking to plot the 50 states in an easy-to-read manner, this can be a really useful tool\n\nNote: it does not move other U.S. territories like Guam by default, so you may have colleges and schools out there you need to deal with\nNote: this can re-project your data, so you need to make sure your CRS is what you want after this (as we will do in our final examples)\n\nNever do spatial analysis on data you’ve done this to, it will be severely off\n\nI recommend only ever using it inside geom_sf()\n\nThis way you never change your data\n\nYou don’t want to accidentally say Hawaii is closer to Austin than Oklahoma City is…\n\n\n\nFinally, to re-illustrate what a CRS does, let’s plot this two more times\n\nFirst, putting it onto our simple EPSG 4326 CRS from earlier\nThen, using the Peters Projection referenced in the video clip at the start of class\n\n\n\n## change CRS to what we used for earlier map\nggplot() +\n  geom_sf(data = shift_geometry(df_st) |&gt; st_transform(4326),\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\n\nSee how the line are now a perfect grid, but the shapes of states (look at Montana) are a little different?\n\nThat’s the power of a CRS!\n\n\n\nQuick Exercise\n\nPick a different projection from the list of EPSG projections and see what changes\n\nPerhaps try a projection for a different part of the world\n\nBelow I tried EPSG 6684 for Tokyo, Japan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s please the Organization of Cartographers for Social Equality and look at the Peters projection\n\nNote: while this projection is great for showing comparably accurate area across the globe, it does that by other trade offs not acknowledged by Dr. Fallow from OCSE\nNo projection is universally better, each is better for the task it was designed for\n\nThat’s the key with CRS, find the best one for the task you’re doing.\n\n\n\n\n## change CRS to requirements for Peters projection\n## h/t https://gis.stackexchange.com/questions/194295/getting-borders-as-svg-using-peters-projection\npp_crs &lt;- \"+proj=cea +lon_0=0 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n\nggplot() +\n  geom_sf(data = shift_geometry(df_st) |&gt; st_transform(pp_crs),\n          aes(),\n          size = 0.1)\n\n\n\n\n\n\n\n\n\nSee how to the gap between 45 and 50 degrees north is much smaller than between 20 and 25 degrees north?\n\nThat’s the projection at work\n\nThink about how this reflects how the globe is shaped and how that affects area (the focus of the Peters projection map)",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#option-one",
    "href": "10-viz-iii.html#option-one",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Option One",
    "text": "Option One\nUsing the tidycensus package and code from this lesson, make a different point map following the instructions below:\n\nModify the tidycensus code to download data for a different geography and a different variable\nMake a new base map\nUse the same academic libraries IPEDS data (or another IPEDS file if you want) to plot points on your new base map that change aes() based on a new variable",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#option-two",
    "href": "10-viz-iii.html#option-two",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Option Two",
    "text": "Option Two\nUsing data from your final project (plus tidycensus and/or tigris as needed) make a map following the guidance from the lesson. This will take a bit more time now, but, if it makes sense for your final project it might save you time later.\n\nNote: It still needs to be a geom_sf()\n\n\nHint: The main challenge will be working out how to either convert you data to spatial data or merge it with existing spatial data",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "10-viz-iii.html#submission",
    "href": "10-viz-iii.html#submission",
    "title": "III: Maps & Spatial Data (Feat. APIs)",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Data Visualization",
      "III: Maps & Spatial Data (Feat. APIs)"
    ]
  },
  {
    "objectID": "12-pro-model.html#data-preparation",
    "href": "12-pro-model.html#data-preparation",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nBefore we can run any kind of models, we need to make sure our data is prepared\n\nThis involves using skills from our data wrangling lessons such as\nData Wrangling I\n\nHandling missing data\nMaking sure our data is the right format (numeric, factor, character, etc.)\nPerforming basic calculations (e.g., percentages, differences, etc.)\n\nData Wrangling II\n\nJoining multiple data sets together\nPivoting data wider and/or longer\n\nData Wrangling III\n\nCleaning up text data\nTransforming dates into\n\nData Wrangling IV\n\nPerforming any of the above tasks across() multiple columns\ncoalesce()-ing multiple columns into one variable\n\n\nFor the purpose of today’s lesson, we are going to focus on two of these tasks, dealing with missing data, and making sure our data is in the right format\n\n\nHandling Missing Data\n\nWhen modeling, by default, R will simply drop any rows that have an NA in any variable you are modeling on (this is a little different to the cautious R we ran into in Data Wrangling I)\nIn real world applications, you need to think carefully about how you handle these…\n\nShould I impute the missing data? If so, using what method?\nShould I use this variable at all if it’s missing for a bunch of observations?\n\nFor this lesson, however, we are just going to drop NA values so we can focus on the main content\nThe below code uses the combines the logic we use for making NAs in Data Wrangling I with the ability to work across multiple columns in Data Wrangling IV\nFirst, we read our data and select() the columns we want to use\n\n\ndata &lt;- read_csv(\"data/hsls-small.csv\") |&gt;\n  select(stu_id, x1sex, x1race, x1txmtscor, x1paredu, x1ses, x1poverty185, x1paredexpct)\n\n\nSecond we use a combination of !, filter(), and if_any() to say…\n“If a row…”\n\n“has a -8 or -9”\n\n.fns = ~ . %in% c(-8, -9)\n\n“in any columns”\n\n.cols = everything()\n\n“do NOT keep it”\n\nfilter(!)\n\n\n\n\ndata &lt;- data |&gt;\n  filter(! if_any(.cols = everything(),\n                  .fns = ~ . %in% c(-8, -9)))\n\n\n\nMaking Sure Our Data is the Right Format\n\nIn our Data Viz I and Data Viz II lessons, we saw that for R to accurately plot categorical variables, we had to convert them into factor()s\n\nThe same is true for using categorical variables in models\nThose more familiar with stats may know that you have to “dummy code” categorical variables as 0 and 1 with one category serving as the “reference level” and all other categories getting their own binary variable\nThe wonderful thing is that R handles that all for us if we tell it to treat the variable as a factor()\n\n\n\n\nThe below code combines the logic of turning variables into a factor() from Data Viz I with working across multiple columns for Data Wrangling IV to say\n“Modify”\n\nmutate()\n\n“Each of these columns”\n\nacross(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185)\n\n“Into a factor”\n\n.fns = ~ factor(.)\n\n\n\ndata &lt;- data |&gt;\n  mutate(across(.cols = c(stu_id, x1sex, x1race, x1paredu, x1poverty185),\n                .fns = ~ factor(.)))\n\n\nWith that, our data is ready for some basic analysis!\nNote: In most real-world projects your data preparation will be much more thorough, usually taking up the vast majority of the lines of code in your entire project, this is just the bare minimum to have to models run",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#t-tests-with-t.test",
    "href": "12-pro-model.html#t-tests-with-t.test",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "t-tests with t.test()",
    "text": "t-tests with t.test()\n\nOne of the first inferential statistical tests you will have learned (or will learn) is the t-test\n\nFor those unfamiliar, the basic concept of a t-test if variance between two groups (i.e., the difference between treatment and control) is greater than the variance within those groups (i.e., random variance between people within the same group)\n\nIf that between-group-variance is great enough compared to the within-group-variance, the t-test will be “statistically significant”\n\nThis means we are (most often) 95% confident that the there is a genuine difference between the groups\n\nThere are also a handful of statistical assumptions we have to satisfy, which are beyond our scope here, but hopefully the general concept will hope those of you yet to take your stats foundations follow along\n\n\n\n\nt.test(x1txmtscor ~ x1sex, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  x1txmtscor by x1sex\nt = 0.38555, df = 16321, p-value = 0.6998\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.2455338  0.3657777\nsample estimates:\nmean in group 1 mean in group 2 \n       52.14309        52.08297 \n\n\n\nLuckily, the code for t.test() is actually very simple (as is the case for regression too)\n\nThe first argument is a forumla, which for a t-test is just outcome ~ group where group must only have 2 levels - In this case, we are looking at math score as our outcome and sex as our group\nThe second argument is data = which we supply our prepared data frame\n\nNote: the pipe |&gt; doesn’t play as nicely with models as it does other commands it’s usually easier to just specify data = in a new line (don’t pipe anything in)\n\n\nThis code simply prints out our t.test() result\n\nAs our p-value is above 0.05, our result is not significant - This indicates there is not a significant difference between male and female math scores in our sample",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#regression-with-lm",
    "href": "12-pro-model.html#regression-with-lm",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Regression with lm()",
    "text": "Regression with lm()\n\nThe problem with t-tests for our research, is that they don’t provide any ability to control for external variables\n\nThey work great in experimental setting with random-treatment-assignment, but in the messy world of educational research, that’s rarely what we have\n\nWhat we far more commonly use is a regression (or more advanced methods that build off regression) which allows use to control for other variables\nThe basic premise of regression very much builds off the logic of t-tests, testing if the variance associated with our treatment variable is great enough compared to a) residual/random variance and b) variance associated with our control variables, to say with confidence that there is a significant difference associated with our treatment\n\nOverall, this looks relatively similar to our code above, with three main differences\n\n\n\nWe use lm() (which stands for linear model) instead of t.test()\nInstead of our formula just being x1txmtscor ~ x1sex we have added + x1poverty185 + x1paredu to “control” for these variables\nWe assigned &lt;- our lm() results to an object rather than just spitting them out\n\n\nThat’s because the summary() function is much more useful for lm() objects, plus, we are going to explore the lm() object more in the next steps\n\n\nregression &lt;- lm(x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\nsummary(regression)\n\n\nCall:\nlm(formula = x1txmtscor ~ x1sex + x1poverty185 + x1paredu, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.748  -5.715   0.160   6.136  30.413 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    49.0485     0.3379 145.139  &lt; 2e-16 ***\nx1sex2         -0.0412     0.1429  -0.288    0.773    \nx1poverty1851  -3.0224     0.1729 -17.483  &lt; 2e-16 ***\nx1paredu2       1.3419     0.3234   4.149 3.36e-05 ***\nx1paredu3       2.4834     0.3584   6.929 4.38e-12 ***\nx1paredu4       6.1227     0.3507  17.460  &lt; 2e-16 ***\nx1paredu5       8.1726     0.3814  21.426  &lt; 2e-16 ***\nx1paredu7      10.7669     0.4294  25.074  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.161 on 16421 degrees of freedom\nMultiple R-squared:  0.1605,    Adjusted R-squared:  0.1601 \nF-statistic: 448.4 on 7 and 16421 DF,  p-value: &lt; 2.2e-16\n\n\n\nOur results show that, sex still had no significant association with math scores, but, our control variables of poverty and parental education seem to have some very strong associations\n\n\nQuick Question\n\nYou may notice we actually have more variables in the regression table than we put in, why? What do they represent?\n\n\n\nThat’s correct, they represent the different levels of our factor()-ed categorical variables",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#creating-pretty-regression-output-tables",
    "href": "12-pro-model.html#creating-pretty-regression-output-tables",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Creating Pretty Regression Output Tables",
    "text": "Creating Pretty Regression Output Tables\n\nRunning regressions in R is all well and good, but the output you see here isn’t exactly “publication ready”\nThere are multiple ways of creating regression (and other model) output tables, each with their own pros and cons\nHere, we will go over three of the most common methods\n\n\nstargazer Package\n\nOne of the most common packages for getting “publication ready” regression tables is stargazer\n\nThe code is very simple, at minimum, provide the regression model you fitted, and the type of table you want\nWe are using “html” here so it formats for the website, you could use “text” or “latex”\nIn general, the biggest drawback of stargazer is the lack of flexibility and limited compatibility with formats other than LaTeX\n\n\n\n\nExtra trick: To get captions and table numbers if we are using Quarto, we use the “chunk options” to cross reference them, not manually adding them to the table code\n\nYou see more about that on the Quarto guide for cross referencing\nTo get the table to appear like this we…\n\nLabeled the chunk {r tbl-stargazer}\nAdded #| tbl-cap: \"Regression Table Using stargazer\" as the first line\n\n\n\n\n\n\nTable 1: Regression Table Using stargazer\n\n\nstargazer(regression, type = \"html\")\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nx1txmtscor\n\n\n\n\n\n\n\n\nx1sex2\n\n\n-0.041\n\n\n\n\n\n\n(0.143)\n\n\n\n\n\n\n\n\n\n\nx1poverty1851\n\n\n-3.022***\n\n\n\n\n\n\n(0.173)\n\n\n\n\n\n\n\n\n\n\nx1paredu2\n\n\n1.342***\n\n\n\n\n\n\n(0.323)\n\n\n\n\n\n\n\n\n\n\nx1paredu3\n\n\n2.483***\n\n\n\n\n\n\n(0.358)\n\n\n\n\n\n\n\n\n\n\nx1paredu4\n\n\n6.123***\n\n\n\n\n\n\n(0.351)\n\n\n\n\n\n\n\n\n\n\nx1paredu5\n\n\n8.173***\n\n\n\n\n\n\n(0.381)\n\n\n\n\n\n\n\n\n\n\nx1paredu7\n\n\n10.767***\n\n\n\n\n\n\n(0.429)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n49.049***\n\n\n\n\n\n\n(0.338)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n16,429\n\n\n\n\nR2\n\n\n0.160\n\n\n\n\nAdjusted R2\n\n\n0.160\n\n\n\n\nResidual Std. Error\n\n\n9.161 (df = 16421)\n\n\n\n\nF Statistic\n\n\n448.368*** (df = 7; 16421)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\n\n\ngtsummary Package\n\nThe gtsummary package (which we saw in our Intro to Quarto lesson) is another option for creating tables\n\ntbl_regression() creates a pretty great table when you just provide the regression model\nOne great thing is how gtsummary handles factors—it makes the it super-duper clear what is going on\n\n\n\ntbl_regression(regression)\n\n\n\nTable 2: Regression Table Using gtsummary (basic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nx1sex\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n-0.04\n-0.32, 0.24\n0.8\n\n\nx1poverty185\n\n\n\n\n\n\n\n\n    0\n—\n—\n\n\n\n\n    1\n-3.0\n-3.4, -2.7\n&lt;0.001\n\n\nx1paredu\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n1.3\n0.71, 2.0\n&lt;0.001\n\n\n    3\n2.5\n1.8, 3.2\n&lt;0.001\n\n\n    4\n6.1\n5.4, 6.8\n&lt;0.001\n\n\n    5\n8.2\n7.4, 8.9\n&lt;0.001\n\n\n    7\n11\n9.9, 12\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\nWith a couple of extra lines to\n\nhandle variable labels\n\nlabel = list(x1sex ~ \"Sex\" ... )\n\nadd significance stars (without hiding the p-value of confidence intervals)\n\nadd_significance_stars(hide_ci = FALSE, hide_p = FALSE)\n\nadd model statistics\n\nadd_glance_source_note(include = c(r.squared, nobs))\n\nforce the standard error column to show\n\nmodify_column_unhide(std.error)\n\n\nwe can get something that looks pretty great\n\n\n\nSimilarly to above, we add table numbers and captions using Quarto cross referencing\n\n{r tbl-gtsummary}\n#| tbl-cap: \"Regression Table Using gtsummary\"\n\n\n\ntbl_regression(regression,\n               label = list(x1sex ~ \"Sex\",\n                            x1poverty185 ~ \"Below Poverty Line\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\nTable 3: Regression Table Using gtsummary (custom)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.143\n-0.32, 0.24\n0.8\n    Below Poverty Line\n\n\n\n\n        0\n—\n—\n—\n\n        1\n-3.0***\n0.173\n-3.4, -2.7\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n1.3***\n0.323\n0.71, 2.0\n&lt;0.001\n        3\n2.5***\n0.358\n1.8, 3.2\n&lt;0.001\n        4\n6.1***\n0.351\n5.4, 6.8\n&lt;0.001\n        5\n8.2***\n0.381\n7.4, 8.9\n&lt;0.001\n        7\n11***\n0.429\n9.9, 12\n&lt;0.001\n  \n  \n    \n      R² = 0.160; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\nReminder: We saw in Intro to Quarto how you can create matching descriptive statistics tables using the tbl_summary() function\n\nWe also saw how you can force them to as_kable() if needed for you output format\nIf you like gtsummary tables and want to learn more, check out\n\ngtsummary documentation\n\n\n\n\n\n“Homemade” Regression Tables with kable()\n\nWhile a little more work, we can also create our own table using the default summary() and kable() like we saw in Intro to Quarto for descriptive tables\n\nYou might want to do this to specifically format a table in a way gtsummary doesn’t allow, or, to match some other tables you already created with kable\n\nFirst things first, let’s save the summary() output to a new object\n\n\nsummary_object &lt;- summary(regression)\n\n\nWe will do this more below, but if you click on the object summary_object in the Environment (top right) you can see all the different pieces of information it holds\n\nWe are most interested in coefficients, if you click on the right hand side of that row, you will see the code summary_object[[\"coefficients\"]] auto-populate\n\n\nTip: This works for most objects like this\n\n\nWe then turn that into as data frame with as.data.frame()\nAdd a new column that contains the correct significance stars using case_when()\nPipe all that into kable() with updated column names and rounded numbers\nSimilarly to above, we add table numbers and captions using Quarto cross referencing\n\n\n{r tbl-manual}\n#| tbl-cap: \"Regression Table Using Kable\"\n\n\n\nsummary_object[[\"coefficients\"]] |&gt;\n  as.data.frame() |&gt;\n  mutate(sig = case_when(`Pr(&gt;|t|)` &lt; 0.001 ~ \"***\",\n                         `Pr(&gt;|t|)` &lt; 0.01 ~ \"**\",\n                         `Pr(&gt;|t|)` &lt; 0.05 ~ \"*\",\n                         TRUE ~ \"\")) |&gt;\n  kable(col.names = c(\"estimate\", \"s.e.\", \"t\", \"p\", \"\"),\n        digits = 3)\n\n\n\nTable 4: Regression Table Using kable\n\n\n\n\n\n\n\nestimate\ns.e.\nt\np\n\n\n\n\n\n(Intercept)\n49.049\n0.338\n145.139\n0.000\n***\n\n\nx1sex2\n-0.041\n0.143\n-0.288\n0.773\n\n\n\nx1poverty1851\n-3.022\n0.173\n-17.483\n0.000\n***\n\n\nx1paredu2\n1.342\n0.323\n4.149\n0.000\n***\n\n\nx1paredu3\n2.483\n0.358\n6.929\n0.000\n***\n\n\nx1paredu4\n6.123\n0.351\n17.460\n0.000\n***\n\n\nx1paredu5\n8.173\n0.381\n21.426\n0.000\n***\n\n\nx1paredu7\n10.767\n0.429\n25.074\n0.000\n***\n\n\n\n\n\n\n\n\n\nThere are other ways as well, but between these three options, you should be able to get what you want!\n\n\n\nPredictions with lm()\n\nWhen you fit a regression model in R, there is a lot more saved than you see with summary()\nSince we have our lm() object saved as regression, let’s start by taking a look inside it by clicking on the object in our environment (top right) panel - Confusing, right?\n\nMost statistical models look something like this, it’s basically a collection of lists and tables containing different information about the model\n\nThere are functions such as summary() that are great at pulling out the most commonly needed information without having to go manually digging through the model object, but sometimes, it can be useful to know it’s there\nAnother great function is predict() which extracts estimated values of the outcome variable based on the predictor variables (some other models use fitted() for the same purpose)\nFor those more familiar with stats, you’ll know predicted values are often compared against the true values to see how strong the model is\n\n\n\nTo start, let’s save a full set of predictions to a new columns in our data frame\n\n\ndata &lt;- data |&gt;\n  mutate(prediction = predict(regression))\n\ndata |&gt; select(stu_id, x1txmtscor, prediction)\n\n# A tibble: 16,429 × 3\n   stu_id x1txmtscor prediction\n   &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n 1 10001        59.4       57.2\n 2 10002        47.7       48.5\n 3 10003        64.2       59.8\n 4 10004        49.3       55.1\n 5 10005        62.6       55.2\n 6 10006        58.1       51.5\n 7 10007        49.5       50.3\n 8 10008        54.6       59.8\n 9 10009        53.2       50.4\n10 10010        63.8       51.5\n# ℹ 16,419 more rows\n\n\n\nNext, we can compare these to our actual results using a simple plot (no formatting) from Data Viz I\n\nThe only new things we add here is\n\ngeom_abline(slope = 1, intercept = 0)\n\nThis adds a reference line that represents a perfect 1 to 1 relationship (which would be if there was 0 prediction error)\n\ncoord_obs_pred() which is from the tidymodels package\n\nThis fixes the axes so that the predictions and observed values are plotted on the same scal\n\n\n\n\n\nQuick Excercise\n\nTry removing the final line coord_obs_pred() and see what happens. Which plot do you think is better?\n\n\n\nggplot(data,\n       aes(x = prediction,\n           y = x1txmtscor)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\n(Easier) Quick Question\n\nWhat do we think about our model? Does it look like it’s doing a great job of predicting? Why/why not?\n\n\n\n(Harder) Quick Question\n\nYou’ll notice our plot looks kind of clumped together, why do you think that it? What about the model would lead to that?\n\n\n\nGiven what we just discussed, can we change one of the variables we are using in the model to make it less “clumpy” but caputre the same information?\n\n\nregression_2 &lt;- lm(x1txmtscor ~ x1sex + x1ses + x1paredu, data = data)\n\ndata &lt;- data |&gt;\n  mutate(prediction_2 = predict(regression_2))\n\nggplot(data,\n       aes(x = prediction_2,\n           y = x1txmtscor)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0) +\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\nQuick Question\n\nDoes that look better? What else is odd about our predictions?\n\n\n\nWe can also use predict() to estimate potential outcome values for new students who don’t have the outcome for\nThis is a common way you evaluate machine learning models\nIf you think you’re model is a really good predictor (which ours is not) you may feel comfortable using something like this to help your office predict student outcomes/identify students in need of additional help\n\n\n\nTo demonstrate this, we are first going to split out 10% of our data using slice_sample() and drop the math score from it\n\n\ndata_outcome_unknown &lt;- data |&gt;\n  slice_sample(prop = 0.1) |&gt;\n  select(-x1txmtscor)\n\n\nThen, we can use anti_join() which is basically the opposite of the joins we used in Data Wrangling II\n\nIt looks for every row in x that isn’t in y and keeps those\n\n\n\ndata_outcome_known &lt;-  anti_join(x = data, y = data_outcome_unknown, by = \"stu_id\")\n\n\nNow, we can fit one more lm() using our data we “know” the outcome for\n\n\nregression_3 &lt;- lm(x1txmtscor ~ x1sex + x1ses + x1paredu, data = data_outcome_known)\n\n\nFinally, we can predict() outcomes for the data we “don’t know” the outcome for\n\nWe add the regression_3 we just fitted as the model, same as before\nBut we also add newdata = data_outcome_unknown to say predict the outcome for this new data, instead of extract the predictions the model originally made\n\n\n\ndata_outcome_unknown &lt;- data_outcome_unknown |&gt;\n  mutate(prediction_3 = predict(regression_3, newdata = data_outcome_unknown))\n\n\nLastly, let’s see how similar our predictions we made using our model without the outcome were to those made when the outcome was known for everyone using cor() to get the correlation\n\n\ncor(data_outcome_unknown$prediction_2, data_outcome_unknown$prediction_3)\n\n[1] 0.9999516\n\n\n\nPretty close!\n\n\n\nChecking Residuals\n\nMany of the assumptions relating to regression are tested by looking at the residuals\n\nWe aren’t going to go over those assumptions, again, this is not a stats class\nBut it might be useful to see how to get them out of a model object\n\nLet’s start by viewing the lm object again (environment, top right panel), then clicking on the little white box on the right hand side of the screen for the row “residuals” - That is a magic tip, if you ever want to get something specific out of a model object, often they’ll be something you can click on to generate the code needed to access it in the console\n\nFor residuals, it is regression_2[[\"residuals\"]]\n\n\n\ndata &lt;- data |&gt;\n  mutate(residual = regression_2[[\"residuals\"]])\n\n\nNow, again, not to get too deep into assumptions, but one of the key things to check is that your residuals have a normal distribution\n\nSo let’s revisit some Data Visualization I content and make a simple ggplot() histogram to of them\n\n\n\nggplot(data) +\n  geom_histogram(aes(x = residual),\n                 color = \"black\",\n                 fill = \"skyblue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nWow, that is almost a perfect normal distribution!\nBonus points: can anyone remember/think of something about the variable x1txmtscor that made this result quite likely? Think about what kind of score it is",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#formula-objects",
    "href": "12-pro-model.html#formula-objects",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "formula() Objects",
    "text": "formula() Objects\n\nThe second from last thing is really simple, but, it can be a time & error saver if you want to get more advanced like our final step\nAbove, we simply put our formula into the t.test() or lm() command\n\nInstead, we can actually specify it as a formula object first, then call that object, which has two advantages\n\n\n\nIf we run multiple tests with the same formula, we only have to change it once in our code for updates\n\n\nHere, we will run both standard lm() and lm_robust() from the estimatr package\n\n\nIf we want to run multiple tests in a loop like below, it makes that possible too\n\n\n\nTo demonstrate this, we will fit the same model using standard lm() and lm_robust() which for those versed in stats, is one option we can use when we have a violation of heteroskedasticity\n\n\nregression_formula &lt;- formula(x1txmtscor ~ x1sex + x1ses + x1paredu)\n\nregression_4 &lt;- lm(regression_formula, data = data)\n\ntbl_regression(regression_4,\n               label = list(x1sex ~ \"Sex\",\n                            x1ses ~ \"Socio-Economic Status\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.142\n-0.32, 0.24\n0.8\n    Socio-Economic Status\n3.8***\n0.163\n3.5, 4.1\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.05\n0.333\n-0.70, 0.61\n0.9\n        3\n-0.16\n0.390\n-0.92, 0.61\n0.7\n        4\n2.0***\n0.424\n1.2, 2.9\n&lt;0.001\n        5\n2.6***\n0.495\n1.6, 3.5\n&lt;0.001\n        7\n2.7***\n0.609\n1.5, 3.9\n&lt;0.001\n  \n  \n    \n      R² = 0.172; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval\n    \n  \n\n\n\nregression_robust &lt;- lm_robust(regression_formula, data = data, se_type = \"stata\")\n\ntbl_regression(regression_robust,\n               label = list(x1sex ~ \"Sex\",\n                            x1ses ~ \"Socio-Economic Status\",\n                            x1paredu ~ \"Parental Education\")) |&gt;\n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n  add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n  modify_column_unhide(std.error)\n\n\n\n\n  \n    \n      Characteristic\n      Beta1\n      SE2\n      95% CI2\n      p-value\n    \n  \n  \n    Sex\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.04\n0.142\n-0.32, 0.24\n0.8\n    Socio-Economic Status\n3.8***\n0.165\n3.5, 4.1\n&lt;0.001\n    Parental Education\n\n\n\n\n        1\n—\n—\n—\n\n        2\n-0.05\n0.320\n-0.67, 0.58\n0.9\n        3\n-0.16\n0.377\n-0.89, 0.58\n0.7\n        4\n2.0***\n0.417\n1.2, 2.9\n&lt;0.001\n        5\n2.6***\n0.489\n1.6, 3.5\n&lt;0.001\n        7\n2.7***\n0.621\n1.5, 3.9\n&lt;0.001\n  \n  \n    \n      R² = 0.172; No. Obs. = 16,429\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n    \n      2 SE = Standard Error, CI = Confidence Interval",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#modeling-programatically-with-loops",
    "href": "12-pro-model.html#modeling-programatically-with-loops",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Modeling Programatically with Loops",
    "text": "Modeling Programatically with Loops\n\nFinally, we can also bring in content from Functions & Loops and fit regression models using loops\n\nThis is kind of thing you might want to do if you are testing the same model on a set of outcomes\n\n\n\nQuick Question\n\nThinking back to that lesson, why might we want to go through the hassle of fitting regressions using loops? What are the advantages of using loops vs coding it all out separately?\n\n\n\nFor example, we might be interested in modeling both a students math score and their parental education expectation\n\nWe make a list containing our outcome variables (x1txmtscor and x1paredexpct)\nUse a for() loop to loop through these outcomes, which paste()s i (which takes on the name of each outcome variable) into the formula and then runs the model\n\n\noutcomes &lt;- c(\"x1txmtscor\", \"x1paredexpct\")\n\nfor(i in outcomes) {\n  \n  print(i)\n  \n  loop_formula &lt;- formula(paste0(i, \"~ x1sex + x1ses + x1paredu\"))\n  \n  loop_regression &lt;- lm(loop_formula, data = data)\n  \n  tbl_regression(loop_regression,\n                 label = list(x1sex ~ \"Sex\",\n                              x1ses ~ \"Socio-Economic Status\",\n                              x1paredu ~ \"Parental Education\")) |&gt;\n    add_significance_stars(hide_ci = FALSE, hide_p = FALSE) |&gt;\n    add_glance_source_note(include = c(r.squared, nobs)) |&gt;\n    modify_column_unhide(std.error) |&gt;\n    print()\n  \n}\n[1] “x1txmtscor”\n\n\n\n\n\n\nCharacteristic\n\n\nBeta1\n\n\nSE2\n\n\n95% CI2\n\n\np-value\n\n\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.04\n\n\n0.142\n\n\n-0.32, 0.24\n\n\n0.8\n\n\n\n\nSocio-Economic Status\n\n\n3.8***\n\n\n0.163\n\n\n3.5, 4.1\n\n\n&lt;0.001\n\n\n\n\nParental Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.05\n\n\n0.333\n\n\n-0.70, 0.61\n\n\n0.9\n\n\n\n\n    3\n\n\n-0.16\n\n\n0.390\n\n\n-0.92, 0.61\n\n\n0.7\n\n\n\n\n    4\n\n\n2.0***\n\n\n0.424\n\n\n1.2, 2.9\n\n\n&lt;0.001\n\n\n\n\n    5\n\n\n2.6***\n\n\n0.495\n\n\n1.6, 3.5\n\n\n&lt;0.001\n\n\n\n\n    7\n\n\n2.7***\n\n\n0.609\n\n\n1.5, 3.9\n\n\n&lt;0.001\n\n\n\n\n\n\nR² = 0.172; No. Obs. = 16,429\n\n\n\n\n\n\n1 p&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n2 SE = Standard Error, CI = Confidence Interval\n\n\n\n\n\n[1] “x1paredexpct”\n\n\n\n\n\n\nCharacteristic\n\n\nBeta1\n\n\nSE2\n\n\n95% CI2\n\n\np-value\n\n\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n0.47***\n\n\n0.040\n\n\n0.39, 0.55\n\n\n&lt;0.001\n\n\n\n\nSocio-Economic Status\n\n\n0.28***\n\n\n0.046\n\n\n0.19, 0.38\n\n\n&lt;0.001\n\n\n\n\nParental Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    1\n\n\n—\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n    2\n\n\n-0.39***\n\n\n0.094\n\n\n-0.58, -0.21\n\n\n&lt;0.001\n\n\n\n\n    3\n\n\n-0.28*\n\n\n0.110\n\n\n-0.50, -0.07\n\n\n0.010\n\n\n\n\n    4\n\n\n-0.08\n\n\n0.119\n\n\n-0.31, 0.16\n\n\n0.5\n\n\n\n\n    5\n\n\n0.29*\n\n\n0.139\n\n\n0.02, 0.56\n\n\n0.037\n\n\n\n\n    7\n\n\n0.85***\n\n\n0.172\n\n\n0.52, 1.2\n\n\n&lt;0.001\n\n\n\n\n\n\nR² = 0.049; No. Obs. = 16,429\n\n\n\n\n\n\n1 p&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n2 SE = Standard Error, CI = Confidence Interval",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#question-one",
    "href": "12-pro-model.html#question-one",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Question One",
    "text": "Question One\na) Conduct an independent samples t-test to compare math scores between students below 185% of the federal poverty line and those above it. Is there a statistically significant difference?\nb) Make a plot to visualize the difference.",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#question-two",
    "href": "12-pro-model.html#question-two",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Question Two",
    "text": "Question Two\na) Fit a logistic regression predicting postsecondary attendance using math score, sex, family income, and parental education\n\nHint: You might want to Google to find out what function you need for fitting a logistic regression.\n\nb) Create a nicely formatted output table using stargazer or gtsummary for the model from 2a.",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "12-pro-model.html#submission",
    "href": "12-pro-model.html#submission",
    "title": "Bringing It All Together (Feat. Basic Models)",
    "section": "Submission",
    "text": "Submission\nOnce complete turn in the .qmd file (it must render/run) and the rendered PDF to Canvas by the due date (usually Tuesday 12:00pm following the lesson). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.",
    "crumbs": [
      "Bringing it All Together (Feat. Basic Models)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Student Folder to Clone/Fork\nStudent Folder to Download as .zip",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#class-overview",
    "href": "index.html#class-overview",
    "title": "Homepage",
    "section": "Class Overview",
    "text": "Class Overview\nContemporary research in higher education (and other disciplines) should be both rigorous and reproducible. This is class will teach you the fundamentals of data management and quantitative research workflow with emphasis on rigor and reproducibility.\nOften referred to informally as “the R class”, you will get an introduction to coding using the R programming language, but many of the skills are directly transferable to future work in Python, Stata, or other software.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#credit-to-ben-skinner-and-matt-capaldi",
    "href": "index.html#credit-to-ben-skinner-and-matt-capaldi",
    "title": "Homepage",
    "section": "Credit to Ben Skinner and Matt Capaldi",
    "text": "Credit to Ben Skinner and Matt Capaldi\nFirst and foremost, credit for the structure and vast majority of the content on this site goes to Dr. Benjamin T. Skinner and Matt Capaldi, who have taught this class in the past.\nIf you want to learn more, see Dr. Skinner’s version of the class website.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#the-im-stuck-workflow",
    "href": "index.html#the-im-stuck-workflow",
    "title": "Homepage",
    "section": "The “I’m Stuck” Workflow",
    "text": "The “I’m Stuck” Workflow\n\nTake a break, go outside, get some food\nTalk to your rubber duck\nTalk to your classmates\n\nPlease acknowledge with a ## h/t\n\nTry Google or Stack Overflow\n\nPlease acknowledge with a ## h/t (it helps you later too!)\nCaution: the internet does strange things to people… Sometimes people offering “help” can be unnecessarily blunt and/or mean, particularly to people just starting out\n\nIf you use Github Copilot to help you with your code\n\nPlease keep your prompts and acknowledge with a ## h/t\n\nOffice hours or email",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#a-note-on-potential-frustration-in-this-class",
    "href": "index.html#a-note-on-potential-frustration-in-this-class",
    "title": "Homepage",
    "section": "A Note on Potential Frustration in this Class",
    "text": "A Note on Potential Frustration in this Class\nFor many of you, particularly those learning to code for the first time, this class will be frustrating (sometimes very frustrating), and that’s okay. Learning to write statistical code is quite literally learning a new language, whilst also coming to grips with new way of thinking. Coding requires you to be very specific and particular and the error messages are often not very intuitive. By far the best way to learn coding is to play around with it until it works, reading will only get you so far. Some of you will be more comfortable with this approach than others, but it’s something you will all have to try your best with, even professional computer programmers solve problems this way. This class is designed to force you into this habit with weekly assignments, some of which are quite tricky.\nYou’re all highly accomplished graduate students and it might have been a long time since you took a class on something entirely new. In a lot of graduate education classes you don’t often get questions “wrong” or fail to even get an answer, in this class, that’s pretty normal. But that’s just it, it’s normal, not a sign you can’t do it, just normal.\nThis frustration will probably peak in a few weeks time after the first couple of assignments. Year after year now we’ve seen this happen, but stick with it, put in the effort, and you will get there, trust us. Often students form informal study groups to work on the assignments together, this can be really helpful, as well as a good group bonding experience. We’re here to support you as you figure it out along the way so don’t hesitate to stop by our office hours or send us an email!",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Homepage",
    "section": "Useful Links",
    "text": "Useful Links\n Email Jue\n Email Matt\n RStudio Keyboard Shortcuts\n Stack Overflow R Questions\n See Examples of Matt’s Code on Github\n See Examples of Dr. Skinner’s Code on Github",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#good-luck-in-the-class",
    "href": "index.html#good-luck-in-the-class",
    "title": "Homepage",
    "section": "Good luck in the class!",
    "text": "Good luck in the class!\n\n\n\n“Rubber duck png sticker, transparent” is marked with CC0 1.0.",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "index.html#previous-semester-archive",
    "href": "index.html#previous-semester-archive",
    "title": "Homepage",
    "section": "Previous Semester Archive",
    "text": "Previous Semester Archive\nSpring 2024",
    "crumbs": [
      "Homepage"
    ]
  },
  {
    "objectID": "x-01-git.html#installing-git",
    "href": "x-01-git.html#installing-git",
    "title": "I: Git & GitHub",
    "section": "Installing git",
    "text": "Installing git\n\nThe first thing you are going to need to do is install git\n\nFrom the git website http://git-scm.com/\n\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\n\n\nIf you’re using a Mac, there’s a decent chance git is already installed, you may already have git on Windows if you’ve used something that installed it in the past\n\nTo check if you have git copy this command into the terminal (note: not the console, the terminal which is next to console in RStudio)\n\nOnce installed, you could keep using the terminal for git, but RStudio has a much more beginner friendly point-and-click system we will use instead\n\n\n\nwhich git\nIf it provides a file path to something called git, you have git, move on!\n\nIf you need to install git\n\nThere’s a great resource “Happy git with R” by Jenny Bryan which cover a whole range of git topics beyond what we need today\n\n“Happy git with R”’s installation page has pretty clear instructions for installing git\nNote: You should always choose “Option 1” unless you\n\n\n\n\nHopefully with those instructions you managed to get git installed if it wasn’t already, which is honestly the hardest part of this lesson!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-github-account",
    "href": "x-01-git.html#creating-a-github-account",
    "title": "I: Git & GitHub",
    "section": "Creating a GitHub Account",
    "text": "Creating a GitHub Account\n\ngit is a language which handles the version control, if you just wanted to use version control and store is all locally, git is all you need\nHowever, the real advantage of git is that you keep the version control both on your computer and in a “remote” repository (similar to OneDrive etc.)\nThere are plenty of git clients that offer this service, but by far the biggest is GitHub, which is what we will use\nSo you need to create an account, which is just like signing up for any other online account\n\n\nGo to https://github.com\nClick “sign up” and follow the prompts on screen…\nDone!\n\n\nFYI: If you plan to use GitHub regularly, students are eligible for free GitHub Pro, which I have, find our more here. This will allow you to create private GitHub repos, by default, all GitHub repos are public (which is how all open source stuff like R works)",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#creating-a-new-github-repo",
    "href": "x-01-git.html#creating-a-new-github-repo",
    "title": "I: Git & GitHub",
    "section": "Creating a New GitHub Repo",
    "text": "Creating a New GitHub Repo\n\nThere are two ways you can create a new GitHub repo\n\nCreate a repo on your computer, then start tracking it with git, then link it to GitHub\nCreate a repo on GitHub and clone it to your computer\n\n\nIMHO, this is far easier, so it’ what we will do!\n\n\n\nStep One: Create Repo on GitHub\n\nWhen on https://github.com\n\nNavigate to “Your Repositories”\nClick the green “new” button\nChoose a name for the new repo\nUnder “Add .gitignore” select the R template\nEverything else is optional, so don’t worry about it for now\n\n\nNote: If you signed up for GitHub pro, you can make the repo private\n\nIf you do, make sure to add me @ttalVlatt so I can see it to give you credit\n\n\n\nYou should be taken to your shiny new repo, yay!\n\n\n\n\nStep Two: Setup GitHub SSH Access\n\nThis is how your computer will have access to edit your repo\nIt sounds scary, but luckily RStudio make it easy-peasy!\n\nIn RStudio go to “Tools” and then “Global Options”\nSelect “Git/SVN” from the left hand menu\nUnder “SSH Key” select “Create SSH Key”\nLeave the optional pass phrase boxes blank and click “create”\nClose the pop-up box that appears\nBack on the “Git/SVN” page select “View Public Key”\nCopy that to your clipboard\nGo to GitHub.com\nGo to “Settings” on the menu under your profile icon\nSelect “SSH and GPG Keys” from the left-hand menu\nSelect the green “New SSH Key”\nGive the key a name\n\n\nIf you’re using RStudio on your computer, this will be set for a while, so just call it “MacBook Pro” or something similar\nIf you’re using RStudio Cloud, you need a new key for each project, so name it accordingly\n\n\nPaste the SSH Key you copied from RStudio into the “key” box\nLeave it set as “authentication key”\nClick “Add New SSH Key” and you’re done!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "href": "x-01-git.html#cloning-a-repo-down-to-your-computer",
    "title": "I: Git & GitHub",
    "section": "Cloning a Repo Down to Your Computer",
    "text": "Cloning a Repo Down to Your Computer\n\nThis step is a little different depending on if you’re using RStudio on your computer or the cloud, so I will outline each separately\n\n\nRStudio on Your Computer\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” select “SSH”\nCopy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nClick on the blue cube in the top right (where we set up projects before)\nClick on “New Project” then “Version Control”\nPaste what you copied from GitHub as the URL\nChoose a file name and location that make sense (this is where the repo will be kept)\nDone!\n\n\n\nposit.cloud\n\nGo to https://github.com/\nGo to “Your Repositories” and select the repo you just created\nSelect the green “Code” button\nOn there, under “Clone” and keep it on “HTTPS”\nCopy the address that should look like https://github.com/&lt;Username&gt;/&lt;Repo&gt;.git\nOn posit.cloud select “New Project” and then “From GitHub Repository”\nPaste the URL you copied in the URL box and select a name for the project\nGo back to your repo on GitHub and reselect the green “Code” button\nThis time select “SSH” and copy the address that should look like git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\nOnce the project is opened, go to the terminal (next to the console)\nType git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git replacing &lt;Username&gt; and &lt;Repo&gt; with the correct names (you can copy from the block below)\nDone!\n\n\ngit remote set-url origin git@github.com:&lt;Username&gt;/&lt;Repo&gt;.git\n\n\nOkay, with git and GitHub set up, the hard part is over! Now we will just go over how to use what we set up\n\nKeep in mind, we are just going to cover one purpose git, this is just the beginning",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#using-git-for-version-control-and-backup",
    "href": "x-01-git.html#using-git-for-version-control-and-backup",
    "title": "I: Git & GitHub",
    "section": "Using git for Version Control and Backup",
    "text": "Using git for Version Control and Backup\n\nGetting a change from your computer to GitHub has three steps\n\n“Stage” the change, which tells git to pay attention to the change\n“Commit” the change, which saves it to your local (on computer) version of git\n“Push” the change, which save it to your remote (GitHub) version of git\n\n\n\nLet’s see what that looks like in RStudio\n\nIn the top right corner panel of RStudio (same area as the “Environment”) there’s a “Git” tab, select it\nYou’ll see a few things here\n\nAlong the top are some buttons for the core git commands of “Commit”, “Pull”, and “Push”\nRight now, there is probably nothing in the main area of the panel\n\nGo ahead and make a new .R script (doesn’t need to be anything in it) and save it in the project folder\n\nNow you’ll see it in the main area of the “Git” panel\n\nAny changes you make to the repo will appear here, new files, changed files, deleted files, etc.\n\n\n\nTo backup these changes to GitHub, follow these steps 1. Click the white square box left of the file in the “Git” main panel - This “stages” the change, i.e., tells git to pay attention to it 2. Click “Commit” - This will open a new box/window - In the top right hand box you can (and should) add an informative message about the change you made - E.g. “Created a test script” - Then hit the “Commit” button right underneath that 3. Finally, hit “Push” - You can do this in the same window, or at the top of the Git panel, it doesn’t matter - This “pushes” the changes you just “committed” up to GitHub - The very first time you do this, you may get a warning that the key isn’t know - Type “yes” as your response, you won’t see this again unless you make a new key\n\nYou can stage, commit, and push lots of change at once, or one by one\n\nThe big difference is that the less each individual commit and push does, the less you have to reverse\n\nFor that reason, always push up things you’re sure about first, then things your not, in separate commits\n\n\nThis process may seem like a lot, but, it will become second nature once you start using it\n\nThe ability to version control your code and easily track back to specific points is alone more than worth it\n\nThat’s not to mention this is only an intro, git can do so much more as you get familiar with it\n\nPlus, if you can use git you will stand out from the crowd in serious data management jobs\n\n\n\n\nThe Need to .gitignore\n\nWhen something appears in the RStudio “Git” panel that you don’t want to push you can right-click and hit the “ignore” option\n\nThis will add that file to a .gitignore file in your repo, and means git will never try and track that file again\nYou can also add file names and/or patterns directly to the .gitignore file\n\nThis is useful for anything you don’t want sharing (as GitHub repos are public by default), or anything too large for git (big data sets etc.)\n\n\n\nSidenote: The Need to pull\n\nAnother git command that is super common is pull, this will just check for any changes in the GitHub copy of your repo and pull them down\n\nIf you keep things simple and only push changes to this project from one computer, there should never be anything to pull down\n\nYou can always hit the button if you’re curious, it will just say “already up to date”\n\nIf you set this up on more than one computer, or start collaborating with someone else, you’ll need to commit and push when you’re finished working then pull from before you start work\n\nNow we’ve covered some of the basics, I just to suggest a few rules you stick by with git",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#git-ground-rules",
    "href": "x-01-git.html#git-ground-rules",
    "title": "I: Git & GitHub",
    "section": "git Ground Rules",
    "text": "git Ground Rules\n\nGenerally, git is best suited for plain text based files, like .R scripts and .qmd files\n\n\ngit can and will track other files, but it’s primarily meant for code, that is where version control is most powerful\nParticularly is a non-code file is large, it is best to ignore it with .gitignore which we will talk about below\n\n\npush regularly and often\n\n\nWhenever you finish something, it’s generally a good idea to push those change up to GitHub\nThis makes each version git stores more granular, so you can undo one thing without undoing a bunch of things. That will make more sense over time, but for, just push\n\n\npull at the start of each work session\n\n\nThis isn’t important if you’re using git in the simple way we are, but the second you start collaberating with git or even using on multiple computers, always pull first\nThis will add any changes that have been push-ed to your files before you edit them, avoiding conflicts and making everyone’s lives easier\n\n\nWrite useful commit messages\n\n\nEverytime you commit then push you have to write a message, if we have to go back in time, this is how you will find the point to go back to, so don’t say make them descriptive\n\n\nDon’t panic\n\n\nSometimes, git can get messed up, particularly when collaborating with others\nThe beuaty is that with version control, we can always go back and fix things\nIf you run into git issues, I am happy to help, and if I can’t I know plenty of people who can!\n\n\nNever, ever, ever, put private or restricted information in git or GitHub\n\n\nBy default GitHub repos are public, and even if they’re private, they are not approved places for private or restricted data\nEven if you’re just backing up code that uses restricted data, you should check-in with your data security/IT team to make sure you’re following institutional rules",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-01-git.html#summary",
    "href": "x-01-git.html#summary",
    "title": "I: Git & GitHub",
    "section": "Summary",
    "text": "Summary\n\nIf you’ve made it to here, congratulations, you’re now officially a git user\n\nI encourage you to keep at it, the more you use it, the easier it becomes\n\nUsing git as a version control and backup for a single computer is the simplest way to use git and more than enough for a lot of people\n\nIf you get comfortable with git, it can do so much more\n\nWorking across multiple computers\nCollaborating with other researchers\nCreating branches of work to try out new approaches\nfork-ing existing repos to make a new version of something someone else did\npull request-ing something you fork-ed and improved/fixed to get your change added to the main project\nHosting websites with gh-pages (like this one!)\nA whole lot more!",
    "crumbs": [
      "Extra Credit",
      "I: Git & GitHub"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "href": "x-03-vanilla.html#re-urgent-data-question-from-the-provost",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Re: Urgent Data Question from the Provost",
    "text": "Re: Urgent Data Question from the Provost\n\nThrough today’s lesson, we will explore some of the basics of data wrangling\n\nBut to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor\n\n\n\nUsing HSLS09 data, figure out average differences in college degree expectations across census regions; for a first pass, ignore missing values and use the higher of student and parental expectations if an observation has both.\n\n\nA primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director — even collaborator — won’t speak to you in the language of R\n\nInstead, it’s up to you to\n\ntranslate a research question into the discrete steps coding steps necessary to provide an answer, and then\ntranslate the answer such that everyone understands what you’ve found\n\n\n\nWhat we need to do is some combination of the following:\n\nRead in the data\nSelect the variables we need\nMutate a new value that’s the higher of student and parental degree expectations\nFilter out observations with missing degree expectation values\nSummarize the data within region to get average degree expectation values\nWrite out the results to a file so we have it for later\n\nLet’s do it!\nNOTE: Since we’re not using the vanilla R, we don’t need to load any packages\n\n## ---------------------------\n##' [Libraries]\n## ---------------------------\n\n## NONE",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#check-working-directory",
    "href": "x-03-vanilla.html#check-working-directory",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Check working directory",
    "text": "Check working directory\n\nBefore we get started, make sure your working directory is set to your class folder\n\n\n## Check working directory is correct\nsetwd(this.path::here())",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#read-in-data",
    "href": "x-03-vanilla.html#read-in-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Read in data",
    "text": "Read in data\n\nFor this lesson, we’ll use a subset of the High School Longitudinal Study of 2009 (HSLS09), an IES /NCES data set that features:\n\n\n\nNationally representative, longitudinal study of 23,000+ 9th graders from 944 schools in 2009, with a first follow-up in 2012 and a second follow-up in 2016\n\nStudents followed throughout secondary and postsecondary years\n\nSurveys of students, their parents, math and science teachers, school administrators, and school counselors\n\nA new student assessment in algebraic skills, reasoning, and problem solving for 9th and 11th grades\n\n10 state representative data sets\n\n\n\nIf you are interested in using HSLS09 for future projects, DO NOT rely on this subset. Be sure to download the full data set with all relevant variables and weights if that’s the case. But for our purposes in this lesson, it will work just fine.\nThroughout, we’ll need to consult the code book. An online version can be found at this link.\n\n\nQuick exercise\nFollow the code book link above in your browser and navigate to the HSLS09 code book.\n\n\n## ---------------------------\n##' [Input]\n## ---------------------------\n\n## data are CSV, so we use read.csv(), which is base R function\ndf &lt;- read.csv(file.path(\"data\", \"hsls-small.csv\"))\n\n\nUnlike the read_csv() function we’ve used before, read.csv() doesn’t print anything\n\nnotice the difference: a . instead of an _\n\nSo that we can see our data, well print to the console. BUT before we do that…\n\n-read.csv() returns a base R data.frame() rather than the special data frame or tibble() that the tidyverse uses. - It’s mostly the same, but one difference is that whereas R will only print the first 10 rows of a tibble, it will print the entire data.frame - We don’t need to see the whole thing, so we’ll use the head() function to print only the first 10 rows.\n\n## show first 10 rows\nhead(df, n = 10)\n\n   stu_id x1sex x1race x1stdob x1txmtscor x1paredu x1hhnumber x1famincome\n1   10001     1      8  199502    59.3710        5          3          10\n2   10002     2      8  199511    47.6821        3          6           3\n3   10003     2      3  199506    64.2431        7          3           6\n4   10004     2      8  199505    49.2690        4          2           5\n5   10005     1      8  199505    62.5897        4          4           9\n6   10006     2      8  199504    58.1268        3          6           5\n7   10007     2      8  199409    49.4960        2          2           4\n8   10008     1      8  199410    54.6249        7          3           7\n9   10009     1      8  199501    53.1875        2          3           4\n10  10010     2      8  199503    63.7986        3          4           4\n   x1poverty185   x1ses x1stuedexpct x1paredexpct x1region x4hscompstat\n1             0  1.5644            8            6        2            1\n2             1 -0.3699           11            6        1            1\n3             0  1.2741           10           10        4            1\n4             0  0.5498           10           10        3            1\n5             0  0.1495            6           10        3            1\n6             0  1.0639           10            8        3           -8\n7             0 -0.4300            8           11        1            1\n8             0  1.5144            8            6        1            1\n9             0 -0.3103           11           11        3            1\n10            0  0.0451            8            6        1           -8\n   x4evratndclg x4hs2psmos\n1             1          3\n2             1          3\n3             1          4\n4             0         -7\n5             0         -7\n6            -8         -8\n7             1          2\n8             1          3\n9             1          8\n10           -8         -8",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#select-variables-columns",
    "href": "x-03-vanilla.html#select-variables-columns",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Select variables (columns)",
    "text": "Select variables (columns)\n\nData frames are like special matrices\n\nThey have rows and columns\nYou can access these rows and columns using square bracket notation ([])\nBecause data frames have two dimensions, you use a comma inside the square brackets to indicate what you mean ([,]):\n\ndf[&lt;rows&gt;,&lt;cols&gt;]\n\n\nAt it’s most basic, you can use numbers to represent the index of the cell or cells you’re interested in\n\nFor example, if you want to access the value of the cell in row 1, column 4, you can use:\n\n\n\n## show value at row 1, col 4\ndf[1, 4]\n\n[1] 199502\n\n\n\nBecause data frames have column names (the variable names in our data set), we can also refer to them by name\n\nThe fourth column is the student date of birth variable, x1stdob\n\nWe can use that instead of 4 (notice the quotation marks \"\"):\n\n\n\n\n## show value at row 1, x1stdob column\ndf[1, \"x1stdob\"]\n\n[1] 199502\n\n\n\nIf we want to see more than one column, we can put the names in a concatenated vector using the c() function:\n\n\n## show values at row 1, stu_id & x1stdob column\ndf[1, c(\"stu_id\", \"x1stdob\")]\n\n  stu_id x1stdob\n1  10001  199502\n\n\n\nSo far, we’ve not assigned these results to anything, so they’ve just printed to the console.\n\nHowever, we can assign them to a new object\nIf we want to slice our data so that we only have selected columns, we can leave the rows section blank (meaning we want all rows) and include all the columns we want to keep in our new data frame object.\n\n\n\n## -----------------\n## select\n## -----------------\n\n## select columns we need and assign to new object\ndf_tmp &lt;- df[, c(\"stu_id\", \"x1stuedexpct\", \"x1paredexpct\", \"x1region\")]\n\n## show 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region\n1   10001            8            6        2\n2   10002           11            6        1\n3   10003           10           10        4\n4   10004           10           10        3\n5   10005            6           10        3\n6   10006           10            8        3\n7   10007            8           11        1\n8   10008            8            6        1\n9   10009           11           11        3\n10  10010            8            6        1",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#mutate-data-into-new-forms",
    "href": "x-03-vanilla.html#mutate-data-into-new-forms",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Mutate data into new forms",
    "text": "Mutate data into new forms\n\nChanging existing variables (columns)\n\nTo conditionally change a variable, we’ll once again use the bracket notation to target our changes\nThis time, however, we do a couple of things differently:\n\ninclude square brackets on the LHS of the assignment\nuse conditions in the &lt;rows&gt; part of the bracket\n\nAs before, we need to account for the fact that our two expectation variables, x1stuedexpct and x1paredexpct, have values that need to be converted to NA: -8, -9, and 11\n\nSee the first data wrangling lesson for the rationale behind these changes.\n\nFirst, let’s look at the unique values using the table() function\n\nThis somewhat similar to count() in tidyverse\n\nSo that we see any missing values, we’ll include an extra argument useNA = \"ifany\"\n\nThis just means we will see counts for NAs if there are any\n\n\n\n## -----------------\n##' [mutate]\n## -----------------\n\n## see unique values for student expectation\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n  -8    1    2    3    4    5    6    7    8    9   10   11 \n2059   93 2619  140 1195  115 3505  231 4278  176 4461 4631 \n\n## see unique values for parental expectation\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n  -9   -8    1    2    3    4    5    6    7    8    9   10   11 \n  32 6715   55 1293  149 1199  133 4952   76 3355   37 3782 1725 \n\n\n\nNotice that we use a dollar sign, $, to call the column name from the data frame\n\nUnlike with the tidyverse, we cannot just use the column name\n\nBase R will look for that column name not as a column in a data frame, but as its own object\nIt probably won’t find it (or worse, you’ll have another object in memory that it will find and you’ll get the wrong thing!).\n\n\nTo modify a variable when it’s a certain value, we can use the [] square brackets in a more advanced way\n\nStart by identifying the column you’d like\n\ne.g, df_tmp$x1stuedexpct -Then add the [] square brackets and inside them\n\nInside them we can add a condition to them, such as when a column is equal to -8\n\ne.g., df_tmp$x1stuedexpct == -8\n\nThink of this a bit like filter() from the tidyverse\n\n\nIf we just print this, you’ll see a load of -8, not that useful…\n\n\n\n## This will just print a bunch of -8s\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8]\n\n   [1] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [25] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [49] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [73] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n  [97] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [121] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [145] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [169] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [193] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [217] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [241] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [265] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [289] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [313] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [337] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [361] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [385] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [409] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [433] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [457] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [481] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [505] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [529] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [553] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [577] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [601] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [625] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [649] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [673] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [697] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [721] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [745] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [769] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [793] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [817] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [841] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [865] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [889] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [913] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [937] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [961] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n [985] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1009] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1033] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1057] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1081] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1105] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1129] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1153] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1177] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1201] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1225] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1249] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1273] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1297] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1321] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1345] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1369] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1393] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1417] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1441] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1465] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1489] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1513] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1537] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1561] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1585] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1609] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1633] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1657] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1681] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1705] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1729] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1753] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1777] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1801] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1825] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1849] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1873] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1897] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1921] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1945] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1969] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[1993] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2017] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n[2041] -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8 -8\n\n\n\nBut, instead of printing it, we can assign NA to it, which will replace all those -8s with NA\n\nWe can do the same for 11 to while we are at it\n\n\n\n## replace student expectation values\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == -8] &lt;- NA\ndf_tmp$x1stuedexpct[df_tmp$x1stuedexpct == 11] &lt;- NA\n\n\nIf you think back to our previous lesson, we can be a little more slick than this though\n\nIf we change the statement to %in% c(-8, -9, 11) it will do it all at once\n\n\n\n## replace parent expectation values\ndf_tmp$x1paredexpct[df_tmp$x1paredexpct %in% c(-8, -9, 11)] &lt;- NA\n\nLet’s confirm using table() again. The values that were in -8, -9, and 11 should now be summed under NA.\n\n## see unique values for student expectation (confirm changes)\ntable(df_tmp$x1stuedexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  93 2619  140 1195  115 3505  231 4278  176 4461 6690 \n\n## see unique values for parental expectation (confirm changes)\ntable(df_tmp$x1paredexpct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  55 1293  149 1199  133 4952   76 3355   37 3782 8472 \n\n\n\n\nAdding new variables (columns)\n\nAdding a new variable to our data frame is just like modifying an existing column\nThe only difference is that instead of putting an existing column name after the first $ sign, we’ll make up a new name\nThis tells R to add a new column to our data frame\nAs with the tidyverse version, we’ll use the ifelse() function to create a new variable that is the higher of student or parental expectations\n\n\n## add new column\ndf_tmp$high_expct &lt;- ifelse(df_tmp$x1stuedexpct &gt; df_tmp$x1paredexpct, # test\n                            df_tmp$x1stuedexpct,                       # if TRUE\n                            df_tmp$x1paredexpct)                       # if FALSE\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1         NA\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1         NA\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nJust like in the original lesson, it doesn’t handle NA values how we want it to\n\nLook at student 10002 in the second row:\n\nWhile the student doesn’t have an expectation (or said “I don’t know”), the parent does.\n\nHowever, our new variable records NA. Let’s fix it with this test:\n\n\n\n\n\nIf high_expct is missing and x1stuedexpct is not missing, replace with that; otherwise replace with itself (leave alone). Repeat, but for x1paredexpct. If still NA, then we can assume both student and parent expectations were missing.\n\nTranslating the bold words to R code:\n\nis missing: is.na()\nand: &\nis not missing: !is.na() (! means NOT)\n\nwe get:\n\n## correct for NA values\n\n## NB: We have to include [is.na(df_tmp$high_expct)] each time so that\n## everything lines up\n\n## step 1 student\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]), \n    ## if TRUE do this...\n    df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n## step 2 parent\ndf_tmp$high_expct[is.na(df_tmp$high_expct)] &lt;- ifelse(\n    ## test\n    !is.na(df_tmp$x1paredexpct[is.na(df_tmp$high_expct)]),\n    ## if TRUE do this...\n    df_tmp$x1paredexpct[is.na(df_tmp$high_expct)],\n    ## ... else do that\n    df_tmp$high_expct[is.na(df_tmp$high_expct)]\n)\n\n\nThat’s a lot of text!\nWhat’s happening is that we are trying to replace a vector of values with another vector of values, which need to line up and be the same length\n\nThat’s why we start with df_tmp$x1stuedexpct[is.na(df_tmp$high_expct)]\n\nWhen our high_expct column has missing values, we want to replace with non-missing x1stuedexpct values in the same row\n\nThat means we also need to subset that column to only include values in rows that have missing high_expct values\n\nBecause we must do this each time, our script gets pretty long and unwieldy.\n\n\n\n\n\nLet’s check to make sure it worked as intended.\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n9   10009           NA           NA        3         NA\n10  10010            8            6        1          8\n\n\n\nLooking at the second observation again, it looks like we’ve fixed our NA issue",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#filter-observations-rows",
    "href": "x-03-vanilla.html#filter-observations-rows",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Filter observations (rows)",
    "text": "Filter observations (rows)\n\nLet’s check the counts of our new variable:\n\n\n## -----------------\n##' [filter]\n## -----------------\n\n## get summary of our new variable\ntable(df_tmp$high_expct, useNA = \"ifany\")\n\n\n   1    2    3    4    5    6    7    8    9   10 &lt;NA&gt; \n  71 2034  163 1282  132 4334  191 5087  168 6578 3463 \n\n\n\nSince we’re can’t use the missing values we’ll drop those observations from our data frame\nJust like when we selected columns above, we’ll use the [] square brackets notation\n\nAs with dplyr’s filter(), we want to filter in what we want (i.e., when it’s not NA)\n\nSince we want to filter rows, we set this condition before the comma in the square brackets\n\nBecause we want all the columns, we leave the space after the comma blank\n\n\n\n## filter in values that aren't missing\ndf_tmp &lt;- df_tmp[!is.na(df_tmp$high_expct),]\n\n## show first 10 rows\nhead(df_tmp, n = 10)\n\n   stu_id x1stuedexpct x1paredexpct x1region high_expct\n1   10001            8            6        2          8\n2   10002           NA            6        1          6\n3   10003           10           10        4         10\n4   10004           10           10        3         10\n5   10005            6           10        3         10\n6   10006           10            8        3         10\n7   10007            8           NA        1          8\n8   10008            8            6        1          8\n10  10010            8            6        1          8\n11  10011            8            6        3          8\n\n\n\nIt looks like we’ve dropped the rows with missing values in our new variable (or, more technically, kept those without missing values)\nSince we haven’t removed rows until now, to double check, we can compare the number of rows in the original data frame, df, to what we have now\n\n\n## is the original # of rows - current # or rows == NA in count?\nnrow(df) - nrow(df_tmp)\n\n[1] 3463\n\n\n\nComparing the difference, we can see it’s the same as the number of missing values in our new column\n\nWhile not a formal test, it does support what we expected\n\nIn other words, if the number were different, we’d definitely want to go back and investigate",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#summarize-data",
    "href": "x-03-vanilla.html#summarize-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Summarize data",
    "text": "Summarize data\nNow we’re ready to get the average of expectations that we need. For an overall average, we can just use the mean() function.\n\n## -----------------\n##' [summarize]\n## -----------------\n\n## get average (without storing)\nmean(df_tmp$high_expct)\n\n[1] 7.272705\n\n\n\nOverall, we can see that students and parents have high post-secondary expectations on average: to earn some graduate credential beyond a bachelor’s degree\nHowever, this isn’t what we want. We want the values across census regions.\n\n\n## check our census regions\ntable(df_tmp$x1region, useNA = \"ifany\")\n\n\n   1    2    3    4 \n3128 5312 8177 3423 \n\n\n\nWe’re not missing any census data, which is good!\nTo calculate our average expectations, we need to use the aggregate function\nThis function allows to compute a FUNction by a group\n\nWe’ll use it to get our summary.\n\n\n\n## get average (assigning this time)\ndf_tmp &lt;- aggregate(df_tmp[\"high_expct\"],                # var of interest\n                    by = list(region = df_tmp$x1region), # by group\n                    FUN = mean)                          # function to run\n\n## show\ndf_tmp\n\n  region high_expct\n1      1   7.389066\n2      2   7.168110\n3      3   7.357833\n4      4   7.125329\n\n\n\nSuccess! Expectations are similar across the country, but not the same by region.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#write-out-updated-data",
    "href": "x-03-vanilla.html#write-out-updated-data",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Write out updated data",
    "text": "Write out updated data\n\nWe can use this new data frame as a table in its own right or to make a figure\nFor now, however, we’ll simply save it using the opposite of read.csv() — write.csv()\n\n\n## write with useful name\nwrite.csv(df_tmp, file.path(\"data\", \"high_expct_mean_region.csv\"))\n\n\nAnd with that, we’ve met our task: we can show average educational expectations by region\nTo be very precise, we can show the higher of student and parental educational expectations among those who answered the question by region\n\nThis caveat doesn’t necessarily make our analysis less useful, but rather sets its scope.\n\nFurthermore, we’ve kept our original data as is (we didn’t overwrite it) for future analyses while saving the results of this analysis for quick reference",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  },
  {
    "objectID": "x-03-vanilla.html#questions",
    "href": "x-03-vanilla.html#questions",
    "title": "III: Data Wrangling I Redux: Vanilla R",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nWhat is the average standardized math test score?\nHow does this differ by gender?\n\n\n\nAmong those students who are under 185% of the federal poverty line in the base year of the survey, what is the median household income category? (include what that category represents)\n\n\n\n\nOf the students who earned a high school credential (traditional diploma or GED), what percentage earned a GED or equivalency?\nHow does this differ by region?\n\n\n\n\n\n\n\nWhat percentage of students ever attended a post-secondary institution by February 2016?\nOptional: Give the cross tabulation for both family incomes above/below $35,000 and region\n\n\nThis means you should have percentages for 8 groups: above/below $35k within each region\n\n\nOnce complete, turn in the .R script (no data etc.) to Canvas by the due date (Sunday 11:59pm following the final lesson). Good faith efforts (as determined by the instructor) at extra credit assignments will earn full credit if submitted on time.",
    "crumbs": [
      "Extra Credit",
      "III: Data Wrangling I Redux: Vanilla R"
    ]
  }
]