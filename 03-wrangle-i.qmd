---
title: "I: Enter the tidyverse"
image: apple-touch-icon.png
---

:::{.panel-tabset}

# Lesson

[{{< fa code >}} R Code](/r-scripts/03-wranlge-i.R)

- Today we take our first full dive into R. Though every data
wrangling / analysis is unique, the best way to learn and practice R
is to answer a question using data, which is how this lesson is set up. 
- By the end of this lesson, you
will have read in a data set, lightly cleaned it, produced results,
and saved your findings in a new file. 
  - As part of this process, you will have practiced translating a research question into data analysis steps, which is a skill every bit as important as technical sophistication with a statistical language like R.
  
## Example Task

- Through today's lesson, we will explore some of the basics of data wrangling
  - But to make it more realistic, we will be doing so to answer a realistic question you may be asked by your advisor or supervisor

> Using HSLS09 data, figure out average differences in college degree expectations
> across census regions; for a first pass, ignore missing values and
> use the higher of student and parental expectations if an
> observation has both.

- A primary skill (often unremarked upon) in data analytic work is translation. Your advisor, IR director, funding agency director --- even collaborator --- won't speak to you in the language of R.
  - Instead, it's up to you to
    1. translate a research question into the discrete steps coding steps necessary to provide an answer, and then
    2. translate the answer such that everyone understands what you've found.

What we need to do is some combination of the following:

1. **Read** in the data
2. **Select** the variables we need
3. **Mutate** a new value that's the higher of student and parental
   degree expectations
4. **Filter** out observations with missing degree expectation values
5. **Summarize** the data within region to get average degree
   expectation values
6. **Write** out the results to a file so we have it for later

Let's do it!

```{r, include = FALSE, purl = TRUE}
################################################################################
##
## <PROJ> EDH7916: Data Wrangling I: Enter the {tidyverse}
## <FILE> dw_one.R 
## <INIT> 20 January 2020
## <AUTH> Benjamin Skinner (GitHub/Twitter: @btskinner)
## <MODI> Matt Capaldi on 12 Jan 2024
##
################################################################################

```

- Throughout this lesson (and class), we are going to lean heavily on the `tidyverse` collection of packages
  - If you don't already have this installed, use `install.packages("tidyverse")`

```{r}
## ---------------------------
## libraries
## ---------------------------

library(tidyverse)
```

## Check working directory

This script --- like the one from the [organizing lesson](02-set-data.qmd) --- assumes that the class folder is the working directory and that the required
data file is in the `data` sub-directory. Like this:

If you need a refresher on setting the working directory, see the
[prior lesson](02-set-data.qmd).

**Notice** that I'm not setting (_i.e._ hard coding) the working
directory in the script. That would not work well for sharing the
code. Instead, I tell you where you need to be (a common landmark),
let you get there, and then rely on relative paths afterwards.

## Example data analysis task



## **Read** in data

For this lesson, we'll use a subset of the [High School Longitudinal Study of 2009 (HSLS09)](https://nces.ed.gov/surveys/hsls09/), an IES /
NCES data set that features:

> * Nationally representative, longitudinal study of 23,000+ 9th graders
> from 944 schools in 2009, with a first follow-up in 2012 and a second
> follow-up in 2016  
> * Students followed throughout secondary and postsecondary years  
> * Surveys of students, their parents, math and science teachers,
>   school administrators, and school counselors   
> * A new student assessment in algebraic skills, reasoning, and problem solving for
>   9th and 11th grades   
> * 10 state representative data sets 

If you are interested in using HSLS09 for future projects, **DO NOT**
rely on this subset. Be sure to download the full data set with all
relevant variables and weights if that's the case. But for our
purposes in this lesson, it will work just fine.

Throughout, we'll need to consult the code book. An [online version canbe found at this link](https://nces.ed.gov/datalab/onlinecodebook). 

> #### Quick exercise 
> Follow the code book link above in your browser and navigate to the
> HSLS09 code book.

```{r, purl = TRUE, include = FALSE}
## -----------------------------------------------------------------------------
## Wrangle data
## -----------------------------------------------------------------------------
```

```{r}
## ---------------------------
## input
## ---------------------------

## data are CSV, so we use read_csv() from the readr library
df <- read_csv(file.path("data", "hsls-small.csv"))
```

You may notice the `read_csv()` prints
out information about the data just read in. Nothing is wrong! The
`read_csv()` function, like many other functions in the tidyverse,
assumes you'd rather have more rather than less information and acts
accordingly.  We can see that all the columns were read in as doubles
(`dbl(16)`), which is just a type of number that the computer
understands in a special way (a distinction that's not important for
us in this case). For other data (or if we had told `read_csv()` how
to parse the columns), we might see other column types like:

- `int()`: another type of number (again, an important
  distinction for the computer, but not usually for us)
- `chr()`: strings (_e.g._, `"Ben"` or `"1"` [notice the quotes]) 
- `lgl()`: Boolean values of `TRUE` or `FALSE`

> #### Quick exercise 
> `read_csv()` is special version of `read_delim()`,
> which can read various _delimited_ file types, that is, tabular data
> in which data cells are separated by a special character. What's the
> special character used to separate _CSV_ files? Once you figure it
> out, re-read in the data using `read_delim()`, being sure to set the
> `delim` argument to the correct character.

Up to know, this should all seem fairly consistent with last week's lesson. This is just the set up, now it's time to dive into really using R!

## Pipe Operator `|>` in R

Remember in our [organizing lesson](02-set-data.qmd) I said `|>` was something I would explain later? Well now's the time...

The pipe is one of the things that makes R code more intuitive than other programming languages, as it allows us write code in the order we think about it, passing it one from one function to another, rather than nesting it like traditional code would be written.

For this example don't worry about the actual processes (we will go over them more below), just look at how much more intuitive the code is with `|>`s.

First, let's say I want to take the data we just read in and `select` the `x1txmtscor` (math test scores) column

#### Without `|>`
```{r}
## Without |>
select(df, x1txmtscor)
```

#### With `|>`
```{r}
## With |>
df |> select(x1txmtscor)
```

But, what if we want to take that output and select only students with a math score above 50?

#### Without `|>`
```{r}
## Without |>
filter(select(df, x1txmtscor), x1txmtscor > 50)
```

#### With `|>`
```{r}
## With |>
df |> select(x1txmtscor) |> filter(x1txmtscor > 50)
```

See how the non-piped version is getting messy? Let's add one more level to really make the point, creating a new variable that is the square root of the test score

#### Without `|>`
```{r}
## Without |>
mutate(filter(select(df, x1txmtscor), x1txmtscor > 50), square_root = sqrt(x1txmtscor))
```

#### With `|>`
```{r}
## With |>
df |> select(x1txmtscor) |> filter(x1txmtscor > 50) |> mutate(square_root = sqrt(x1txmtscor))
```

- As we are getting longer, I'm going to use a new line for each pipe, just to make it even clearer (it makes no difference to R)

```{r}
df |>
  select(x1txmtscor) |>
  filter(x1txmtscor > 50) |>
  mutate(square_root = sqrt(x1txmtscor))
```

Even though we haven't covered any of these commands yet, I think (tell me if I'm wrong) the `|>` is still pretty easy to know roughly what's going on. Whereas, the traditional nested way gets really tricky beyond a couple of commands.

> Note for later: In general, the pipe works intuitively and typically passes some kind of data from step-to-step. Rarely, when working with more complicated functions that take multiple arguments, you might need to specify where the piped data is going, which you do by placing an underscore. For example, `data |> function(argument = _)`

Of course, if you wanted, you could do each step separately, constantly assigning and overwriting an object like below

```{r}
## Without the |>, we could technically break it down step by step assigning
## after each step, but again it's confusing 
temp <- select(df, x1txmtscor)
temp <- filter(temp, x1txmtscor > 50)
temp <- mutate(temp, square_root = sqrt(x1txmtscor))
temp
```

But again, I think it's less intuitive than simply piping the results.

- If we do want to see step-by-step output in a piped command, you can either
  a. Run the code as you write it line-by-line (usually what I do)
  b. Highlight sections of the piped code to run up to a point

#### Assigning Output from Pipes

- Assigning output from pipes is the same as we have covered a few times, we use a `<-` to pass it backwards to an object (in this case we called that object  `df_backward_pass`)

```{r}
df_backward_pass <- df |>
  select(x1txmtscor) |>
  filter(x1txmtscor > 50) |>
  mutate(square_root = sqrt(x1txmtscor))
```

- If you want to think of it this way, we are effectively continuing to pass it forward like this...

```{r}
df |>
  select(x1txmtscor) |>
  filter(x1txmtscor > 50) |>
  mutate(square_root = sqrt(x1txmtscor)) ->
  df_forward_pass
```

- That's how I first thought about how pipes and assignment work together, and you
can see the outputs are `identical()`

```{r}
identical(df_backward_pass, df_forward_pass)
```

- However, you *really* shouldn't write code like this, as although it runs, it's
then hard to later find where you created `df_forward_pass`.
- Starting the string of code with where you store the result is **MUCH** clearer.
  - But, it really helped me to think about it this way at first, so hopefully it will help some of you understand how `|>` and `<-` work together

#### A Brief History of the R Pipe

<span style="display:block;text-align:center">
[![badge](https://raw.githubusercontent.com/tidyverse/magrittr/main/man/figures/logo.png)](https://www.fine-arts-museum.be/uploads/exhibitions/images/magritte_la_trahison_des_images_large@2x.jpg)
</span>

- The pipe was originally a `tidyverse` invention, and used `%>%` symbol, which is probably still the more common pipe you see in the wild
- The pipe we are using was brought into the base (vanilla) version of R a few years ago as `|>` 
- The reason for the change is some benefits that are beyond the scope of this class, but I wanted you to be aware that <ins>`|>` and `%>%` are essentially the same thing</ins>
  - You can intermingle them, but, the new pipe is one less character to type, doesn't need you to have loaded `tidyverse` loaded, and I think it's more clear, so I suggest you stick with `|>`

## Basic Tidyverse Commands

- Now we have the pipe `|>` covered, it's time to dig into some basic data wrangling commands

### `select()` variables (columns)

- Often data sets contain hundreds, thousands, or even tens of thousands of variables, when we are only interested in a handful. Our first `tidyverse` command `select()` helps us deal with this, by, as you may have guessed `select()`-ing the variables we want
- Since we are going to pipe our R commands, we start with our data then pipe it into the `select()` command
- In the `select()` command, we list out the variables we want
  - `stu_id, x1stuedexpct, x1paredexpct, x1region`
    - Notice, in this (and most tidyverse) command(s) we don't have to use "quotes" around the variable names
      - Hint: a common error message when you needed to put something in quotes is `Error: object xxx not found`

```{r, include = FALSE, purl = TRUE}
df |> select(stu_id, x1stuedexpct, x1paredexpct, x1region)
```

> Quick question, if we want to use this reduced data-frame going forward, what should we do?

That's right, assign it to an object! Let's call that `df_small`

```{r, include = FALSE, purl = TRUE}
df_small <- df |> select(stu_id, x1stuedexpct, x1paredexpct, x1region)
```

## `mutate()` data into new forms

- When we want to add variables and change existing ones, we can use the `mutate()` function
- In this case, we want to create a new variable `high_expect` that is the larger of `x1stuedexpct` and `x1paredexpct`

`mutate()` enables us to make a new variable or modify an existing variable, just like in the piping demonstration above when I made a new variable that was the square root of the math test score
  1. I took our `df` and piped it into `mutate()`
  2. Inside mutate I created a new variable called `square_root` 
    - note: this uses `=` not `<-` to assign, confusing I know...
  3. I assigned to new variable `square_root` the `sqrt()` (an R function for square root) of `x1txmtscor` (math score)

```{r}
df |> mutate(square_root = sqrt(x1txmtscor))
```

So, let's take that same logic and apply it to our missing values!

But wait, we have a problem...

### Understanding our data

First things first, however, we need to check the [code book](https://nces.ed.gov/datalab/onlinecodebook) to see what the numerical values for our two education expectation variables represent. To save time, I've copied them here:

#### `x1stuedexpct`

_How far in school 9th grader thinks he/she will get_

|value|label|
|:---:|:----|
|1 |Less than high school|
|2 |High school diploma or GED|
|3 |Start an Associate's degree|
|4 |Complete an Associate's degree|
|5 |Start a Bachelor's degree|
|6 |Complete a Bachelor's degree|
|7 |Start a Master's degree|
|8 |Complete a Master's degree|
|9 |Start Ph.D/M.D/Law/other prof degree|
|10|Complete Ph.D/M.D/Law/other prof degree|
|11|Don't know|
|-8|Unit non-response|

#### `x1paredexpct`

_How far in school parent thinks 9th grader will go_

|value|label|
|:---:|:----|
|1 |Less than high school|
|2 |High school diploma or GED|
|3 |Start an Associate's degree|
|4 |Complete an Associate's degree|
|5 |Start a Bachelor's degree|
|6 |Complete a Bachelor's degree|
|7 |Start a Master's degree|
|8 |Complete a Master's degree|
|9 |Start Ph.D/M.D/Law/other prof degree|
|10|Complete Ph.D/M.D/Law/other prof degree|
|11|Don't know|
|-8|Unit non-response|
|-9|Missing|

- The good news is that the categorical values are the same for both variables (meaning we can make an easy comparison) and move in a logical progression. 
- The bad news is that we have three values --- `-8`, `-9`, and `11` --- that we need to deal with so that the averages we compute later represent what we mean.

## `count()` how many students in each category

- Let's see how many observations are affected by these values using `count()` 
> notice that we don't assign to a new object; this means we'll see the result in the console, but nothing in our data or object will change

```{r}
## -----------------
## mutate
## -----------------

## see unique values for student expectation
df_small |> count(x1stuedexpct)

## see unique values for parental expectation
df_small |> count(x1paredexpct)
```

- Dealing with `-8` and `-9` is straight forward --- we'll convert it missing.
  - In R, missing values are technically stored as `NA`.
    - Not all statistical software uses the same values to represent missing values (for example, Stata uses a dot `.`). Likely because they want to be software agnostic,
  - NCES has decided to represent missing values as a limited number of negative values. In this case, `-8` and `-9`
represent missing values.

- Note: how to handle missing values is a **very** important topic, one we could spend all semester discussing
  - For now, we are just going to drop observations with missing values; but be forewarned that how you handle missing values can have real ramifications for the quality of your final results.
  - In real research, a better approach is usually to impute missing values, but that is beyond our scope right now

- Deciding what to do with `11` is a little trickier. While it's not a missing value _per se_, it also doesn't make much sense in its current ordering, that is, to be "higher" than completing a professional degree
  - We'll make a decision to convert these to `NA` as well, effectively deciding that an answer of "I don't know" is the same as missing an answer.

## Changing existing variables (columns)

- So first step: convert `-8`, `-9`, and `11` in both variables to `NA`. We can do this by overwriting cells in each variable with `NA` when they equal one of these two values. For this, we'll use the `ifelse()` function

## `ifelse()` conditional values

`ifelse()` is a really common command in `R` and has three parts
  1. `statement` that can be `TRUE` or `FALSE`
  2. What to return `if` the `statement` is `TRUE`
  3. `else` what to return when the `statement` is `FALSE`

Now, we have three values we want to covert to NA, so we *could* do them one-at-a-time, like below

- Let's walk through this code
1. Assign the results back to `df_small` (which will overwrite our previous `df_small`)
2. Take `df_small` and pipe `|>` it into `mutate()`
  - we use `df_small` over `df` to keep our previous steps
3. Inside `mutate()` create a new variable called `student_exp`
- Notice, I'm creating a new column `student_exp` rather than modifying the original column. We could modify the original column by just putting `x1stuedexpct` as the first argument, but, this is clearer for demonstration purposes
4. Make our new variable `student_exp` with an `ifelse()` statement, which remember has 3 parts
  i. `statement` which is asking "is `x1stuedexpct == -8`"?
    - Notice `==` means "is equal to", while `=` means "assign to". Yes it's confusing, but you'll get it over time!
  ii. `if` that `statement` is true, make it `NA`
  iii. `else` (if the statement is false) return `x1stuedexpct`

```{r}
df_small <- df_small |>
  mutate(student_exp = ifelse(x1stuedexpct == -8, NA, x1stuedexpct))
```

Okay, make sense? Let's see what we just did (look at row 26)

```{r}
print(df_small, n = 26)
```

This is fine, but we have to do it 3 times for both parent and student expectation

- Instead, can anyone think (not in `R` code, just in terms of logic) how we could change our `statement` piece of the `ifelse()` to be more efficient?

## `%in%` and `list()`

- What we can do, is group -8, -9, and 11 together into a list using `list()`
  - Then, we can use the `%in%` operator to ask if that result is any of the numbers in that list
    - This keeps our code shorter and easier to read

```{r}
df_small <- df_small |>
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct))
```

- The code now works just as above, but instead of asking if `x1stuedexpct` is equal to -8, it asks if it's in the list of -8, -9, and 11, then does the same for parental expectations!

Let's view those first 26 rows again to see what we did

```{r}
print(df_small, n = 26)
```

And just to be doubly-sure, lets check count again

```{r}
df_small |> count(student_exp) 
df_small |> count(parent_exp)
```

Success!

So, let's get back to our original task, creating a new variable that is the highest of parental and student expectations!

## Back on track!

To make a new variable which is the highest of two variables, we can use our friend `ifelse()` some more

```{r}
df_small <- df_small |>
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp))
```

- That code is *almost* what we want to do
  - If `student_exp` is higher then take that, if not, take `parent_exp`
    - There's two things I haven't fully accounted for though
      - One doesn't actually matter here, but might in other circumstances
      - One definitely matters here
        - Without scrolling past the duck, can you tell me what they might be?

!["[Rubber duck png sticker, transparent](https://www.rawpixel.com/image/6772898/png-sticker-public-domain)" is marked with [CC0 1.0.](https://creativecommons.org/publicdomain/zero/1.0/?ref=openverse)](site-attachments/Rubber-Duck.png){width=200px fig-align="left"}

The one that doesn't actually matter here
- I was a little sloppy with the `statement` piece, I just asked if `student_exp` was greater than `parent_exp` or not
  - If I was being more careful, I might have said "greater than or equal to"
    - Why doesn't this matter in this context, and when might it matter?
    
Now let's check our data frame to see the one that does matter

```{r}
print(df_small, n = 26)
```

- Hmm, that seems odd, why would R consider `NA` to be greater than `6`?
  - Any thoughts?
  
- Generally, R is overly-cautious when dealing with `NA`s to ensure you don't accidentally drop them without realizing it
  - For example, if I asked you what the `mean(c(5, 6, 4, NA))` would be, you'd probably say 5, right?
    - R is never going to just ignore the NA values like that unless we tell it to

```{r}
mean(c(5, 6, 4, NA))
```

See, what have to explictly tell it to remove the `NA` values

```{r}
mean(c(5, 6, 4, NA), na.rm = T)
```

- So in our case of trying to get the highest expectation, R doesn't want us to forget we have `NA` values, so it throws them at us.
- For now, let's keep the results we got, but if it gave us an `NA` and there is a non-`NA` value in other other column, sub that in

## `is.na()` and `!`

To do this, we will add another couple of code helpers
  -`is.na(high_exp)` simply asks if the `high_exp` is `NA` or not
    - R doesn't let you just say `high_exp == NA`
  -`!` is really helpful tool, which can be used to negate or invert a command
      - `!is.na(student_exp)` just returns the opposite of `is.na(student_exp)` so it tells us that the column is not `NA` 
      
Used together, we can now ensure we have as few `NA`s as possible in the data (there will still be some when both student and parent were `NA`)
      
```{r}
df_small <- df_small |>
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp))
```

Let's print this one last time to check our work

```{r}
print(df_small, n = 26)
```

Okay, looks good!

- There are other ways we could have gone about this analysis, some more sophisticated and slicker
  - But, what matters more is that we checked our work as we went, caught the issues, and finished with the correct data
    - We will learn more tools along the way that speed things up, especially in the brand new [Data Wrangling IV Lesson](09-wrangle-iv.qmd)
    - However, the more steps you take at once, the more you have to check at once!
    - There's often never a single correct way to get to the right answer, so long as you get there, it's clear what you did, and you catch issues as they come up

> "The point to keep in mind that the process is often
iterative (two steps forward, one step back...) and that there's
seldom an single _correct way_." B.T. Skinner

- Now it's made correctly, let's check the counts of our new variable

```{r}
## -----------------
## filter
## -----------------

## get summary of our new variable
df_small |> count(high_exp)
```

- Hmm... We still have a large number of `NA`s, meaning neither the parent or student provided an expectation
    - In more sophisticated analyses, this is where we might need to think about imputation or something else to handle the missing-ness
      - For the sake of this lesson, however, we just want to drop the observations with `NA high_exp`

## **Filter** observations (rows)

- To do this, we are going to use the `filter()` command from `tidyverse`
- `filter()` works by only keeping observations that *meet* the condition(s) we set
  - As in, to make it through the filter, a row must answer "yes" to "does it meet this condition?"

```{r}
## filter out missing values
df_small_cut <- df_small |> filter(!is.na(high_exp))
```

- Above we re-used our `!is.na()` helper to keep all rows that answer "yes" to "are you not `NA`?" 
  - Double negatives are something you'll have to get used to in coding, sorry
  - Notice, instead of overwriting `df_small` we assigned this to a new object `df_small_cut`
    - Generally, when making substantial changes to a data set like dropping observations, I like to be able to double check what I did, which is easier if we make a new `df`

> Quick Question: A commmon confusion from this lesson is between `filter()` and `select()`. Can someone explain when you'd use `select()` over `filter()`?

- To see if that worked, let's `count()` our new variable one more time

```{r}
df_small_cut |> count(high_exp)
```

- Okay, so `NA`s, perfect!
- Just to be extra sure we only removed `NA`s, we can check the difference in how many rows our original `df_small` has with `df_small_cut`
  - Who can tell me how many rows *should* have been dropped?

```{r}
## does the original # of rows - current # or rows == NA in count?
nrow(df_small) - nrow(df_small_cut)
```

- `nrow()` does what you'd expect, counts the number of rows
  - You could also just check by looking at the `df`s in the environment tab, but this way leaves no room for mental math errors

## **summarize()** data

- Okay, so we have our data, we made our `high_exp` variable, and we've done some work to make sure there aren't any missing values we haven't accounted for
- Now, we talked in our week one data talk about how massive tables of data are not particularly helpful to for someone to read or interpret
  - We will cover how to make graphs of our data in a few weeks
  - For our final task today, we are going to make some summary tables using `summarize()` from the `tidyverse`
  
  
- `summarize()` allows us to apply a summary statistic (mean, sd, median, etc.) to a column in our data
- `summarize()` takes an entire data frame as an input, and spits out a small data frame with the just the summary variables
  - Note: for this reason, you rarely ever want to assign `<-` the output of `summarize()` back to the main data frame object, as you'll overwrite it
    - You can either spit the summary tables out into the console without assignment (which we will do) or if you need to use them for something else, assign them to a new object

```{r}
## -----------------
## summarize
## -----------------

## get average (without storing)
df_small_cut |> summarize(mean(high_exp))
```

- See, the output is a 1x1 table with the mean expectation `mean(high_exp)` of 7.27, just above a bachelors degree
  - Note: if we want to name the summary variable, we can name it just like we did earlier in `mutate()` with a single `=`
  
```{r}
df_small_cut |> summarize(mean_exp = mean(high_exp))
```

- But, that wasn't quite the question we were asked
  - We were asked if it varied by region...
    - For time's sake, I can tell you the region variable is `x1region` and splits the US in 4 Census regions

## `group_by()` 

- The `group_by()` function, following the `tidyverse` principle of intuitive naming, *groups* the data and outputs *by* the variable(s) you say
  - So, since we want to calculate the average high expectation by region, we `group_by(x1region)`
    - Since we just want it for our `summarize()`, we just add it to the pipe
      - If you wanted to save the data in it's `group_by()`-ed state, you could assign it to something

```{r}
## get grouped average
df_small_cut |>
  group_by(x1region) |>
  summarize(mean_exp = mean(high_exp))
```

- Success! While expectations are similar across the country, there's some variance by region
  - While there are few things we could do to make this a little fancier (e.g., changing the region numbers to names, formatting the table, etc.) we have answered our question, and have clear documentation of how we got here, so I will call that a win!

## Using **write_csv()** to Export Data

- Sometimes we want to be able to access objects from scripts without having to re-run the whole thing
  - Remember: one of the main advantages of R is the data we read in is untouched
- To do this, we want to `write_` a new `csv()` file, containing our modified data
  - unlike `read_csv()` which only needed a file name/path, `write_csv()` needs to know what you're trying to save and the file name/path you want to save it to
    - The only way you can overwrite or change the original data is by saving to the same file name as the original data, so **NEVER** do that!
- Since we didn't assign our summary table to anything, we can just add `write_csv()` to the end of the pipe and add a `file.path()`
  - If you want to save a data frame you already assigned to an object `write_csv(<object>, file.path(<path>))` would work just fine!

```{r, include = FALSE, purl = TRUE}
## ---------------------------
## output
## ---------------------------

## write with useful name

df_small_cut |>
  group_by(x1region) |>
  summarize(mean_exp = mean(high_exp)) |>
  write_csv(file.path("data", "region-expects.csv"))

```

Phew!

## Appendix: All at Once

We went through that piece by piece to demonstrate each function, but, there's no reason we can't just `|>` pipe it all together

```{r, include = FALSE, purl = TRUE}
## ---------------------------
## appendix
## ---------------------------

## Let's redo the analysis above, but with a fully chained set of
## functions.

## start with original df
df |>
  ## select columns we want
  select(stu_id, x1stuedexpct, x1paredexpct, x1region) |>
  ## If expectation is -8, -9. or 11, make it NA
  mutate(student_exp = ifelse(x1stuedexpct %in% list(-8, -9, 11), NA, x1stuedexpct),
         parent_exp = ifelse(x1paredexpct %in% list(-8, -9, 11), NA, x1paredexpct)) |>
  ## Make a new variable called high_exp that is the higher or parent and student exp
  mutate(high_exp = ifelse(student_exp > parent_exp, student_exp, parent_exp)) |>
  ## If one exp is NA but the other isn't, keep the value not the NA
  mutate(high_exp = ifelse(is.na(high_exp) & !is.na(student_exp), student_exp, high_exp),
         high_exp = ifelse(is.na(high_exp) & !is.na(parent_exp), parent_exp, high_exp)) |>
  ## Drop is high_exp is still NA (neither parent or student answereed)
  filter(!is.na(high_exp)) |>
  ## Group the results by region
  group_by(x1region) |>
  ## Get the mean of high_exp (by region)
  summarize(mean_exp = mean(high_exp)) |>
  ## Write that to a .csv file
  write_csv(file.path("data", "region-expects-chain.csv"))
  
```

To double check, let's just check these are the same
  
```{r}

non_chain <- read_csv(file.path("data", "region-expects.csv"))
chain <- read_csv(file.path("data", "region-expects-chain.csv"))
 
all.equal(non_chain, chain)
      
```

Yep!

## Final notes

- This rather lengthy lesson has thrown you in the (medium) deep end of the coding pool
  - By no means are you expected to get everything we just did
    - We will continue to revisit all these commands throughout the class, by the end of the semester, they will be second nature!
- We also saw how to use code to answer a realistic question we might be asked in a data management job, a translation skill that will prove invaluable later on!
  - We had to plan out steps and then make some adjustments along the way (e.g., our `NA` issues), that's all part of the process!

> "Becoming a better quantitative researcher mostly means becoming a better translator: question --> data/coding --> answer." B.T. Skinner


```{r, include = FALSE, purl = TRUE}
## -----------------------------------------------------------------------------
## end script
## -----------------------------------------------------------------------------
```

# Assignment

Using the `hsls_small.csv` data set and the online code book, answer
the following questions. You **do not** need to save the final output
as a data file: just having the final result print to the console is
fine. For each question, I would like you to try to pipe all the
commands together. You can account for missing values by dropping
them.

For each question, show your data work and then answer the question in
a short (1-2 sentence(s)) comment. 

## Questions

1) What is the average standardized math test score?
2) What is the average standardized math test score by gender?
3) In what year and month were the oldest students in the data set
   born? The youngest?
4) Among those students who are under 185% of the federal poverty line
   in the base year of the survey, what is the median household income
   (give the category and what that category reprents).
5) Of the students who earned a high school credential (diploma or
   GED), what percentage earned a GED or equivalency? How does this
   differ by region?
6) What percentage of students ever attended a postsecondary
   institution by February 2016? Give the cross tabulations for:  
     - family incomes less than or equal to $35,000 and greater than
       $35,000   
	 - region  
	 
   This means you should have percentages for 8 groups: above/below
   $35k within each region.   
   
   **HINT** You can `group_by()` on more than one group.
  
#### Submission details

- Save your script (`<lastname>_assignment_3.R`) in your `scripts`
  directory.
- Push changes to your repo (the new script and new folder) to GitHub
  prior to the next class session.


:::
