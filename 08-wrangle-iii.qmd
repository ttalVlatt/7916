---
title: "III: Working with strings & dates"
image: apple-touch-icon.png
solution: false
---

::: panel-tabset
# Lesson

[{{< fa code >}} R Code](/r-scripts/08-wrangle-iii.R)

```{r, include = FALSE, purl = TRUE}
## -----------------------------------------------------------------------------
##
##' [PROJ: EDH 7916]
##' [FILE: Data Wrangling III: Strings and Dates]
##' [INIT: 18 March 2024]
##' [AUTH: Benjamin Skinner @btskinner]
##' [EDIT: Matt Capaldi @ttalVlatt]
##' [EDIT: Jue Wu]
##' [UPDT: 12 February 2025]
##
## -----------------------------------------------------------------------------
```

The data we've used so far in this course have been almost entirely numerical.
Even when the field represented an expected level of education, for example, we didn't see "complete a Bachelor's degree." Instead, we saw the number `6` --- sometimes with a label and sometimes without (meaning we had to look up what `6` stood for).
In the few cases in which we've seen strings or dates, the values have been very regular.

Much education-related data, however, are not this uniform.
Particularly when using administrative data files, you are likely to read in columns that contain *unstructured strings*: names, addresses, dates, *etc*.
Why are they unstructured?
Almost always the answer is that person who initially keyed in the data neither had a dropdown menu of options to choose from nor separate fields for each part of the data element (*e.g.*, `first name`, `last name`).
Instead, they have a blank field in which they type:

**Enter name**: `Stephen Curry`

Why is this a problem?
With an open field, the variations are (often) unlimited:

-   `S. Curry`
-   `Steph Curry`
-   `stephen curry`
-   `Curry, Stephen`

Similarly, the same date can be written any number of ways:

**Enter date**: `February 11, 2020`

-   `11 February 2020`
-   `11 Feb 2020`
-   `Feb. 11, 2020`
-   `2/11/2020` (American)
-   `11/2/2020` ([most everyone else](https://en.wikipedia.org/wiki/Date_and_time_notation_in_the_United_States))
-   `2/11/20`

To be clear, this is not to impugn those who enter the data.
Rather, it's an acknowledgment that the original uses of the data we analyze may differ from our own, *e.g.*, data input to meet compliance with an administrative task versus data input for statistical analysis.

Now, imagine gaining access to an administrative data set with these two irregular columns, `name` and `date`, and thousands or even millions of rows.
To complete your analytic task, you must clean the data such that for each observation, you keep only last names and need to convert the date into a format that will allow you to easily calculate time periods between dates.
With so many observations, it's an impossible task to do by hand.
But because the data are irregular, you can't, for example, just select the second word from the name (what if the last name is first in some rows?) or the second number after a forward slash, `/` (what about when the date uses hyphens, month name, or a different order?).

You won't always need to work with strings and dates, but when you do, having a few specialty tools in your toolbox will be greatly beneficial.
Sometimes they can mean the difference between being able to answer your question and not.
In this lesson, we'll discuss two: regular expressions and dates.

## Setup

As before, we'll continue working within the tidyverse.
We'll focus, however, on using two specific libraries:

-   [stringr for strings](https://stringr.tidyverse.org)
-   [lubridate for dates](https://lubridate.tidyverse.org)

You may have noticed already that when we load the tidyverse library with `library(tidyverse)`, the stringr library and lubridate library are already loaded.
However, if you have an older version of tidyverse, you may need to load lubridate separately.

```{r}
## ---------------------------
##' [Libraries]
## ---------------------------

library(tidyverse)

```

**NB**: As we have done in the past few lessons, we'll run this script assuming that our working directory is set to the `scripts` directory.

## Part 1: Working with strings

To practice working with strings, we'll use data from [Integrated Postsecondary Education Data System (IPEDS)](https://nces.ed.gov/ipeds/):

> The National Center for Education Statistics (NCES) administers the Integrated Postsecondary Education Data System (IPEDS), which is a large-scale survey that collects institution-level data from postsecondary institutions in the United States (50 states and the District of Columbia) and other U.S. jurisdictions.
> IPEDS defines a postsecondary institution as an organization that is open to the public and has the provision of postsecondary education or training beyond the high school level as one of its primary missions.
> This definition includes institutions that offer academic, vocational and continuing professional education programs and excludes institutions that offer only avocational (leisure) and adult basic education programs.
> Definitions for other terms used in this report may be found in the IPEDS online glossary.
>
> NCES annually releases national-level statistics on postsecondary institutions based on the IPEDS data.
> National statistics include tuition and fees, number and types of degrees and certificates conferred, number of students applying and enrolled, number of employees, financial statistics, graduation rates, student outcomes, student financial aid, and academic libraries.

[You can find more information about IPEDS here](https://nces.ed.gov/ipeds/ReportYourData/IpedsSurveyMethodology).
As higher education scholars, IPEDS data are a valuable resource that you may often turn to.

We'll use one file ([which can be found here](https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx)), that covers institutional characteristics for one year:

-   Directory information, 2007 (`hd2007.csv`)

```{r}
## ---------------------------
##' [Input]
## ---------------------------

## read in data and lower all names using rename_all(tolower)
data <- read_csv("data/hd2007.csv") |>
    rename_all(tolower)
```

```{r, purl = TRUE, include = FALSE}
## -----------------------------------------------------------------------------
##' [Working with strings]
## -----------------------------------------------------------------------------
```

## Finding: `str_detect()`

Previously, we've filtered data using dplyr's `filter()` function.
When matching a string, we have used `==` (or `!=` for negative match).
For example, if we wanted to limit our data to only those institutions in Florida, we could filter using the `stabbr` column:

```{r}
## filter using state abbreviation (not saving, just viewing)
data |>
    filter(stabbr == "FL")
```

This works well because the `stabbr` column, even though it uses strings, is regular.
But what happens when the strings aren't so regular?
For example, let's look the different titles chief college administrators take.

```{r}
## see first few rows of distinct chief titles
data |>
    distinct(chftitle)
```

We find over 500 unique titles.
Just looking at the first 10 rows, we see that some titles are pretty similar --- *President* vs. *CEO* vs. *President/CEO* --- but not exactly the same.
Let's look again, but this time get counts of each distinct title and arrange from most common to least.

```{r}
## return the most common titles
data |>
    ## get counts of each type
    count(chftitle) |>
    ## arrange in descending order so we see most popular at top
    arrange(desc(n))
```

> #### Quick exercise
>
> What do you notice about the data frames returned by `distinct()` and `count()`?
> What's the same?
> What does `count()` do that `distinct()` does not?

Getting our counts and arranging, we can see that *President* is by far the most common title.
That said, we also see *Campus President* and *Interim President* (and before we saw *Acting President* as well).

If your research question asked, *how many chief administrators use the title of "President"?* regardless the various iterations, you can't really use a simple `==` filter any more.
In theory, you could inspect your data, find the unique versions, get counts of each of those using `==`, and then sum them up --- but that's a lot of work and likely to be error prone!

Instead, we can use the stringr function `str_detect()`, which looks for a *pattern* in a vector of strings:

``` r
str_detect(< vector of strings >, < pattern >)
```

Going item by item in the vector, it compares what it sees to the pattern.
If it matches, then it returns `TRUE`; it not, then `FALSE`.
Here's a toy example:

```{r}
## string vector example
fruits <- c("green apple", "banana", "red apple")

## search for "apple", which should be true for the first and third item
str_detect(fruits, "apple")
```

We can use `str_detect()` inside `filter()` to select only certain rows in our data frame.
In our case, we want only those observations in which the title `"President"` occurs in the `chftitle` column.
Because we're only detecting, as long as `"President"` occurs *anywhere* in the title, we'll get that row back.

```{r}
## how many use some form of the title president?
data |>
    ## still starting with our count
    count(chftitle) |>
    ## ...but keeping only those titles that contain "President"
    filter(str_detect(chftitle, "President")) |>
    ## arranging as before
    arrange(desc(n))
```

Now we're seeing many more versions.
We can even more clearly see a few titles that are almost certainly the same title, but were just inputted differently --- *President/CEO* vs. *President and CEO* vs. *President & CEO*.

> #### Quick exercise
>
> Ignoring the sub-counts of the various versions, how many chief administrators have the word "President" in their title?

Seeing the different versions of basically the same title should have us stopping to think: since it seems that this data column contains free form input (*e.g.* **Input chief administrator title:**), maybe we should allow for typos?
The easiest: Is there any reason to assume that "President" will be capitalized?

> #### Quick exercise
>
> What happens if we search for "president" with a lowercase "p"?

Ah!
We find a few stragglers.
How can we restructure our filter so that we get these, too?
There are at least two solutions.

#### 1. Use regular expressions

[Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) (aka *regex*) are strings that use a special syntax to create patterns that can be used to match other strings. Some of you may have used them when you try to search for literature in your work. 
They are very useful when you need to match strings that have some general form, but may differ in specifics.

We already used this technique in the a prior lesson when we matched columns in the `all_schools_wide.csv` with `contains("19")` so that we could `pivot_longer()`.
Instead of naming all the columns specifically, we recognized that each column took the form of `<test>_19<YY>`.
This is a type of regular expression.

In the tidyverse some of the stringr and [tidyselect helper functions](https://www.rdocumentation.org/packages/tidyselect/versions/1.0.0/topics/select_helpers) abstract-away some of the nitty-gritty behind regular expressions.
Knowing a little about regular expression syntax, particularly [how it is used in R](https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html), can go a long way.

In our first case, we can match strings that have a capital **P** *President* or lowercase **p** *president* using square brackets (`[]`).
If we want either "P" or "p", then we can use the regex, `[Pp]`, in place of the first character: `"[Pp]resident"`.
This will match either `"President"` or `"president"`.

```{r}
## solution 1: look for either P or p
data |>
    count(chftitle) |>
    filter(str_detect(chftitle, "[Pp]resident")) |>
    arrange(desc(n))

```

Though we don't see the new observations in the abbreviated output, we note that the number of rows has increased by two.
This means that there are at least two title formats in which `"president"` is lowercase and that we weren't picking up when we only used the uppercase version of `"President"` before.

#### 2. Put everything in the same case and match with that case

Another solution, which is probably much easier in this particular case, is to set all potential values in `chftitle` to the same case and then match using that case.
In many situations, this is preferable since you don't need to guess cases up front.

We won't change the values in `chftitle` permanently --- only while filtering.
To compare apples to apples (rather than `"Apples"` to `"apples"`), we'll wrap our column name with the function `str_to_lower()`, which will make character lowercase, and match using lowercase `"president"`.

```{r}
## solution 2: make everything lowercase so that case doesn't matter
data |>
    count(chftitle) |>
    filter(str_detect(str_to_lower(chftitle), "president")) |>
    arrange(desc(n))

```

We recover another two titles when using this second solution.
Clearly, our first solution didn't account for other cases (perhaps "`PRESIDENT"`?).

In general, it may be a good idea to try a solution like the second one before a more complicated one like the first.
But because every problem is different, so too are the solutions.
You may find yourself using a combination of the two.

> #### Not-so-quick exercise
>
> Another chief title that was high on the list was "Owner." How many institutions have an "Owner" as their chief administrator?
> Of these, how many are private, for-profit institutions (`control == 3`)?
> How many have the word "Beauty" in their name?

### Replace using string position: `str_sub()`

In addition to filtering data, we sometimes need to create new variables from pieces of exiting variables.
For example, let's look at the zip code values that are included in the file.

```{r}
## show first few zip code values
data |>
    select(unitid, zip)
```

We can see that we have both regular 5 digit zip codes as well as those that include the extra 4 digits (ZIP+4).
Let's say we don't need those last four digits for our analysis (particularly because not every school uses them anyway).
Our task is to create a new column that pulls out only the main part of the zip code.
It has to work both for `zip` values that include the additional hyphen and 4 digits as well as those that only have the primary 5 digits to begin with.

One solution in this case is to take advantage of the fact that zip codes --- minus the sometimes extra 4 digits --- should be regular: 5 digits.
If want the sub-part of a string and that sub-part is always in the same spot, we can use the function, `str_sub()`, which takes a string or column name first, and has arguments for the starting and ending character that mark the sub-string of interest.

In our case, we want the first 5 digits so we should `start == 1` and `end == 5`:

```{r}
## pull out first 5 digits of zip code
data <- data |>
    mutate(zip5 = str_sub(zip, start = 1, end = 5))

## show (use select() to subset so we can see new columns)
data |>
    select(unitid, zip, zip5)
```

A quick visual inspection of the first few rows shows that our `str_sub()` function performed as expected (for a real analysis, you'll want to do more formal checks).

### Replace using regular expressions: `str_replace()`

We can also use a more sophisticated regex pattern with the function `str_replace()`.
The pieces of our regex pattern, `"([0-9]+)(-[0-9]+)?"`, are translated as this:

-   `[0-9]` := any digit, 0 1 2 3 4 5 6 7 8 9
-   `+` := match the preceding one or more times
-   `?` := match the preceding 0 or more times
-   `()` := subexpression

Put together, we have:

-   `([0-9]+)` := first, look for 1 or more digits
-   `(-[0-9]+)?` := second, look for a hyphen and one or more digits, but you may not find any of that

Because we used parentheses, `()`, to separate our subexpressions, we can call them using their numbers (in order) in the last argument of `str_replace()`:

-   `"\\1"` := return the first subexpression

So what's happening?
If given a zip code that is `"32605"`, the regex pattern will collect each digit --- `"3"` `"2"` `"6"` `"0"` `"5"` --- into the first subexpression because it never sees a hyphen.
That first subexpression, `"\\1"`, is returned: `"32605"`.
That's what we want.

If given `"32605-1234"`, it will collect the first 5 digits in the first subexpression, but will stop adding characters there when it sees the hyphen.
From then on, it adds everything it sees the second subexpression: `"-"` `"1"` `"2"` `"3"` `"4"`.
But because `str_replace()` only returns the first subexpression, we still get the same answer: `"32605"`.
This is what we want.

Let's try it on the data.

```{r}
## drop last four digits of extended zip code if they exist
data <- data |>
    mutate(zip5_v2 = str_replace(zip, "([0-9]+)(-[0-9]+)?", "\\1"))

## show (use select() to subset so we can see new columns)
data |>
    select(unitid, zip, zip5, zip5_v2)
```

> #### Quick exercise
>
> What if you wanted to the get the last 4 digits (after the hyphen)?
> What two bits of code above would you change so that you can store the last 4 digits without including the hyphen?
> Make a new variable called `zip_plus4` and store these values.
> **HINT** Look at the help file for `str_replace()`.

Let's compare our two versions: do we get the same results?

```{r}
## check if both versions of new zip column are equal
identical(data |> select(zip5), data |> select(zip5_v2))
```

No!
Let's see where they are different:

```{r}
## filter to rows where zip5 != zip5_v2 (not storing...just looking)
data |>
    filter(zip5 != zip5_v2) |>
    select(unitid, zip, zip5, zip5_v2)

```

> #### Quick exercise
>
> What happened?
> In this scenario, which string subsetting technique worked better?

Depending on the task, regular expressions can either feel like a blessing or a curse.
However, regular expressions are often *the only* way to perform a data wrangling task on unstructured string data.
They are also a cornerstone of natural language processing techniques, which are increasingly of interest to education researchers.

We've only scratched the surface of what regular expressions can do.
If you face string data in the future, taking a little time to craft a regular expression can be well worth it.

## Part II: Working with dates

-   Much like names, dates are often saved as text/strings, and can be messy and formatted differently

    -   For example, "2026-07-04", "July 4th 2026", "4th July 26", and "04/07/26" all refer to the United State's upcoming 250th birthday, but how do we make the computer understand they all mean that?

-   What's trickier is that even once we have dealt with the formatting, we sometimes need to be able to make calculations with dates

    -   For example, if we have students SAT scores from multiple attempts, somone might be interested in knowing how many days passed between attempts

        -   This involves knowing how many days are in each month, if that year was a leap year, etc.

```{r, purl = TRUE, include = FALSE}
## -----------------------------------------------------------------------------
##' [Working with Dates]
## -----------------------------------------------------------------------------
```

-   In our IPEDS data, see which institutions closed in 2007 and 2008 in the `closedat` column

    -   `-2` means the institution didn't close in this period, so let's drop them

```{r}
## subset to schools who closed during this period
data <- data |>
  filter(closedat != -2) |>
  select(unitid, instnm, closedat)

data
```

#### `parse`-ing String into Dates

-   So, we can see `closedat` is a `<chr>` or string variable

    -   From a combination of looking at the values and knowing this is a US document, we can see most of these dates are month/day/year

-   We are really only going to use one function from `lubridate` the `parse_date_time()` function

    -   There are other ways you can handle dates, some in base `R`, some other in `lubridate`, but this is a way that works well in most situations

    -   The function takes two main arguments

        -   `x`: The string you are trying to turn into a date

        -   `orders`: The format(s) the string date is written in

            -   This take one or more date formats to try

            -   Since we think most of our dates are "month, day, year" this will be `"mdy"`

            -   You can see a full list of options [on the lubridate reference page](https://lubridate.tidyverse.org/reference/index.html)

            -   It tries these in order, seeing if it can "parse" or "figure out" the date looking for anything that divides dates up (" ", "/", "-") and/or spelt out date elements like "August"

                -   If it can work out the date with the the first format, it will move on, if not, it will try and others you gave it (in order) and then if it can't make the string fit into any of those formats, it will "fail to parse"

    -   It then outputs a `<dttm>` (date time) type variable, which we are going to assign to a new variable `clean_date`

        -   This involves a *lot* of well thought out code on the back-end, but makes our lives so much easier

-   Let's give this a go!

```{r}

## create a new clean_date column 
data <- data |>
    mutate(clean_date = parse_date_time(closedat,
                                        orders = "mdy"))

## show
data
```

-   Okay, that worked for the majority of our colleges, but we see some like `Sep-2007` that it didn't like

> ### Quick Dicussion
>
> Why wasn't it able to "parse" `Sep-2007`?
>
> Do we know enough about when this institution closed?
> Why or why not?
>
> What might we be able to do with the information we have?

-   Now, remember `orders` can take more than one value, so maybe we could try another date format that can pick up some of these dates

    -   Looking through the [lubridate reference page](https://lubridate.tidyverse.org/reference/index.html), we can see that `"my"` (month year) is a format it will take as well, so let's try that

```{r}
## Try adding another date format
data <- data |>
    mutate(clean_date = parse_date_time(closedat,
                                        orders = c("mdy", "my")))

## show
data
```

-   Okay, this time only `7 failed to parse` , so that's a lot better

-   Take a look at what it did with `Sep-2007`

    -   Do we like this or not? Is it trustworthy?

-   Let's take a look at the 7 that didn't go through this time

```{r}
data |>
  filter(is.na(clean_date))
```

-   Hmm, those ones might be a little hard to approximate, so let's drop them

```{r}
data <- data |>
  drop_na(clean_date)
```

#### Working with `parse`-ed Date Objects

-   We have successfully turn as many of the close dates into date objects that R can understand, but why?

    -   We can now make comparisons and calculations ***so*** much more easily

        -   As we go through these examples, just imagine having to do this by hand

##### Numerical Calculations

-   Let's say we wanted to quickly find the earliest date

    -   We wouldn't have got far with the `<chr>` string date, but our `<dttm>` object R can understand and tell us the `min()` value of it

```{r}
data |> filter(clean_date == min(clean_date))
```

> ### Quick Exercise
>
> Find the school with the most recent closure date

#### Comparing to Reference Dates

-   Or, lets say we want to see how many schools closed before Christmas Day 2007

    -   We can use `parse_date_time()` again to store December 25th 2007 as a date time object `christmas_07`

    -   Then we can just filter schools whose date is less than that

```{r}
christmas_07 <- parse_date_time("Dec 25 2007", "mdy")

data |> filter(clean_date < christmas_07)
```

-   What about within 30 days of Christmas

    -   For this we need to use `interval(clean_date, christmas_07)` to look between two dates

    -   Then `time_length(, "day")` converts that to the number of days

    -   Then `abs()` to get the absolute value (otherwise dates long after Christmas 2007 will be kept)

    -   This gets a little nested, so we can use more pipes `|>` inside our `filter()` statement if that's easier

```{r}
## Nested version
data |> filter(abs(time_length(interval(clean_date, christmas_07), "day")) < 30)

## Internal pipes version
data |> filter(interval(clean_date, christmas_07) |>
               time_length("day") |>
               abs() < 30)
```

> ### Quick Exercise
>
> -   Did any schools close within a week of Christmas?

#### Extracting Info From Dates

-   What if we want to see which fiscal quarter more schools closed in?

    -   The handy `quarter()` function tell us that

```{r}
data |> 
  mutate(quarter = quarter(clean_date)) |>
  count(quarter)
```

-   What about the day of the week they closed on?

    -   Similarly `wday()` can tell us that

        -   We need to say `label = TRUE` to get "Sun" instead of 1

```{r}
data |>
  mutate(day = wday(clean_date, label = TRUE)) |>
  count(day)
```

> ### Quick Question
>
> -   Can we 100% trust these week day counts? Why or why not?

## Summary

-   In this lesson we've looked at working with both generic strings and with dates

    -   For both of these we have only begun to scratch the surface, but it should have given you enough of an idea to go out and get your hands dirty

    -   If you found this interesting, Dr. Jinnie Shin teaches a class on Natural Language Processing (in python programming language), which gets into how make the computer begin to understand text

-   These are both messy types of data and you're often going to have to make subjective decisions (e.g., how to handle `Sep-2007`)

    -   What's most important is that you document/comment what you did and why you did it

-   As you will see over the next few weeks, while this stuff can be tricky, working with strings efficiently can be really powerful in more advanced programming

```{r, include = FALSE, purl = TRUE}
## -----------------------------------------------------------------------------
##' *END SCRIPT*
## -----------------------------------------------------------------------------
```

# Assignment

[{{< fa question >}} Assignment Template](/site-attachments/assignments/assignment-07.qmd)

{{< include /site-attachments/assignments/assignment-07.qmd >}}

## Submission

Once complete turn in the .qmd file (**it must render/run**) and the rendered PDF to Canvas by the due date (usually **Tuesday 12:00pm following the lesson**). Assignments will be graded before next lesson on Wednesday in line with the grading policy outlined in the syllabus.

::: {.content-hidden unless-meta="solution"}
## Solution

[{{< fa code >}} R Solution Code](site-attachments/solutions/data-wrangling-iii-solution.R)

```{r}
#| echo: true
#| eval: false
#| file: site-attachments/solutions/data-wrangling-iii-solution.R
```
:::
:::
